<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<search>
    
     <entry>
        <title>Gradle构建ReactJS前端实践</title>
        <url>https://mryqu.github.io/post/gradle%E6%9E%84%E5%BB%BAreactjs%E5%89%8D%E7%AB%AF%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>npm</tag><tag>webpack</tag>
        </tags>
        <content type="html">  frontend-maven-plugin使用介绍 Spring指南里面有个示例React.js and Spring Data REST，技术架构为：
* 后端采用Spring Data Rest
* 前端采用React.js
* 构建工具为Maven
下面看一下其pom.xml构建前端的片段:
&amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;com.github.eirslett&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;frontend-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.6&amp;lt;/version&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;installDirectory&amp;gt;target&amp;lt;/installDirectory&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;install node and npm&amp;lt;/id&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;install-node-and-npm&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;nodeVersion&amp;gt;v10.11.0&amp;lt;/nodeVersion&amp;gt; &amp;lt;npmVersion&amp;gt;6.4.1&amp;lt;/npmVersion&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;npm install&amp;lt;/id&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;npm&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;arguments&amp;gt;install&amp;lt;/arguments&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;webpack build&amp;lt;/id&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;webpack&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt;  frontend-maven-plugin用于构建JavaScript部分：
* install-node-and-npm命令将安装node.js及其包管理工具npm到target目录。 （这确保这些二进制文件不在源代码控制范围内并且能被clean命令清除）。
* npm命令将执行使用参数install的npm二进制文件，它会安装定义在package.json内的模块。
* webpack命令将执行webpack二进制文件，它会基于webpack.config.js编译所有JavaScript代码。
这些步骤依次运行，完成安装node.js、下载JavaScript模块、构建JS部分。
备选Gradle前端构建插件 frontend-maven-plugin是专用于Maven的插件，在Gradle上并没有直接对应的插件。
我查找后，重点考察了下面两个插件：
* Frontend Gradle plugin
* Gradle Plugin for Node
Frontend Gradle plugin实践 代码修改 package.json稍作修改，在scripts里面添加build命令：
 &amp;quot;scripts&amp;quot;: { &amp;quot;build&amp;quot;: &amp;quot;webpack --config webpack.config.js -d&amp;quot;, &amp;quot;watch&amp;quot;: &amp;quot;webpack --watch -d&amp;quot; }  build.gradle:
plugins { id &#39;application&#39; id &#39;org.siouan.frontend&#39; version &#39;2.0.0&#39; } apply plugin: &#39;org.siouan.frontend&#39; frontend { // [OPTIONAL] Node version, used to build the URL to download the corresponding distribution, if the // &#39;nodeDistributionUrl&#39; property is not set. By default, this property is &#39;null&#39; nodeVersion = &#39;10.15.3&#39; // [OPTIONAL] Install directory where the distribution archive shall be exploded. nodeInstallDirectory = file(&amp;quot;${project.buildDir}/nodejs&amp;quot;) ////// SCRIPT SETTINGS ////// // Name of NPM/Yarn scripts (see &#39;package.json&#39; file) that shall be executed depending on this // plugin task. The values below are passed as arguments of the &#39;npm&#39; or &#39;yarn&#39; executables. // Under Linux-like O/S, white space characters &#39; &#39; in an argument value must be escaped with a // backslash character &#39;\&#39;. Under Windows O/S, the whole argument must be enclosed between // double-quotes. Example: assembleScript = &#39;run assemble single\ argument&#39; assembleScript = &#39;run build&#39; ////// GENERAL SETTINGS ////// // [OPTIONAL] Location of the directory containing the &#39;package.json&#39; file. By default, this // file is considered to be located in the project&#39;s directory, at the same level than this // &#39;build.gradle[.kts]&#39; file. If the &#39;package.json&#39; file is located in another directory, it is // recommended either to set up a Gradle multi-project build, or to set this property with the // appropriate directory. This directory being used as the working directory when running JS // scripts, consequently, the &#39;node_modules&#39; directory would be created at this location after // the &#39;installFrontend&#39; task is executed. // [OPTIONAL] Whether messages logged by the plugin in Gradle with INFO level shall be visible // whatever the active Gradle logging level is. This property allows to track the plugin // execution without activating Gradle INFO or DEBUG levels that may be too much verbose on a // global point of view. The plugin also logs some messages at lower levels, (e.g. debugging // data). The visibility of these messages does not depend on this property. verboseModeEnabled = true } clean.delete &amp;lt;&amp;lt; file(&#39;node_modules&#39;) clean.delete &amp;lt;&amp;lt; file(&#39;src/main/resources/static/built&#39;)  操作 查看构建需要执行的任务：
C:\ws\HelloSpringDataRestReactjs&amp;gt;gradle -m build :HelloSpringDataRestReactjs:installNode SKIPPED :HelloSpringDataRestReactjs:installYarn SKIPPED :HelloSpringDataRestReactjs:installFrontend SKIPPED :HelloSpringDataRestReactjs:assembleFrontend SKIPPED :HelloSpringDataRestReactjs:compileJava SKIPPED :HelloSpringDataRestReactjs:processResources SKIPPED :HelloSpringDataRestReactjs:classes SKIPPED :HelloSpringDataRestReactjs:bootJar SKIPPED :HelloSpringDataRestReactjs:bootStartScripts SKIPPED :HelloSpringDataRestReactjs:bootDistTar SKIPPED :HelloSpringDataRestReactjs:bootDistZip SKIPPED :HelloSpringDataRestReactjs:jar SKIPPED :HelloSpringDataRestReactjs:startScripts SKIPPED :HelloSpringDataRestReactjs:distTar SKIPPED :HelloSpringDataRestReactjs:distZip SKIPPED :HelloSpringDataRestReactjs:assemble SKIPPED :HelloSpringDataRestReactjs:checkFrontend SKIPPED :HelloSpringDataRestReactjs:compileTestJava SKIPPED :HelloSpringDataRestReactjs:processTestResources SKIPPED :HelloSpringDataRestReactjs:testClasses SKIPPED :HelloSpringDataRestReactjs:test SKIPPED :HelloSpringDataRestReactjs:check SKIPPED :HelloSpringDataRestReactjs:build SKIPPED BUILD SUCCESSFUL in 1s  构建：
C:\ws\HelloSpringDataRestReactjs&amp;gt;gradle build --info Selected primary task &#39;build&#39; from project :HelloSpringDataRestReactjs Tasks to be executed: [task &#39;:HelloSpringDataRestReactjs:installNode&#39;, task &#39;:HelloSpringDataRestReactjs:installYarn&#39;, task &#39;:HelloSpringDataRestReactjs:installFrontend&#39;, task &#39;:HelloSpringDataRestReactjs:assembleFrontend&#39;, task &#39;:HelloSpringDataRestReactjs:compileJava&#39;, task &#39;:HelloSpringDataRestReactjs:processResources&#39;, task &#39;:HelloSpringDataRestReactjs:classes&#39;, task &#39;:HelloSpringDataRestReactjs:bootJar&#39;, task &#39;:HelloSpringDataRestReactjs:bootStartScripts&#39;, task &#39;:HelloSpringDataRestReactjs:bootDistTar&#39;, task &#39;:HelloSpringDataRestReactjs:bootDistZip&#39;, task &#39;:HelloSpringDataRestReactjs:jar&#39;, task &#39;:HelloSpringDataRestReactjs:startScripts&#39;, task &#39;:HelloSpringDataRestReactjs:distTar&#39;, task &#39;:HelloSpringDataRestReactjs:distZip&#39;, task &#39;:HelloSpringDataRestReactjs:assemble&#39;, task &#39;:HelloSpringDataRestReactjs:checkFrontend&#39;, task &#39;:HelloSpringDataRestReactjs:compileTestJava&#39;, task &#39;:HelloSpringDataRestReactjs:processTestResources&#39;, task &#39;:HelloSpringDataRestReactjs:testClasses&#39;, task &#39;:HelloSpringDataRestReactjs:test&#39;, task &#39;:HelloSpringDataRestReactjs:check&#39;, task &#39;:HelloSpringDataRestReactjs:build&#39;] :HelloSpringDataRestReactjs:installNode (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:installNode Caching disabled for task &#39;:HelloSpringDataRestReactjs:installNode&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:installNode&#39; is not up-to-date because: Output property &#39;nodeInstallDirectory&#39; file C:\ws\HelloSpringDataRestReactjs\build\nodejs has been removed. Output property &#39;nodeInstallDirectory&#39; file C:\ws\HelloSpringDataRestReactjs\build\nodejs\CHANGELOG.md has been removed. Output property &#39;nodeInstallDirectory&#39; file C:\ws\HelloSpringDataRestReactjs\build\nodejs\LICENSE has been removed. [installNode] Removing install directory &#39; C:\ws\HelloSpringDataRestReactjs\build\nodejs&#39; [installNode] Downloading distribution at &#39;https://nodejs.org/dist/v10.15.3/node-v10.15.3-win-x64.zip&#39; [installNode] Downloading shasums at &#39;https://nodejs.org/dist/v10.15.3/SHASUMS256.txt&#39; [installNode] Verifying distribution integrity [installNode] Exploding distribution into &#39; C:\ws\HelloSpringDataRestReactjs\build\tmp\installNode\extract&#39; [installNode] Moving distribution into &#39; C:\ws\HelloSpringDataRestReactjs\build\nodejs&#39; [installNode] Removing explode directory &#39; C:\ws\HelloSpringDataRestReactjs\build\tmp\installNode\extract&#39; [installNode] Removing distribution file &#39; C:\ws\HelloSpringDataRestReactjs\build\tmp\installNode\node-v10.15.3-win-x64.zip&#39; [installNode] Distribution installed in &#39; C:\ws\HelloSpringDataRestReactjs\build\nodejs&#39; :HelloSpringDataRestReactjs:installNode (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 1 mins 0.902 secs. :HelloSpringDataRestReactjs:installYarn (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:installYarn SKIPPED Skipping task &#39;:HelloSpringDataRestReactjs:installYarn&#39; as task onlyIf is false. :HelloSpringDataRestReactjs:installYarn (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:installFrontend (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:installFrontend Caching disabled for task &#39;:HelloSpringDataRestReactjs:installFrontend&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:installFrontend&#39; is not up-to-date because: Task has not declared any outputs despite executing actions. [installFrontend] Running &#39;cmd&#39; with arguments: [/c], [&amp;quot; C:\ws\HelloSpringDataRestReactjs\build\nodejs\npm.cmd&amp;quot; install] Starting process &#39;command &#39;cmd&#39;&#39;. Working directory: C:\ws\HelloSpringDataRestReactjs Command: cmd /c &amp;quot; C:\ws\HelloSpringDataRestReactjs\build\nodejs\npm.cmd&amp;quot; install Successfully started process &#39;command &#39;cmd&#39;&#39; npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) added 543 packages from 245 contributors in 15.172s :HelloSpringDataRestReactjs:installFrontend (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 16.825 secs. :HelloSpringDataRestReactjs:assembleFrontend (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:assembleFrontend Caching disabled for task &#39;:HelloSpringDataRestReactjs:assembleFrontend&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:assembleFrontend&#39; is not up-to-date because: Task has not declared any outputs despite executing actions. [assembleFrontend] Running &#39;cmd&#39; with arguments: [/c], [&amp;quot; C:\ws\HelloSpringDataRestReactjs\build\nodejs\npm.cmd&amp;quot; run build] Starting process &#39;command &#39;cmd&#39;&#39;. Working directory: C:\ws\HelloSpringDataRestReactjs Command: cmd /c &amp;quot; C:\ws\HelloSpringDataRestReactjs\build\nodejs\npm.cmd&amp;quot; run build Successfully started process &#39;command &#39;cmd&#39;&#39; &amp;gt; spring-data-rest-and-reactjs@0.1.0 build C:\ws\HelloSpringDataRestReactjs &amp;gt; webpack --config webpack.config.js -d Hash: a2e26d521660d96c361d Version: webpack 4.43.0 Time: 1501ms Built at: 2020-04-25 09:50:55 Asset Size Chunks Chunk Names ./src/main/resources/static/built/bundle.js 2.58 MiB main [emitted] main Entrypoint main = ./src/main/resources/static/built/bundle.js [./src/main/js/api/uriListConverter.js] 614 bytes {main} [built] [./src/main/js/api/uriTemplateInterceptor.js] 497 bytes {main} [built] [./src/main/js/app.js] 5.7 KiB {main} [built] [./src/main/js/client.js] 712 bytes {main} [built] &#43; 37 hidden modules :HelloSpringDataRestReactjs:assembleFrontend (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 3.545 secs. :HelloSpringDataRestReactjs:compileJava (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:compileJava Resolving global dependency management for project &#39;HelloSpringDataRestReactjs&#39; Excluding [org.apache.tomcat:tomcat-annotations-api, ognl:ognl] Excluding [] Caching disabled for task &#39;:HelloSpringDataRestReactjs:compileJava&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:compileJava&#39; is not up-to-date because: Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\classes\java\main has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\classes\java\main\com has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\classes\java\main\com\yqu has been removed. All input files are considered out-of-date for incremental task &#39;:HelloSpringDataRestReactjs:compileJava&#39;. Full recompilation is required because no incremental change information is available. This is usually caused by clean builds or changing compiler arguments. Compiling with JDK Java compiler API. Created classpath snapshot for incremental compilation in 0.073 secs. 1 duplicate classes found in classpath (see all with --debug). :HelloSpringDataRestReactjs:compileJava (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 1.305 secs. :HelloSpringDataRestReactjs:processResources (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:processResources Caching disabled for task &#39;:HelloSpringDataRestReactjs:processResources&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:processResources&#39; is not up-to-date because: Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\resources\main has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\resources\main\application.properties has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\resources\main\static has been removed. :HelloSpringDataRestReactjs:processResources (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.061 secs. :HelloSpringDataRestReactjs:classes (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:classes Skipping task &#39;:HelloSpringDataRestReactjs:classes&#39; as it has no actions. :HelloSpringDataRestReactjs:classes (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:bootJar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootJar Excluding [] Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootJar&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootJar&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\libs\HelloSpringDataRestReactjs.jar has been removed. :HelloSpringDataRestReactjs:bootJar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.686 secs. :HelloSpringDataRestReactjs:bootStartScripts (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootStartScripts Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootStartScripts&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootStartScripts&#39; is not up-to-date because: Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\bootScripts has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\bootScripts\HelloSpringDataRestReactjs has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\bootScripts\HelloSpringDataRestReactjs.bat has been removed. :HelloSpringDataRestReactjs:bootStartScripts (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.183 secs. :HelloSpringDataRestReactjs:bootDistTar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootDistTar Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootDistTar&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootDistTar&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs-boot.tar has been removed. :HelloSpringDataRestReactjs:bootDistTar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.209 secs. :HelloSpringDataRestReactjs:bootDistZip (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootDistZip Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootDistZip&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootDistZip&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs-boot.zip has been removed. :HelloSpringDataRestReactjs:bootDistZip (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 1.351 secs. :HelloSpringDataRestReactjs:jar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:jar SKIPPED Skipping task &#39;:HelloSpringDataRestReactjs:jar&#39; as task onlyIf is false. :HelloSpringDataRestReactjs:jar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:startScripts (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:startScripts Caching disabled for task &#39;:HelloSpringDataRestReactjs:startScripts&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:startScripts&#39; is not up-to-date because: Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\scripts has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\scripts\HelloSpringDataRestReactjs has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\scripts\HelloSpringDataRestReactjs.bat has been removed. :HelloSpringDataRestReactjs:startScripts (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.055 secs. :HelloSpringDataRestReactjs:distTar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:distTar Caching disabled for task &#39;:HelloSpringDataRestReactjs:distTar&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:distTar&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs.tar has been removed. :HelloSpringDataRestReactjs:distTar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.235 secs. :HelloSpringDataRestReactjs:distZip (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:distZip Caching disabled for task &#39;:HelloSpringDataRestReactjs:distZip&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:distZip&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs.zip has been removed. :HelloSpringDataRestReactjs:distZip (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 1.462 secs. :HelloSpringDataRestReactjs:assemble (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:assemble Skipping task &#39;:HelloSpringDataRestReactjs:assemble&#39; as it has no actions. :HelloSpringDataRestReactjs:assemble (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:checkFrontend (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:checkFrontend SKIPPED Skipping task &#39;:HelloSpringDataRestReactjs:checkFrontend&#39; as task onlyIf is false. :HelloSpringDataRestReactjs:checkFrontend (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:compileTestJava (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:compileTestJava NO-SOURCE file or directory &#39; C:\ws\HelloSpringDataRestReactjs\src\test\java&#39;, not found Skipping task &#39;:HelloSpringDataRestReactjs:compileTestJava&#39; as it has no source files and no previous output files. :HelloSpringDataRestReactjs:compileTestJava (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.002 secs. :HelloSpringDataRestReactjs:processTestResources (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:processTestResources NO-SOURCE file or directory &#39; C:\ws\HelloSpringDataRestReactjs\src\test\resources&#39;, not found Skipping task &#39;:HelloSpringDataRestReactjs:processTestResources&#39; as it has no source files and no previous output files. :HelloSpringDataRestReactjs:processTestResources (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:testClasses (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:testClasses UP-TO-DATE Skipping task &#39;:HelloSpringDataRestReactjs:testClasses&#39; as it has no actions. :HelloSpringDataRestReactjs:testClasses (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:test (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:test NO-SOURCE Skipping task &#39;:HelloSpringDataRestReactjs:test&#39; as it has no source files and no previous output files. :HelloSpringDataRestReactjs:test (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.001 secs. :HelloSpringDataRestReactjs:check (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:check UP-TO-DATE Skipping task &#39;:HelloSpringDataRestReactjs:check&#39; as it has no actions. :HelloSpringDataRestReactjs:check (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:build (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:build Skipping task &#39;:HelloSpringDataRestReactjs:build&#39; as it has no actions. :HelloSpringDataRestReactjs:build (Thread[Execution worker for &#39;:&#39; Thread 3,5,main]) completed. Took 0.0 secs. BUILD SUCCESSFUL in 1m 28s 12 actionable tasks: 12 executed  清除：
C:\ws\HelloSpringDataRestReactjs&amp;gt;gradle clean --info Selected primary task &#39;clean&#39; from project :HelloSpringDataRestReactjs Tasks to be executed: [task &#39;:HelloSpringDataRestReactjs:cleanFrontend&#39;, task &#39;:HelloSpringDataRestReactjs:clean&#39;] :HelloSpringDataRestReactjs:cleanFrontend (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:cleanFrontend SKIPPED Skipping task &#39;:HelloSpringDataRestReactjs:cleanFrontend&#39; as task onlyIf is false. :HelloSpringDataRestReactjs:cleanFrontend (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.005 secs. :HelloSpringDataRestReactjs:clean (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:clean Caching disabled for task &#39;:HelloSpringDataRestReactjs:clean&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:clean&#39; is not up-to-date because: Task has not declared any outputs despite executing actions. :HelloSpringDataRestReactjs:clean (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 5.01 secs. BUILD SUCCESSFUL in 6s 1 actionable task: 1 executed  Gradle Plugin for Node实践 代码修改 build.gradle:
plugins { id &#39;com.moowork.node&#39; version &#39;1.3.1&#39; } apply plugin: &#39;com.moowork.node&#39; node { // Version of node to use. version = &#39;10.15.3&#39; // Version of npm to use. npmVersion = &#39;6.4.1&#39; // Base URL for fetching node distributions (change if you have a mirror). distBaseUrl = &#39;https://nodejs.org/dist&#39; // If true, it will download node using above parameters. // If false, it will try to use globally installed node. download = true // Set the work directory for unpacking node workDir = file(&amp;quot;${project.buildDir}/nodejs&amp;quot;) // Set the work directory for NPM npmWorkDir = file(&amp;quot;${project.buildDir}/npm&amp;quot;) // Set the work directory where node_modules should be located nodeModulesDir = file(&amp;quot;${project.projectDir}&amp;quot;) } task buildReactApp(type: NodeTask, dependsOn: &#39;npmInstall&#39;) { script = project.file(&#39;node_modules/webpack/bin/webpack.js&#39;) } processResources.dependsOn &#39;buildReactApp&#39; clean.delete &amp;lt;&amp;lt; file(&#39;node_modules&#39;) clean.delete &amp;lt;&amp;lt; file(&#39;src/main/resources/static/built&#39;)  操作 查看构建需要执行的任务：
C:\ws\HelloSpringDataRestReactjs&amp;gt;gradle -m build :HelloSpringDataRestReactjs:compileJava SKIPPED :HelloSpringDataRestReactjs:nodeSetup SKIPPED :HelloSpringDataRestReactjs:npmSetup SKIPPED :HelloSpringDataRestReactjs:npmInstall SKIPPED :HelloSpringDataRestReactjs:buildReactApp SKIPPED :HelloSpringDataRestReactjs:processResources SKIPPED :HelloSpringDataRestReactjs:classes SKIPPED :HelloSpringDataRestReactjs:bootJar SKIPPED :HelloSpringDataRestReactjs:bootStartScripts SKIPPED :HelloSpringDataRestReactjs:bootDistTar SKIPPED :HelloSpringDataRestReactjs:bootDistZip SKIPPED :HelloSpringDataRestReactjs:jar SKIPPED :HelloSpringDataRestReactjs:startScripts SKIPPED :HelloSpringDataRestReactjs:distTar SKIPPED :HelloSpringDataRestReactjs:distZip SKIPPED :HelloSpringDataRestReactjs:assemble SKIPPED :HelloSpringDataRestReactjs:compileTestJava SKIPPED :HelloSpringDataRestReactjs:processTestResources SKIPPED :HelloSpringDataRestReactjs:testClasses SKIPPED :HelloSpringDataRestReactjs:test SKIPPED :HelloSpringDataRestReactjs:check SKIPPED :HelloSpringDataRestReactjs:build SKIPPED BUILD SUCCESSFUL in 985ms  构建：
Selected primary task &#39;build&#39; from project :HelloSpringDataRestReactjs Tasks to be executed: [task &#39;:HelloSpringDataRestReactjs:compileJava&#39;, task &#39;:HelloSpringDataRestReactjs:nodeSetup&#39;, task &#39;:HelloSpringDataRestReactjs:npmSetup&#39;, task &#39;:HelloSpringDataRestReactjs:npmInstall&#39;, task &#39;:HelloSpringDataRestReactjs:buildReactApp&#39;, task &#39;:HelloSpringDataRestReactjs:processResources&#39;, task &#39;:HelloSpringDataRestReactjs:classes&#39;, task &#39;:HelloSpringDataRestReactjs:bootJar&#39;, task &#39;:HelloSpringDataRestReactjs:bootStartScripts&#39;, task &#39;:HelloSpringDataRestReactjs:bootDistTar&#39;, task &#39;:HelloSpringDataRestReactjs:bootDistZip&#39;, task &#39;:HelloSpringDataRestReactjs:jar&#39;, task &#39;:HelloSpringDataRestReactjs:startScripts&#39;, task &#39;:HelloSpringDataRestReactjs:distTar&#39;, task &#39;:HelloSpringDataRestReactjs:distZip&#39;, task &#39;:HelloSpringDataRestReactjs:assemble&#39;, task &#39;:HelloSpringDataRestReactjs:compileTestJava&#39;, task &#39;:HelloSpringDataRestReactjs:processTestResources&#39;, task &#39;:HelloSpringDataRestReactjs:testClasses&#39;, task &#39;:HelloSpringDataRestReactjs:test&#39;, task &#39;:HelloSpringDataRestReactjs:check&#39;, task &#39;:HelloSpringDataRestReactjs:build&#39;] :HelloSpringDataRestReactjs:compileJava (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:compileJava Resolving global dependency management for project &#39;HelloSpringDataRestReactjs&#39; Excluding [org.apache.tomcat:tomcat-annotations-api, ognl:ognl] Excluding [] Caching disabled for task &#39;:HelloSpringDataRestReactjs:compileJava&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:compileJava&#39; is not up-to-date because: Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\classes\java\main has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\classes\java\main\com has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\classes\java\main\com\yqu has been removed. All input files are considered out-of-date for incremental task &#39;:HelloSpringDataRestReactjs:compileJava&#39;. Full recompilation is required because no incremental change information is available. This is usually caused by clean builds or changing compiler arguments. Compiling with JDK Java compiler API. Created classpath snapshot for incremental compilation in 0.002 secs. 1 duplicate classes found in classpath (see all with --debug). :HelloSpringDataRestReactjs:compileJava (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.57 secs. :HelloSpringDataRestReactjs:nodeSetup (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:nodeSetup Caching disabled for task &#39;:HelloSpringDataRestReactjs:nodeSetup&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:nodeSetup&#39; is not up-to-date because: Output property &#39;nodeDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64 has been removed. Output property &#39;nodeDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\CHANGELOG.md has been removed. Output property &#39;nodeDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\LICENSE has been removed. :HelloSpringDataRestReactjs:nodeSetup (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 26.34 secs. :HelloSpringDataRestReactjs:npmSetup (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:npmSetup Caching disabled for task &#39;:HelloSpringDataRestReactjs:npmSetup&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:npmSetup&#39; is not up-to-date because: Output property &#39;npmDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1 has been removed. Output property &#39;npmDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\node_modules has been removed. Output property &#39;npmDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\node_modules\npm has been removed. Starting process &#39;command &#39; C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\node.exe&#39;&#39;. Working directory: C:\ws\HelloSpringDataRestReactjs Command: C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\node.exe C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\node_modules\npm\bin\npm-cli.js install --global --no-save --prefix C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1 npm@6.4.1 Successfully started process &#39;command &#39; C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\node.exe&#39;&#39; C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\npx -&amp;gt; C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\node_modules\npm\bin\npx-cli.js C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\npm -&amp;gt; C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\node_modules\npm\bin\npm-cli.js &#43; npm@6.4.1 added 387 packages from 770 contributors in 16.644s :HelloSpringDataRestReactjs:npmSetup (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 18.55 secs. :HelloSpringDataRestReactjs:npmInstall (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:npmInstall Caching disabled for task &#39;:HelloSpringDataRestReactjs:npmInstall&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:npmInstall&#39; is not up-to-date because: Output property &#39;$1&#39; file C:\ws\HelloSpringDataRestReactjs\node_modules has been removed. Output property &#39;$1&#39; file C:\ws\HelloSpringDataRestReactjs\node_modules\.bin has been removed. Output property &#39;$1&#39; file C:\ws\HelloSpringDataRestReactjs\node_modules\.bin\acorn has been removed. Starting process &#39;command &#39; C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\npm.cmd&#39;&#39;. Working directory: C:\ws\HelloSpringDataRestReactjs Command: C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\npm.cmd install Successfully started process &#39;command &#39; C:\ws\HelloSpringDataRestReactjs\build\npm\npm-v6.4.1\npm.cmd&#39;&#39; npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) added 543 packages from 245 contributors in 15.314s :HelloSpringDataRestReactjs:npmInstall (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 16.978 secs. :HelloSpringDataRestReactjs:buildReactApp (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:buildReactApp Caching disabled for task &#39;:HelloSpringDataRestReactjs:buildReactApp&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:buildReactApp&#39; is not up-to-date because: Task has not declared any outputs despite executing actions. Starting process &#39;command &#39; C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\node.exe&#39;&#39;. Working directory: C:\ws\HelloSpringDataRestReactjs Command: C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\node.exe C:\ws\HelloSpringDataRestReactjs\node_modules\webpack\bin\webpack.js Successfully started process &#39;command &#39; C:\ws\HelloSpringDataRestReactjs\build\nodejs\node-v10.15.3-win-x64\node.exe&#39;&#39; Hash: f1e3a07435d35106a59b Version: webpack 4.43.0 Time: 1764ms Built at: 2020-04-25 10:31:04 Asset Size Chunks Chunk Names ./src/main/resources/static/built/bundle.js 1020 KiB main [emitted] main ./src/main/resources/static/built/bundle.js.map 1.16 MiB main [emitted] [dev] main Entrypoint main = ./src/main/resources/static/built/bundle.js ./src/main/resources/static/built/bundle.js.map [./src/main/js/api/uriListConverter.js] 614 bytes {main} [built] [./src/main/js/api/uriTemplateInterceptor.js] 497 bytes {main} [built] [./src/main/js/app.js] 5.7 KiB {main} [built] [./src/main/js/client.js] 712 bytes {main} [built] &#43; 37 hidden modules :HelloSpringDataRestReactjs:buildReactApp (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 2.958 secs. :HelloSpringDataRestReactjs:processResources (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:processResources Caching disabled for task &#39;:HelloSpringDataRestReactjs:processResources&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:processResources&#39; is not up-to-date because: Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\resources\main has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\resources\main\application.properties has been removed. Output property &#39;destinationDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\resources\main\static has been removed. :HelloSpringDataRestReactjs:processResources (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.155 secs. :HelloSpringDataRestReactjs:classes (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:classes Skipping task &#39;:HelloSpringDataRestReactjs:classes&#39; as it has no actions. :HelloSpringDataRestReactjs:classes (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:bootJar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootJar Excluding [] Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootJar&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootJar&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\libs\HelloSpringDataRestReactjs.jar has been removed. :HelloSpringDataRestReactjs:bootJar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.405 secs. :HelloSpringDataRestReactjs:bootStartScripts (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootStartScripts Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootStartScripts&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootStartScripts&#39; is not up-to-date because: Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\bootScripts has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\bootScripts\HelloSpringDataRestReactjs has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\bootScripts\HelloSpringDataRestReactjs.bat has been removed. :HelloSpringDataRestReactjs:bootStartScripts (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.031 secs. :HelloSpringDataRestReactjs:bootDistTar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootDistTar Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootDistTar&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootDistTar&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs-boot.tar has been removed. :HelloSpringDataRestReactjs:bootDistTar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.204 secs. :HelloSpringDataRestReactjs:bootDistZip (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:bootDistZip Caching disabled for task &#39;:HelloSpringDataRestReactjs:bootDistZip&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:bootDistZip&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs-boot.zip has been removed. :HelloSpringDataRestReactjs:bootDistZip (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 1.345 secs. :HelloSpringDataRestReactjs:jar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:jar SKIPPED Skipping task &#39;:HelloSpringDataRestReactjs:jar&#39; as task onlyIf is false. :HelloSpringDataRestReactjs:jar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:startScripts (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:startScripts Caching disabled for task &#39;:HelloSpringDataRestReactjs:startScripts&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:startScripts&#39; is not up-to-date because: Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\scripts has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\scripts\HelloSpringDataRestReactjs has been removed. Output property &#39;outputDir&#39; file C:\ws\HelloSpringDataRestReactjs\build\scripts\HelloSpringDataRestReactjs.bat has been removed. :HelloSpringDataRestReactjs:startScripts (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.047 secs. :HelloSpringDataRestReactjs:distTar (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:distTar Caching disabled for task &#39;:HelloSpringDataRestReactjs:distTar&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:distTar&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs.tar has been removed. :HelloSpringDataRestReactjs:distTar (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.23 secs. :HelloSpringDataRestReactjs:distZip (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:distZip Caching disabled for task &#39;:HelloSpringDataRestReactjs:distZip&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:distZip&#39; is not up-to-date because: Output property &#39;archiveFile&#39; file C:\ws\HelloSpringDataRestReactjs\build\distributions\HelloSpringDataRestReactjs.zip has been removed. :HelloSpringDataRestReactjs:distZip (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 1.439 secs. :HelloSpringDataRestReactjs:assemble (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:assemble Skipping task &#39;:HelloSpringDataRestReactjs:assemble&#39; as it has no actions. :HelloSpringDataRestReactjs:assemble (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:compileTestJava (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:compileTestJava NO-SOURCE file or directory &#39; C:\ws\HelloSpringDataRestReactjs\src\test\java&#39;, not found Skipping task &#39;:HelloSpringDataRestReactjs:compileTestJava&#39; as it has no source files and no previous output files. :HelloSpringDataRestReactjs:compileTestJava (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:processTestResources (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:processTestResources NO-SOURCE file or directory &#39; C:\ws\HelloSpringDataRestReactjs\src\test\resources&#39;, not found Skipping task &#39;:HelloSpringDataRestReactjs:processTestResources&#39; as it has no source files and no previous output files. :HelloSpringDataRestReactjs:processTestResources (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:testClasses (Thread[Execution worker for &#39;:&#39;,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:testClasses UP-TO-DATE Skipping task &#39;:HelloSpringDataRestReactjs:testClasses&#39; as it has no actions. :HelloSpringDataRestReactjs:testClasses (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:test (Thread[Execution worker for &#39;:&#39; Thread 9,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:test NO-SOURCE Skipping task &#39;:HelloSpringDataRestReactjs:test&#39; as it has no source files and no previous output files. :HelloSpringDataRestReactjs:test (Thread[Execution worker for &#39;:&#39; Thread 9,5,main]) completed. Took 0.002 secs. :HelloSpringDataRestReactjs:check (Thread[Execution worker for &#39;:&#39; Thread 9,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:check UP-TO-DATE Skipping task &#39;:HelloSpringDataRestReactjs:check&#39; as it has no actions. :HelloSpringDataRestReactjs:check (Thread[Execution worker for &#39;:&#39; Thread 9,5,main]) completed. Took 0.0 secs. :HelloSpringDataRestReactjs:build (Thread[Execution worker for &#39;:&#39; Thread 9,5,main]) started. &amp;gt; Task :HelloSpringDataRestReactjs:build Skipping task &#39;:HelloSpringDataRestReactjs:build&#39; as it has no actions. :HelloSpringDataRestReactjs:build (Thread[Execution worker for &#39;:&#39; Thread 9,5,main]) completed. Took 0.0 secs. Deprecated Gradle features were used in this build, making it incompatible with Gradle 6.0. Use &#39;--warning-mode all&#39; to show the individual deprecation warnings. See https://docs.gradle.org/5.6/userguide/command_line_interface.html#sec:command_line_warnings BUILD SUCCESSFUL in 1m 10s  清除：
C:\ws\HelloSpringDataRestReactjs&amp;gt;gradle clean --info &amp;gt; Task :HelloSpringDataRestReactjs:clean Caching disabled for task &#39;:HelloSpringDataRestReactjs:clean&#39; because: Build cache is disabled Task &#39;:HelloSpringDataRestReactjs:clean&#39; is not up-to-date because: Task has not declared any outputs despite executing actions. :HelloSpringDataRestReactjs:clean (Thread[Execution worker for &#39;:&#39;,5,main]) completed. Took 6.71 secs. BUILD SUCCESSFUL in 9s  备选Gradle前端构建插件对比分析 星数对比 Frontend Gradle plugin 项目星数 3.2k；
Gradle Plugin for Node 项目星数 824；
Frontend Gradle plugin 胜出
支持系统 Frontend Gradle plugin 支持NodeJS、Npm、Yarn；
Gradle Plugin for Node 支持NodeJS、Npm、Yarn、Grunt、Gulp；
Gradle Plugin for Node 胜出
Node/Npm配置 Frontend Gradle plugin 支持NodeJS版本，支持NodeJS安装路径；
Gradle Plugin for Node 支持NodeJS版本，支持NodeJS安装路径，支持Npm版本，支持Npm安装路径，支持node_modules安装路径；
Gradle Plugin for Node 胜出
总结 Frontend Gradle plugin项目星数远远高于Gradle Plugin for Node，不过当前支持的特性却不如后者多。
这两款插件都能胜任我当前的游戏要求。 所以我选择现阶段使用Gradle Plugin for Node，对Frontend Gradle plugin保持关注。
执行Webpack的其他方式 Running Webpack with Gradle是Gradle的官方指南，它是用commandLine执行的。
但是我发现这种方式有个缺陷，在Windows平台执行webpack shell命令就报错了，需要判断平台来决定使用webpack shell命令还是webpack.cmd命令。所以需要做如下修改：
task webpack(type: Exec) { if (System.getProperty(&amp;quot;os.name&amp;quot;).toUpperCase().contains(&amp;quot;WINDOWS&amp;quot;)) { commandLine &amp;quot;$projectDir/node_modules/.bin/webpack.cmd&amp;quot;, &amp;quot;app/index.js&amp;quot;, &amp;quot;$buildDir/js/bundle.js&amp;quot; } else { commandLine &amp;quot;$projectDir/node_modules/.bin/webpack&amp;quot;, &amp;quot;app/index.js&amp;quot;, &amp;quot;$buildDir/js/bundle.js&amp;quot; } }  参考  frontend-maven-plugin
 React.js and Spring Data REST
 Frontend Gradle plugin
 Gradle Plugin for Node
 Running Webpack with Gradle
 Angular &#43; Spring Boot integration using Gradle
  </content>
    </entry>
    
     <entry>
        <title>React.js开发环境设置</title>
        <url>https://mryqu.github.io/post/react.js%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>react</tag>
        </tags>
        <content type="html">  设置Node.js和NPM 升级Node(n不支持Windows操作系统)：
node -v #查看Node版本 npm cache clean -f #清除Node的缓存 npm install -g n #安装n工具，该工具是专门管理Node版本的工具 n stable #安装最新稳定的Node版本  升级NPM：
npm -v #查看NPM版本 npm install npm@latest -g #安装最新稳定的NPM版本  我在两台机器上安装了Node 12.16.2 TLS，其中一台机器上npm死活有问题，从Node 10.16版本开始总是报错verbose stack TypeError: Cannot read property &#39;resolve&#39; of undefined。
最后在那台机器上重新安装了Node 10.15.3,才避免了问题。
安装Chrome插件React Developer Tools 从Chrome Extensions上搜索React Developer Tools进行安装。
安装后，在Chrome开发者工具中可以看到React的Components和Profiler两个Tab并使用。
安装脚手架create-react-app npm install -g create-react-app  使用create-react-app创建React项目 C:\devpg&amp;gt;create-react-app hello-react Creating a new React app in C:\devpg\hello-react. Installing packages. This might take a couple of minutes. Installing react, react-dom, and react-scripts with cra-template... &amp;gt; core-js@2.6.11 postinstall C:\devpg\hello-react\node_modules\babel-runtime\node_modules\core-js &amp;gt; node -e &amp;quot;try{require(&#39;./postinstall&#39;)}catch(e){}&amp;quot; &amp;gt; core-js@3.6.5 postinstall C:\devpg\hello-react\node_modules\core-js &amp;gt; node -e &amp;quot;try{require(&#39;./postinstall&#39;)}catch(e){}&amp;quot; &amp;gt; core-js-pure@3.6.5 postinstall C:\devpg\hello-react\node_modules\core-js-pure &amp;gt; node -e &amp;quot;try{require(&#39;./postinstall&#39;)}catch(e){}&amp;quot; &#43; cra-template@1.0.3 &#43; react-scripts@3.4.1 &#43; react@16.13.1 &#43; react-dom@16.13.1 added 1606 packages from 750 contributors and audited 931196 packages in 2128.923s found 0 vulnerabilities Initialized a git repository. Installing template dependencies using npm... npm WARN react-scripts@3.4.1 requires a peer of typescript@^3.2.1 but none is installed. You must install peer dependencies yourself. npm WARN sass-loader@8.0.2 requires a peer of node-sass@^4.0.0 but none is installed. You must install peer dependencies yourself. npm WARN sass-loader@8.0.2 requires a peer of sass@^1.3.0 but none is installed. You must install peer dependencies yourself. npm WARN sass-loader@8.0.2 requires a peer of fibers@&amp;gt;= 3.1.0 but none is installed. You must install peer dependencies yourself. npm WARN tsutils@3.17.1 requires a peer of typescript@&amp;gt;=2.8.0 || &amp;gt;= 3.2.0-dev || &amp;gt;= 3.3.0-dev || &amp;gt;= 3.4.0-dev || &amp;gt;= 3.5.0-dev || &amp;gt;= 3.6.0-dev || &amp;gt;= 3.6.0-beta || &amp;gt;= 3.7.0-dev || &amp;gt;= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\webpack-dev-server\node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\watchpack\node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\jest-haste-map\node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) &#43; @testing-library/jest-dom@4.2.4 &#43; @testing-library/react@9.5.0 &#43; @testing-library/user-event@7.2.1 added 36 packages from 56 contributors and audited 931402 packages in 81.133s found 0 vulnerabilities Removing template package using npm... npm WARN react-scripts@3.4.1 requires a peer of typescript@^3.2.1 but none is installed. You must install peer dependencies yourself. npm WARN sass-loader@8.0.2 requires a peer of node-sass@^4.0.0 but none is installed. You must install peer dependencies yourself. npm WARN sass-loader@8.0.2 requires a peer of sass@^1.3.0 but none is installed. You must install peer dependencies yourself. npm WARN sass-loader@8.0.2 requires a peer of fibers@&amp;gt;= 3.1.0 but none is installed. You must install peer dependencies yourself. npm WARN tsutils@3.17.1 requires a peer of typescript@&amp;gt;=2.8.0 || &amp;gt;= 3.2.0-dev || &amp;gt;= 3.3.0-dev || &amp;gt;= 3.4.0-dev || &amp;gt;= 3.5.0-dev || &amp;gt;= 3.6.0-dev || &amp;gt;= 3.6.0-beta || &amp;gt;= 3.7.0-dev || &amp;gt;= 3.7.0-beta but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\webpack-dev-server\node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\watchpack\node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.12 (node_modules\jest-haste-map\node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.12: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) removed 1 package and audited 931401 packages in 11.837s found 0 vulnerabilities Created git commit. Success! Created hello-react at C:\devpg\hello-react Inside that directory, you can run several commands: npm start Starts the development server. npm run build Bundles the app into static files for production. npm test Starts the test runner. npm run eject Removes this tool and copies build dependencies, configuration files and scripts into the app directory. If you do this, you can’t go back! We suggest that you begin by typing: cd hello-react npm start Happy hacking! C:\devpg&amp;gt;  要是觉得npm下载挺慢的，可以提前改用国内taobao的npm源：npm config set registry https://registry.npm.taobao.org。
使用IntelliJ创建React项目 参考  n – Interactively Manage Your Node.js Versions
 Cannot read property &amp;lsquo;resolve&amp;rsquo; of undefined #1941
 Try the latest stable version of npm
  </content>
    </entry>
    
     <entry>
        <title>前端框架对比资料</title>
        <url>https://mryqu.github.io/post/%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%E8%B5%84%E6%96%99/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>web</tag><tag>openui5</tag><tag>react</tag><tag>vue</tag>
        </tags>
        <content type="html"> 资料如下：
1. Side by Side: SAPUI5 vs. React &amp;amp; Angular2
2. Vue: Comparison with Other Frameworks
3. Angular 2 vs React: The Ultimate Dance Off
4. React vs Angular vs Vue.js — What to choose in 2019? (updated)
5. React.js与Vue.js：流行框架的比较
6. Reactjs vs. Vuejs
7. React与Vue的对比
8. 关于Vue.js和React.js，听听国外的开发者怎么说？
9. web前端技术框架选型参考
10. 前端框架及组件库选型分析
11. 前端架构师对于框架的技术选型
</content>
    </entry>
    
     <entry>
        <title>Gradle：解决error: unmappable character for encoding GBK</title>
        <url>https://mryqu.github.io/post/gradle_%E8%A7%A3%E5%86%B3error-unmappable-character-for-encoding-gbk/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>encoding</tag><tag>unmappable character</tag><tag>javadoc</tag>
        </tags>
        <content type="html"> 在学习某个项目时，.\gradlew build总是遇到error: unmappable character for encoding GBK。至少确定源文件至少会是UTF8的，所以尝试设置文件编码格式来解决这个问题。
一般使用javac编译和java执行程序时，可以使用：
javac -encoding UTF-8 Test.java java -Dfile.encoding=UTF-8 Test  对于Gradle项目，可以设置gradlew.bat:
set DEFAULT_JVM_OPTS=&amp;quot;-Dfile.encoding=UTF-8&amp;quot;  对于IntelliJ Idea，可在配置文件vmoption文件底部添加一行：
-Dfile.encoding=UTF-8  经过上述尝试，问题依旧存在，仔细一看错误是发生在javadoc任务阶段，一个java文件注释中包含一个字符“ß”导致这个问题的出现。
在build.gradle文件中添加：
javadoc { options.encoding = &#39;UTF-8&#39; }  搞定！！！
</content>
    </entry>
    
     <entry>
        <title>[JS] 图算法实践</title>
        <url>https://mryqu.github.io/post/js_%E5%9B%BE%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>algorithm</tag><tag>digraph</tag>
        </tags>
        <content type="html">  最近需要用JavaScript处理图算法，没找到适合的库，就自己写一套玩玩。
Graph.js 仿照Graph.java写的，实现无向图API。
(function(){ return Graph = (function () { // create empty Graph with V vertices function Graph(V) { this._V = V; this._E = 0; this._adj = []; for(var i=0;i&amp;lt;V;i&#43;&#43;) this._adj.push([]); } Object.defineProperty(Graph.prototype, &amp;quot;V&amp;quot;, { get: function () { return this._V; }, enumerable: true, configurable: true }); Object.defineProperty(Graph.prototype, &amp;quot;E&amp;quot;, { get: function () { return this._E; }, enumerable: true, configurable: true }); // Adds the undirected edge v-w to this graph. Graph.prototype.addEdge = function (v, w) { this._E&#43;&#43;; this._adj[v].push(w); this._adj[w].push(v); }; // Returns the vertices adjacent to vertex v Graph.prototype.adj = function (v) { return this._adj[v]; }; return Graph; }()); })();  测试数据使用tinyG.txt，结果如下：
CC.js 仿照CC.java写的，使用DFS算法为无向图计算连通图。
(function(){ return CC = (function () { // Computes the connected components of the DiGraph function CC(G) { this._marked = []; // marked[v] = has vertex v been marked? this._id = []; // id[v] = id of connected component containing v this._size = []; // size[id] = number of vertices in given component this._count = 0; // number of connected components for(var i=0;i&amp;lt;G.V;i&#43;&#43;) { this._id.push(0); this._size.push(0); this._marked.push(false); } for(var v=0;v&amp;lt;G.V;v&#43;&#43;) { if (!this._marked[v]) { this.dfs(G, v); this._count&#43;&#43;; } } } // Returns the number of strong components. Object.defineProperty(CC.prototype, &amp;quot;count&amp;quot;, { get: function () { return this._count; }, enumerable: true, configurable: true }); Object.defineProperty(CC.prototype, &amp;quot;ids&amp;quot;, { get: function () { return this._id; }, enumerable: true, configurable: true }); // DFS on graph G CC.prototype.dfs = function (G, v) { this._marked[v] = true; this._id[v] = this._count; this._size[this._count]&#43;&#43;; var adj = G.adj(v); for(var i=0;i&amp;lt;adj.length;i&#43;&#43;) { var w = adj[i]; if(!this._marked[w]) this.dfs(G, w); } }; // Are vertices v and w in the same connected component? CC.prototype.areConnected = function (v, w) { return this._id[v] === this._id[w]; }; // Returns the component id of the connected component containing vertex v CC.prototype.id = function (v) { return this._id[v]; }; // Returns the number of vertices in the connected component containing vertex v CC.prototype.size = function (v) { return this._size[v]; }; return CC; }()); })();  测试数据使用tinyG.txt，结果如下：
DiGraph.js 仿照Digraph.java写的，实现有向图API。
(function(){ return DiGraph = (function () { // create empty digraph with V vertices function DiGraph(V) { this._V = V; this._E = 0; this._adj = []; for(var i=0;i&amp;lt;V;i&#43;&#43;) this._adj.push([]); } Object.defineProperty(DiGraph.prototype, &amp;quot;V&amp;quot;, { get: function () { return this._V; }, enumerable: true, configurable: true }); Object.defineProperty(DiGraph.prototype, &amp;quot;E&amp;quot;, { get: function () { return this._E; }, enumerable: true, configurable: true }); // add edge v→w DiGraph.prototype.addEdge = function (v, w) { this._adj[v].unshift(w); this._E&#43;&#43;; }; // vertices pointing from v DiGraph.prototype.adj = function (v) { return this._adj[v]; }; DiGraph.prototype.reverse = function() { var reverse = new DiGraph(this._V); for (var v = 0; v &amp;lt; this._V; v&#43;&#43;) { var adj = this.adj(v); for(var i=0;i&amp;lt;adj.length;i&#43;&#43;) { var w = adj[i]; reverse.addEdge(w, v); } } return reverse; }; return DiGraph; }()); })();  测试数据使用tinyDAG.txt，结果如下：
DepthFirstOrder.js 仿照DepthFirstOrder.java写的，为有向图计算前序和后序。
(function(){ return DepthFirstOrder = (function () { // Determines a depth-first order for the DiGraph function DepthFirstOrder(G) { this._marked = []; //marked[v] = has v been marked in dfs? this._pre = []; // pre[v] = preorder number of v this._post = []; // post[v] = postorder number of v this._preorder = []; // vertices in preorder this._postorder = []; // vertices in postorder this._preCounter = 0; // counter or preorder numbering this._postCounter = 0; // counter for postorder numbering for(var i=0;i&amp;lt;G.V;i&#43;&#43;) { this._marked.push(false); this._pre.push(0); this._post.push(0); } for (var v = 0; v &amp;lt; G.V; v&#43;&#43;) { if (!this._marked[v]) this.dfs(G, v); } } // Returns the vertices in pre-order. Object.defineProperty(DepthFirstOrder.prototype, &amp;quot;preorder&amp;quot;, { get: function () { return this._preorder; }, enumerable: true, configurable: true }); // Returns the vertices in post-order. Object.defineProperty(DepthFirstOrder.prototype, &amp;quot;postorder&amp;quot;, { get: function () { return this._postorder; }, enumerable: true, configurable: true }); // Returns the vertices in reverse post-order. Object.defineProperty(DepthFirstOrder.prototype, &amp;quot;reversePost&amp;quot;, { get: function () { return this._postorder.reverse(); }, enumerable: true, configurable: true }); // Run DFS in digraph G from vertex v and compute pre-order/post-order DepthFirstOrder.prototype.dfs = function (G, v) { this._marked[v] = true; this._pre[v] = this._preCounter&#43;&#43;; this._preorder.push(v); var adj = G.adj(v); for(var i=0;i&amp;lt;adj.length;i&#43;&#43;) { var w = adj[i]; if(!this._marked[w]) this.dfs(G, w); } this._postorder.push(v); this._post[v] = this._postCounter&#43;&#43;; }; // Returns the pre-order number of vertex DepthFirstOrder.prototype.pre = function (v) { return this._pre[v]; }; // Returns the post-order number of verte DepthFirstOrder.prototype.post = function (v) { return this._post[v]; }; return DepthFirstOrder; }()); })();  测试数据使用tinyDAG.txt，结果如下：
KosarajuSharirSCC.js 仿照KosarajuSharirSCC.java写的，使用Kosaraju-Sharir算法为有向图计算强连通图。
(function(){ // Kosaraju-Sharir algorithm implementation to compute strong components in a DiGraph (with two DFSs). return KosarajuSharirSCC = (function () { // Computes the strong components of the DiGraph function KosarajuSharirSCC(G) { this._id = []; // id[v] = id of connected component containing v this._marked = []; // marked[v] = has vertex v been marked? this._count = 0; // number of strong connected components for(var i=0;i&amp;lt;G.V;i&#43;&#43;) { this._id.push(0); this._marked.push(false); } // compute reverse post-order of reverse graph var dfs = new DepthFirstOrder(G.reverse()); var reversePost = dfs.reversePost; // run DFS on G, using reverse postorder to guide calculation for(var i=0;i&amp;lt;reversePost.length;i&#43;&#43;) { var v = reversePost[i]; if (!this._marked[v]) { this.dfs(G, v); this._count&#43;&#43;; } } } // Returns the number of strong components. Object.defineProperty(KosarajuSharirSCC.prototype, &amp;quot;count&amp;quot;, { get: function () { return this._count; }, enumerable: true, configurable: true }); Object.defineProperty(KosarajuSharirSCC.prototype, &amp;quot;ids&amp;quot;, { get: function () { return this._id; }, enumerable: true, configurable: true }); // DFS on graph G KosarajuSharirSCC.prototype.dfs = function (G, v) { this._marked[v] = true; this._id[v] = this._count; var adj = G.adj(v); for(var i=0;i&amp;lt;adj.length;i&#43;&#43;) { var w = adj[i]; if(!this._marked[w]) this.dfs(G, w); } }; // Are vertices v and w in the same strong component? KosarajuSharirSCC.prototype.stronglyConnected = function (v, w) { return this._id[v] === this._id[w]; }; // Returns the component id of the strong component containing vertex v KosarajuSharirSCC.prototype.id = function (v) { return this._id[v]; }; return KosarajuSharirSCC; }()); })();  测试数据使用tinyDG.txt，结果如下：
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 监控Model属性变动</title>
        <url>https://mryqu.github.io/post/openui5_%E7%9B%91%E6%8E%A7model%E5%B1%9E%E6%80%A7%E5%8F%98%E5%8A%A8/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>web</tag><tag>openui5</tag><tag>model</tag><tag>listener</tag><tag>attachChange</tag>
        </tags>
        <content type="html"> 设计了某个OpenUI5控件，当对控件的某些子控件进行设置时，想监控模型的变动。
下面的代码完成了这样的功能：
1. 该控件绑定路径为/someItems/{itemId}/objInfo
2. 当控件下某些子控件修改设置，则路径为/someItems/{itemId}/objInfo的模型属性会发生变动
3. 路径为/someItems/{itemId}/isModified的模型属性将被设置为true
(function () { &amp;quot;use strict&amp;quot;; .... var PATH_PART_OBJINFO = &amp;quot;/objInfo&amp;quot;; var PATH_PART_ISMODIFIED = &amp;quot;/isModified&amp;quot;; SomeControl.extend(&amp;quot;com.yqu.MySomeControl&amp;quot;, { metadata: { properties: {}, publicMethods: [], events: {} }, rb: sap.ui.getCore().getLibraryResourceBundle(&amp;quot;com.yqu&amp;quot;), renderer: &amp;quot;SomeControlRenderer&amp;quot;, init: function() { .... }, onBeforeRendering: function() { if(SomeControl.prototype.onBeforeRendering) SomeControl.prototype.onBeforeRendering.apply(this, arguments); .... var context = this.getBindingContext(); if (!!context &amp;amp;&amp;amp; !!context.oModel &amp;amp;&amp;amp; !!context.sPath) { var binding = new sap.ui.model.Binding(context.oModel, &amp;quot;/&amp;quot;, context); binding.attachChange($.proxy(this._onDataModified, this)); } }, _onDataModified: function() { var context = this.getBindingContext(); if (!!context &amp;amp;&amp;amp; !!context.oModel &amp;amp;&amp;amp; !!context.sPath &amp;amp;&amp;amp; context.sPath.indexOf(PATH_PART_OBJINFO)&amp;gt;0) { var flagPath = context.sPath.replace(PATH_PART_OBJINFO, PATH_PART_ISMODIFIED); context.oModel.setProperty(flagPath, true); } }, _getText: function(sKey, aArgs, bCustomBundle, bSuppressAssertions) { return this.rb.getText(sKey, aArgs, bCustomBundle, bSuppressAssertions); }, exit: function() { .... } }); }());  </content>
    </entry>
    
     <entry>
        <title>CTRL&#43;C无法中断Git Bash中运行的Spring Boot程序</title>
        <url>https://mryqu.github.io/post/ctrl_c%E6%97%A0%E6%B3%95%E4%B8%AD%E6%96%ADgitbash%E4%B8%AD%E8%BF%90%E8%A1%8C%E7%9A%84spring-boot%E7%A8%8B%E5%BA%8F/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>CTRL&#43;C</tag><tag>Git Bash</tag><tag>MinTTY</tag>
        </tags>
        <content type="html"> 在Git Bash中通过gradle bootRun的方式运行Spring Boot程序，使用CTRL&#43;C无法中断运行的程序，重启计算机才能重新运行Spring Boot程序。
忍了很久，最近查了查，发现是Msys2使用的MinTTY终端无法争取地将CTRL&#43;C传递给应用导致的。
CTRL-C doesn&amp;rsquo;t interrupt the running process #684
CTRL-C doesn&amp;rsquo;t stop running app in Windows #773
Re: Ctrl-C and non-cygwin programs
Unable to use CTRL-C, &amp;lsquo;n&amp;rsquo; and &amp;lsquo;q&amp;rsquo; keyboard commands in Cygwin and Msys2 shells #112
Ctrl&#43;C no longer kills running process in Git Bash
在最后一个帖子中查找到适用于我使用场景的workaround：
1. 通过文件浏览器在Git目录中删除usr\bin\mintty.exe文件
2. 重新运行Git Bash（或在文件浏览器中直接双击git-bash.exe)
# 删除mintty前 $ echo $TERM xterm # 删除mintty后 $ echo $TERM cygwin  </content>
    </entry>
    
     <entry>
        <title>学习使用Minikube</title>
        <url>https://mryqu.github.io/post/%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8minikube/</url>
        <categories>
          <category>tool</category><category>kubernetes</category>
        </categories>
        <tags>
          <tag>kubernetes</tag><tag>minikube</tag>
        </tags>
        <content type="html">  在管理容器化应用方面，Kubernetes是目前最好的工具之一。而Minikube能够在macOS、Linux和Windows平台上实现一个本地Kubernetes集群，用于Kubernetes的学习。
Minikube是由Go语言实现，当前支持如下Kubernetes特性：
- DNS
- NodePort
- ConfigMap和Secret
- 仪表盘
- 容器运行时: Docker、rkt、CRI-O和containerd
- 使能容器网络接口
- Ingres
由上图可知，要使用Minikube，需要安装Minikube、kubectl和虚拟机。好在有Katacoda提供浏览器内的带有Kubernetes环境的免费虚拟终端，省却了安装这一步骤。
按照Kubernetes的Hello Minikube教程走了一遍。
minikube version; minikube start $ $ minikube version; minikube start minikube version: v0.28.2 Starting local Kubernetes v1.10.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. $ $ minikube --help Minikube is a CLI tool that provisions and manages single-node Kubernetes clusters optimized for development workflows. Usage: minikube [command] Available Commands: addons Modify minikube&#39;s kubernetes addons cache Add or delete an image from the local cache. completion Outputs minikube shell completion for the given shell (bash or zsh) config Modify minikube config dashboard Opens/displays the kubernetes dashboard URL for your local cluster delete Deletes a local kubernetes cluster docker-env Sets up docker env variables; similar to &#39;$(docker-machine env)&#39; get-k8s-versions Gets the list of Kubernetes versions available for minikube when using the localkube bootstrapper help Help about any command ip Retrieves the IP address of the running cluster logs Gets the logs of the running localkube instance, used for debugging minikube, not user code mount Mounts the specified directory into minikube profile Profile sets the current minikube profile service Gets the kubernetes URL(s) for the specified service in your local cluster ssh Log into or run a command on a machine with SSH; similar to &#39;docker-machine ssh&#39; ssh-key Retrieve the ssh identity key path of the specified cluster start Starts a local kubernetes cluster status Gets the status of a local kubernetes cluster stop Stops a running local kubernetes cluster update-check Print current and latest version number update-context Verify the IP address of the running cluster in kubeconfig. version Print the version of minikube Flags: --alsologtostderr log to standard error as well as files -b, --bootstrapper string The name of the cluster bootstrapper that will set up the kubernetes cluster. (default &amp;quot;kubeadm&amp;quot;) -h, --help help for minikube --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --loglevel int Log level (0 = DEBUG, 5 = FATAL) (default 1) --logtostderr log to standard error instead of files -p, --profile string The name of the minikube VM being used. This can be modified to allow for multiple minikube instances to be run independently (default &amp;quot;minikube&amp;quot;) --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging Use &amp;quot;minikube [command] --help&amp;quot; for more information about a command. $ $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 172.17.0.48 $ minikube dashboard Opening kubernetes dashboard in default browser... $ kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node deployment.apps/hello-node created $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE hello-node 1 1 1 0 6s $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE hello-node 1 1 1 1 1m $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-node-7f5b6bd6b8-xsv8q 1/1 Running 0 1m $ kubectl get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 8m 8m 1 minikube.15a7b294ca3e3206 Node Normal Starting kubelet, minikube Starting kubelet. 8m 8m 1 minikube.15a7b294fd3f5d0a Node Normal Starting kube-proxy, minikube Starting kube-proxy. 8m 8m 1 minikube.15a7b294d0df5af8 Node Normal NodeHasSufficientPID kubelet, minikube Node minikube status is now: NodeHasSufficientPID 8m 8m 1 minikube.15a7b294d0df44a0 Node Normal NodeHasNoDiskPressure kubelet, minikube Node minikube status is now: NodeHasNoDiskPressure 8m 8m 1 minikube.15a7b294d0df2ab9 Node Normal NodeHasSufficientMemory kubelet, minikube Node minikube status is now: NodeHasSufficientMemory 8m 8m 1 minikube.15a7b294d0df04be Node Normal NodeHasSufficientDisk kubelet, minikube Node minikube status is now: NodeHasSufficientDisk 8m 8m 1 minikube.15a7b2954df56145 Node Normal NodeAllocatableEnforced kubelet, minikube Updated Node Allocatable limit across pods 8m 8m 1 minikube.15a7b2972a0b97fc Node Normal NodeReady kubelet, minikube Node minikube status is now: NodeReady 1m 1m 1 hello-node.15a7b2f9dca4ac42 Deployment Normal ScalingReplicaSet deployment-controller Scaled up replica set hello-node-7f5b6bd6b8 to 1 1m 1m 1 hello-node-7f5b6bd6b8.15a7b2f9dcb8c32e ReplicaSet Normal SuccessfulCreate replicaset-controller Created pod: hello-node-7f5b6bd6b8-xsv8q 1m 1m 1 hello-node-7f5b6bd6b8-xsv8q.15a7b2f9fe7cf48f Pod spec.containers{hello-node} Normal Pulling kubelet, minikube pulling image &amp;quot;gcr.io/hello-minikube-zero-install/hello-node&amp;quot; 1m 1m 1 hello-node-7f5b6bd6b8-xsv8q.15a7b2f9ef33a6b7 Pod Normal SuccessfulMountVolume kubelet, minikube MountVolume.SetUp succeeded for volume &amp;quot;default-token-kmdrg&amp;quot; 1m 1m 1 hello-node-7f5b6bd6b8-xsv8q.15a7b2f9dda4da8c Pod Normal Scheduled default-scheduler Successfully assigned hello-node-7f5b6bd6b8-xsv8q to minikube 59s 59s 1 hello-node-7f5b6bd6b8-xsv8q.15a7b301584221b0 Pod spec.containers{hello-node} Normal Started kubelet, minikube Started container 59s 59s 1 hello-node-7f5b6bd6b8-xsv8q.15a7b301517672cd Pod spec.containers{hello-node} Normal Created kubelet, minikube Created container 59s 59s 1 hello-node-7f5b6bd6b8-xsv8q.15a7b3014df0fe95 Pod spec.containers{hello-node} Normal Pulled kubelet, minikube Successfully pulled image &amp;quot;gcr.io/hello-minikube-zero-install/hello-node&amp;quot; $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /root/.minikube/ca.crt server: https://172.17.0.48:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /root/.minikube/client.crt client-key: /root/.minikube/client.key $ kubectl expose deployment hello-node --type=LoadBalancer --port=8080 service/hello-node exposed $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node LoadBalancer 10.103.104.233 &amp;lt;pending&amp;gt; 8080:31739/TCP 4s kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 9m $ minikube service hello-node Opening kubernetes service default/hello-node in default browser... $ minikube addons list - addon-manager: enabled - coredns: disabled - dashboard: enabled - default-storageclass: enabled - efk: disabled - freshpod: disabled - heapster: disabled - ingress: disabled - kube-dns: enabled - metrics-server: disabled - nvidia-driver-installer: disabled - nvidia-gpu-device-plugin: disabled - registry: disabled - registry-creds: disabled - storage-provisioner: enabled $ minikube addons enable heapster heapster was successfully enabled $ kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE pod/heapster-2mztm 1/1 Running 0 3m pod/influxdb-grafana-v5j5q 2/2 Running 0 3m pod/kube-addon-manager-minikube 1/1 Running 0 17m pod/kube-dns-6dcb57bcc8-84zs7 3/3 Running 0 17m pod/kubernetes-dashboard-5498ccf677-xc4kr 1/1 Running 0 17m pod/storage-provisioner 1/1 Running 0 17m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/heapster ClusterIP 10.107.109.88 &amp;lt;none&amp;gt; 80/TCP 3m service/kube-dns ClusterIP 10.96.0.10 &amp;lt;none&amp;gt; 53/UDP,53/TCP 17m service/kubernetes-dashboard NodePort 10.111.86.255 &amp;lt;none&amp;gt; 80:30000/TCP 17m service/monitoring-grafana NodePort 10.111.167.239 &amp;lt;none&amp;gt; 80:30002/TCP 3m service/monitoring-influxdb ClusterIP 10.98.229.238 &amp;lt;none&amp;gt; 8083/TCP,8086/TCP 3m $ minikube addons disable heapster heapster was successfully disabled $ kubectl delete service hello-node service &amp;quot;hello-node&amp;quot; deleted $ kubectl delete deployment hello-node deployment.extensions &amp;quot;hello-node&amp;quot; deleted $ minikube stop Stopping local Kubernetes cluster... Machine stopped. $  唯一要注意的是，教程里提到打开Kubernetes仪表盘使用Select port to view on Host 1菜单，结果是502 Bad GateWay错误。可使用View HTTP port 80 on Host 1然后修改端口为30000即可。 使用Katacoda，下列命令也可以一试。
minikube ip # Get IP address of the running cluster kubectl cluster-info # Get cluster information minikube dashboard --url # Get the Dashboard URL minikube service --url SERVICE # Get the URL of service  参考 Install Minikube
Installing Kubernetes with Minikube
Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what?
MINIKUBE CHEATSHEET
Minikube体验
Minikube: easily run Kubernetes locally
</content>
    </entry>
    
     <entry>
        <title>玩一道正则表达式练习题</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E4%B8%80%E9%81%93%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%BB%83%E4%B9%A0%E9%A2%98/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>regex</tag>
        </tags>
        <content type="html"> 前一段时间在网上碰到一道题，要求进行用户名检查：
- 用户名长度要求最短6字符，最长16字符；
- 用户名可以由字母、数字和可选的一个连字号(-)组成；
- 用户名首字符必须为字母，末字符不可以是连字号。
这道题属于送分题，很容易写出下面的Java代码：
public class Username { public static int MIN_LEN = 6; public static int MAX_LEN = 16; public static boolean validate(String username) { if(username==null || username.length()&amp;lt;MIN_LEN || username.length()&amp;gt;MAX_LEN || !Character.isLetter(username.charAt(0))) return false; int hyphenNum = 0; for(int i=1;i&amp;lt;username.length();i&#43;&#43;) { char ch = username.charAt(i); if (!Character.isLetterOrDigit(ch)) { if (ch==&#39;-&#39;) { if(i== username.length()-1 || &#43;&#43;hyphenNum&amp;gt;1) return false; } else return false; } } return true; } public static void main(String[] args) { String[] tests = new String[] { null, &amp;quot;mryqu123&amp;quot;, &amp;quot;mryqu-123&amp;quot;, &amp;quot;mryqu 123&amp;quot;, &amp;quot;mryqu123-&amp;quot;, &amp;quot;mryqu-123-bj&amp;quot;, &amp;quot;123mryqu&amp;quot;, &amp;quot;MryQu-123&amp;quot; }; for(String test:tests) { System.out.println(test&#43;&amp;quot;,&amp;quot;&#43;validate(test)); } } }  跟一老友聊天，老友觉得还是用正则表达式代码少，高大上。好吧，添一个方法即可：
 public static boolean validateByRegex(String username) { return username!=null &amp;amp;&amp;amp; username.matches(&amp;quot;(?=^[A-Za-z].*$)(?!^.*\\-$)(?!^.*\\-.*\\-.*$)^[A-Za-z\\d\\-]{6,16}$&amp;quot;); }  最后买一送一，送个JS版的：
这个正则分四部分：
1. 使用正向肯定预查匹配字母开始的用户名；
2. 使用正向否定预查匹配非连字符结尾的用户名；
3. 使用正向否定预查匹配最多含一个连字符的用户名；
4. 匹配字母、数字和连字号组成的最短6字符、最长16字符的用户名。
</content>
    </entry>
    
     <entry>
        <title>使用Postman动态调试Salesforce API</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8postman%E5%8A%A8%E6%80%81%E8%B0%83%E8%AF%95salesforce-api/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>Postman</tag>
        </tags>
        <content type="html">  在使用Postman调试Salesforce API时，觉得每次输入Access Token很low，应该充分利用它的脚本能力进行自动化。
创建环境变量 创建新环境变量： 设置环境变量： 选择环境变量： 创建OAuth集合 创建OAuth Collection：
创建Password Authorization请求：
Password Authorization请求头：
Password Authorization请求体：
Password Authorization响应脚本： 脚本用于将响应中信息写入ACCESS_TOKEN和INSTANCE_URL环境变量以用于后继API调用。
Server Authorization请求：
创建Export集合 创建Export Collection： 编辑Export Collection： 主要是让集合内请求共用Bearer Token。 获取版本： 脚本用于将响应中信息写入LATEST_VER_PATH环境变量以用于后继API调用。
注： 通过快捷键(CMD/CTRL &#43; ALT &#43; C)可以打开Postman应用的控制台。
获取资源： 获取对象列表： 获取对象元数据： 获取对象列信息： 脚本用于通过响应信息生成QUERY环境变量以用于SOQL查询API调用。
执行SOQL查询： 查询限额： 最后将环境变量和两个请求集合导出成JSON文件，可以共享给其他人使用。
</content>
    </entry>
    
     <entry>
        <title>体验Salesforce API</title>
        <url>https://mryqu.github.io/post/%E4%BD%93%E9%AA%8Csalesforce-api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>salesforce</tag><tag>api</tag><tag>soap</tag><tag>rest</tag>
        </tags>
        <content type="html">  继前一博文Salesforce dataLoader一览，这次就开始体验一把Salesforce API了。
准备工作  通过developer.salesforce.com/signup获得Salesforce开发版
 确保激活了API权限
 创建connected app：scnydqTest1
 获取scnydqTest1的sonsumer key和secret   OAuth鉴权 Salesforce的OAuth鉴权支持三种流程：
* web服务器流程：适用于服务器可以保存consumer secret
* 用户代理流程：适用于应用无法安全保存consumer secret
* 用户密码流程：应用使用用户凭证直接访问
OAuth鉴权之web服务器流程 验证按照Understanding the User-Agent OAuth Authentication Flow进行。
https://login.salesforce.com/services/oauth2/authorize?response_type=code&amp;amp;client_id={scnydqTest1_appConsumerKey}&amp;amp;redirect_uri=https%3A%2F%2Flogin.salesforce.com%2Fservices%2Foauth2%2Fsuccess  鉴权：
通过跳转URL获取code：
通过code、consumer key和consumer secret获取access token：
OAuth鉴权之用户代理流程 验证按照How Are Apps Authenticated with the Web Server OAuth Authentication Flow?进行。
https://login.salesforce.com/services/oauth2/authorize?response_type=token&amp;amp;client_id={scnydqTest1_appConsumerKey}&amp;amp;redirect_uri=https%3A%2F%2Flogin.salesforce.com%2Fservices%2Foauth2%2Fsuccess  认证： 鉴权：
通过跳转URL获取access_token：
返回URL为：
https://login.salesforce.com/services/oauth2/success#access_token=00D2v000000R9tt%21AXXXXXi.&amp;amp;instance_url=https%3A%2F%2Fap15.salesforce.com&amp;amp;id=https%3A%2F%2Flogin.salesforce.com%2Fid%2F00D2XXXXXC%2F005XXXXXL&amp;amp;issued_at=1XXXXX8&amp;amp;signature=iXXXXXI%3D&amp;amp;scope=id&#43;api&amp;amp;token_type=Bearer  OAuth鉴权之用户密码流程 验证按照Understanding the Username-Password OAuth Authentication Flow进行。
REST调用 此处走一下Salesforce官方的快速入门示例。
获取Salesforce版本 关于版本的介绍详见Apex Code Versions。
使用Salesforce版本获取可用资源 使用一个资源获取可用对象列表 获取一个对象的元数据描述 获取一个对象的列信息 执行SOQL查询获取Account记录的某些指定列的值 限额 REST API与SOAP API使用相同的数据模型和标准对象。REST API遵循SOAP API的限额。
获取Limits信息详见Salesforce示例。
参考 Salesforce SOAP API Developer Guide
Salesforce REST API Developer Guide
SOQL and SOSL Reference
Salesforce Bulk API Developer Guide
Enhance Salesforce with Code
</content>
    </entry>
    
     <entry>
        <title>Salesforce Data Loader一览</title>
        <url>https://mryqu.github.io/post/salesforce-dataloader%E4%B8%80%E8%A7%88/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>salesforce</tag><tag>dataloader</tag>
        </tags>
        <content type="html">  最近研究一下如何从Salesforce抓取数据，所以找到了dataloader.io和dataloader这两款软件进行学习。
dataloader.io dataloader.io是salesforce AppExchange上的应用。它分为免费版、专业版和企业版。
入口
认证
授权
主界面
导出 - 浏览对象 导出 - 选择列
导出 - 设置
导出 - 运行结果邮件
Data Loader Data Loader是Salesforce开源的一款桌面版数据连接器，基于Java语言，底层依赖Force Wsc和Patner API。 Data Loader有两种登陆方式：OAuth和Password Authentication。 OAuth认证 认证 授权 登陆成功 密码认证 注：
* 假设用户密码为mypwd，此处要输入的密码为用户密码和下面提到的Salesforce 安全标记组合，即mypwdXXXXXXXXXXXX。
* 此处Salesforce登陆URL为用户所在的实例地址。如果使用沙箱的话，则使用https://test.salesforce.com。
导出 导出 - 浏览对象 导出 - 选择列
导出 - 运行
导出 - 查看结果
其他Salesforce Data Loader Jitterbit Cloud Data Loader for Salesforce
Salesforce Connector - Mule 4
参考 Data Loader Guide
Data Loader help
重置您的安全标记
更新 Connect Offline、Connect for Office 和 Data Loader 中的标记
</content>
    </entry>
    
     <entry>
        <title>初探Salesforce API</title>
        <url>https://mryqu.github.io/post/%E5%88%9D%E6%8E%A2salesforce-api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>salesforce</tag><tag>api</tag><tag>soap</tag><tag>rest</tag>
        </tags>
        <content type="html"> 最近研究一下如何从Salesforce抓取数据，首先看了一下几个ODBC driver。
* DataDirect 文档
* easysoft
* devart
* cdata
怎么没有Salesforce自家的驱动，都是第三方的。
再找找看，找到了Salesforce SOAP API Developer Guide和REST API Developer Guide。
既然有了SOAP/REST API，何必再看ODBC driver了。
</content>
    </entry>
    
     <entry>
        <title>升级Idea IntelliJ</title>
        <url>https://mryqu.github.io/post/%E5%8D%87%E7%BA%A7idea-intellij/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>gradle</tag>
        </tags>
        <content type="html"> 上次重装机器时，看看自己用的IntelliJ是ideaIU-2016.3.5，而可以升级的最新版本是ideaIU-2018.3.3。犹豫了一下，害怕万一将来跟公司的插件、设置冲突没法用，就没敢升级那么大，只升到ideaIU-2016.3.8而已。
结果从已有项目导入Idea，不仅报错&amp;rdquo;Failed to notify progress listener.&amp;ldquo;，而且依赖库也找不到。上网查了查才发现ideaIU-2018.2之前的版本不支持Gradle 5。
好吧，升级到ideaIU-2018.3.4了。
</content>
    </entry>
    
     <entry>
        <title>Chrome Extension记录</title>
        <url>https://mryqu.github.io/post/chrome-extension%E8%AE%B0%E5%BD%95/</url>
        <categories>
          <category>Tool</category><category>Chrome</category>
        </categories>
        <tags>
          <tag>chrome</tag><tag>extension</tag><tag>LinkedIn</tag><tag>OneTab</tag><tag>Screenshot</tag>
        </tags>
        <content type="html"> 重装机器后发现Chrome扩展工具需要重装，现搜了事。为了便于以后重装，特此记录。
- LinkedIn Extension
- OneTab
- 网页截图 - Screenshot Extension
 Postman
 HTTP Archive Viewer
  </content>
    </entry>
    
     <entry>
        <title>使用了Widnows包管理器Chocolatey</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8%E4%BA%86widnows%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8chocolatey/</url>
        <categories>
          <category>Tool</category><category>Chocolatey</category>
        </categories>
        <tags>
          <tag>package</tag><tag>windows</tag><tag>chocolatey</tag><tag>choco</tag>
        </tags>
        <content type="html"> 系统由于升级微软安全补丁起不来了，在IT同事的帮助下重装了系统。
然后就开始重装大量软件，看到有些软件可以用Widnows包管理器Chocolatey安装，一时轻浮，按捺不住对新事物的向往，就装了Chocolatey。
通过Chocolatey安装了Gradle，后来IntelliJ问我Gradle装哪里了，我一时就蒙了。
想想Linux下的软件都不问，为啥到Windows就不成了？估计还是Linux下包管理是主流，判断依赖软件装没装直接先问deb/apt-get。
估计Windows下用Chocolatey的太少，不但人而且软件的习惯都还没培养起来。这是我的不对，赶紧自己搜出来“C:\ProgramData\chocolatey\lib\gradle\tools\gradle-5.1.1”应付交差了。
</content>
    </entry>
    
     <entry>
        <title>使用Packer创建基于Ubuntu的Vagrant Box</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8packer%E5%88%9B%E5%BB%BA%E5%9F%BA%E4%BA%8Eubuntu%E7%9A%84vagrant_box/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>packer</tag><tag>vagrant</tag><tag>box</tag><tag>create</tag><tag>ubuntu</tag>
        </tags>
        <content type="html">  前一博客创建基于Ubuntu的Vagrant Box是手工完成的，这意味着每次实施后需要写出详细文档，以便于以后查档、定位问题或者更新。
持续交付和DevOps技术的快速进步和演化，使得基础设施的配置不得不频繁变化。基础设施即代码(IaC)是一种使用新的技术来构建和管理动态基础设施的方式。它把基础设施、工具和服务以及对基础设施的管理本身作为一个软件系统，采纳软件工程实践以结构化的安全的方式来管理对系统的变更。
Packer是由HashiCorp推出的一款轻量级镜像定义工具，用于从单一配置来源为多平台创建相同的机器映像。目前支持的平台包括Alicloud ECS、Amazon EC2、Azure、CloudStack、DigitalOcean、Docker、File、Google Cloud、Hetzner Cloud、Hper-V、LXC、LXD、NAVER Cloud、1&amp;amp;1、OpenStack、Oracle、Parallels、ProfitBricks、QEMU、Scaleway、Triton、VirtualBox和VMware。
配置好一个模版文件，用pakcer命令就可以按需构建机器镜像。也可以根据需求及时更改配置。 加入软件版本控制（主要用的是 Git）后，就可以很方便的追溯更改。
尝试了用Packer模板创建Ubuntu基于VirtualBox provider的Vagrant Box，日志如下：
C:\quTemp\PackerTest&amp;gt;REM set PACKER_LOG=1 C:\quTemp\PackerTest&amp;gt;packer build -var &amp;quot;mirror=c:/quTemp&amp;quot; -var &amp;quot;mirror_directory=PackerTest&amp;quot; ubuntu-14.04-amd64.json virtualbox-iso output will be in this color. ==&amp;gt; virtualbox-iso: Retrieving Guest additions virtualbox-iso: Using file in-place: file:///C:/quTools/Oracle/VirtualBox/VBoxGuestAdditions.iso ==&amp;gt; virtualbox-iso: Retrieving ISO virtualbox-iso: Using file in-place: file:///C:/quTemp/PackerTest/ubuntu-14.04.5-server-amd64.iso ==&amp;gt; virtualbox-iso: Starting HTTP server on port 8711 ==&amp;gt; virtualbox-iso: Creating virtual machine... ==&amp;gt; virtualbox-iso: Creating hard drive... ==&amp;gt; virtualbox-iso: Creating forwarded port mapping for communicator (SSH, WinRM, etc) (host port 3620) ==&amp;gt; virtualbox-iso: Executing custom VBoxManage commands... virtualbox-iso: Executing: modifyvm ubuntu-14.04-amd64 --memory 1024 virtualbox-iso: Executing: modifyvm ubuntu-14.04-amd64 --cpus 1 ==&amp;gt; virtualbox-iso: Starting the virtual machine... virtualbox-iso: The VM will be run headless, without a GUI. If you want to virtualbox-iso: view the screen of the VM, connect via VRDP without a password to virtualbox-iso: rdp://127.0.0.1:5924 ==&amp;gt; virtualbox-iso: Waiting 10s for boot... ==&amp;gt; virtualbox-iso: Typing the boot command... ==&amp;gt; virtualbox-iso: Using ssh communicator to connect: 127.0.0.1 ==&amp;gt; virtualbox-iso: Waiting for SSH to become available... ==&amp;gt; virtualbox-iso: Connected to SSH! ==&amp;gt; virtualbox-iso: Uploading VirtualBox version info (5.2.12) ==&amp;gt; virtualbox-iso: Uploading VirtualBox guest additions ISO... ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/common/virtualbox.sh virtualbox-iso: &#43; cat /root/.vbox_version virtualbox-iso: &#43; VER=5.2.12 virtualbox-iso: &#43; ISO=VBoxGuestAdditions_5.2.12.iso virtualbox-iso: &#43; mkdir -p /tmp/vbox virtualbox-iso: &#43; mount -o loop /root/VBoxGuestAdditions_5.2.12.iso /tmp/vbox virtualbox-iso: mount: block device /root/VBoxGuestAdditions_5.2.12.iso is write-protected, mounting read-only virtualbox-iso: &#43; sh /tmp/vbox/VBoxLinuxAdditions.run virtualbox-iso: Verifying archive integrity... All good. virtualbox-iso: Uncompressing VirtualBox 5.2.12 Guest Additions for Linux........ virtualbox-iso: VirtualBox Guest Additions installer virtualbox-iso: Copying additional installer modules ... virtualbox-iso: Installing additional modules ... virtualbox-iso: VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel modules. virtualbox-iso: VirtualBox Guest Additions: Running kernel modules will not be replaced until the system is restarted virtualbox-iso: VirtualBox Guest Additions: Starting. virtualbox-iso: VirtualBox Guest Additions: modprobe vboxsf failed virtualbox-iso: &#43; echo VBoxLinuxAdditions.run exited 2 and is suppressed. For more read https://www.virtualbox.org/ticket/12479 virtualbox-iso: VBoxLinuxAdditions.run exited 2 and is suppressed. For more read https://www.virtualbox.org/ticket/12479 virtualbox-iso: &#43; umount /tmp/vbox virtualbox-iso: &#43; rm -rf /tmp/vbox virtualbox-iso: &#43; rm -f /root/VBoxGuestAdditions_5.2.12.iso ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/common/vagrant-user.sh virtualbox-iso: &#43; groupadd -g 1000 vagrant virtualbox-iso: &#43; useradd -g 1000 -u 1000 -md /home/vagrant -s /bin/bash vagrant virtualbox-iso: &#43; test -d /etc/sudoers.d virtualbox-iso: &#43; echo %vagrant ALL=(ALL) NOPASSWD: ALL ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/common/vagrant.sh virtualbox-iso: &#43; VAGRANT_HOME=/home/vagrant virtualbox-iso: &#43; pubkey_url=https://raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub virtualbox-iso: &#43; mkdir -p /home/vagrant/.ssh virtualbox-iso: &#43; chown -R vagrant: /home/vagrant/.ssh virtualbox-iso: &#43; command -v wget virtualbox-iso: &#43; wget --no-check-certificate https://raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub -O /home/vagrant/.ssh/authorized_keys virtualbox-iso: --2019-01-08 12:39:02-- https://raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub virtualbox-iso: Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... virtualbox-iso: Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. virtualbox-iso: HTTP request sent, awaiting response... 200 OK virtualbox-iso: Length: 409 [text/plain] virtualbox-iso: Saving to: ‘/home/vagrant/.ssh/authorized_keys’ virtualbox-iso: virtualbox-iso: 0K 100% 58.8M=0s virtualbox-iso: virtualbox-iso: 2019-01-08 12:39:03 (58.8 MB/s) - ‘/home/vagrant/.ssh/authorized_keys’ saved [409/409] virtualbox-iso: virtualbox-iso: &#43; chown -R vagrant /home/vagrant/.ssh virtualbox-iso: &#43; chmod -R go-rwsx /home/vagrant/.ssh ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/ubuntu/update.sh virtualbox-iso: &#43; awk {print $2} virtualbox-iso: &#43; lsb_release -r virtualbox-iso: &#43; UBUNTU_VERSION=14.04 virtualbox-iso: &#43; awk -F. {print $1} virtualbox-iso: &#43; echo 14.04 virtualbox-iso: &#43; UBUNTU_MAJOR_VERSION=14 virtualbox-iso: &#43; sed -i.bak s/^Prompt=.*$/Prompt=never/ /etc/update-manager/release-upgrades virtualbox-iso: &#43; apt-get -y update virtualbox-iso: Hit http://security.ubuntu.com trusty-security InRelease virtualbox-iso: Ign http://archive.ubuntu.com trusty InRelease virtualbox-iso: Get:1 http://archive.ubuntu.com trusty-updates InRelease [65.9 kB] virtualbox-iso: Hit http://security.ubuntu.com trusty-security/main Sources virtualbox-iso: Hit http://security.ubuntu.com trusty-security/restricted Sources virtualbox-iso: Hit http://security.ubuntu.com trusty-security/universe Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports InRelease virtualbox-iso: Hit http://security.ubuntu.com trusty-security/multiverse Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty Release.gpg virtualbox-iso: Hit http://security.ubuntu.com trusty-security/main amd64 Packages virtualbox-iso: Get:2 http://archive.ubuntu.com trusty-updates/main Sources [425 kB] virtualbox-iso: Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages virtualbox-iso: Hit http://security.ubuntu.com trusty-security/universe amd64 Packages virtualbox-iso: Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages virtualbox-iso: Hit http://security.ubuntu.com trusty-security/main i386 Packages virtualbox-iso: Hit http://security.ubuntu.com trusty-security/restricted i386 Packages virtualbox-iso: Hit http://security.ubuntu.com trusty-security/universe i386 Packages virtualbox-iso: Hit http://security.ubuntu.com trusty-security/multiverse i386 Packages virtualbox-iso: Hit http://security.ubuntu.com trusty-security/main Translation-en virtualbox-iso: Hit http://security.ubuntu.com trusty-security/multiverse Translation-en virtualbox-iso: Hit http://security.ubuntu.com trusty-security/restricted Translation-en virtualbox-iso: Hit http://security.ubuntu.com trusty-security/universe Translation-en virtualbox-iso: Get:3 http://archive.ubuntu.com trusty-updates/restricted Sources [6,322 B] virtualbox-iso: Get:4 http://archive.ubuntu.com trusty-updates/universe Sources [226 kB] virtualbox-iso: Get:5 http://archive.ubuntu.com trusty-updates/multiverse Sources [7,437 B] virtualbox-iso: Get:6 http://archive.ubuntu.com trusty-updates/main amd64 Packages [1,135 kB] virtualbox-iso: Get:7 http://archive.ubuntu.com trusty-updates/restricted amd64 Packages [17.2 kB] virtualbox-iso: Get:8 http://archive.ubuntu.com trusty-updates/universe amd64 Packages [510 kB] virtualbox-iso: Get:9 http://archive.ubuntu.com trusty-updates/multiverse amd64 Packages [14.6 kB] virtualbox-iso: Get:10 http://archive.ubuntu.com trusty-updates/main i386 Packages [1,065 kB] virtualbox-iso: Get:11 http://archive.ubuntu.com trusty-updates/restricted i386 Packages [17.1 kB] virtualbox-iso: Get:12 http://archive.ubuntu.com trusty-updates/universe i386 Packages [493 kB] virtualbox-iso: Get:13 http://archive.ubuntu.com trusty-updates/multiverse i386 Packages [15.0 kB] virtualbox-iso: Hit http://archive.ubuntu.com trusty-updates/main Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty-updates/multiverse Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty-updates/restricted Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty-updates/universe Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty Release virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/main Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/restricted Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/universe Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/multiverse Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/main amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/restricted amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/universe amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/multiverse amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/main i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/restricted i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/universe i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/multiverse i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/main Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/multiverse Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/restricted Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty-backports/universe Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty/main Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty/restricted Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty/universe Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty/multiverse Sources virtualbox-iso: Hit http://archive.ubuntu.com trusty/main amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/restricted amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/universe amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/multiverse amd64 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/main i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/restricted i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/universe i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/multiverse i386 Packages virtualbox-iso: Hit http://archive.ubuntu.com trusty/main Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty/multiverse Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty/restricted Translation-en virtualbox-iso: Hit http://archive.ubuntu.com trusty/universe Translation-en virtualbox-iso: Ign http://archive.ubuntu.com trusty/main Translation-en_US virtualbox-iso: Ign http://archive.ubuntu.com trusty/multiverse Translation-en_US virtualbox-iso: Ign http://archive.ubuntu.com trusty/restricted Translation-en_US virtualbox-iso: Ign http://archive.ubuntu.com trusty/universe Translation-en_US virtualbox-iso: Fetched 3,998 kB in 2min 2s (32.7 kB/s) virtualbox-iso: Reading package lists... virtualbox-iso: &#43; apt-get -y upgrade linux-server linux-headers-server virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following NEW packages will be installed: virtualbox-iso: amd64-microcode intel-microcode iucode-tool linux-generic virtualbox-iso: linux-headers-3.13.0-164 linux-headers-3.13.0-164-generic virtualbox-iso: linux-headers-generic linux-headers-server linux-image-3.13.0-164-generic virtualbox-iso: linux-image-extra-3.13.0-164-generic linux-image-generic linux-server virtualbox-iso: The following packages have been kept back: virtualbox-iso: linux-generic-lts-xenial linux-headers-generic-lts-xenial virtualbox-iso: linux-image-generic-lts-xenial ubuntu-minimal virtualbox-iso: The following packages will be upgraded: virtualbox-iso: accountsservice apparmor apport apt apt-transport-https apt-utils bash virtualbox-iso: bind9-host bsdutils ca-certificates curl dbus dnsutils dpkg eject file virtualbox-iso: gettext-base gnupg gpgv ifupdown init-system-helpers initramfs-tools virtualbox-iso: initramfs-tools-bin iproute2 isc-dhcp-client isc-dhcp-common klibc-utils virtualbox-iso: kmod krb5-locales landscape-common libaccountsservice0 libapparmor-perl virtualbox-iso: libapparmor1 libapt-inst1.5 libapt-pkg4.12 libasn1-8-heimdal libasprintf0c2 virtualbox-iso: libbind9-90 libblkid1 libc-bin libcurl3 libcurl3-gnutls libdb5.3 libdbus-1-3 virtualbox-iso: libdns100 libdrm2 libelf1 libevent-2.0-5 libexpat1 libffi6 libgc1c2 virtualbox-iso: libgcrypt11 libglib2.0-0 libglib2.0-data libgnutls-openssl27 libgnutls26 virtualbox-iso: libgssapi-krb5-2 libgssapi3-heimdal libhcrypto4-heimdal libheimbase1-heimdal virtualbox-iso: libheimntlm0-heimdal libhx509-5-heimdal libidn11 libisc95 libisccc90 virtualbox-iso: libisccfg90 libk5crypto3 libklibc libkmod2 libkrb5-26-heimdal libkrb5-3 virtualbox-iso: libkrb5support0 libldap-2.4-2 liblwres90 libmagic1 libmount1 libnl-3-200 virtualbox-iso: libnl-genl-3-200 libpam-systemd libpcsclite1 libplymouth2 libpng12-0 virtualbox-iso: libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libprocps3 virtualbox-iso: libpython2.7 libpython2.7-minimal libpython2.7-stdlib libpython3.4-minimal virtualbox-iso: libpython3.4-stdlib libroken18-heimdal librtmp0 libsystemd-daemon0 virtualbox-iso: libsystemd-login0 libtasn1-6 libudev1 libuuid1 libwind0-heimdal libx11-6 virtualbox-iso: libx11-data libxml2 linux-firmware login logrotate lsb-base lsb-release lshw virtualbox-iso: makedev module-init-tools mount multiarch-support ntpdate openssl passwd virtualbox-iso: patch plymouth plymouth-theme-ubuntu-text policykit-1 ppp procps python-apt virtualbox-iso: python-apt-common python-six python-twisted-bin python-twisted-core virtualbox-iso: python2.7 python2.7-minimal python3-apport python3-apt python3-distupgrade virtualbox-iso: python3-problem-report python3-software-properties python3-update-manager virtualbox-iso: python3.4 python3.4-minimal resolvconf rsync rsyslog sensible-utils virtualbox-iso: software-properties-common sudo systemd-services tar tcpdump tzdata virtualbox-iso: ubuntu-release-upgrader-core ubuntu-standard udev unattended-upgrades virtualbox-iso: update-manager-core update-notifier-common util-linux uuid-runtime vim virtualbox-iso: vim-common vim-runtime vim-tiny w3m wget wpasupplicant virtualbox-iso: 161 upgraded, 12 newly installed, 0 to remove and 4 not upgraded. virtualbox-iso: Need to get 140 MB of archives. virtualbox-iso: After this operation, 283 MB of additional disk space will be used. virtualbox-iso: Get:1 http://archive.ubuntu.com/ubuntu/ trusty-updates/main bash amd64 4.3-7ubuntu1.7 [575 kB] virtualbox-iso: Get:2 http://archive.ubuntu.com/ubuntu/ trusty-updates/main dpkg amd64 1.17.5ubuntu5.8 [1,958 kB] virtualbox-iso: Get:3 http://archive.ubuntu.com/ubuntu/ trusty-updates/main login amd64 1:4.1.5.1-1ubuntu9.5 [302 kB] virtualbox-iso: Get:4 http://archive.ubuntu.com/ubuntu/ trusty-updates/main mount amd64 2.20.1-5.1ubuntu20.9 [115 kB] virtualbox-iso: Get:5 http://archive.ubuntu.com/ubuntu/ trusty-updates/main tar amd64 1.27.1-1ubuntu0.1 [196 kB] virtualbox-iso: Get:6 http://archive.ubuntu.com/ubuntu/ trusty-updates/main lsb-base all 4.1&#43;Debian11ubuntu6.2 [13.3 kB] virtualbox-iso: Get:7 http://archive.ubuntu.com/ubuntu/ trusty-updates/main tzdata all 2018i-0ubuntu0.14.04 [168 kB] virtualbox-iso: Get:8 http://archive.ubuntu.com/ubuntu/ trusty-updates/main util-linux amd64 2.20.1-5.1ubuntu20.9 [458 kB] virtualbox-iso: Get:9 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libapt-pkg4.12 amd64 1.0.1ubuntu2.18 [638 kB] virtualbox-iso: Get:10 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libapt-pkg4.12 amd64 1.0.1ubuntu2.18 [638 kB] virtualbox-iso: Get:11 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gpgv amd64 1.4.16-1ubuntu2.6 [161 kB] virtualbox-iso: Get:12 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gnupg amd64 1.4.16-1ubuntu2.6 [611 kB] virtualbox-iso: Get:13 http://archive.ubuntu.com/ubuntu/ trusty-updates/main apt amd64 1.0.1ubuntu2.18 [954 kB] virtualbox-iso: Get:14 http://archive.ubuntu.com/ubuntu/ trusty-updates/main bsdutils amd64 1:2.20.1-5.1ubuntu20.9 [33.9 kB] virtualbox-iso: Get:15 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libc-bin amd64 2.19-0ubuntu6.14 [1,166 kB] virtualbox-iso: Get:16 http://archive.ubuntu.com/ubuntu/ trusty-updates/main passwd amd64 1:4.1.5.1-1ubuntu9.5 [759 kB] virtualbox-iso: Get:17 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libuuid1 amd64 2.20.1-5.1ubuntu20.9 [10.8 kB] virtualbox-iso: Get:18 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libblkid1 amd64 2.20.1-5.1ubuntu20.9 [62.6 kB] virtualbox-iso: Get:19 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libdb5.3 amd64 5.3.28-3ubuntu3.1 [646 kB] virtualbox-iso: Get:20 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libmount1 amd64 2.20.1-5.1ubuntu20.9 [60.3 kB] virtualbox-iso: Get:21 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libapt-inst1.5 amd64 1.0.1ubuntu2.18 [58.6 kB] virtualbox-iso: Get:22 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libexpat1 amd64 2.1.0-4ubuntu1.4 [70.9 kB] virtualbox-iso: Get:23 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libffi6 amd64 3.1~rc1&#43;r3.0.13-12ubuntu0.2 [17.9 kB] virtualbox-iso: Get:24 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgcrypt11 amd64 1.5.3-2ubuntu4.6 [238 kB] virtualbox-iso: Get:25 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libtasn1-6 amd64 3.4-3ubuntu0.6 [43.6 kB] virtualbox-iso: Get:26 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgnutls-openssl27 amd64 2.12.23-12ubuntu2.8 [18.3 kB] virtualbox-iso: Get:27 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgnutls26 amd64 2.12.23-12ubuntu2.8 [395 kB] virtualbox-iso: Get:28 http://archive.ubuntu.com/ubuntu/ trusty-updates/main file amd64 1:5.14-2ubuntu3.4 [19.4 kB] virtualbox-iso: Get:29 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libmagic1 amd64 1:5.14-2ubuntu3.4 [185 kB] virtualbox-iso: Get:30 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3.4 amd64 3.4.3-1ubuntu1~14.04.7 [178 kB] virtualbox-iso: Get:31 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3.4-minimal amd64 3.4.3-1ubuntu1~14.04.7 [1,224 kB] virtualbox-iso: Get:32 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpython3.4-stdlib amd64 3.4.3-1ubuntu1~14.04.7 [1,986 kB] virtualbox-iso: Get:33 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpython3.4-minimal amd64 3.4.3-1ubuntu1~14.04.7 [461 kB] virtualbox-iso: Get:34 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ntpdate amd64 1:4.2.6.p5&#43;dfsg-3ubuntu2.14.04.13 [57.4 kB] virtualbox-iso: Get:35 http://archive.ubuntu.com/ubuntu/ trusty-updates/main resolvconf all 1.69ubuntu1.4 [55.1 kB] virtualbox-iso: Get:36 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libdbus-1-3 amd64 1.6.18-0ubuntu4.5 [132 kB] virtualbox-iso: Get:37 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libdrm2 amd64 2.4.67-1ubuntu0.14.04.2 [27.2 kB] virtualbox-iso: Get:38 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libkmod2 amd64 15-0ubuntu7 [38.1 kB] virtualbox-iso: Get:39 http://archive.ubuntu.com/ubuntu/ trusty-updates/main klibc-utils amd64 2.0.3-0ubuntu1.14.04.3 [107 kB] virtualbox-iso: Get:40 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libklibc amd64 2.0.3-0ubuntu1.14.04.3 [40.6 kB] virtualbox-iso: Get:41 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libprocps3 amd64 1:3.3.9-1ubuntu2.3 [31.7 kB] virtualbox-iso: Get:42 http://archive.ubuntu.com/ubuntu/ trusty-updates/main procps amd64 1:3.3.9-1ubuntu2.3 [210 kB] virtualbox-iso: Get:43 http://archive.ubuntu.com/ubuntu/ trusty-updates/main udev amd64 204-5ubuntu20.29 [736 kB] virtualbox-iso: Get:44 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libudev1 amd64 204-5ubuntu20.29 [34.3 kB] virtualbox-iso: Get:45 http://archive.ubuntu.com/ubuntu/ trusty-updates/main initramfs-tools all 0.103ubuntu4.11 [44.6 kB] virtualbox-iso: Get:46 http://archive.ubuntu.com/ubuntu/ trusty-updates/main initramfs-tools-bin amd64 0.103ubuntu4.11 [8,610 B] virtualbox-iso: Get:47 http://archive.ubuntu.com/ubuntu/ trusty-updates/main kmod amd64 15-0ubuntu7 [84.9 kB] virtualbox-iso: Get:48 http://archive.ubuntu.com/ubuntu/ trusty-updates/main module-init-tools all 15-0ubuntu7 [1,944 B] virtualbox-iso: Get:49 http://archive.ubuntu.com/ubuntu/ trusty-updates/main plymouth amd64 0.8.8-0ubuntu17.2 [98.4 kB] virtualbox-iso: Get:50 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpng12-0 amd64 1.2.50-1ubuntu2.14.04.3 [118 kB] virtualbox-iso: Get:51 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libplymouth2 amd64 0.8.8-0ubuntu17.2 [76.6 kB] virtualbox-iso: Get:52 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libroken18-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [39.9 kB] virtualbox-iso: Get:53 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libasn1-8-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [160 kB] virtualbox-iso: Get:54 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libasprintf0c2 amd64 0.18.3.1-1ubuntu3.1 [6,466 B] virtualbox-iso: Get:55 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libk5crypto3 amd64 1.12&#43;dfsg-2ubuntu5.3 [79.6 kB] virtualbox-iso: Get:56 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgssapi-krb5-2 amd64 1.12&#43;dfsg-2ubuntu5.3 [114 kB] virtualbox-iso: Get:57 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libkrb5-3 amd64 1.12&#43;dfsg-2ubuntu5.3 [262 kB] virtualbox-iso: Get:58 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libkrb5support0 amd64 1.12&#43;dfsg-2ubuntu5.3 [30.7 kB] virtualbox-iso: Get:59 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libidn11 amd64 1.28-1ubuntu2.2 [94.6 kB] virtualbox-iso: Get:60 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libhcrypto4-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [84.1 kB] virtualbox-iso: Get:61 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libheimbase1-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [29.0 kB] virtualbox-iso: Get:62 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libwind0-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [47.9 kB] virtualbox-iso: Get:63 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libhx509-5-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [104 kB] virtualbox-iso: Get:64 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libkrb5-26-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [196 kB] virtualbox-iso: Get:65 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libheimntlm0-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [15.2 kB] virtualbox-iso: Get:66 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgssapi3-heimdal amd64 1.6~git20131207&#43;dfsg-1ubuntu1.2 [89.7 kB] virtualbox-iso: Get:67 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libldap-2.4-2 amd64 2.4.31-1&#43;nmu2ubuntu8.5 [153 kB] virtualbox-iso: Get:68 http://archive.ubuntu.com/ubuntu/ trusty-updates/main librtmp0 amd64 2.4&#43;20121230.gitdf6c518-1ubuntu0.1 [50.4 kB] virtualbox-iso: Get:69 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libcurl3-gnutls amd64 7.35.0-1ubuntu2.19 [166 kB] virtualbox-iso: Get:70 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libelf1 amd64 0.158-0ubuntu5.3 [38.3 kB] virtualbox-iso: Get:71 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libglib2.0-0 amd64 2.40.2-0ubuntu1.1 [1,059 kB] virtualbox-iso: Get:72 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpam-systemd amd64 204-5ubuntu20.29 [25.5 kB] virtualbox-iso: Get:73 http://archive.ubuntu.com/ubuntu/ trusty-updates/main systemd-services amd64 204-5ubuntu20.29 [197 kB] virtualbox-iso: Get:74 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libsystemd-daemon0 amd64 204-5ubuntu20.29 [10.8 kB] virtualbox-iso: Get:75 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libapparmor1 amd64 2.10.95-0ubuntu2.6~14.04.4 [31.2 kB] virtualbox-iso: Get:76 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libsystemd-login0 amd64 204-5ubuntu20.29 [27.8 kB] virtualbox-iso: Get:77 http://archive.ubuntu.com/ubuntu/ trusty-updates/main dbus amd64 1.6.18-0ubuntu4.5 [232 kB] virtualbox-iso: Get:78 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpolkit-gobject-1-0 amd64 0.105-4ubuntu3.14.04.2 [35.1 kB] virtualbox-iso: Get:79 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libx11-data all 2:1.6.2-1ubuntu2.1 [111 kB] virtualbox-iso: Get:80 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libx11-6 amd64 2:1.6.2-1ubuntu2.1 [561 kB] virtualbox-iso: Get:81 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libxml2 amd64 2.9.1&#43;dfsg1-3ubuntu4.13 [573 kB] virtualbox-iso: Get:82 http://archive.ubuntu.com/ubuntu/ trusty-updates/main curl amd64 7.35.0-1ubuntu2.19 [123 kB] virtualbox-iso: Get:83 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libcurl3 amd64 7.35.0-1ubuntu2.19 [173 kB] virtualbox-iso: Get:84 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libevent-2.0-5 amd64 2.0.21-stable-1ubuntu1.14.04.2 [126 kB] virtualbox-iso: Get:85 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libgc1c2 amd64 1:7.2d-5ubuntu2.1 [74.8 kB] virtualbox-iso: Get:86 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libnl-genl-3-200 amd64 3.2.21-1ubuntu4.1 [10.2 kB] virtualbox-iso: Get:87 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libnl-3-200 amd64 3.2.21-1ubuntu4.1 [45.3 kB] virtualbox-iso: Get:88 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpcsclite1 amd64 1.8.10-1ubuntu1.1 [21.1 kB] virtualbox-iso: Get:89 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpolkit-agent-1-0 amd64 0.105-4ubuntu3.14.04.2 [14.9 kB] virtualbox-iso: Get:90 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpolkit-backend-1-0 amd64 0.105-4ubuntu3.14.04.2 [35.2 kB] virtualbox-iso: Get:91 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python2.7 amd64 2.7.6-8ubuntu0.5 [197 kB] virtualbox-iso: Get:92 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpython2.7 amd64 2.7.6-8ubuntu0.5 [1,041 kB] virtualbox-iso: Get:93 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpython2.7-stdlib amd64 2.7.6-8ubuntu0.5 [1,872 kB] virtualbox-iso: Get:94 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python2.7-minimal amd64 2.7.6-8ubuntu0.5 [1,186 kB] virtualbox-iso: Get:95 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libpython2.7-minimal amd64 2.7.6-8ubuntu0.5 [308 kB] virtualbox-iso: Get:96 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-3.13.0-164-generic amd64 3.13.0-164.214 [15.6 MB] virtualbox-iso: Get:97 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python-apt-common all 0.9.3.5ubuntu3 [15.0 kB] virtualbox-iso: Get:98 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3-apt amd64 0.9.3.5ubuntu3 [139 kB] virtualbox-iso: Get:99 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python-apt amd64 0.9.3.5ubuntu3 [141 kB] virtualbox-iso: Get:100 http://archive.ubuntu.com/ubuntu/ trusty-updates/main patch amd64 2.7.1-4ubuntu2.4 [86.9 kB] virtualbox-iso: Get:101 http://archive.ubuntu.com/ubuntu/ trusty-updates/main lsb-release all 4.1&#43;Debian11ubuntu6.2 [11.6 kB] virtualbox-iso: Get:102 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ubuntu-release-upgrader-core all 1:0.220.10 [24.2 kB] virtualbox-iso: Get:103 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3-distupgrade all 1:0.220.10 [104 kB] virtualbox-iso: Get:104 http://archive.ubuntu.com/ubuntu/ trusty-updates/main update-manager-core all 1:0.196.25 [8,280 B] virtualbox-iso: Get:105 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3-update-manager all 1:0.196.25 [32.2 kB] virtualbox-iso: Get:106 http://archive.ubuntu.com/ubuntu/ trusty-updates/main update-notifier-common all 0.154.1ubuntu3 [164 kB] virtualbox-iso: Get:107 http://archive.ubuntu.com/ubuntu/ trusty-updates/main multiarch-support amd64 2.19-0ubuntu6.14 [4,482 B] virtualbox-iso: Get:108 http://archive.ubuntu.com/ubuntu/ trusty-updates/main sensible-utils all 0.0.9ubuntu0.14.04.1 [10.0 kB] virtualbox-iso: Get:109 http://archive.ubuntu.com/ubuntu/ trusty-updates/main apt-utils amd64 1.0.1ubuntu2.18 [172 kB] virtualbox-iso: Get:110 http://archive.ubuntu.com/ubuntu/ trusty-updates/main eject amd64 2.1.5&#43;deb1&#43;cvs20081104-13.1ubuntu0.14.04.1 [23.4 kB] virtualbox-iso: Get:111 http://archive.ubuntu.com/ubuntu/ trusty-updates/main init-system-helpers all 1.14ubuntu1 [10.7 kB] virtualbox-iso: Get:112 http://archive.ubuntu.com/ubuntu/ trusty-updates/main iproute2 amd64 3.12.0-2ubuntu1.2 [401 kB] virtualbox-iso: Get:113 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ifupdown amd64 0.7.47.2ubuntu4.5 [53.3 kB] virtualbox-iso: Get:114 http://archive.ubuntu.com/ubuntu/ trusty-updates/main isc-dhcp-client amd64 4.2.4-7ubuntu12.13 [640 kB] virtualbox-iso: Get:115 http://archive.ubuntu.com/ubuntu/ trusty-updates/main isc-dhcp-common amd64 4.2.4-7ubuntu12.13 [709 kB] virtualbox-iso: Get:116 http://archive.ubuntu.com/ubuntu/ trusty-updates/main logrotate amd64 3.8.7-1ubuntu1.2 [38.4 kB] virtualbox-iso: Get:117 http://archive.ubuntu.com/ubuntu/ trusty-updates/main rsyslog amd64 7.4.4-1ubuntu2.7 [353 kB] virtualbox-iso: Get:118 http://archive.ubuntu.com/ubuntu/ trusty-updates/main sudo amd64 1.8.9p5-1ubuntu1.4 [342 kB] virtualbox-iso: Get:119 http://archive.ubuntu.com/ubuntu/ trusty-updates/main vim amd64 2:7.4.052-1ubuntu3.1 [955 kB] virtualbox-iso: Get:120 http://archive.ubuntu.com/ubuntu/ trusty-updates/main vim-tiny amd64 2:7.4.052-1ubuntu3.1 [391 kB] virtualbox-iso: Get:121 http://archive.ubuntu.com/ubuntu/ trusty-updates/main vim-runtime all 2:7.4.052-1ubuntu3.1 [4,882 kB] virtualbox-iso: Get:122 http://archive.ubuntu.com/ubuntu/ trusty-updates/main vim-common amd64 2:7.4.052-1ubuntu3.1 [95.2 kB] virtualbox-iso: Get:123 http://archive.ubuntu.com/ubuntu/ trusty-updates/main makedev all 2.3.1-93ubuntu2~ubuntu14.04.1 [24.5 kB] virtualbox-iso: Get:124 http://archive.ubuntu.com/ubuntu/ trusty-updates/main accountsservice amd64 0.6.35-0ubuntu7.3 [60.6 kB] virtualbox-iso: Get:125 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libaccountsservice0 amd64 0.6.35-0ubuntu7.3 [69.8 kB] virtualbox-iso: Get:126 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libapparmor-perl amd64 2.10.95-0ubuntu2.6~14.04.4 [31.2 kB] virtualbox-iso: Get:127 http://archive.ubuntu.com/ubuntu/ trusty-updates/main apparmor amd64 2.10.95-0ubuntu2.6~14.04.4 [362 kB] virtualbox-iso: Get:128 http://archive.ubuntu.com/ubuntu/ trusty-updates/main apt-transport-https amd64 1.0.1ubuntu2.18 [25.0 kB] virtualbox-iso: Get:129 http://archive.ubuntu.com/ubuntu/ trusty-updates/main bind9-host amd64 1:9.9.5.dfsg-3ubuntu0.18 [46.5 kB] virtualbox-iso: Get:130 http://archive.ubuntu.com/ubuntu/ trusty-updates/main dnsutils amd64 1:9.9.5.dfsg-3ubuntu0.18 [97.0 kB] virtualbox-iso: Get:131 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libisc95 amd64 1:9.9.5.dfsg-3ubuntu0.18 [148 kB] virtualbox-iso: Get:132 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libdns100 amd64 1:9.9.5.dfsg-3ubuntu0.18 [645 kB] virtualbox-iso: Get:133 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libisccc90 amd64 1:9.9.5.dfsg-3ubuntu0.18 [15.7 kB] virtualbox-iso: Get:134 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libisccfg90 amd64 1:9.9.5.dfsg-3ubuntu0.18 [35.9 kB] virtualbox-iso: Get:135 http://archive.ubuntu.com/ubuntu/ trusty-updates/main liblwres90 amd64 1:9.9.5.dfsg-3ubuntu0.18 [33.1 kB] virtualbox-iso: Get:136 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libbind9-90 amd64 1:9.9.5.dfsg-3ubuntu0.18 [22.1 kB] virtualbox-iso: Get:137 http://archive.ubuntu.com/ubuntu/ trusty-updates/main openssl amd64 1.0.1f-1ubuntu2.27 [489 kB] virtualbox-iso: Get:138 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ca-certificates all 20170717~14.04.2 [166 kB] virtualbox-iso: Get:139 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gettext-base amd64 0.18.3.1-1ubuntu3.1 [48.6 kB] virtualbox-iso: Get:140 http://archive.ubuntu.com/ubuntu/ trusty-updates/main krb5-locales all 1.12&#43;dfsg-2ubuntu5.3 [13.5 kB] virtualbox-iso: Get:141 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libglib2.0-data all 2.40.2-0ubuntu1.1 [116 kB] virtualbox-iso: Get:142 http://archive.ubuntu.com/ubuntu/ trusty-updates/main lshw amd64 02.16-2ubuntu1.4 [225 kB] virtualbox-iso: Get:143 http://archive.ubuntu.com/ubuntu/ trusty-updates/main plymouth-theme-ubuntu-text amd64 0.8.8-0ubuntu17.2 [7,958 B] virtualbox-iso: Get:144 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ppp amd64 2.4.5-5.1ubuntu2.3 [311 kB] virtualbox-iso: Get:145 http://archive.ubuntu.com/ubuntu/ trusty-updates/main rsync amd64 3.1.0-2ubuntu0.4 [284 kB] virtualbox-iso: Get:146 http://archive.ubuntu.com/ubuntu/ trusty-updates/main tcpdump amd64 4.9.2-0ubuntu0.14.04.1 [380 kB] virtualbox-iso: Get:147 http://archive.ubuntu.com/ubuntu/ trusty-updates/main wget amd64 1.15-1ubuntu1.14.04.4 [270 kB] virtualbox-iso: Get:148 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ubuntu-standard amd64 1.325.1 [2,682 B] virtualbox-iso: Get:149 http://archive.ubuntu.com/ubuntu/ trusty-updates/main uuid-runtime amd64 2.20.1-5.1ubuntu20.9 [12.2 kB] virtualbox-iso: Get:150 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3-problem-report all 2.14.1-0ubuntu3.29 [9,504 B] virtualbox-iso: Get:151 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3-apport all 2.14.1-0ubuntu3.29 [75.5 kB] virtualbox-iso: Get:152 http://archive.ubuntu.com/ubuntu/ trusty-updates/main apport all 2.14.1-0ubuntu3.29 [183 kB] virtualbox-iso: Get:153 http://archive.ubuntu.com/ubuntu/ trusty/multiverse iucode-tool amd64 1.0.1-1 [28.1 kB] virtualbox-iso: Get:154 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python-twisted-bin amd64 13.2.0-1ubuntu1.2 [11.9 kB] virtualbox-iso: Get:155 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python-twisted-core all 13.2.0-1ubuntu1.2 [983 kB] virtualbox-iso: Get:156 http://archive.ubuntu.com/ubuntu/ trusty-updates/main landscape-common amd64 14.12-0ubuntu6.14.04.3 [168 kB] virtualbox-iso: Get:157 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:158 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:159 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:160 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:161 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:162 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:163 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:164 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:165 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-firmware all 1.127.24 [33.9 MB] virtualbox-iso: Get:166 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-extra-3.13.0-164-generic amd64 3.13.0-164.214 [37.0 MB] virtualbox-iso: Get:167 http://archive.ubuntu.com/ubuntu/ trusty-updates/main intel-microcode amd64 3.20180807a.0ubuntu0.14.04.1 [1,275 kB] virtualbox-iso: Get:168 http://archive.ubuntu.com/ubuntu/ trusty-updates/main amd64-microcode amd64 3.20180524.1~ubuntu0.14.04.2&#43;really20130710.1ubuntu1 [26.3 kB] virtualbox-iso: Get:169 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-generic amd64 3.13.0.164.174 [2,360 B] virtualbox-iso: Get:170 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-3.13.0-164 all 3.13.0-164.214 [8,924 kB] virtualbox-iso: Get:171 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-3.13.0-164-generic amd64 3.13.0-164.214 [702 kB] virtualbox-iso: Get:172 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-generic amd64 3.13.0.164.174 [2,336 B] virtualbox-iso: Get:173 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-generic amd64 3.13.0.164.174 [1,788 B] virtualbox-iso: Get:174 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-server amd64 3.13.0.164.174 [1,762 B] virtualbox-iso: Get:175 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-server amd64 3.13.0.164.174 [1,752 B] virtualbox-iso: Get:176 http://archive.ubuntu.com/ubuntu/ trusty-updates/main policykit-1 amd64 0.105-4ubuntu3.14.04.2 [52.0 kB] virtualbox-iso: Get:177 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python-six all 1.5.2-1ubuntu1.1 [8,348 B] virtualbox-iso: Get:178 http://archive.ubuntu.com/ubuntu/ trusty-updates/main software-properties-common all 0.92.37.8 [9,384 B] virtualbox-iso: Get:179 http://archive.ubuntu.com/ubuntu/ trusty-updates/main unattended-upgrades all 0.82.1ubuntu2.5 [25.6 kB] virtualbox-iso: Get:180 http://archive.ubuntu.com/ubuntu/ trusty-updates/main python3-software-properties all 0.92.37.8 [19.2 kB] virtualbox-iso: Get:181 http://archive.ubuntu.com/ubuntu/ trusty-updates/main w3m amd64 0.5.3-15ubuntu0.2 [876 kB] virtualbox-iso: Get:182 http://archive.ubuntu.com/ubuntu/ trusty-updates/main wpasupplicant amd64 2.1-0ubuntu1.6 [749 kB] virtualbox-iso: Err http://archive.ubuntu.com/ubuntu/ trusty-updates/main wpasupplicant amd64 2.1-0ubuntu1.6 virtualbox-iso: Connection failed [IP: 91.189.88.152 80] virtualbox-iso: Get:183 http://security.ubuntu.com/ubuntu/ trusty-security/main wpasupplicant amd64 2.1-0ubuntu1.6 [749 kB] virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: dpkg-preconfigure: unable to re-open stdin: virtualbox-iso: Fetched 115 MB in 1h 25min 20s (22.5 kB/s) virtualbox-iso: (Reading database ... 67874 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../bash_4.3-7ubuntu1.7_amd64.deb ... virtualbox-iso: Unpacking bash (4.3-7ubuntu1.7) over (4.3-7ubuntu1.5) ... virtualbox-iso: Processing triggers for install-info (5.2.0.dfsg.1-2) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up bash (4.3-7ubuntu1.7) ... virtualbox-iso: update-alternatives: using /usr/share/man/man7/bash-builtins.7.gz to provide /usr/share/man/man7/builtins.7.gz (builtins.7.gz) in auto mode virtualbox-iso: (Reading database ... 67874 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../dpkg_1.17.5ubuntu5.8_amd64.deb ... virtualbox-iso: Unpacking dpkg (1.17.5ubuntu5.8) over (1.17.5ubuntu5.7) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up dpkg (1.17.5ubuntu5.8) ... virtualbox-iso: (Reading database ... 67874 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../login_1%3a4.1.5.1-1ubuntu9.5_amd64.deb ... virtualbox-iso: Unpacking login (1:4.1.5.1-1ubuntu9.5) over (1:4.1.5.1-1ubuntu9.2) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up login (1:4.1.5.1-1ubuntu9.5) ... virtualbox-iso: (Reading database ... 67874 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../mount_2.20.1-5.1ubuntu20.9_amd64.deb ... virtualbox-iso: Unpacking mount (2.20.1-5.1ubuntu20.9) over (2.20.1-5.1ubuntu20.7) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up mount (2.20.1-5.1ubuntu20.9) ... virtualbox-iso: (Reading database ... 67874 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../tar_1.27.1-1ubuntu0.1_amd64.deb ... virtualbox-iso: Unpacking tar (1.27.1-1ubuntu0.1) over (1.27.1-1) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Processing triggers for mime-support (3.54ubuntu1.1) ... virtualbox-iso: Setting up tar (1.27.1-1ubuntu0.1) ... virtualbox-iso: (Reading database ... 67874 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../lsb-base_4.1&#43;Debian11ubuntu6.2_all.deb ... virtualbox-iso: Unpacking lsb-base (4.1&#43;Debian11ubuntu6.2) over (4.1&#43;Debian11ubuntu6.1) ... virtualbox-iso: Setting up lsb-base (4.1&#43;Debian11ubuntu6.2) ... virtualbox-iso: (Reading database ... 67874 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../tzdata_2018i-0ubuntu0.14.04_all.deb ... virtualbox-iso: Unpacking tzdata (2018i-0ubuntu0.14.04) over (2016f-0ubuntu0.14.04) ... virtualbox-iso: Setting up tzdata (2018i-0ubuntu0.14.04) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: virtualbox-iso: Current default time zone: &#39;Etc/UTC&#39; virtualbox-iso: Local time is now: Tue Jan 8 14:08:25 UTC 2019. virtualbox-iso: Universal Time is now: Tue Jan 8 14:08:25 UTC 2019. virtualbox-iso: Run &#39;dpkg-reconfigure tzdata&#39; if you wish to change it. virtualbox-iso: virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../util-linux_2.20.1-5.1ubuntu20.9_amd64.deb ... virtualbox-iso: Unpacking util-linux (2.20.1-5.1ubuntu20.9) over (2.20.1-5.1ubuntu20.7) ... virtualbox-iso: Processing triggers for install-info (5.2.0.dfsg.1-2) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Processing triggers for mime-support (3.54ubuntu1.1) ... virtualbox-iso: Processing triggers for ureadahead (0.100.0-16) ... virtualbox-iso: ureadahead will be reprofiled on next reboot virtualbox-iso: Setting up util-linux (2.20.1-5.1ubuntu20.9) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../libapt-pkg4.12_1.0.1ubuntu2.18_amd64.deb ... virtualbox-iso: Unpacking libapt-pkg4.12:amd64 (1.0.1ubuntu2.18) over (1.0.1ubuntu2.14) ... virtualbox-iso: Setting up libapt-pkg4.12:amd64 (1.0.1ubuntu2.18) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.9) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../gpgv_1.4.16-1ubuntu2.6_amd64.deb ... virtualbox-iso: Unpacking gpgv (1.4.16-1ubuntu2.6) over (1.4.16-1ubuntu2.3) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up gpgv (1.4.16-1ubuntu2.6) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../gnupg_1.4.16-1ubuntu2.6_amd64.deb ... virtualbox-iso: Unpacking gnupg (1.4.16-1ubuntu2.6) over (1.4.16-1ubuntu2.3) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Processing triggers for install-info (5.2.0.dfsg.1-2) ... virtualbox-iso: Setting up gnupg (1.4.16-1ubuntu2.6) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../apt_1.0.1ubuntu2.18_amd64.deb ... virtualbox-iso: Unpacking apt (1.0.1ubuntu2.18) over (1.0.1ubuntu2.14) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up apt (1.0.1ubuntu2.18) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.9) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../bsdutils_1%3a2.20.1-5.1ubuntu20.9_amd64.deb ... virtualbox-iso: Unpacking bsdutils (1:2.20.1-5.1ubuntu20.9) over (1:2.20.1-5.1ubuntu20.7) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up bsdutils (1:2.20.1-5.1ubuntu20.9) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../libc-bin_2.19-0ubuntu6.14_amd64.deb ... virtualbox-iso: Unpacking libc-bin (2.19-0ubuntu6.14) over (2.19-0ubuntu6.9) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up libc-bin (2.19-0ubuntu6.14) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../passwd_1%3a4.1.5.1-1ubuntu9.5_amd64.deb ... virtualbox-iso: Unpacking passwd (1:4.1.5.1-1ubuntu9.5) over (1:4.1.5.1-1ubuntu9.2) ... virtualbox-iso: Processing triggers for ureadahead (0.100.0-16) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up passwd (1:4.1.5.1-1ubuntu9.5) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../libuuid1_2.20.1-5.1ubuntu20.9_amd64.deb ... virtualbox-iso: Unpacking libuuid1:amd64 (2.20.1-5.1ubuntu20.9) over (2.20.1-5.1ubuntu20.7) ... virtualbox-iso: Setting up libuuid1:amd64 (2.20.1-5.1ubuntu20.9) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.14) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../libblkid1_2.20.1-5.1ubuntu20.9_amd64.deb ... virtualbox-iso: Unpacking libblkid1:amd64 (2.20.1-5.1ubuntu20.9) over (2.20.1-5.1ubuntu20.7) ... virtualbox-iso: Setting up libblkid1:amd64 (2.20.1-5.1ubuntu20.9) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.14) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../libdb5.3_5.3.28-3ubuntu3.1_amd64.deb ... virtualbox-iso: Unpacking libdb5.3:amd64 (5.3.28-3ubuntu3.1) over (5.3.28-3ubuntu3) ... virtualbox-iso: Setting up libdb5.3:amd64 (5.3.28-3ubuntu3.1) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.14) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../libmount1_2.20.1-5.1ubuntu20.9_amd64.deb ... virtualbox-iso: Unpacking libmount1:amd64 (2.20.1-5.1ubuntu20.9) over (2.20.1-5.1ubuntu20.7) ... virtualbox-iso: Setting up libmount1:amd64 (2.20.1-5.1ubuntu20.9) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.14) ... virtualbox-iso: (Reading database ... 67889 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../libapt-inst1.5_1.0.1ubuntu2.18_amd64.deb ... virtualbox-iso: Unpacking libapt-inst1.5:amd64 (1.0.1ubuntu2.18) over (1.0.1ubuntu2.14) ... virtualbox-iso: Preparing to unpack .../libexpat1_2.1.0-4ubuntu1.4_amd64.deb ... virtualbox-iso: Unpacking libexpat1:amd64 (2.1.0-4ubuntu1.4) over (2.1.0-4ubuntu1.3) ... virtualbox-iso: Preparing to unpack .../libffi6_3.1~rc1&#43;r3.0.13-12ubuntu0.2_amd64.deb ... virtualbox-iso: Unpacking libffi6:amd64 (3.1~rc1&#43;r3.0.13-12ubuntu0.2) over (3.1~rc1&#43;r3.0.13-12ubuntu0.1) ... virtualbox-iso: Preparing to unpack .../libgcrypt11_1.5.3-2ubuntu4.6_amd64.deb ... virtualbox-iso: Unpacking libgcrypt11:amd64 (1.5.3-2ubuntu4.6) over (1.5.3-2ubuntu4.3) ... virtualbox-iso: Preparing to unpack .../libtasn1-6_3.4-3ubuntu0.6_amd64.deb ... virtualbox-iso: Unpacking libtasn1-6:amd64 (3.4-3ubuntu0.6) over (3.4-3ubuntu0.4) ... virtualbox-iso: Preparing to unpack .../libgnutls-openssl27_2.12.23-12ubuntu2.8_amd64.deb ... virtualbox-iso: Unpacking libgnutls-openssl27:amd64 (2.12.23-12ubuntu2.8) over (2.12.23-12ubuntu2.5) ... virtualbox-iso: Preparing to unpack .../libgnutls26_2.12.23-12ubuntu2.8_amd64.deb ... virtualbox-iso: Unpacking libgnutls26:amd64 (2.12.23-12ubuntu2.8) over (2.12.23-12ubuntu2.5) ... virtualbox-iso: Preparing to unpack .../file_1%3a5.14-2ubuntu3.4_amd64.deb ... virtualbox-iso: Unpacking file (1:5.14-2ubuntu3.4) over (1:5.14-2ubuntu3.3) ... virtualbox-iso: Preparing to unpack .../libmagic1_1%3a5.14-2ubuntu3.4_amd64.deb ... virtualbox-iso: Unpacking libmagic1:amd64 (1:5.14-2ubuntu3.4) over (1:5.14-2ubuntu3.3) ... virtualbox-iso: Preparing to unpack .../python3.4_3.4.3-1ubuntu1~14.04.7_amd64.deb ... virtualbox-iso: Unpacking python3.4 (3.4.3-1ubuntu1~14.04.7) over (3.4.3-1ubuntu1~14.04.3) ... virtualbox-iso: Preparing to unpack .../python3.4-minimal_3.4.3-1ubuntu1~14.04.7_amd64.deb ... virtualbox-iso: Unpacking python3.4-minimal (3.4.3-1ubuntu1~14.04.7) over (3.4.3-1ubuntu1~14.04.3) ... virtualbox-iso: Preparing to unpack .../libpython3.4-stdlib_3.4.3-1ubuntu1~14.04.7_amd64.deb ... virtualbox-iso: Unpacking libpython3.4-stdlib:amd64 (3.4.3-1ubuntu1~14.04.7) over (3.4.3-1ubuntu1~14.04.3) ... virtualbox-iso: Preparing to unpack .../libpython3.4-minimal_3.4.3-1ubuntu1~14.04.7_amd64.deb ... virtualbox-iso: Unpacking libpython3.4-minimal:amd64 (3.4.3-1ubuntu1~14.04.7) over (3.4.3-1ubuntu1~14.04.3) ... virtualbox-iso: Preparing to unpack .../ntpdate_1%3a4.2.6.p5&#43;dfsg-3ubuntu2.14.04.13_amd64.deb ... virtualbox-iso: Unpacking ntpdate (1:4.2.6.p5&#43;dfsg-3ubuntu2.14.04.13) over (1:4.2.6.p5&#43;dfsg-3ubuntu2.14.04.8) ... virtualbox-iso: Preparing to unpack .../resolvconf_1.69ubuntu1.4_all.deb ... virtualbox-iso: Unpacking resolvconf (1.69ubuntu1.4) over (1.69ubuntu1.1) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Preparing to unpack .../libdbus-1-3_1.6.18-0ubuntu4.5_amd64.deb ... virtualbox-iso: Unpacking libdbus-1-3:amd64 (1.6.18-0ubuntu4.5) over (1.6.18-0ubuntu4.3) ... virtualbox-iso: Preparing to unpack .../libdrm2_2.4.67-1ubuntu0.14.04.2_amd64.deb ... virtualbox-iso: Unpacking libdrm2:amd64 (2.4.67-1ubuntu0.14.04.2) over (2.4.67-1ubuntu0.14.04.1) ... virtualbox-iso: Preparing to unpack .../libkmod2_15-0ubuntu7_amd64.deb ... virtualbox-iso: Unpacking libkmod2:amd64 (15-0ubuntu7) over (15-0ubuntu6) ... virtualbox-iso: Preparing to unpack .../klibc-utils_2.0.3-0ubuntu1.14.04.3_amd64.deb ... virtualbox-iso: Unpacking klibc-utils (2.0.3-0ubuntu1.14.04.3) over (2.0.3-0ubuntu1.14.04.1) ... virtualbox-iso: Preparing to unpack .../libklibc_2.0.3-0ubuntu1.14.04.3_amd64.deb ... virtualbox-iso: Unpacking libklibc (2.0.3-0ubuntu1.14.04.3) over (2.0.3-0ubuntu1.14.04.1) ... virtualbox-iso: Preparing to unpack .../libprocps3_1%3a3.3.9-1ubuntu2.3_amd64.deb ... virtualbox-iso: Unpacking libprocps3:amd64 (1:3.3.9-1ubuntu2.3) over (1:3.3.9-1ubuntu2.2) ... virtualbox-iso: Preparing to unpack .../procps_1%3a3.3.9-1ubuntu2.3_amd64.deb ... virtualbox-iso: Unpacking procps (1:3.3.9-1ubuntu2.3) over (1:3.3.9-1ubuntu2.2) ... virtualbox-iso: Preparing to unpack .../udev_204-5ubuntu20.29_amd64.deb ... virtualbox-iso: Adding &#39;diversion of /bin/udevadm to /bin/udevadm.upgrade by fake-udev&#39; virtualbox-iso: Unpacking udev (204-5ubuntu20.29) over (204-5ubuntu20.19) ... virtualbox-iso: Preparing to unpack .../libudev1_204-5ubuntu20.29_amd64.deb ... virtualbox-iso: Unpacking libudev1:amd64 (204-5ubuntu20.29) over (204-5ubuntu20.19) ... virtualbox-iso: Preparing to unpack .../initramfs-tools_0.103ubuntu4.11_all.deb ... virtualbox-iso: Unpacking initramfs-tools (0.103ubuntu4.11) over (0.103ubuntu4.4) ... virtualbox-iso: Preparing to unpack .../initramfs-tools-bin_0.103ubuntu4.11_amd64.deb ... virtualbox-iso: Unpacking initramfs-tools-bin (0.103ubuntu4.11) over (0.103ubuntu4.4) ... virtualbox-iso: Preparing to unpack .../kmod_15-0ubuntu7_amd64.deb ... virtualbox-iso: Unpacking kmod (15-0ubuntu7) over (15-0ubuntu6) ... virtualbox-iso: Preparing to unpack .../module-init-tools_15-0ubuntu7_all.deb ... virtualbox-iso: Unpacking module-init-tools (15-0ubuntu7) over (15-0ubuntu6) ... virtualbox-iso: Preparing to unpack .../plymouth_0.8.8-0ubuntu17.2_amd64.deb ... virtualbox-iso: Unpacking plymouth (0.8.8-0ubuntu17.2) over (0.8.8-0ubuntu17.1) ... virtualbox-iso: Preparing to unpack .../libpng12-0_1.2.50-1ubuntu2.14.04.3_amd64.deb ... virtualbox-iso: Unpacking libpng12-0:amd64 (1.2.50-1ubuntu2.14.04.3) over (1.2.50-1ubuntu2.14.04.2) ... virtualbox-iso: Preparing to unpack .../libplymouth2_0.8.8-0ubuntu17.2_amd64.deb ... virtualbox-iso: Unpacking libplymouth2:amd64 (0.8.8-0ubuntu17.2) over (0.8.8-0ubuntu17.1) ... virtualbox-iso: Preparing to unpack .../libroken18-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libroken18-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libasn1-8-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libasn1-8-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libasprintf0c2_0.18.3.1-1ubuntu3.1_amd64.deb ... virtualbox-iso: Unpacking libasprintf0c2:amd64 (0.18.3.1-1ubuntu3.1) over (0.18.3.1-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../libk5crypto3_1.12&#43;dfsg-2ubuntu5.3_amd64.deb ... virtualbox-iso: Unpacking libk5crypto3:amd64 (1.12&#43;dfsg-2ubuntu5.3) over (1.12&#43;dfsg-2ubuntu5.2) ... virtualbox-iso: Preparing to unpack .../libgssapi-krb5-2_1.12&#43;dfsg-2ubuntu5.3_amd64.deb ... virtualbox-iso: Unpacking libgssapi-krb5-2:amd64 (1.12&#43;dfsg-2ubuntu5.3) over (1.12&#43;dfsg-2ubuntu5.2) ... virtualbox-iso: Preparing to unpack .../libkrb5-3_1.12&#43;dfsg-2ubuntu5.3_amd64.deb ... virtualbox-iso: Unpacking libkrb5-3:amd64 (1.12&#43;dfsg-2ubuntu5.3) over (1.12&#43;dfsg-2ubuntu5.2) ... virtualbox-iso: Preparing to unpack .../libkrb5support0_1.12&#43;dfsg-2ubuntu5.3_amd64.deb ... virtualbox-iso: Unpacking libkrb5support0:amd64 (1.12&#43;dfsg-2ubuntu5.3) over (1.12&#43;dfsg-2ubuntu5.2) ... virtualbox-iso: Preparing to unpack .../libidn11_1.28-1ubuntu2.2_amd64.deb ... virtualbox-iso: Unpacking libidn11:amd64 (1.28-1ubuntu2.2) over (1.28-1ubuntu2) ... virtualbox-iso: Preparing to unpack .../libhcrypto4-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libhcrypto4-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libheimbase1-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libheimbase1-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libwind0-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libwind0-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libhx509-5-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libhx509-5-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libkrb5-26-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libkrb5-26-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libheimntlm0-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libheimntlm0-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libgssapi3-heimdal_1.6~git20131207&#43;dfsg-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking libgssapi3-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) over (1.6~git20131207&#43;dfsg-1ubuntu1.1) ... virtualbox-iso: Preparing to unpack .../libldap-2.4-2_2.4.31-1&#43;nmu2ubuntu8.5_amd64.deb ... virtualbox-iso: Unpacking libldap-2.4-2:amd64 (2.4.31-1&#43;nmu2ubuntu8.5) over (2.4.31-1&#43;nmu2ubuntu8.3) ... virtualbox-iso: Preparing to unpack .../librtmp0_2.4&#43;20121230.gitdf6c518-1ubuntu0.1_amd64.deb ... virtualbox-iso: Unpacking librtmp0:amd64 (2.4&#43;20121230.gitdf6c518-1ubuntu0.1) over (2.4&#43;20121230.gitdf6c518-1) ... virtualbox-iso: Preparing to unpack .../libcurl3-gnutls_7.35.0-1ubuntu2.19_amd64.deb ... virtualbox-iso: Unpacking libcurl3-gnutls:amd64 (7.35.0-1ubuntu2.19) over (7.35.0-1ubuntu2.7) ... virtualbox-iso: Preparing to unpack .../libelf1_0.158-0ubuntu5.3_amd64.deb ... virtualbox-iso: Unpacking libelf1:amd64 (0.158-0ubuntu5.3) over (0.158-0ubuntu5.2) ... virtualbox-iso: Preparing to unpack .../libglib2.0-0_2.40.2-0ubuntu1.1_amd64.deb ... virtualbox-iso: Unpacking libglib2.0-0:amd64 (2.40.2-0ubuntu1.1) over (2.40.2-0ubuntu1) ... virtualbox-iso: Preparing to unpack .../libpam-systemd_204-5ubuntu20.29_amd64.deb ... virtualbox-iso: Unpacking libpam-systemd:amd64 (204-5ubuntu20.29) over (204-5ubuntu20.19) ... virtualbox-iso: Preparing to unpack .../systemd-services_204-5ubuntu20.29_amd64.deb ... virtualbox-iso: Unpacking systemd-services (204-5ubuntu20.29) over (204-5ubuntu20.19) ... virtualbox-iso: Preparing to unpack .../libsystemd-daemon0_204-5ubuntu20.29_amd64.deb ... virtualbox-iso: Unpacking libsystemd-daemon0:amd64 (204-5ubuntu20.29) over (204-5ubuntu20.19) ... virtualbox-iso: Preparing to unpack .../libapparmor1_2.10.95-0ubuntu2.6~14.04.4_amd64.deb ... virtualbox-iso: Unpacking libapparmor1:amd64 (2.10.95-0ubuntu2.6~14.04.4) over (2.8.95~2430-0ubuntu5.3) ... virtualbox-iso: Preparing to unpack .../libsystemd-login0_204-5ubuntu20.29_amd64.deb ... virtualbox-iso: Unpacking libsystemd-login0:amd64 (204-5ubuntu20.29) over (204-5ubuntu20.19) ... virtualbox-iso: Preparing to unpack .../dbus_1.6.18-0ubuntu4.5_amd64.deb ... virtualbox-iso: Unpacking dbus (1.6.18-0ubuntu4.5) over (1.6.18-0ubuntu4.3) ... virtualbox-iso: Preparing to unpack .../libpolkit-gobject-1-0_0.105-4ubuntu3.14.04.2_amd64.deb ... virtualbox-iso: Unpacking libpolkit-gobject-1-0:amd64 (0.105-4ubuntu3.14.04.2) over (0.105-4ubuntu3.14.04.1) ... virtualbox-iso: Preparing to unpack .../libx11-data_2%3a1.6.2-1ubuntu2.1_all.deb ... virtualbox-iso: Unpacking libx11-data (2:1.6.2-1ubuntu2.1) over (2:1.6.2-1ubuntu2) ... virtualbox-iso: Preparing to unpack .../libx11-6_2%3a1.6.2-1ubuntu2.1_amd64.deb ... virtualbox-iso: Unpacking libx11-6:amd64 (2:1.6.2-1ubuntu2.1) over (2:1.6.2-1ubuntu2) ... virtualbox-iso: Preparing to unpack .../libxml2_2.9.1&#43;dfsg1-3ubuntu4.13_amd64.deb ... virtualbox-iso: Unpacking libxml2:amd64 (2.9.1&#43;dfsg1-3ubuntu4.13) over (2.9.1&#43;dfsg1-3ubuntu4.8) ... virtualbox-iso: Preparing to unpack .../curl_7.35.0-1ubuntu2.19_amd64.deb ... virtualbox-iso: Unpacking curl (7.35.0-1ubuntu2.19) over (7.35.0-1ubuntu2.7) ... virtualbox-iso: Preparing to unpack .../libcurl3_7.35.0-1ubuntu2.19_amd64.deb ... virtualbox-iso: Unpacking libcurl3:amd64 (7.35.0-1ubuntu2.19) over (7.35.0-1ubuntu2.7) ... virtualbox-iso: Preparing to unpack .../libevent-2.0-5_2.0.21-stable-1ubuntu1.14.04.2_amd64.deb ... virtualbox-iso: Unpacking libevent-2.0-5:amd64 (2.0.21-stable-1ubuntu1.14.04.2) over (2.0.21-stable-1ubuntu1.14.04.1) ... virtualbox-iso: Preparing to unpack .../libgc1c2_1%3a7.2d-5ubuntu2.1_amd64.deb ... virtualbox-iso: Unpacking libgc1c2:amd64 (1:7.2d-5ubuntu2.1) over (1:7.2d-5ubuntu2) ... virtualbox-iso: Preparing to unpack .../libnl-genl-3-200_3.2.21-1ubuntu4.1_amd64.deb ... virtualbox-iso: Unpacking libnl-genl-3-200:amd64 (3.2.21-1ubuntu4.1) over (3.2.21-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../libnl-3-200_3.2.21-1ubuntu4.1_amd64.deb ... virtualbox-iso: Unpacking libnl-3-200:amd64 (3.2.21-1ubuntu4.1) over (3.2.21-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../libpcsclite1_1.8.10-1ubuntu1.1_amd64.deb ... virtualbox-iso: Unpacking libpcsclite1:amd64 (1.8.10-1ubuntu1.1) over (1.8.10-1ubuntu1) ... virtualbox-iso: Preparing to unpack .../libpolkit-agent-1-0_0.105-4ubuntu3.14.04.2_amd64.deb ... virtualbox-iso: Unpacking libpolkit-agent-1-0:amd64 (0.105-4ubuntu3.14.04.2) over (0.105-4ubuntu3.14.04.1) ... virtualbox-iso: Preparing to unpack .../libpolkit-backend-1-0_0.105-4ubuntu3.14.04.2_amd64.deb ... virtualbox-iso: Unpacking libpolkit-backend-1-0:amd64 (0.105-4ubuntu3.14.04.2) over (0.105-4ubuntu3.14.04.1) ... virtualbox-iso: Preparing to unpack .../python2.7_2.7.6-8ubuntu0.5_amd64.deb ... virtualbox-iso: Unpacking python2.7 (2.7.6-8ubuntu0.5) over (2.7.6-8ubuntu0.2) ... virtualbox-iso: Preparing to unpack .../libpython2.7_2.7.6-8ubuntu0.5_amd64.deb ... virtualbox-iso: Unpacking libpython2.7:amd64 (2.7.6-8ubuntu0.5) over (2.7.6-8ubuntu0.2) ... virtualbox-iso: Preparing to unpack .../libpython2.7-stdlib_2.7.6-8ubuntu0.5_amd64.deb ... virtualbox-iso: Unpacking libpython2.7-stdlib:amd64 (2.7.6-8ubuntu0.5) over (2.7.6-8ubuntu0.2) ... virtualbox-iso: Preparing to unpack .../python2.7-minimal_2.7.6-8ubuntu0.5_amd64.deb ... virtualbox-iso: Unpacking python2.7-minimal (2.7.6-8ubuntu0.5) over (2.7.6-8ubuntu0.2) ... virtualbox-iso: Preparing to unpack .../libpython2.7-minimal_2.7.6-8ubuntu0.5_amd64.deb ... virtualbox-iso: Unpacking libpython2.7-minimal:amd64 (2.7.6-8ubuntu0.5) over (2.7.6-8ubuntu0.2) ... virtualbox-iso: Selecting previously unselected package linux-image-3.13.0-164-generic. virtualbox-iso: Preparing to unpack .../linux-image-3.13.0-164-generic_3.13.0-164.214_amd64.deb ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Done. virtualbox-iso: Unpacking linux-image-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Preparing to unpack .../python-apt-common_0.9.3.5ubuntu3_all.deb ... virtualbox-iso: Unpacking python-apt-common (0.9.3.5ubuntu3) over (0.9.3.5ubuntu2) ... virtualbox-iso: Preparing to unpack .../python3-apt_0.9.3.5ubuntu3_amd64.deb ... virtualbox-iso: Unpacking python3-apt (0.9.3.5ubuntu3) over (0.9.3.5ubuntu2) ... virtualbox-iso: Preparing to unpack .../python-apt_0.9.3.5ubuntu3_amd64.deb ... virtualbox-iso: Unpacking python-apt (0.9.3.5ubuntu3) over (0.9.3.5ubuntu2) ... virtualbox-iso: Preparing to unpack .../patch_2.7.1-4ubuntu2.4_amd64.deb ... virtualbox-iso: Unpacking patch (2.7.1-4ubuntu2.4) over (2.7.1-4ubuntu2.3) ... virtualbox-iso: Preparing to unpack .../lsb-release_4.1&#43;Debian11ubuntu6.2_all.deb ... virtualbox-iso: Unpacking lsb-release (4.1&#43;Debian11ubuntu6.2) over (4.1&#43;Debian11ubuntu6.1) ... virtualbox-iso: Preparing to unpack .../ubuntu-release-upgrader-core_1%3a0.220.10_all.deb ... virtualbox-iso: Unpacking ubuntu-release-upgrader-core (1:0.220.10) over (1:0.220.8) ... virtualbox-iso: Preparing to unpack .../python3-distupgrade_1%3a0.220.10_all.deb ... virtualbox-iso: Unpacking python3-distupgrade (1:0.220.10) over (1:0.220.8) ... virtualbox-iso: Preparing to unpack .../update-manager-core_1%3a0.196.25_all.deb ... virtualbox-iso: Unpacking update-manager-core (1:0.196.25) over (1:0.196.14) ... virtualbox-iso: Preparing to unpack .../python3-update-manager_1%3a0.196.25_all.deb ... virtualbox-iso: Unpacking python3-update-manager (1:0.196.25) over (1:0.196.14) ... virtualbox-iso: Preparing to unpack .../update-notifier-common_0.154.1ubuntu3_all.deb ... virtualbox-iso: Unpacking update-notifier-common (0.154.1ubuntu3) over (0.154.1ubuntu1) ... virtualbox-iso: Preparing to unpack .../multiarch-support_2.19-0ubuntu6.14_amd64.deb ... virtualbox-iso: Unpacking multiarch-support (2.19-0ubuntu6.14) over (2.19-0ubuntu6.9) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Processing triggers for mime-support (3.54ubuntu1.1) ... virtualbox-iso: Processing triggers for ureadahead (0.100.0-16) ... virtualbox-iso: Setting up multiarch-support (2.19-0ubuntu6.14) ... virtualbox-iso: (Reading database ... 68811 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../sensible-utils_0.0.9ubuntu0.14.04.1_all.deb ... virtualbox-iso: Unpacking sensible-utils (0.0.9ubuntu0.14.04.1) over (0.0.9) ... virtualbox-iso: Processing triggers for mime-support (3.54ubuntu1.1) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up sensible-utils (0.0.9ubuntu0.14.04.1) ... virtualbox-iso: (Reading database ... 68811 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../apt-utils_1.0.1ubuntu2.18_amd64.deb ... virtualbox-iso: Unpacking apt-utils (1.0.1ubuntu2.18) over (1.0.1ubuntu2.14) ... virtualbox-iso: Preparing to unpack .../eject_2.1.5&#43;deb1&#43;cvs20081104-13.1ubuntu0.14.04.1_amd64.deb ... virtualbox-iso: Unpacking eject (2.1.5&#43;deb1&#43;cvs20081104-13.1ubuntu0.14.04.1) over (2.1.5&#43;deb1&#43;cvs20081104-13.1) ... virtualbox-iso: Preparing to unpack .../init-system-helpers_1.14ubuntu1_all.deb ... virtualbox-iso: Unpacking init-system-helpers (1.14ubuntu1) over (1.14) ... virtualbox-iso: Preparing to unpack .../iproute2_3.12.0-2ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking iproute2 (3.12.0-2ubuntu1.2) over (3.12.0-2ubuntu1) ... virtualbox-iso: Preparing to unpack .../ifupdown_0.7.47.2ubuntu4.5_amd64.deb ... virtualbox-iso: Unpacking ifupdown (0.7.47.2ubuntu4.5) over (0.7.47.2ubuntu4.4) ... virtualbox-iso: Preparing to unpack .../isc-dhcp-client_4.2.4-7ubuntu12.13_amd64.deb ... virtualbox-iso: Unpacking isc-dhcp-client (4.2.4-7ubuntu12.13) over (4.2.4-7ubuntu12.5) ... virtualbox-iso: Preparing to unpack .../isc-dhcp-common_4.2.4-7ubuntu12.13_amd64.deb ... virtualbox-iso: Unpacking isc-dhcp-common (4.2.4-7ubuntu12.13) over (4.2.4-7ubuntu12.5) ... virtualbox-iso: Preparing to unpack .../logrotate_3.8.7-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking logrotate (3.8.7-1ubuntu1.2) over (3.8.7-1ubuntu1) ... virtualbox-iso: Preparing to unpack .../rsyslog_7.4.4-1ubuntu2.7_amd64.deb ... virtualbox-iso: Unpacking rsyslog (7.4.4-1ubuntu2.7) over (7.4.4-1ubuntu2.6) ... virtualbox-iso: Preparing to unpack .../sudo_1.8.9p5-1ubuntu1.4_amd64.deb ... virtualbox-iso: Unpacking sudo (1.8.9p5-1ubuntu1.4) over (1.8.9p5-1ubuntu1.2) ... virtualbox-iso: Preparing to unpack .../vim_2%3a7.4.052-1ubuntu3.1_amd64.deb ... virtualbox-iso: Unpacking vim (2:7.4.052-1ubuntu3.1) over (2:7.4.052-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../vim-tiny_2%3a7.4.052-1ubuntu3.1_amd64.deb ... virtualbox-iso: Unpacking vim-tiny (2:7.4.052-1ubuntu3.1) over (2:7.4.052-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../vim-runtime_2%3a7.4.052-1ubuntu3.1_all.deb ... virtualbox-iso: Unpacking vim-runtime (2:7.4.052-1ubuntu3.1) over (2:7.4.052-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../vim-common_2%3a7.4.052-1ubuntu3.1_amd64.deb ... virtualbox-iso: Unpacking vim-common (2:7.4.052-1ubuntu3.1) over (2:7.4.052-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../makedev_2.3.1-93ubuntu2~ubuntu14.04.1_all.deb ... virtualbox-iso: Unpacking makedev (2.3.1-93ubuntu2~ubuntu14.04.1) over (2.3.1-93ubuntu1) ... virtualbox-iso: Preparing to unpack .../accountsservice_0.6.35-0ubuntu7.3_amd64.deb ... virtualbox-iso: Unpacking accountsservice (0.6.35-0ubuntu7.3) over (0.6.35-0ubuntu7.2) ... virtualbox-iso: Preparing to unpack .../libaccountsservice0_0.6.35-0ubuntu7.3_amd64.deb ... virtualbox-iso: Unpacking libaccountsservice0:amd64 (0.6.35-0ubuntu7.3) over (0.6.35-0ubuntu7.2) ... virtualbox-iso: Preparing to unpack .../libapparmor-perl_2.10.95-0ubuntu2.6~14.04.4_amd64.deb ... virtualbox-iso: Unpacking libapparmor-perl (2.10.95-0ubuntu2.6~14.04.4) over (2.8.95~2430-0ubuntu5.3) ... virtualbox-iso: Preparing to unpack .../apparmor_2.10.95-0ubuntu2.6~14.04.4_amd64.deb ... virtualbox-iso: Unpacking apparmor (2.10.95-0ubuntu2.6~14.04.4) over (2.8.95~2430-0ubuntu5.3) ... virtualbox-iso: Preparing to unpack .../apt-transport-https_1.0.1ubuntu2.18_amd64.deb ... virtualbox-iso: Unpacking apt-transport-https (1.0.1ubuntu2.18) over (1.0.1ubuntu2.14) ... virtualbox-iso: Preparing to unpack .../bind9-host_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking bind9-host (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../dnsutils_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking dnsutils (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../libisc95_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking libisc95 (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../libdns100_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking libdns100 (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../libisccc90_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking libisccc90 (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../libisccfg90_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking libisccfg90 (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../liblwres90_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking liblwres90 (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../libbind9-90_1%3a9.9.5.dfsg-3ubuntu0.18_amd64.deb ... virtualbox-iso: Unpacking libbind9-90 (1:9.9.5.dfsg-3ubuntu0.18) over (1:9.9.5.dfsg-3ubuntu0.8) ... virtualbox-iso: Preparing to unpack .../openssl_1.0.1f-1ubuntu2.27_amd64.deb ... virtualbox-iso: Unpacking openssl (1.0.1f-1ubuntu2.27) over (1.0.1f-1ubuntu2.19) ... virtualbox-iso: Preparing to unpack .../ca-certificates_20170717~14.04.2_all.deb ... virtualbox-iso: Unpacking ca-certificates (20170717~14.04.2) over (20160104ubuntu0.14.04.1) ... virtualbox-iso: Preparing to unpack .../gettext-base_0.18.3.1-1ubuntu3.1_amd64.deb ... virtualbox-iso: Unpacking gettext-base (0.18.3.1-1ubuntu3.1) over (0.18.3.1-1ubuntu3) ... virtualbox-iso: Preparing to unpack .../krb5-locales_1.12&#43;dfsg-2ubuntu5.3_all.deb ... virtualbox-iso: Unpacking krb5-locales (1.12&#43;dfsg-2ubuntu5.3) over (1.12&#43;dfsg-2ubuntu5.2) ... virtualbox-iso: Preparing to unpack .../libglib2.0-data_2.40.2-0ubuntu1.1_all.deb ... virtualbox-iso: Unpacking libglib2.0-data (2.40.2-0ubuntu1.1) over (2.40.2-0ubuntu1) ... virtualbox-iso: Preparing to unpack .../lshw_02.16-2ubuntu1.4_amd64.deb ... virtualbox-iso: Unpacking lshw (02.16-2ubuntu1.4) over (02.16-2ubuntu1.3) ... virtualbox-iso: Preparing to unpack .../plymouth-theme-ubuntu-text_0.8.8-0ubuntu17.2_amd64.deb ... virtualbox-iso: Unpacking plymouth-theme-ubuntu-text (0.8.8-0ubuntu17.2) over (0.8.8-0ubuntu17.1) ... virtualbox-iso: Preparing to unpack .../ppp_2.4.5-5.1ubuntu2.3_amd64.deb ... virtualbox-iso: Unpacking ppp (2.4.5-5.1ubuntu2.3) over (2.4.5-5.1ubuntu2.2) ... virtualbox-iso: Preparing to unpack .../rsync_3.1.0-2ubuntu0.4_amd64.deb ... virtualbox-iso: Unpacking rsync (3.1.0-2ubuntu0.4) over (3.1.0-2ubuntu0.2) ... virtualbox-iso: Preparing to unpack .../tcpdump_4.9.2-0ubuntu0.14.04.1_amd64.deb ... virtualbox-iso: Unpacking tcpdump (4.9.2-0ubuntu0.14.04.1) over (4.5.1-2ubuntu1.2) ... virtualbox-iso: Preparing to unpack .../wget_1.15-1ubuntu1.14.04.4_amd64.deb ... virtualbox-iso: Unpacking wget (1.15-1ubuntu1.14.04.4) over (1.15-1ubuntu1.14.04.2) ... virtualbox-iso: Preparing to unpack .../ubuntu-standard_1.325.1_amd64.deb ... virtualbox-iso: Unpacking ubuntu-standard (1.325.1) over (1.325) ... virtualbox-iso: Preparing to unpack .../uuid-runtime_2.20.1-5.1ubuntu20.9_amd64.deb ... virtualbox-iso: Unpacking uuid-runtime (2.20.1-5.1ubuntu20.9) over (2.20.1-5.1ubuntu20.7) ... virtualbox-iso: Preparing to unpack .../python3-problem-report_2.14.1-0ubuntu3.29_all.deb ... virtualbox-iso: Unpacking python3-problem-report (2.14.1-0ubuntu3.29) over (2.14.1-0ubuntu3.21) ... virtualbox-iso: Preparing to unpack .../python3-apport_2.14.1-0ubuntu3.29_all.deb ... virtualbox-iso: Unpacking python3-apport (2.14.1-0ubuntu3.29) over (2.14.1-0ubuntu3.21) ... virtualbox-iso: Preparing to unpack .../apport_2.14.1-0ubuntu3.29_all.deb ... virtualbox-iso: apport stop/waiting virtualbox-iso: Unpacking apport (2.14.1-0ubuntu3.29) over (2.14.1-0ubuntu3.21) ... virtualbox-iso: Selecting previously unselected package iucode-tool. virtualbox-iso: Preparing to unpack .../iucode-tool_1.0.1-1_amd64.deb ... virtualbox-iso: Unpacking iucode-tool (1.0.1-1) ... virtualbox-iso: Preparing to unpack .../python-twisted-bin_13.2.0-1ubuntu1.2_amd64.deb ... virtualbox-iso: Unpacking python-twisted-bin (13.2.0-1ubuntu1.2) over (13.2.0-1ubuntu1) ... virtualbox-iso: Preparing to unpack .../python-twisted-core_13.2.0-1ubuntu1.2_all.deb ... virtualbox-iso: Unpacking python-twisted-core (13.2.0-1ubuntu1.2) over (13.2.0-1ubuntu1) ... virtualbox-iso: Preparing to unpack .../landscape-common_14.12-0ubuntu6.14.04.3_amd64.deb ... virtualbox-iso: Unpacking landscape-common (14.12-0ubuntu6.14.04.3) over (14.12-0ubuntu0.14.04) ... virtualbox-iso: Preparing to unpack .../linux-firmware_1.127.24_all.deb ... virtualbox-iso: Unpacking linux-firmware (1.127.24) over (1.127.22) ... virtualbox-iso: Selecting previously unselected package linux-image-extra-3.13.0-164-generic. virtualbox-iso: Preparing to unpack .../linux-image-extra-3.13.0-164-generic_3.13.0-164.214_amd64.deb ... virtualbox-iso: Unpacking linux-image-extra-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Selecting previously unselected package intel-microcode. virtualbox-iso: Preparing to unpack .../intel-microcode_3.20180807a.0ubuntu0.14.04.1_amd64.deb ... virtualbox-iso: Unpacking intel-microcode (3.20180807a.0ubuntu0.14.04.1) ... virtualbox-iso: Selecting previously unselected package amd64-microcode. virtualbox-iso: Preparing to unpack .../amd64-microcode_3.20180524.1~ubuntu0.14.04.2&#43;really20130710.1ubuntu1_amd64.deb ... virtualbox-iso: Unpacking amd64-microcode (3.20180524.1~ubuntu0.14.04.2&#43;really20130710.1ubuntu1) ... virtualbox-iso: Selecting previously unselected package linux-image-generic. virtualbox-iso: Preparing to unpack .../linux-image-generic_3.13.0.164.174_amd64.deb ... virtualbox-iso: Unpacking linux-image-generic (3.13.0.164.174) ... virtualbox-iso: Selecting previously unselected package linux-headers-3.13.0-164. virtualbox-iso: Preparing to unpack .../linux-headers-3.13.0-164_3.13.0-164.214_all.deb ... virtualbox-iso: Unpacking linux-headers-3.13.0-164 (3.13.0-164.214) ... virtualbox-iso: Selecting previously unselected package linux-headers-3.13.0-164-generic. virtualbox-iso: Preparing to unpack .../linux-headers-3.13.0-164-generic_3.13.0-164.214_amd64.deb ... virtualbox-iso: Unpacking linux-headers-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Selecting previously unselected package linux-headers-generic. virtualbox-iso: Preparing to unpack .../linux-headers-generic_3.13.0.164.174_amd64.deb ... virtualbox-iso: Unpacking linux-headers-generic (3.13.0.164.174) ... virtualbox-iso: Selecting previously unselected package linux-generic. virtualbox-iso: Preparing to unpack .../linux-generic_3.13.0.164.174_amd64.deb ... virtualbox-iso: Unpacking linux-generic (3.13.0.164.174) ... virtualbox-iso: Selecting previously unselected package linux-headers-server. virtualbox-iso: Preparing to unpack .../linux-headers-server_3.13.0.164.174_amd64.deb ... virtualbox-iso: Unpacking linux-headers-server (3.13.0.164.174) ... virtualbox-iso: Selecting previously unselected package linux-server. virtualbox-iso: Preparing to unpack .../linux-server_3.13.0.164.174_amd64.deb ... virtualbox-iso: Unpacking linux-server (3.13.0.164.174) ... virtualbox-iso: Preparing to unpack .../policykit-1_0.105-4ubuntu3.14.04.2_amd64.deb ... virtualbox-iso: Unpacking policykit-1 (0.105-4ubuntu3.14.04.2) over (0.105-4ubuntu3.14.04.1) ... virtualbox-iso: Preparing to unpack .../python-six_1.5.2-1ubuntu1.1_all.deb ... virtualbox-iso: Unpacking python-six (1.5.2-1ubuntu1.1) over (1.5.2-1ubuntu1) ... virtualbox-iso: Preparing to unpack .../software-properties-common_0.92.37.8_all.deb ... virtualbox-iso: Unpacking software-properties-common (0.92.37.8) over (0.92.37.7) ... virtualbox-iso: Preparing to unpack .../unattended-upgrades_0.82.1ubuntu2.5_all.deb ... virtualbox-iso: Unpacking unattended-upgrades (0.82.1ubuntu2.5) over (0.82.1ubuntu2.4) ... virtualbox-iso: Preparing to unpack .../python3-software-properties_0.92.37.8_all.deb ... virtualbox-iso: Unpacking python3-software-properties (0.92.37.8) over (0.92.37.7) ... virtualbox-iso: Preparing to unpack .../w3m_0.5.3-15ubuntu0.2_amd64.deb ... virtualbox-iso: Unpacking w3m (0.5.3-15ubuntu0.2) over (0.5.3-15) ... virtualbox-iso: Preparing to unpack .../wpasupplicant_2.1-0ubuntu1.6_amd64.deb ... virtualbox-iso: Unpacking wpasupplicant (2.1-0ubuntu1.6) over (2.1-0ubuntu1.4) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Processing triggers for ureadahead (0.100.0-16) ... virtualbox-iso: Processing triggers for mime-support (3.54ubuntu1.1) ... virtualbox-iso: Processing triggers for install-info (5.2.0.dfsg.1-2) ... virtualbox-iso: Processing triggers for shared-mime-info (1.2-0ubuntu3) ... virtualbox-iso: Setting up libapt-inst1.5:amd64 (1.0.1ubuntu2.18) ... virtualbox-iso: Setting up libexpat1:amd64 (2.1.0-4ubuntu1.4) ... virtualbox-iso: Setting up libffi6:amd64 (3.1~rc1&#43;r3.0.13-12ubuntu0.2) ... virtualbox-iso: Setting up libgcrypt11:amd64 (1.5.3-2ubuntu4.6) ... virtualbox-iso: Setting up libtasn1-6:amd64 (3.4-3ubuntu0.6) ... virtualbox-iso: Setting up libgnutls26:amd64 (2.12.23-12ubuntu2.8) ... virtualbox-iso: Setting up libgnutls-openssl27:amd64 (2.12.23-12ubuntu2.8) ... virtualbox-iso: Setting up libmagic1:amd64 (1:5.14-2ubuntu3.4) ... virtualbox-iso: Setting up file (1:5.14-2ubuntu3.4) ... virtualbox-iso: Setting up libpython3.4-minimal:amd64 (3.4.3-1ubuntu1~14.04.7) ... virtualbox-iso: Setting up python3.4-minimal (3.4.3-1ubuntu1~14.04.7) ... virtualbox-iso: Setting up libpython3.4-stdlib:amd64 (3.4.3-1ubuntu1~14.04.7) ... virtualbox-iso: Setting up python3.4 (3.4.3-1ubuntu1~14.04.7) ... virtualbox-iso: Setting up ntpdate (1:4.2.6.p5&#43;dfsg-3ubuntu2.14.04.13) ... virtualbox-iso: Installing new version of config file /etc/network/if-up.d/ntpdate ... virtualbox-iso: Setting up resolvconf (1.69ubuntu1.4) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Setting up libdbus-1-3:amd64 (1.6.18-0ubuntu4.5) ... virtualbox-iso: Setting up libdrm2:amd64 (2.4.67-1ubuntu0.14.04.2) ... virtualbox-iso: Setting up libkmod2:amd64 (15-0ubuntu7) ... virtualbox-iso: Setting up libklibc (2.0.3-0ubuntu1.14.04.3) ... virtualbox-iso: Setting up klibc-utils (2.0.3-0ubuntu1.14.04.3) ... virtualbox-iso: Setting up libprocps3:amd64 (1:3.3.9-1ubuntu2.3) ... virtualbox-iso: Setting up procps (1:3.3.9-1ubuntu2.3) ... virtualbox-iso: procps stop/waiting virtualbox-iso: Setting up libudev1:amd64 (204-5ubuntu20.29) ... virtualbox-iso: Setting up udev (204-5ubuntu20.29) ... virtualbox-iso: udev stop/waiting virtualbox-iso: udev start/running, process 18613 virtualbox-iso: Removing &#39;diversion of /bin/udevadm to /bin/udevadm.upgrade by fake-udev&#39; virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Setting up initramfs-tools-bin (0.103ubuntu4.11) ... virtualbox-iso: Setting up kmod (15-0ubuntu7) ... virtualbox-iso: Setting up module-init-tools (15-0ubuntu7) ... virtualbox-iso: Setting up initramfs-tools (0.103ubuntu4.11) ... virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Setting up libpng12-0:amd64 (1.2.50-1ubuntu2.14.04.3) ... virtualbox-iso: Setting up libplymouth2:amd64 (0.8.8-0ubuntu17.2) ... virtualbox-iso: Setting up plymouth (0.8.8-0ubuntu17.2) ... virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Setting up libroken18-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libasn1-8-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libasprintf0c2:amd64 (0.18.3.1-1ubuntu3.1) ... virtualbox-iso: Setting up libkrb5support0:amd64 (1.12&#43;dfsg-2ubuntu5.3) ... virtualbox-iso: Setting up libk5crypto3:amd64 (1.12&#43;dfsg-2ubuntu5.3) ... virtualbox-iso: Setting up libkrb5-3:amd64 (1.12&#43;dfsg-2ubuntu5.3) ... virtualbox-iso: Setting up libgssapi-krb5-2:amd64 (1.12&#43;dfsg-2ubuntu5.3) ... virtualbox-iso: Setting up libidn11:amd64 (1.28-1ubuntu2.2) ... virtualbox-iso: Setting up libhcrypto4-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libheimbase1-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libwind0-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libhx509-5-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libkrb5-26-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libheimntlm0-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libgssapi3-heimdal:amd64 (1.6~git20131207&#43;dfsg-1ubuntu1.2) ... virtualbox-iso: Setting up libldap-2.4-2:amd64 (2.4.31-1&#43;nmu2ubuntu8.5) ... virtualbox-iso: Setting up librtmp0:amd64 (2.4&#43;20121230.gitdf6c518-1ubuntu0.1) ... virtualbox-iso: Setting up libcurl3-gnutls:amd64 (7.35.0-1ubuntu2.19) ... virtualbox-iso: Setting up libelf1:amd64 (0.158-0ubuntu5.3) ... virtualbox-iso: Setting up libglib2.0-0:amd64 (2.40.2-0ubuntu1.1) ... virtualbox-iso: No schema files found: doing nothing. virtualbox-iso: Setting up libsystemd-daemon0:amd64 (204-5ubuntu20.29) ... virtualbox-iso: Setting up libapparmor1:amd64 (2.10.95-0ubuntu2.6~14.04.4) ... virtualbox-iso: Setting up libsystemd-login0:amd64 (204-5ubuntu20.29) ... virtualbox-iso: Setting up dbus (1.6.18-0ubuntu4.5) ... virtualbox-iso: Installing new version of config file /etc/dbus-1/system.conf ... virtualbox-iso: Setting up systemd-services (204-5ubuntu20.29) ... virtualbox-iso: Setting up libpam-systemd:amd64 (204-5ubuntu20.29) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Setting up libpolkit-gobject-1-0:amd64 (0.105-4ubuntu3.14.04.2) ... virtualbox-iso: Setting up libx11-data (2:1.6.2-1ubuntu2.1) ... virtualbox-iso: Setting up libx11-6:amd64 (2:1.6.2-1ubuntu2.1) ... virtualbox-iso: Setting up libxml2:amd64 (2.9.1&#43;dfsg1-3ubuntu4.13) ... virtualbox-iso: Setting up libcurl3:amd64 (7.35.0-1ubuntu2.19) ... virtualbox-iso: Setting up curl (7.35.0-1ubuntu2.19) ... virtualbox-iso: Setting up libevent-2.0-5:amd64 (2.0.21-stable-1ubuntu1.14.04.2) ... virtualbox-iso: Setting up libgc1c2:amd64 (1:7.2d-5ubuntu2.1) ... virtualbox-iso: Setting up libnl-3-200:amd64 (3.2.21-1ubuntu4.1) ... virtualbox-iso: Setting up libnl-genl-3-200:amd64 (3.2.21-1ubuntu4.1) ... virtualbox-iso: Setting up libpcsclite1:amd64 (1.8.10-1ubuntu1.1) ... virtualbox-iso: Setting up libpolkit-agent-1-0:amd64 (0.105-4ubuntu3.14.04.2) ... virtualbox-iso: Setting up libpolkit-backend-1-0:amd64 (0.105-4ubuntu3.14.04.2) ... virtualbox-iso: Setting up libpython2.7-minimal:amd64 (2.7.6-8ubuntu0.5) ... virtualbox-iso: Setting up python2.7-minimal (2.7.6-8ubuntu0.5) ... virtualbox-iso: Setting up libpython2.7-stdlib:amd64 (2.7.6-8ubuntu0.5) ... virtualbox-iso: Setting up python2.7 (2.7.6-8ubuntu0.5) ... virtualbox-iso: Setting up libpython2.7:amd64 (2.7.6-8ubuntu0.5) ... virtualbox-iso: Setting up linux-image-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Running depmod. virtualbox-iso: update-initramfs: deferring update (hook will be called later) virtualbox-iso: Examining /etc/kernel/postinst.d. virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/apt-auto-removal 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/dkms 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/initramfs-tools 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: update-initramfs: Generating /boot/initrd.img-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/update-notifier 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/vboxadd 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel modules. virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/zz-update-grub 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Found linux image: /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Found initrd image: /boot/initrd.img-3.13.0-164-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Setting up python-apt-common (0.9.3.5ubuntu3) ... virtualbox-iso: Setting up python3-apt (0.9.3.5ubuntu3) ... virtualbox-iso: Setting up python-apt (0.9.3.5ubuntu3) ... virtualbox-iso: Setting up patch (2.7.1-4ubuntu2.4) ... virtualbox-iso: Setting up lsb-release (4.1&#43;Debian11ubuntu6.2) ... virtualbox-iso: Setting up apt-utils (1.0.1ubuntu2.18) ... virtualbox-iso: Setting up eject (2.1.5&#43;deb1&#43;cvs20081104-13.1ubuntu0.14.04.1) ... virtualbox-iso: Setting up init-system-helpers (1.14ubuntu1) ... virtualbox-iso: Setting up iproute2 (3.12.0-2ubuntu1.2) ... virtualbox-iso: Setting up ifupdown (0.7.47.2ubuntu4.5) ... virtualbox-iso: Setting up isc-dhcp-common (4.2.4-7ubuntu12.13) ... virtualbox-iso: Setting up isc-dhcp-client (4.2.4-7ubuntu12.13) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Setting up logrotate (3.8.7-1ubuntu1.2) ... virtualbox-iso: Setting up rsyslog (7.4.4-1ubuntu2.7) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: The user `syslog&#39; is already a member of `adm&#39;. virtualbox-iso: Skipping profile in /etc/apparmor.d/disable: usr.sbin.rsyslogd virtualbox-iso: rsyslog stop/waiting virtualbox-iso: rsyslog start/running, process 31704 virtualbox-iso: Setting up sudo (1.8.9p5-1ubuntu1.4) ... virtualbox-iso: Installing new version of config file /etc/sudoers ... virtualbox-iso: Setting up vim-common (2:7.4.052-1ubuntu3.1) ... virtualbox-iso: Setting up vim-runtime (2:7.4.052-1ubuntu3.1) ... virtualbox-iso: Processing /usr/share/vim/addons/doc virtualbox-iso: Setting up vim (2:7.4.052-1ubuntu3.1) ... virtualbox-iso: Setting up vim-tiny (2:7.4.052-1ubuntu3.1) ... virtualbox-iso: Setting up makedev (2.3.1-93ubuntu2~ubuntu14.04.1) ... virtualbox-iso: Setting up libaccountsservice0:amd64 (0.6.35-0ubuntu7.3) ... virtualbox-iso: Setting up accountsservice (0.6.35-0ubuntu7.3) ... virtualbox-iso: Setting up libapparmor-perl (2.10.95-0ubuntu2.6~14.04.4) ... virtualbox-iso: Setting up apparmor (2.10.95-0ubuntu2.6~14.04.4) ... virtualbox-iso: Installing new version of config file /etc/init.d/apparmor ... virtualbox-iso: Installing new version of config file /etc/apparmor.d/abstractions/ubuntu-browsers.d/user-files ... virtualbox-iso: Installing new version of config file /etc/apparmor.d/abstractions/private-files ... virtualbox-iso: Installing new version of config file /etc/apparmor.d/abstractions/private-files-strict ... virtualbox-iso: Installing new version of config file /etc/apparmor.d/tunables/kernelvars ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: * Starting AppArmor profiles virtualbox-iso: Skipping profile in /etc/apparmor.d/disable: usr.sbin.rsyslogd virtualbox-iso: ...done. virtualbox-iso: * Reloading AppArmor profiles virtualbox-iso: Skipping profile in /etc/apparmor.d/disable: usr.sbin.rsyslogd virtualbox-iso: ...done. virtualbox-iso: Setting up apt-transport-https (1.0.1ubuntu2.18) ... virtualbox-iso: Setting up libisc95 (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up libdns100 (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up libisccc90 (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up libisccfg90 (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up libbind9-90 (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up liblwres90 (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up bind9-host (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up dnsutils (1:9.9.5.dfsg-3ubuntu0.18) ... virtualbox-iso: Setting up openssl (1.0.1f-1ubuntu2.27) ... virtualbox-iso: Setting up ca-certificates (20170717~14.04.2) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Setting up gettext-base (0.18.3.1-1ubuntu3.1) ... virtualbox-iso: Setting up krb5-locales (1.12&#43;dfsg-2ubuntu5.3) ... virtualbox-iso: Setting up libglib2.0-data (2.40.2-0ubuntu1.1) ... virtualbox-iso: Setting up lshw (02.16-2ubuntu1.4) ... virtualbox-iso: Setting up plymouth-theme-ubuntu-text (0.8.8-0ubuntu17.2) ... virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Setting up ppp (2.4.5-5.1ubuntu2.3) ... virtualbox-iso: Setting up rsync (3.1.0-2ubuntu0.4) ... virtualbox-iso: update-rc.d: warning: default stop runlevel arguments (0 1 6) do not match rsync Default-Stop values (none) virtualbox-iso: System start/stop links for /etc/init.d/rsync already exist. virtualbox-iso: Setting up tcpdump (4.9.2-0ubuntu0.14.04.1) ... virtualbox-iso: Installing new version of config file /etc/apparmor.d/usr.sbin.tcpdump ... virtualbox-iso: Setting up wget (1.15-1ubuntu1.14.04.4) ... virtualbox-iso: Setting up ubuntu-standard (1.325.1) ... virtualbox-iso: Setting up uuid-runtime (2.20.1-5.1ubuntu20.9) ... virtualbox-iso: Setting up python3-problem-report (2.14.1-0ubuntu3.29) ... virtualbox-iso: Setting up python3-apport (2.14.1-0ubuntu3.29) ... virtualbox-iso: Setting up apport (2.14.1-0ubuntu3.29) ... virtualbox-iso: Installing new version of config file /etc/init/apport.conf ... virtualbox-iso: Installing new version of config file /etc/init.d/apport ... virtualbox-iso: apport start/running virtualbox-iso: Setting up iucode-tool (1.0.1-1) ... virtualbox-iso: Setting up python-twisted-bin (13.2.0-1ubuntu1.2) ... virtualbox-iso: Setting up python-twisted-core (13.2.0-1ubuntu1.2) ... virtualbox-iso: Setting up landscape-common (14.12-0ubuntu6.14.04.3) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Setting up linux-firmware (1.127.24) ... virtualbox-iso: update-initramfs: Generating /boot/initrd.img-4.4.0-31-generic virtualbox-iso: update-initramfs: Generating /boot/initrd.img-3.13.0-164-generic virtualbox-iso: Setting up linux-image-extra-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/apt-auto-removal 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/dkms 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/initramfs-tools 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: update-initramfs: Generating /boot/initrd.img-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/update-notifier 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/vboxadd 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel modules. virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/zz-update-grub 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Found linux image: /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Found initrd image: /boot/initrd.img-3.13.0-164-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Setting up intel-microcode (3.20180807a.0ubuntu0.14.04.1) ... virtualbox-iso: intel-microcode: microcode will be updated at next boot virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Setting up amd64-microcode (3.20180524.1~ubuntu0.14.04.2&#43;really20130710.1ubuntu1) ... virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Setting up linux-image-generic (3.13.0.164.174) ... virtualbox-iso: Setting up linux-headers-3.13.0-164 (3.13.0-164.214) ... virtualbox-iso: Setting up linux-headers-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Examining /etc/kernel/header_postinst.d. virtualbox-iso: run-parts: executing /etc/kernel/header_postinst.d/dkms 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Setting up linux-headers-generic (3.13.0.164.174) ... virtualbox-iso: Setting up linux-generic (3.13.0.164.174) ... virtualbox-iso: Setting up linux-headers-server (3.13.0.164.174) ... virtualbox-iso: Setting up linux-server (3.13.0.164.174) ... virtualbox-iso: Setting up policykit-1 (0.105-4ubuntu3.14.04.2) ... virtualbox-iso: Setting up python-six (1.5.2-1ubuntu1.1) ... virtualbox-iso: Setting up unattended-upgrades (0.82.1ubuntu2.5) ... virtualbox-iso: Installing new version of config file /etc/apt/apt.conf.d/50unattended-upgrades ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Setting up python3-software-properties (0.92.37.8) ... virtualbox-iso: Setting up software-properties-common (0.92.37.8) ... virtualbox-iso: Setting up w3m (0.5.3-15ubuntu0.2) ... virtualbox-iso: Setting up wpasupplicant (2.1-0ubuntu1.6) ... virtualbox-iso: Setting up python3-distupgrade (1:0.220.10) ... virtualbox-iso: Setting up python3-update-manager (1:0.196.25) ... virtualbox-iso: Setting up ubuntu-release-upgrader-core (1:0.220.10) ... virtualbox-iso: Setting up update-manager-core (1:0.196.25) ... virtualbox-iso: Setting up update-notifier-common (0.154.1ubuntu3) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.14) ... virtualbox-iso: Processing triggers for initramfs-tools (0.103ubuntu4.11) ... virtualbox-iso: update-initramfs: Generating /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Processing triggers for ureadahead (0.100.0-16) ... virtualbox-iso: Processing triggers for ca-certificates (20170717~14.04.2) ... virtualbox-iso: Updating certificates in /etc/ssl/certs... 17 added, 42 removed; done. virtualbox-iso: Running hooks in /etc/ca-certificates/update.d....done. virtualbox-iso: &#43; uname -r virtualbox-iso: &#43; apt-get -y install linux-headers-4.4.0-31-generic virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: linux-headers-4.4.0-31-generic is already the newest version. virtualbox-iso: &#43; cat virtualbox-iso: 0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded. virtualbox-iso: &#43; cat virtualbox-iso: &#43; apt-get -y dist-upgrade --force-yes virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following NEW packages will be installed: virtualbox-iso: linux-headers-4.4.0-141 linux-headers-4.4.0-141-generic virtualbox-iso: linux-image-4.4.0-141-generic linux-image-extra-4.4.0-141-generic virtualbox-iso: ubuntu-advantage-tools virtualbox-iso: The following packages will be upgraded: virtualbox-iso: linux-generic-lts-xenial linux-headers-generic-lts-xenial virtualbox-iso: linux-image-generic-lts-xenial ubuntu-minimal virtualbox-iso: 4 upgraded, 5 newly installed, 0 to remove and 0 not upgraded. virtualbox-iso: Need to get 68.0 MB of archives. virtualbox-iso: After this operation, 307 MB of additional disk space will be used. virtualbox-iso: Get:1 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ubuntu-advantage-tools all 10ubuntu0.14.04.2 [11.5 kB] virtualbox-iso: Get:2 http://archive.ubuntu.com/ubuntu/ trusty-updates/main ubuntu-minimal amd64 1.325.1 [2,658 B] virtualbox-iso: Get:3 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:4 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:5 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:6 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:7 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:8 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:9 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:10 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:11 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:12 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:13 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:14 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:15 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:16 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [21.1 MB] virtualbox-iso: Get:17 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-extra-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [36.1 MB] virtualbox-iso: Err http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-extra-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 virtualbox-iso: Connection failed [IP: 91.189.88.161 80] virtualbox-iso: Get:18 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-generic-lts-xenial amd64 4.4.0.141.121 [1,802 B] virtualbox-iso: Get:19 http://security.ubuntu.com/ubuntu/ trusty-security/main linux-image-extra-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [36.1 MB] virtualbox-iso: Get:20 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-image-generic-lts-xenial amd64 4.4.0.141.121 [2,334 B] virtualbox-iso: Get:21 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-4.4.0-141 all 4.4.0-141.167~14.04.1 [9,946 kB] virtualbox-iso: Get:22 http://security.ubuntu.com/ubuntu/ trusty-security/main linux-image-extra-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [36.1 MB] virtualbox-iso: Get:23 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-4.4.0-141 all 4.4.0-141.167~14.04.1 [9,946 kB] virtualbox-iso: Get:24 http://security.ubuntu.com/ubuntu/ trusty-security/main linux-image-extra-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [36.1 MB] virtualbox-iso: Get:25 http://security.ubuntu.com/ubuntu/ trusty-security/main linux-image-extra-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [36.1 MB] virtualbox-iso: Get:26 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-4.4.0-141 all 4.4.0-141.167~14.04.1 [9,946 kB] virtualbox-iso: Get:27 http://security.ubuntu.com/ubuntu/ trusty-security/main linux-image-extra-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [36.1 MB] virtualbox-iso: Get:28 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-4.4.0-141 all 4.4.0-141.167~14.04.1 [9,946 kB] virtualbox-iso: Get:29 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-4.4.0-141-generic amd64 4.4.0-141.167~14.04.1 [805 kB] virtualbox-iso: Get:30 http://archive.ubuntu.com/ubuntu/ trusty-updates/main linux-headers-generic-lts-xenial amd64 4.4.0.141.121 [2,296 B] virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: dpkg-preconfigure: unable to re-open stdin: virtualbox-iso: Fetched 30.6 MB in 1h 55min 35s (4,409 B/s) virtualbox-iso: Selecting previously unselected package ubuntu-advantage-tools. virtualbox-iso: (Reading database ... 97786 files and directories currently installed.) virtualbox-iso: Preparing to unpack .../ubuntu-advantage-tools_10ubuntu0.14.04.2_all.deb ... virtualbox-iso: Unpacking ubuntu-advantage-tools (10ubuntu0.14.04.2) ... virtualbox-iso: Preparing to unpack .../ubuntu-minimal_1.325.1_amd64.deb ... virtualbox-iso: Unpacking ubuntu-minimal (1.325.1) over (1.325) ... virtualbox-iso: Selecting previously unselected package linux-image-4.4.0-141-generic. virtualbox-iso: Preparing to unpack .../linux-image-4.4.0-141-generic_4.4.0-141.167~14.04.1_amd64.deb ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Examining /etc/kernel/preinst.d/ virtualbox-iso: run-parts: executing /etc/kernel/preinst.d/amd64-microcode 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/preinst.d/intel-microcode 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Done. virtualbox-iso: Unpacking linux-image-4.4.0-141-generic (4.4.0-141.167~14.04.1) ... virtualbox-iso: Selecting previously unselected package linux-image-extra-4.4.0-141-generic. virtualbox-iso: Preparing to unpack .../linux-image-extra-4.4.0-141-generic_4.4.0-141.167~14.04.1_amd64.deb ... virtualbox-iso: Unpacking linux-image-extra-4.4.0-141-generic (4.4.0-141.167~14.04.1) ... virtualbox-iso: Preparing to unpack .../linux-generic-lts-xenial_4.4.0.141.121_amd64.deb ... virtualbox-iso: Unpacking linux-generic-lts-xenial (4.4.0.141.121) over (4.4.0.31.21) ... virtualbox-iso: Preparing to unpack .../linux-image-generic-lts-xenial_4.4.0.141.121_amd64.deb ... virtualbox-iso: Unpacking linux-image-generic-lts-xenial (4.4.0.141.121) over (4.4.0.31.21) ... virtualbox-iso: Selecting previously unselected package linux-headers-4.4.0-141. virtualbox-iso: Preparing to unpack .../linux-headers-4.4.0-141_4.4.0-141.167~14.04.1_all.deb ... virtualbox-iso: Unpacking linux-headers-4.4.0-141 (4.4.0-141.167~14.04.1) ... virtualbox-iso: Selecting previously unselected package linux-headers-4.4.0-141-generic. virtualbox-iso: Preparing to unpack .../linux-headers-4.4.0-141-generic_4.4.0-141.167~14.04.1_amd64.deb ... virtualbox-iso: Unpacking linux-headers-4.4.0-141-generic (4.4.0-141.167~14.04.1) ... virtualbox-iso: Preparing to unpack .../linux-headers-generic-lts-xenial_4.4.0.141.121_amd64.deb ... virtualbox-iso: Unpacking linux-headers-generic-lts-xenial (4.4.0.141.121) over (4.4.0.31.21) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Setting up ubuntu-advantage-tools (10ubuntu0.14.04.2) ... virtualbox-iso: Setting up ubuntu-minimal (1.325.1) ... virtualbox-iso: Setting up linux-image-4.4.0-141-generic (4.4.0-141.167~14.04.1) ... virtualbox-iso: Running depmod. virtualbox-iso: update-initramfs: deferring update (hook will be called later) virtualbox-iso: Examining /etc/kernel/postinst.d. virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/apt-auto-removal 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/dkms 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/initramfs-tools 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: update-initramfs: Generating /boot/initrd.img-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/update-notifier 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/vboxadd 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel modules. virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/zz-update-grub 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Found linux image: /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Found initrd image: /boot/initrd.img-3.13.0-164-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Setting up linux-image-extra-4.4.0-141-generic (4.4.0-141.167~14.04.1) ... virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/apt-auto-removal 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/dkms 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/initramfs-tools 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: update-initramfs: Generating /boot/initrd.img-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/update-notifier 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/vboxadd 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: VirtualBox Guest Additions: Building the VirtualBox Guest Additions kernel modules. virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/zz-update-grub 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Found linux image: /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Found initrd image: /boot/initrd.img-3.13.0-164-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Setting up linux-image-generic-lts-xenial (4.4.0.141.121) ... virtualbox-iso: Setting up linux-headers-4.4.0-141 (4.4.0-141.167~14.04.1) ... virtualbox-iso: Setting up linux-headers-4.4.0-141-generic (4.4.0-141.167~14.04.1) ... virtualbox-iso: Examining /etc/kernel/header_postinst.d. virtualbox-iso: run-parts: executing /etc/kernel/header_postinst.d/dkms 4.4.0-141-generic /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Setting up linux-headers-generic-lts-xenial (4.4.0.141.121) ... virtualbox-iso: Setting up linux-generic-lts-xenial (4.4.0.141.121) ... virtualbox-iso: &#43; reboot virtualbox-iso: &#43; sleep 60 ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/common/sshd.sh virtualbox-iso: &#43; SSHD_CONFIG=/etc/ssh/sshd_config virtualbox-iso: &#43; sed -i -e $a\ /etc/ssh/sshd_config virtualbox-iso: &#43; USEDNS=UseDNS no virtualbox-iso: &#43; grep -q -E ^[[:space:]]*UseDNS /etc/ssh/sshd_config virtualbox-iso: &#43; echo UseDNS no virtualbox-iso: &#43; GSSAPI=GSSAPIAuthentication no virtualbox-iso: &#43; grep -q -E ^[[:space:]]*GSSAPIAuthentication /etc/ssh/sshd_config virtualbox-iso: &#43; echo GSSAPIAuthentication no ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/ubuntu/networking.sh virtualbox-iso: &#43; awk {print $2} virtualbox-iso: &#43; lsb_release -r virtualbox-iso: &#43; UBUNTU_VERSION=14.04 virtualbox-iso: &#43; awk -F. {print $1} virtualbox-iso: &#43; echo 14.04 virtualbox-iso: &#43; MAJOR_VERSION=14 virtualbox-iso: &#43; [ 14 -le 15 ] virtualbox-iso: &#43; [ 14.04 != 15.10 ] virtualbox-iso: &#43; echo Disabling automatic udev rules for network interfaces in Ubuntu virtualbox-iso: Disabling automatic udev rules for network interfaces in Ubuntu virtualbox-iso: &#43; rm -f /etc/udev/rules.d/70-persistent-net.rules virtualbox-iso: &#43; mkdir -p /etc/udev/rules.d/70-persistent-net.rules virtualbox-iso: &#43; rm -f /lib/udev/rules.d/75-persistent-net-generator.rules virtualbox-iso: &#43; rm -rf /dev/.udev/ /var/lib/dhcp3/* /var/lib/dhcp/dhclient.eth0.leases /var/lib/dhcp/dhclient.leases virtualbox-iso: &#43; echo pre-up sleep 2 ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/ubuntu/sudoers.sh virtualbox-iso: &#43; sed -i -e /Defaults\s\&#43;env_reset/a Defaults\texempt_group=sudo /etc/sudoers virtualbox-iso: &#43; echo vagrant ALL=(ALL) NOPASSWD:ALL virtualbox-iso: &#43; chmod 440 /etc/sudoers.d/123_vagrant ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/ubuntu/cleanup.sh virtualbox-iso: &#43; &#43; &#43; awk { print $2 }grep linux-headers&#43; dpkg --list virtualbox-iso: xargs virtualbox-iso: apt-get virtualbox-iso: -y purge virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode intel-microcode iucode-tool linux-image-3.13.0-164-generic virtualbox-iso: linux-image-extra-3.13.0-164-generic linux-image-generic virtualbox-iso: linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: linux-generic* linux-generic-lts-xenial* linux-headers-3.13.0-164* virtualbox-iso: linux-headers-3.13.0-164-generic* linux-headers-4.4.0-141* virtualbox-iso: linux-headers-4.4.0-141-generic* linux-headers-4.4.0-31* virtualbox-iso: linux-headers-4.4.0-31-generic* linux-headers-generic* virtualbox-iso: linux-headers-generic-lts-xenial* linux-headers-server* linux-server* virtualbox-iso: 0 upgraded, 0 newly installed, 12 to remove and 0 not upgraded. virtualbox-iso: After this operation, 252 MB disk space will be freed. virtualbox-iso: (Reading database ... 130200 files and directories currently installed.) virtualbox-iso: Removing linux-server (3.13.0.164.174) ... virtualbox-iso: Removing linux-generic (3.13.0.164.174) ... virtualbox-iso: Removing linux-generic-lts-xenial (4.4.0.141.121) ... virtualbox-iso: Removing linux-headers-server (3.13.0.164.174) ... virtualbox-iso: Removing linux-headers-generic (3.13.0.164.174) ... virtualbox-iso: Removing linux-headers-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Removing linux-headers-3.13.0-164 (3.13.0-164.214) ... virtualbox-iso: Removing linux-headers-generic-lts-xenial (4.4.0.141.121) ... virtualbox-iso: Removing linux-headers-4.4.0-141-generic (4.4.0-141.167~14.04.1) ... virtualbox-iso: Removing linux-headers-4.4.0-141 (4.4.0-141.167~14.04.1) ... virtualbox-iso: Removing linux-headers-4.4.0-31-generic (4.4.0-31.50~14.04.1) ... virtualbox-iso: Removing linux-headers-4.4.0-31 (4.4.0-31.50~14.04.1) ... virtualbox-iso: &#43; xargs apt-get -y purge virtualbox-iso: &#43; grep linux-image-.*-generic virtualbox-iso: &#43; awk&#43; { print $2 } virtualbox-iso: uname -r virtualbox-iso: &#43; dpkg --list virtualbox-iso: &#43; grep -v 4.4.0-141-generic virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode intel-microcode iucode-tool linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: linux-image-3.13.0-164-generic* linux-image-4.4.0-31-generic* virtualbox-iso: linux-image-extra-3.13.0-164-generic* linux-image-extra-4.4.0-31-generic* virtualbox-iso: linux-image-generic* virtualbox-iso: 0 upgraded, 0 newly installed, 5 to remove and 0 not upgraded. virtualbox-iso: After this operation, 412 MB disk space will be freed. virtualbox-iso: (Reading database ... 51884 files and directories currently installed.) virtualbox-iso: Removing linux-image-generic (3.13.0.164.174) ... virtualbox-iso: Removing linux-image-extra-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/apt-auto-removal 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/dkms 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/initramfs-tools 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: update-initramfs: Generating /boot/initrd.img-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/update-notifier 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/vboxadd 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/zz-update-grub 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Found linux image: /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Found initrd image: /boot/initrd.img-3.13.0-164-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Purging configuration files for linux-image-extra-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Removing linux-image-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Examining /etc/kernel/prerm.d. virtualbox-iso: run-parts: executing /etc/kernel/prerm.d/dkms 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/prerm.d/vboxadd 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Examining /etc/kernel/postrm.d . virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/initramfs-tools 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: update-initramfs: Deleting /boot/initrd.img-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/zz-update-grub 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: The link /vmlinuz.old is a damaged link virtualbox-iso: Removing symbolic link vmlinuz.old virtualbox-iso: you may need to re-run your boot loader[grub] virtualbox-iso: The link /initrd.img.old is a damaged link virtualbox-iso: Removing symbolic link initrd.img.old virtualbox-iso: you may need to re-run your boot loader[grub] virtualbox-iso: Purging configuration files for linux-image-3.13.0-164-generic (3.13.0-164.214) ... virtualbox-iso: Examining /etc/kernel/postrm.d . virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/initramfs-tools 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/zz-update-grub 3.13.0-164-generic /boot/vmlinuz-3.13.0-164-generic virtualbox-iso: Removing linux-image-extra-4.4.0-31-generic (4.4.0-31.50~14.04.1) ... virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/apt-auto-removal 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/dkms 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/initramfs-tools 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: update-initramfs: Generating /boot/initrd.img-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/update-notifier 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/vboxadd 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/postinst.d/zz-update-grub 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-31-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Purging configuration files for linux-image-extra-4.4.0-31-generic (4.4.0-31.50~14.04.1) ... virtualbox-iso: Removing linux-image-4.4.0-31-generic (4.4.0-31.50~14.04.1) ... virtualbox-iso: Examining /etc/kernel/prerm.d. virtualbox-iso: run-parts: executing /etc/kernel/prerm.d/dkms 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/prerm.d/vboxadd 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Examining /etc/kernel/postrm.d . virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/initramfs-tools 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: update-initramfs: Deleting /boot/initrd.img-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/zz-update-grub 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Purging configuration files for linux-image-4.4.0-31-generic (4.4.0-31.50~14.04.1) ... virtualbox-iso: Examining /etc/kernel/postrm.d . virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/initramfs-tools 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: run-parts: executing /etc/kernel/postrm.d/zz-update-grub 4.4.0-31-generic /boot/vmlinuz-4.4.0-31-generic virtualbox-iso: &#43; xargs apt-get -y purge virtualbox-iso: &#43; grep linux-source virtualbox-iso: &#43; awk { print $2 } virtualbox-iso: &#43; dpkg --list virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode intel-microcode iucode-tool linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: linux-source* linux-source-3.13.0* virtualbox-iso: 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded. virtualbox-iso: After this operation, 115 MB disk space will be freed. virtualbox-iso: (Reading database ... 41484 files and directories currently installed.) virtualbox-iso: Removing linux-source (3.13.0.164.174) ... virtualbox-iso: Removing linux-source-3.13.0 (3.13.0-164.214) ... virtualbox-iso: &#43; xargs apt-get -y purge virtualbox-iso: &#43; grep -- -dev$ virtualbox-iso: &#43; awk { print $2 } virtualbox-iso: &#43; dpkg --list virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode intel-microcode iucode-tool linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: build-essential* dpkg-dev* manpages-dev* virtualbox-iso: 0 upgraded, 0 newly installed, 3 to remove and 0 not upgraded. virtualbox-iso: After this operation, 3,590 kB disk space will be freed. virtualbox-iso: (Reading database ... 41216 files and directories currently installed.) virtualbox-iso: Removing build-essential (11.6ubuntu6) ... virtualbox-iso: Removing dpkg-dev (1.17.5ubuntu5.8) ... virtualbox-iso: Purging configuration files for dpkg-dev (1.17.5ubuntu5.8) ... virtualbox-iso: Removing manpages-dev (3.54-1ubuntu1) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: &#43; xargs apt-get -y purge virtualbox-iso: &#43; grep -- -doc$ virtualbox-iso: &#43; awk { print $2 } virtualbox-iso: &#43; dpkg --list virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode g&#43;&#43; g&#43;&#43;-4.8 intel-microcode iucode-tool virtualbox-iso: libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl virtualbox-iso: libdpkg-perl libfile-fcntllock-perl libstdc&#43;&#43;-4.8-dev virtualbox-iso: linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: libssl-doc* virtualbox-iso: 0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded. virtualbox-iso: After this operation, 1,500 kB disk space will be freed. virtualbox-iso: (Reading database ... 39035 files and directories currently installed.) virtualbox-iso: Removing libssl-doc (1.0.1f-1ubuntu2.27) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: &#43; apt-get -y purge libx11-data xauth libxmuu1 libxcb1 libx11-6 libxext6 virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode g&#43;&#43; g&#43;&#43;-4.8 intel-microcode iucode-tool virtualbox-iso: libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl virtualbox-iso: libdpkg-perl libfile-fcntllock-perl libstdc&#43;&#43;-4.8-dev virtualbox-iso: linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: libx11-6* libx11-data* libxcb1* libxext6* libxmuu1* xauth* virtualbox-iso: 0 upgraded, 0 newly installed, 6 to remove and 0 not upgraded. virtualbox-iso: After this operation, 3,629 kB disk space will be freed. virtualbox-iso: (Reading database ... 37701 files and directories currently installed.) virtualbox-iso: Removing xauth (1:1.0.7-1ubuntu1) ... virtualbox-iso: Removing libxext6:amd64 (2:1.3.2-1ubuntu0.0.14.04.1) ... virtualbox-iso: Purging configuration files for libxext6:amd64 (2:1.3.2-1ubuntu0.0.14.04.1) ... virtualbox-iso: Removing libxmuu1:amd64 (2:1.1.1-1) ... virtualbox-iso: Purging configuration files for libxmuu1:amd64 (2:1.1.1-1) ... virtualbox-iso: Removing libx11-6:amd64 (2:1.6.2-1ubuntu2.1) ... virtualbox-iso: Purging configuration files for libx11-6:amd64 (2:1.6.2-1ubuntu2.1) ... virtualbox-iso: Removing libx11-data (2:1.6.2-1ubuntu2.1) ... virtualbox-iso: Removing libxcb1:amd64 (1.10-2ubuntu1) ... virtualbox-iso: Purging configuration files for libxcb1:amd64 (1.10-2ubuntu1) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Processing triggers for libc-bin (2.19-0ubuntu6.14) ... virtualbox-iso: &#43; apt-get -y purge ppp pppconfig pppoeconf virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode g&#43;&#43; g&#43;&#43;-4.8 intel-microcode iucode-tool virtualbox-iso: libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl virtualbox-iso: libdpkg-perl libfile-fcntllock-perl libstdc&#43;&#43;-4.8-dev virtualbox-iso: linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: ppp* pppconfig* pppoeconf* virtualbox-iso: 0 upgraded, 0 newly installed, 3 to remove and 0 not upgraded. virtualbox-iso: After this operation, 1,311 kB disk space will be freed. virtualbox-iso: (Reading database ... 37423 files and directories currently installed.) virtualbox-iso: Removing pppoeconf (1.20ubuntu1) ... virtualbox-iso: Purging configuration files for pppoeconf (1.20ubuntu1) ... virtualbox-iso: Removing pppconfig (2.3.19ubuntu1) ... virtualbox-iso: Purging configuration files for pppconfig (2.3.19ubuntu1) ... virtualbox-iso: rmdir: failed to remove ‘/var/cache/pppconfig’: No such file or directory virtualbox-iso: Removing ppp (2.4.5-5.1ubuntu2.3) ... virtualbox-iso: Stopping all PPP connections...done. virtualbox-iso: Purging configuration files for ppp (2.4.5-5.1ubuntu2.3) ... virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: &#43; apt-get -y purge popularity-contest installation-report command-not-found command-not-found-data friendly-recovery bash-completion fonts-ubuntu-font-family-console laptop-detect virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode g&#43;&#43; g&#43;&#43;-4.8 intel-microcode iucode-tool virtualbox-iso: libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl virtualbox-iso: libdpkg-perl libfile-fcntllock-perl libstdc&#43;&#43;-4.8-dev virtualbox-iso: linux-image-generic-lts-xenial virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: bash-completion* command-not-found* command-not-found-data* virtualbox-iso: fonts-ubuntu-font-family-console* friendly-recovery* installation-report* virtualbox-iso: laptop-detect* popularity-contest* python3-commandnotfound* ubuntu-standard* virtualbox-iso: 0 upgraded, 0 newly installed, 10 to remove and 0 not upgraded. virtualbox-iso: After this operation, 5,621 kB disk space will be freed. virtualbox-iso: (Reading database ... 37292 files and directories currently installed.) virtualbox-iso: Removing bash-completion (1:2.1-4ubuntu0.2) ... virtualbox-iso: Purging configuration files for bash-completion (1:2.1-4ubuntu0.2) ... virtualbox-iso: Removing command-not-found (0.3ubuntu12) ... virtualbox-iso: Purging configuration files for command-not-found (0.3ubuntu12) ... virtualbox-iso: Removing python3-commandnotfound (0.3ubuntu12) ... virtualbox-iso: Removing command-not-found-data (0.3ubuntu12) ... virtualbox-iso: Removing fonts-ubuntu-font-family-console (0.80-0ubuntu6) ... virtualbox-iso: Removing friendly-recovery (0.2.25) ... virtualbox-iso: Generating grub configuration file ... virtualbox-iso: Warning: Setting GRUB_TIMEOUT to a non-zero value when GRUB_HIDDEN_TIMEOUT is set is no longer supported. virtualbox-iso: Found linux image: /boot/vmlinuz-4.4.0-141-generic virtualbox-iso: Found initrd image: /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.elf virtualbox-iso: Found memtest86&#43; image: /memtest86&#43;.bin virtualbox-iso: done virtualbox-iso: Purging configuration files for friendly-recovery (0.2.25) ... virtualbox-iso: dpkg: warning: while removing friendly-recovery, directory &#39;/lib/recovery-mode&#39; not empty so not removed virtualbox-iso: Removing installation-report (2.54ubuntu1) ... virtualbox-iso: Purging configuration files for installation-report (2.54ubuntu1) ... virtualbox-iso: Removing laptop-detect (0.13.7ubuntu2) ... virtualbox-iso: Removing ubuntu-standard (1.325.1) ... virtualbox-iso: Removing popularity-contest (1.57ubuntu1) ... virtualbox-iso: Purging configuration files for popularity-contest (1.57ubuntu1) ... virtualbox-iso: debconf: unable to initialize frontend: Dialog virtualbox-iso: debconf: (TERM is not set, so the dialog frontend is not usable.) virtualbox-iso: debconf: falling back to frontend: Readline virtualbox-iso: debconf: unable to initialize frontend: Readline virtualbox-iso: debconf: (This frontend requires a controlling tty.) virtualbox-iso: debconf: falling back to frontend: Teletype virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: Processing triggers for ureadahead (0.100.0-16) ... virtualbox-iso: ureadahead will be reprofiled on next reboot virtualbox-iso: &#43; apt-get -y purge linux-firmware virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages were automatically installed and are no longer required: virtualbox-iso: amd64-microcode g&#43;&#43; g&#43;&#43;-4.8 intel-microcode iucode-tool virtualbox-iso: libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl virtualbox-iso: libdpkg-perl libfile-fcntllock-perl libstdc&#43;&#43;-4.8-dev virtualbox-iso: Use &#39;apt-get autoremove&#39; to remove them. virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: linux-firmware* linux-image-generic-lts-xenial* virtualbox-iso: 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded. virtualbox-iso: After this operation, 127 MB disk space will be freed. virtualbox-iso: (Reading database ... 36608 files and directories currently installed.) virtualbox-iso: Removing linux-image-generic-lts-xenial (4.4.0.141.121) ... virtualbox-iso: Removing linux-firmware (1.127.24) ... virtualbox-iso: &#43; apt-get -y autoremove virtualbox-iso: Reading package lists... virtualbox-iso: Building dependency tree... virtualbox-iso: Reading state information... virtualbox-iso: The following packages will be REMOVED: virtualbox-iso: amd64-microcode g&#43;&#43; g&#43;&#43;-4.8 intel-microcode iucode-tool virtualbox-iso: libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl virtualbox-iso: libdpkg-perl libfile-fcntllock-perl libstdc&#43;&#43;-4.8-dev virtualbox-iso: 0 upgraded, 0 newly installed, 11 to remove and 0 not upgraded. virtualbox-iso: After this operation, 43.9 MB disk space will be freed. virtualbox-iso: (Reading database ... 35398 files and directories currently installed.) virtualbox-iso: Removing amd64-microcode (3.20180524.1~ubuntu0.14.04.2&#43;really20130710.1ubuntu1) ... virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Removing g&#43;&#43; (4:4.8.2-1ubuntu6) ... virtualbox-iso: Removing g&#43;&#43;-4.8 (4.8.4-2ubuntu1~14.04.4) ... virtualbox-iso: Removing intel-microcode (3.20180807a.0ubuntu0.14.04.1) ... virtualbox-iso: update-initramfs: deferring update (trigger activated) virtualbox-iso: Removing iucode-tool (1.0.1-1) ... virtualbox-iso: Removing libalgorithm-merge-perl (0.08-2) ... virtualbox-iso: Removing libalgorithm-diff-xs-perl (0.04-2build4) ... virtualbox-iso: Removing libalgorithm-diff-perl (1.19.02-3) ... virtualbox-iso: Removing libdpkg-perl (1.17.5ubuntu5.8) ... virtualbox-iso: Removing libfile-fcntllock-perl (0.14-2build1) ... virtualbox-iso: Removing libstdc&#43;&#43;-4.8-dev:amd64 (4.8.4-2ubuntu1~14.04.4) ... virtualbox-iso: Processing triggers for initramfs-tools (0.103ubuntu4.11) ... virtualbox-iso: update-initramfs: Generating /boot/initrd.img-4.4.0-141-generic virtualbox-iso: Processing triggers for man-db (2.6.7.1-1ubuntu1) ... virtualbox-iso: &#43; apt-get -y clean virtualbox-iso: &#43; rm -rf /usr/share/doc/accountsservice /usr/share/doc/acpid /usr/share/doc/adduser /usr/share/doc/apparmor /usr/share/doc/apport /usr/share/doc/apport-symptoms /usr/share/doc/apt /usr/share/doc/apt-transport-https /usr/share/doc/apt-utils /usr/share/doc/apt-xapian-index /usr/share/doc/aptitude /usr/share/doc/aptitude-common /usr/share/doc/at /usr/share/doc/base-files /usr/share/doc/base-passwd /usr/share/doc/bash /usr/share/doc/bc /usr/share/doc/bind9-host /usr/share/doc/binutils /usr/share/doc/biosdevname /usr/share/doc/bsdmainutils /usr/share/doc/bsdutils /usr/share/doc/busybox-initramfs /usr/share/doc/busybox-static /usr/share/doc/byobu /usr/share/doc/bzip2 /usr/share/doc/ca-certificates /usr/share/doc/console-setup /usr/share/doc/coreutils /usr/share/doc/cpio /usr/share/doc/cpp /usr/share/doc/cpp-4.8 /usr/share/doc/crda /usr/share/doc/cron /usr/share/doc/cryptsetup /usr/share/doc/cryptsetup-bin /usr/share/doc/curl /usr/share/doc/dash /usr/share/doc/dbus /usr/share/doc/debconf /usr/share/doc/debconf-i18n /usr/share/doc/debianutils /usr/share/doc/dh-python /usr/share/doc/diffutils /usr/share/doc/dkms /usr/share/doc/dmidecode /usr/share/doc/dmsetup /usr/share/doc/dnsutils /usr/share/doc/dosfstools /usr/share/doc/dpkg /usr/share/doc/e2fslibs /usr/share/doc/e2fsprogs /usr/share/doc/ed /usr/share/doc/eject /usr/share/doc/ethtool /usr/share/doc/fakeroot /usr/share/doc/file /usr/share/doc/findutils /usr/share/doc/ftp /usr/share/doc/fuse /usr/share/doc/gawk /usr/share/doc/gcc /usr/share/doc/gcc-4.8 /usr/share/doc/gcc-4.8-base /usr/share/doc/gcc-4.9-base /usr/share/doc/geoip-database /usr/share/doc/gettext-base /usr/share/doc/gir1.2-glib-2.0 /usr/share/doc/gnupg /usr/share/doc/gpgv /usr/share/doc/grep /usr/share/doc/groff-base /usr/share/doc/grub-common /usr/share/doc/grub-gfxpayload-lists /usr/share/doc/grub-pc /usr/share/doc/grub-pc-bin /usr/share/doc/grub2-common /usr/share/doc/gzip /usr/share/doc/hdparm /usr/share/doc/hostname /usr/share/doc/ifupdown /usr/share/doc/info /usr/share/doc/init-system-helpers /usr/share/doc/initramfs-tools /usr/share/doc/initramfs-tools-bin /usr/share/doc/initscripts /usr/share/doc/insserv /usr/share/doc/install-info /usr/share/doc/iproute2 /usr/share/doc/iptables /usr/share/doc/iputils-ping /usr/share/doc/iputils-tracepath /usr/share/doc/irqbalance /usr/share/doc/isc-dhcp-client /usr/share/doc/isc-dhcp-common /usr/share/doc/iso-codes /usr/share/doc/kbd /usr/share/doc/keyboard-configuration /usr/share/doc/keyutils /usr/share/doc/klibc-utils /usr/share/doc/kmod /usr/share/doc/krb5-locales /usr/share/doc/landscape-common /usr/share/doc/language-pack-en /usr/share/doc/language-pack-en-base /usr/share/doc/language-pack-gnome-en /usr/share/doc/language-pack-gnome-en-base /usr/share/doc/language-selector-common /usr/share/doc/less /usr/share/doc/libaccountsservice0 /usr/share/doc/libacl1 /usr/share/doc/libapparmor-perl /usr/share/doc/libapparmor1 /usr/share/doc/libapt-inst1.5 /usr/share/doc/libapt-pkg4.12 /usr/share/doc/libarchive-extract-perl /usr/share/doc/libasan0 /usr/share/doc/libasn1-8-heimdal /usr/share/doc/libasprintf0c2 /usr/share/doc/libatomic1 /usr/share/doc/libattr1 /usr/share/doc/libaudit-common /usr/share/doc/libaudit1 /usr/share/doc/libbind9-90 /usr/share/doc/libblkid1 /usr/share/doc/libboost-iostreams1.54.0 /usr/share/doc/libbsd0 /usr/share/doc/libbz2-1.0 /usr/share/doc/libc-bin /usr/share/doc/libc-dev-bin /usr/share/doc/libc6 /usr/share/doc/libc6-dev /usr/share/doc/libcap-ng0 /usr/share/doc/libcap2 /usr/share/doc/libcap2-bin /usr/share/doc/libcgmanager0 /usr/share/doc/libck-connector0 /usr/share/doc/libclass-accessor-perl /usr/share/doc/libcloog-isl4 /usr/share/doc/libcomerr2 /usr/share/doc/libcryptsetup4 /usr/share/doc/libcurl3 /usr/share/doc/libcurl3-gnutls /usr/share/doc/libcwidget3 /usr/share/doc/libdb5.3 /usr/share/doc/libdbus-1-3 /usr/share/doc/libdbus-glib-1-2 /usr/share/doc/libdebconfclient0 /usr/share/doc/libdevmapper-event1.02.1 /usr/share/doc/libdevmapper1.02.1 /usr/share/doc/libdns100 /usr/share/doc/libdrm2 /usr/share/doc/libedit2 /usr/share/doc/libelf1 /usr/share/doc/libept1.4.12 /usr/share/doc/libestr0 /usr/share/doc/libevent-2.0-5 /usr/share/doc/libexpat1 /usr/share/doc/libfakeroot /usr/share/doc/libffi6 /usr/share/doc/libfreetype6 /usr/share/doc/libfribidi0 /usr/share/doc/libfuse2 /usr/share/doc/libgc1c2 /usr/share/doc/libgcc-4.8-dev /usr/share/doc/libgcc1 /usr/share/doc/libgck-1-0 /usr/share/doc/libgcr-3-common /usr/share/doc/libgcr-base-3-1 /usr/share/doc/libgcrypt11 /usr/share/doc/libgdbm3 /usr/share/doc/libgeoip1 /usr/share/doc/libgirepository-1.0-1 /usr/share/doc/libglib2.0-0 /usr/share/doc/libglib2.0-data /usr/share/doc/libgmp10 /usr/share/doc/libgnutls-openssl27 /usr/share/doc/libgnutls26 /usr/share/doc/libgomp1 /usr/share/doc/libgpg-error0 /usr/share/doc/libgpm2 /usr/share/doc/libgssapi-krb5-2 /usr/share/doc/libgssapi3-heimdal /usr/share/doc/libgssglue1 /usr/share/doc/libhcrypto4-heimdal /usr/share/doc/libheimbase1-heimdal /usr/share/doc/libheimntlm0-heimdal /usr/share/doc/libhx509-5-heimdal /usr/share/doc/libidn11 /usr/share/doc/libio-string-perl /usr/share/doc/libisc95 /usr/share/doc/libisccc90 /usr/share/doc/libisccfg90 /usr/share/doc/libisl10 /usr/share/doc/libitm1 /usr/share/doc/libiw30 /usr/share/doc/libjson-c2 /usr/share/doc/libjson0 /usr/share/doc/libk5crypto3 /usr/share/doc/libkeyutils1 /usr/share/doc/libklibc /usr/share/doc/libkmod2 /usr/share/doc/libkrb5-26-heimdal /usr/share/doc/libkrb5-3 /usr/share/doc/libkrb5support0 /usr/share/doc/libldap-2.4-2 /usr/share/doc/liblocale-gettext-perl /usr/share/doc/liblockfile-bin /usr/share/doc/liblockfile1 /usr/share/doc/liblog-message-simple-perl /usr/share/doc/liblwres90 /usr/share/doc/liblzma5 /usr/share/doc/libmagic1 /usr/share/doc/libmodule-pluggable-perl /usr/share/doc/libmount1 /usr/share/doc/libmpc3 /usr/share/doc/libmpdec2 /usr/share/doc/libmpfr4 /usr/share/doc/libncurses5 /usr/share/doc/libncursesw5 /usr/share/doc/libnewt0.52 /usr/share/doc/libnfnetlink0 /usr/share/doc/libnfsidmap2 /usr/share/doc/libnih-dbus1 /usr/share/doc/libnih1 /usr/share/doc/libnl-3-200 /usr/share/doc/libnl-genl-3-200 /usr/share/doc/libnuma1 /usr/share/doc/libp11-kit0 /usr/share/doc/libpam-cap /usr/share/doc/libpam-modules /usr/share/doc/libpam-modules-bin /usr/share/doc/libpam-runtime /usr/share/doc/libpam-systemd /usr/share/doc/libpam0g /usr/share/doc/libparse-debianchangelog-perl /usr/share/doc/libparted0debian1 /usr/share/doc/libpcap0.8 /usr/share/doc/libpci3 /usr/share/doc/libpcre3 /usr/share/doc/libpcsclite1 /usr/share/doc/libpipeline1 /usr/share/doc/libplymouth2 /usr/share/doc/libpng12-0 /usr/share/doc/libpod-latex-perl /usr/share/doc/libpolkit-agent-1-0 /usr/share/doc/libpolkit-backend-1-0 /usr/share/doc/libpolkit-gobject-1-0 /usr/share/doc/libpopt0 /usr/share/doc/libprocps3 /usr/share/doc/libpython-stdlib /usr/share/doc/libpython2.7 /usr/share/doc/libpython2.7-minimal /usr/share/doc/libpython2.7-stdlib /usr/share/doc/libpython3-stdlib /usr/share/doc/libpython3.4-minimal /usr/share/doc/libpython3.4-stdlib /usr/share/doc/libquadmath0 /usr/share/doc/libreadline-dev /usr/share/doc/libreadline5 /usr/share/doc/libreadline6 /usr/share/doc/libreadline6-dev /usr/share/doc/libroken18-heimdal /usr/share/doc/librtmp0 /usr/share/doc/libsasl2-2 /usr/share/doc/libsasl2-modules /usr/share/doc/libsasl2-modules-db /usr/share/doc/libselinux1 /usr/share/doc/libsemanage-common /usr/share/doc/libsemanage1 /usr/share/doc/libsepol1 /usr/share/doc/libsigc&#43;&#43;-2.0-0c2a /usr/share/doc/libsigsegv2 /usr/share/doc/libslang2 /usr/share/doc/libsqlite3-0 /usr/share/doc/libss2 /usr/share/doc/libssl-dev /usr/share/doc/libssl1.0.0 /usr/share/doc/libstdc&#43;&#43;6 /usr/share/doc/libsub-name-perl /usr/share/doc/libsystemd-daemon0 /usr/share/doc/libsystemd-login0 /usr/share/doc/libtasn1-6 /usr/share/doc/libterm-ui-perl /usr/share/doc/libtext-charwidth-perl /usr/share/doc/libtext-iconv-perl /usr/share/doc/libtext-soundex-perl /usr/share/doc/libtext-wrapi18n-perl /usr/share/doc/libtimedate-perl /usr/share/doc/libtinfo-dev /usr/share/doc/libtinfo5 /usr/share/doc/libtirpc1 /usr/share/doc/libtsan0 /usr/share/doc/libudev1 /usr/share/doc/libusb-0.1-4 /usr/share/doc/libusb-1.0-0 /usr/share/doc/libustr-1.0-1 /usr/share/doc/libuuid1 /usr/share/doc/libwind0-heimdal /usr/share/doc/libwrap0 /usr/share/doc/libxapian22 /usr/share/doc/libxau6 /usr/share/doc/libxdmcp6 /usr/share/doc/libxml2 /usr/share/doc/libxtables10 /usr/share/doc/linux-image-4.4.0-141-generic /usr/share/doc/linux-image-extra-4.4.0-141-generic /usr/share/doc/linux-libc-dev /usr/share/doc/locales /usr/share/doc/lockfile-progs /usr/share/doc/login /usr/share/doc/logrotate /usr/share/doc/lsb-base /usr/share/doc/lsb-release /usr/share/doc/lshw /usr/share/doc/lsof /usr/share/doc/ltrace /usr/share/doc/lvm2 /usr/share/doc/make /usr/share/doc/makedev /usr/share/doc/man-db /usr/share/doc/manpages /usr/share/doc/mawk /usr/share/doc/memtest86&#43; /usr/share/doc/mime-support /usr/share/doc/mlocate /usr/share/doc/module-init-tools /usr/share/doc/mount /usr/share/doc/mountall /usr/share/doc/mtr-tiny /usr/share/doc/multiarch-support /usr/share/doc/nano /usr/share/doc/ncurses-base /usr/share/doc/ncurses-bin /usr/share/doc/ncurses-term /usr/share/doc/net-tools /usr/share/doc/netbase /usr/share/doc/netcat-openbsd /usr/share/doc/nfs-common /usr/share/doc/ntfs-3g /usr/share/doc/ntpdate /usr/share/doc/openssh-client /usr/share/doc/openssh-server /usr/share/doc/openssh-sftp-server /usr/share/doc/openssl /usr/share/doc/os-prober /usr/share/doc/parted /usr/share/doc/passwd /usr/share/doc/patch /usr/share/doc/pciutils /usr/share/doc/perl /usr/share/doc/perl-base /usr/share/doc/perl-modules /usr/share/doc/plymouth /usr/share/doc/plymouth-theme-ubuntu-text /usr/share/doc/policykit-1 /usr/share/doc/powermgmt-base /usr/share/doc/procps /usr/share/doc/psmisc /usr/share/doc/python /usr/share/doc/python-apt /usr/share/doc/python-apt-common /usr/share/doc/python-chardet /usr/share/doc/python-configobj /usr/share/doc/python-debian /usr/share/doc/python-gdbm /usr/share/doc/python-minimal /usr/share/doc/python-openssl /usr/share/doc/python-pam /usr/share/doc/python-pkg-resources /usr/share/doc/python-requests /usr/share/doc/python-serial /usr/share/doc/python-six /usr/share/doc/python-twisted-bin /usr/share/doc/python-twisted-core /usr/share/doc/python-urllib3 /usr/share/doc/python-xapian /usr/share/doc/python-zope.interface /usr/share/doc/python2.7 /usr/share/doc/python2.7-minimal /usr/share/doc/python3 /usr/share/doc/python3-apport /usr/share/doc/python3-apt /usr/share/doc/python3-dbus /usr/share/doc/python3-distupgrade /usr/share/doc/python3-gdbm /usr/share/doc/python3-gi /usr/share/doc/python3-minimal /usr/share/doc/python3-newt /usr/share/doc/python3-problem-report /usr/share/doc/python3-pycurl /usr/share/doc/python3-software-properties /usr/share/doc/python3-update-manager /usr/share/doc/python3.4 /usr/share/doc/python3.4-minimal /usr/share/doc/readline-common /usr/share/doc/resolvconf /usr/share/doc/rpcbind /usr/share/doc/rsync /usr/share/doc/rsyslog /usr/share/doc/run-one /usr/share/doc/screen /usr/share/doc/sed /usr/share/doc/sensible-utils /usr/share/doc/sgml-base /usr/share/doc/shared-mime-info /usr/share/doc/software-properties-common /usr/share/doc/ssh-import-id /usr/share/doc/strace /usr/share/doc/sudo /usr/share/doc/systemd-services /usr/share/doc/systemd-shim /usr/share/doc/sysv-rc /usr/share/doc/sysvinit-utils /usr/share/doc/tar /usr/share/doc/tasksel /usr/share/doc/tasksel-data /usr/share/doc/tcpd /usr/share/doc/tcpdump /usr/share/doc/telnet /usr/share/doc/time /usr/share/doc/tmux /usr/share/doc/tzdata /usr/share/doc/ubuntu-advantage-tools /usr/share/doc/ubuntu-keyring /usr/share/doc/ubuntu-minimal /usr/share/doc/ubuntu-release-upgrader-core /usr/share/doc/ucf /usr/share/doc/udev /usr/share/doc/ufw /usr/share/doc/unattended-upgrades /usr/share/doc/update-manager-core /usr/share/doc/update-notifier-common /usr/share/doc/upstart /usr/share/doc/ureadahead /usr/share/doc/usbutils /usr/share/doc/util-linux /usr/share/doc/uuid-runtime /usr/share/doc/vim /usr/share/doc/vim-common /usr/share/doc/vim-runtime /usr/share/doc/vim-tiny /usr/share/doc/w3m /usr/share/doc/watershed /usr/share/doc/wget /usr/share/doc/whiptail /usr/share/doc/wireless-regdb /usr/share/doc/wireless-tools /usr/share/doc/wpa_supplicant /usr/share/doc/wpasupplicant /usr/share/doc/xkb-data /usr/share/doc/xml-core /usr/share/doc/xz-utils /usr/share/doc/zlib1g /usr/share/doc/zlib1g-dev virtualbox-iso: &#43; find /var/cache -type f -exec rm -rf {} ; virtualbox-iso: &#43; find /var/log/ -name *.log -exec rm -f {} ; ==&amp;gt; virtualbox-iso: Provisioning with shell script: scripts/common/minimize.sh virtualbox-iso: &#43; awk -F {print $4} virtualbox-iso: &#43; tail -n1 virtualbox-iso: &#43; df --sync -kP / virtualbox-iso: &#43; count=17237060 virtualbox-iso: &#43; count=17237059 virtualbox-iso: &#43; dd if=/dev/zero of=/tmp/whitespace bs=1M count=17237059 virtualbox-iso: dd: error writing ‘/tmp/whitespace’: No space left on device virtualbox-iso: 17794&#43;0 records in virtualbox-iso: dd exit code 1 is suppressed virtualbox-iso: 17793&#43;0 records out virtualbox-iso: 18657771520 bytes (19 GB) copied, 72.3498 s, 258 MB/s virtualbox-iso: &#43; echo dd exit code 1 is suppressed virtualbox-iso: &#43; rm /tmp/whitespace virtualbox-iso: &#43; awk -F {print $4} virtualbox-iso: &#43; tail -n1 virtualbox-iso: &#43; df --sync -kP /boot virtualbox-iso: &#43; count=187593 virtualbox-iso: &#43; count=187592 virtualbox-iso: &#43; dd if=/dev/zero of=/boot/whitespace bs=1M count=187592 virtualbox-iso: dd: error writing ‘/boot/whitespace’: No space left on device virtualbox-iso: 195&#43;0 records in virtualbox-iso: 194&#43;0 records out virtualbox-iso: dd exit code 1 is suppressed virtualbox-iso: 204029952 bytes (204 MB) copied, 1.74466 s, 117 MB/s virtualbox-iso: &#43; echo dd exit code 1 is suppressed virtualbox-iso: &#43; rm /boot/whitespace virtualbox-iso: &#43; set &#43;e virtualbox-iso: &#43; /sbin/blkid -o value -l -s UUID -t TYPE=swap virtualbox-iso: &#43; swapuuid=718137af-be54-4f61-847c-fc40e5c9f917 virtualbox-iso: &#43; set -e virtualbox-iso: &#43; [ x718137af-be54-4f61-847c-fc40e5c9f917 != x ] virtualbox-iso: &#43; readlink -f /dev/disk/by-uuid/718137af-be54-4f61-847c-fc40e5c9f917 virtualbox-iso: &#43; swappart=/dev/dm-1 virtualbox-iso: &#43; /sbin/swapoff /dev/dm-1 virtualbox-iso: &#43; dd if=/dev/zero of=/dev/dm-1 bs=1M virtualbox-iso: dd: error writing ‘/dev/dm-1’: No space left on device virtualbox-iso: 1025&#43;0 records in virtualbox-iso: dd exit code 1 is suppressed virtualbox-iso: 1024&#43;0 records out virtualbox-iso: 1073741824 bytes (1.1 GB) copied, 1.85048 s, 580 MB/s virtualbox-iso: &#43; echo dd exit code 1 is suppressed virtualbox-iso: &#43; /sbin/mkswap -U 718137af-be54-4f61-847c-fc40e5c9f917 /dev/dm-1 virtualbox-iso: mkswap: /dev/dm-1: warning: don&#39;t erase bootbits sectors virtualbox-iso: on whole disk. Use -f to force. virtualbox-iso: Setting up swapspace version 1, size = 1048572 KiB virtualbox-iso: no label, UUID=718137af-be54-4f61-847c-fc40e5c9f917 virtualbox-iso: &#43; sync ==&amp;gt; virtualbox-iso: Gracefully halting virtual machine... ==&amp;gt; virtualbox-iso: Preparing to export machine... virtualbox-iso: Deleting forwarded port mapping for the communicator (SSH, WinRM, etc) (host port 3620) ==&amp;gt; virtualbox-iso: Exporting virtual machine... virtualbox-iso: Executing: export ubuntu-14.04-amd64 --output packer-ubuntu-14.04-amd64-virtualbox\ubuntu-14.04-amd64.ova ==&amp;gt; virtualbox-iso: Deregistering and deleting VM... ==&amp;gt; virtualbox-iso: Running post-processor: vagrant ==&amp;gt; virtualbox-iso (vagrant): Creating Vagrant box for &#39;virtualbox&#39; provider virtualbox-iso (vagrant): Unpacking OVA: packer-ubuntu-14.04-amd64-virtualbox\ubuntu-14.04-amd64.ova virtualbox-iso (vagrant): Renaming the OVF to box.ovf... virtualbox-iso (vagrant): Compressing: Vagrantfile virtualbox-iso (vagrant): Compressing: box.ovf virtualbox-iso (vagrant): Compressing: metadata.json virtualbox-iso (vagrant): Compressing: ubuntu-14.04-amd64-disk001.vmdk ==&amp;gt; virtualbox-iso: Running post-processor: manifest Build &#39;virtualbox-iso&#39; finished. ==&amp;gt; Builds finished. The artifacts of successful builds are: --&amp;gt; virtualbox-iso: VM files in directory: packer-ubuntu-14.04-amd64-virtualbox --&amp;gt; virtualbox-iso: &#39;virtualbox&#39; provider box: builds/ubuntu-14.04.virtualbox.box --&amp;gt; virtualbox-iso: C:\quTemp\PackerTest&amp;gt;  生成的ova镜像文件：生成的box：生成的ubuntu-14.04-manifest.json：
{ &amp;quot;builds&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;virtualbox-iso&amp;quot;, &amp;quot;builder_type&amp;quot;: &amp;quot;virtualbox-iso&amp;quot;, &amp;quot;build_time&amp;quot;: XXXXXXXXXX, &amp;quot;files&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;ubuntu-14.04-amd64.ova&amp;quot;, &amp;quot;size&amp;quot;: 337966592 } ], &amp;quot;artifact_id&amp;quot;: &amp;quot;VM&amp;quot;, &amp;quot;packer_run_uuid&amp;quot;: &amp;quot;592c1dba-a3a6-9b0d-9901-4c0088b0d8a1&amp;quot; } ], &amp;quot;last_run_uuid&amp;quot;: &amp;quot;592c1dba-a3a6-9b0d-9901-4c0088b0d8a1&amp;quot; }  参考 Packer Documentation
Using Packer and Vagrant to Build Virtual Machines
GitHub: express42/packer-definitions
使用Packer创建自定义镜像
玩转云镜像制作之packer篇
Packer: Build Command Packer: VirtualBox Builder (from an ISO)
Packer: Templates
Using preseeding
</content>
    </entry>
    
     <entry>
        <title>创建基于Ubuntu的Vagrant Box</title>
        <url>https://mryqu.github.io/post/%E5%88%9B%E5%BB%BA%E5%9F%BA%E4%BA%8Eubuntu%E7%9A%84vagrant_box/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>box</tag><tag>create</tag><tag>ubuntu</tag>
        </tags>
        <content type="html">  学习一下创建Ubuntu基于VirtualBox provider的Vagrant Box。
准备环境  Vagrant VirtualBox 下载Ubuntu 14.04.5 LTS (Trusty Tahr)服务器版ISO文件  创建虚拟机 创建VirtualBox VM mryqu-ubuntu 内存1024MB 创建VMDK类型、自动分配的虚拟硬盘 安装Ubuntu 点击Setting，在Storage配置页的Controller:IDE中增加CD，选择所下载的Ubuntu ISO文件。然后启动虚拟机。 安装过程除了下列项之外使用默认选择：
- Configure the network - Hostname: vagrant
- Set up users and passwords: vagrant/vagrant
- Encrypt your home directory? Select No
- Select your time zone: UTC
- Partitioning method: Guided – use entire disk and set up LVM
- When prompted which software to install, select OpenSSH server
安装成功后，会提示进行登陆。 使用vagrant用户登陆后，使用sudo su -切到root账号，重新同步安装包索引并更新安装包。
apt-get update apt-get upgrade  为vagrant用户设置无密码sudo vagrant用户每次执行sudo命令会要求输入密码。通过visudo命令在配置文件末增加vagrant ALL=(ALL) NOPASSWD:ALL并保存即可实现无密码sudo。
为vagrant用户设置无密码ssh 为了让Vagrant可以通过SSH登入虚拟机，需要使用公开密钥认证。仍旧以root账户身份执行下列操作：
cd /home/vagrant mkdir .ssh wget https://raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub -O .ssh/authorized_keys chmod 700 .ssh chmod 600 .ssh/authorized_keys chown -R vagrant:vagrant .ssh  为了加速与虚拟机的SSH连接，可在/etc/ssh文件增加UseDNS no。 通过service ssh restart重启SSH服务器，通过exit命令推出root账户。
安装VirtualBox guest additions 为了让Vagrant可在客户机和宿主机操作系统之间共享目录，需要安装VirtualBox guest additions。 首先安装必须的包：
sudo apt-get install linux-headers-generic build-essential dkms -y  在虚拟机窗口，选择Devices菜单的Insert Guest Additions CD Image。!(vagrantbox-installGuestAdditions)(/images/2019/1/vagrantbox-installGuestAdditions.png) 安装VirtualBox guest additions：
sudo mount /dev/cdrom /media/cdrom cd /media/cdrom sudo ./VBoxLinuxAdditions.run  操作完成后，通过sudo reboot命令重启以使设置生效。
设置宿主机与客户机之间SSH端口转发。然后可在宿主机通过ssh -p2222 vagrant@127.0.0.1登陆客户机。
清理并优化Box体积 删除Ubuntu MOTD、Landscape服务和软件更新管理器 Landscape服务可在Ubuntu MOTD（message of the day）欢迎信息中显示服务器性能信息，会增加启动时间，没有太多意义。 软件更新管理器对于Box来说也没什么用。 可以通过下列命令删除：sudo apt-get remove landscape-client landscape-common update-notifier-common update-manager-core
删除不需要的包 使用sudo apt-get -y autoremove删除为了满足其他软件包的依赖而安装的、但现在不再需要的安装包 使用sudo apt-get clean删除包缓存中的所有安装包。
删除临时文件、登陆信息和命令历史 通过last或lastb可以看到以往的登陆历史，通过history可以看到命令历史。 以root账户登录，执行清除：
rm -rf /tmp/* echo &amp;gt;/var/log/wtmp echo &amp;gt;/var/log/btmp history -c  创建Vagrant Box包 创建Vagrant Box包为：vagrant package --base &amp;lt;virtual machine name&amp;gt;，其中虚拟机名为最开始设置的虚拟机名，也可以通过VBoxManage list vms命令查询。在宿主机执行下列命令创建Box包：vagrant package --base &amp;quot;mryqu-ubuntu&amp;quot; --output mryqu-ubuntu_0.1.0.box 测试 通过vagrant box add &#39;mryqu-ubuntu&#39; /c/quTemp/mryqu-ubuntu_0.1.0.box添加Box：通过vagrant init创建vagrantfile，然后将vagrantfile中的config.vm.box改为&amp;rsquo;mryqu-ubuntu&amp;rsquo;，最后通过vagrant up启动Box：参考 Vagrant Boxes: Creating a Base Box
Vagrant Provider VirtualBox: Creating a Base Box
How to Create and Share a Vagrant Base Box
How to set up a self-hosted &amp;ldquo;vagrant cloud&amp;rdquo; with versioned, self-packaged vagrant boxes
GitHub: ebmeierj/local_vagrant_box_hosting
</content>
    </entry>
    
     <entry>
        <title>博客从Hexo转向Hugo</title>
        <url>https://mryqu.github.io/post/%E5%8D%9A%E5%AE%A2%E4%BB%8Ehexo%E8%BD%AC%E5%90%91hugo/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>github</tag><tag>hexo</tag><tag>hugo</tag><tag>theme next</tag><tag>travis</tag>
        </tags>
        <content type="html">  起因 使用Hexo构建我的博客网站，感觉功能丰富、插件齐全，使用的不能再爽。
但是我开始把我以前的新浪博客帖子搬家，就不美了。我就搬了自己原创的部分，总共六百多个帖子，总是内存溢出。
&amp;lt;--- Last few GCs ---&amp;gt; 17611169 ms: Mark-sweep 1389.3 (1404.7) -&amp;gt; 1388.2 (1406.7) MB, 529.2 / 0.0 ms [allocation failure] [GC in old space requested]. 17611746 ms: Mark-sweep 1388.2 (1406.7) -&amp;gt; 1388.2 (1406.7) MB, 577.3 / 0.0 ms [allocation failure] [GC in old space requested]. 17612313 ms: Mark-sweep 1388.2 (1406.7) -&amp;gt; 1395.2 (1403.7) MB, 566.6 / 0.0 ms [last resort gc]. 17612859 ms: Mark-sweep 1395.2 (1403.7) -&amp;gt; 1402.0 (1403.7) MB, 545.6 / 0.0 ms [last resort gc]. &amp;lt;--- JS stacktrace ---&amp;gt; ==== JS stack trace ========================================= Security context: 0000017558DCFB61 &amp;lt;JS Object&amp;gt; 1: charAt [native string.js:~42] [pc=000000875B941596] (this=0000032AB82E7A79 &amp;lt;Very long string[115368]&amp;gt;,t=0) 2: _parse [C:\Users\scnydq\blog\node_modules\htmlparser2\lib\Tokenizer.js:~632] [pc=000000875B95A279] (this=000002668E5C0EF9 &amp;lt;a Tokenizer with map 000001917B7FC0E1&amp;gt;) 3: write [C:\Users\scnydq\blog\node_modules\htmlparser2\lib\Tokenizer.js:~625] [pc=000000876843955C] (this=... FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory  查阅了下面的帖子：
- Hexo Troubleshooting - Process Out of Memory
- OutOfMemory with 3000 docs 
- how to use hexo generate more than 1000&#43; posts
对C:\users\mryqu\AppData\Roaming\npm\node_modules\hexo-cli\bin\hexo进行如下设置，还是不顶：
#!/usr/bin/env node --max_old_space_size=20480 --optimize_for_size --max_executable_size=20480 --stack_size=20480 &#39;use strict&#39;; require(&#39;../lib/hexo&#39;)();  好吧，Node实现的博客工具我就不碰了。,我是写的GoLang程序从新浪下载我的博文并转化成Markdown文件的，所以这次就挑个GoLang实现的、号称最快的博客工具Hugo。
Hugo是由前Docker的重量级员工(2015年8月末从Docker离职)：Steve Francia实现的一个开源静态站点生成工具框架，类似于Jekyll、Octopress或Hexo，都是将特定格式(最常见的是Markdown格式)的文本文件转换为静态html文件而生成一个静态站点。在这些工具中，Hugo算是后起之秀了，它最大的优点就是Fast! 一个中等规模的站点在几分之一秒内就可以生成出来。其次是良好的跨平台特性、配置简单、使用方便等。这一切均源于其良好的基因：采用Go语言实现。Steve Francia除了Hugo平台自身外，还维护了一个Hugo Theme的仓库，这个Hugo主题库可以帮助Hugo使用者快速找到自己心仪的主题并快速搭建起静态站点。
安装Hugo Hugo的安装方式有两种，一种是直接下载编译好的Hugo二进制文件。如果只是使用Hugo推荐用这种方式。另一种方式是获取Hugo的源码，自己编译。
Hugo二进制下载地址：https://github.com/spf13/hugo/releases
这里我直接下载了最新的0.52 Hugo二进制文件。
C:\temp&amp;gt;hugo new site blog Congratulations! Your new Hugo site is created in C:\temp\blog. Just a few more steps and you&#39;re ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/, or create your own with the &amp;quot;hugo new theme &amp;lt;THEMENAME&amp;gt;&amp;quot; command. 2. Perhaps you want to add some content. You can add single files with &amp;quot;hugo new &amp;lt;SECTIONNAME&amp;gt;\&amp;lt;FILENAME&amp;gt;.&amp;lt;FORMAT&amp;gt;&amp;quot;. 3. Start the built-in live server via &amp;quot;hugo server&amp;quot;. Visit https://gohugo.io/ for quickstart guide and full documentation. C:\temp&amp;gt;dir blog Directory of C:\temp\blog 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; . 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; .. 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; archetypes 12/13/2018 08:04 PM 82 config.toml 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; content 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; data 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; layouts 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; static 12/13/2018 08:04 PM &amp;lt;DIR&amp;gt; themes 1 File(s) 82 bytes 8 Dir(s) 62,302,044,160 bytes free  选择主题 在https://themes.gohugo.io/上浏览各种Hugo的主题后，结果还是喜欢hexo-theme-next那样的主题。
继续搜索，找到了两款hugo-theme-next，分别是xtfly/hugo-theme-next和leopku/hugo-theme-next。
前一款跟hexo-theme-next更像，GitHub Star也更多。试用了一下，前款可以使用，但是也有不足。
就在GitHub上fork出自己的mryqu/hugo-theme-next ，进行再次开发。
.Next and .Prev are deprecated 我用的是Hugo 0.52，所以出现“.Next and .Prev are deprecated”提示，在layouts/partials/post/prenext.html文件中将“.Next”替换为“.NextPage”，“.Prev”替换为“.PrevPage”。
更新了I8N信息 move copyright from wexin.html to copyright.html xtfly/hugo-theme-next中将著作权信息放在wexin widget里，我不想用wexin widget但是想在帖子里显示版权，所以另建layouts/partials/post/copyright.html。
allow empty category or tag 在layouts/partials/post/category.html中将{{ if not (eq (len .Params.categories) 0) }}改为{{ if not .Params.categories}} {{ else }}; 在layouts/partials/post/tags.html中将{{ if not (eq (len .Params.tags) 0) }}改为{{ if not .Params.tags}} {{ else }}; 这样归类和标签为空时不会抛nil错误。
删除toc设置，对帖子自动显示文章目录 xtfly/hugo-theme-next需要每个Markdown文件都设置toc: true, 我对layouts/partials/sidebar.html和layouts/partials/sidebar/toc.html进行修改，自动显示文章目录
配置 添加next主题 git submodule add https://github.com/mryqu/hugo-theme-next.git themes/next  设置config.toml baseURL = &amp;quot;https://mryqu.github.io/&amp;quot; languageCode = &amp;quot;zh-CN&amp;quot; DefaultContentLanguage = &amp;quot;zh&amp;quot; title = &amp;quot;Mryqu&#39;s Notes&amp;quot; theme = &amp;quot;next&amp;quot; [Author] DisplayName = &amp;quot;mryqu&amp;quot;  修改Markdown文件 对于Markdown文件，Hexo与Hugo的差异：
- 使用Hexo时，我将图像文件位于content/images目录；使用Hugo后图像文件位于static目录。 可在static目录创建content子目录，然后在content子目录下创建images子目录，然后将Hexo的content/images目录下内容复制到Hugo的static/content/images目录，则原有Markdown文件无需修改。
- 使用Hexo时，每个Markdown文件会生成/yyyy/MM/dd/&amp;lt;Markdown文件名&amp;gt;这样的页面，而Hugo和mryqu/hugo-theme-next会生成/post/&amp;lt;Markdown文件名处理后的结果&amp;gt;
这个处理见Hugo的helpers/path.go的UnicodeSanitize函数。
Markdown文件对自己网站其他帖子的相对链接必须进行修改。
Travis CI自动生成网站 Hugo生成的网站及我自己的Markdown和图像文件将提交到https://github.com/mryqu/mryqu.github.io/ 的hugo分支。下列操作将实现通过Travis CI自动生成静态网页并提交到master分支。
让GitHub通过Travis CI认证 以GitHub身份登陆travis-ci.org，接受Travis CI认证。 生成GitHub的Personal access token 在travis-ci.org集成mryqu.github.io 项目 在travis-ci.org对mryqu/mryqu.github.io进行设置，其中GH_TOKEN为第二步获得的结果。 在https://github.com/mryqu/mryqu.github.io/ 的hugo分支提交.travis.yml --- git: submodules: false before_install: - git submodule update --init --recursive install: - wget -O /tmp/hugo.deb https://github.com/gohugoio/hugo/releases/download/v0.52/hugo_0.52_Linux-64bit.deb - sudo dpkg -i /tmp/hugo.deb script: - hugo after_script: - rm -rf deployment - git clone -b master &amp;quot;https://${GH_TOKEN}@${GH_REF}&amp;quot; deployment - rsync -av --delete --exclude &amp;quot;.git&amp;quot; public/ deployment - git config user.name &amp;quot;${U_NAME}&amp;quot; - git config user.email &amp;quot;${U_EMAIL}&amp;quot; - git config --global push.default simple - cd deployment - git add -A - git commit -m &amp;quot;rebuilding site on `date`, commit ${TRAVIS_COMMIT} and job ${TRAVIS_JOB_NUMBER}&amp;quot; || true - git push --force --quiet &amp;quot;https://${GH_TOKEN}@${GH_REF}&amp;quot; HEAD:${P_BRANCH} - cd .. - rm -rf deployment branches: only: - hugo  至此，大功告成！
参考 11个最流行的静态(博客)网站生成工具
使用travis-ci自动部署github上的项目
Hugo on GitHub Pages with Travis CI
</content>
    </entry>
    
     <entry>
        <title>Facebook的Page Access Token</title>
        <url>https://mryqu.github.io/post/facebook%E7%9A%84pageaccesstoken/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>page</tag><tag>access token</tag>
        </tags>
        <content type="html">  忽然发现原本可用的Facebook App Access Token无法获取Page内容的，甚至是自己的主页，错误提示为： &amp;gt; &amp;ldquo;(#10) To use &amp;lsquo;Page Public Content Access&amp;rsquo;, your use of this endpoint must be reviewed and approved by Facebook. To submit this &amp;lsquo;Page Public Content Access&amp;rsquo; feature for review please read our documentation on reviewable features: https://developers.facebook.com/docs/apps/review.&amp;quot;。
注：我的App yquTest当前App版本为2.8。祭出Access Token Tool武器，开始实验User Access Token。 发现结果如下： - 使用Facebook App Access Token无法读取自己或他人的主页内容 - 使用Facebook User Access Token可以读取自己主页内容，但无法读取他人的主页内容
Facebook Page Access Token调查 获取自己多个主页的Page Access Toke 获取自己单个主页的Page Access Toke 使用Page Access Token获取自己的主页内容 尝试获取他人主页的Page Access Token 结果自然是嘿嘿嘿。
结论 还是赶紧向Facebook提交并通过使用Page Public Content Access特性的评审吧。
参考 Page Access Tokens, Permissions, and Roles
</content>
    </entry>
    
     <entry>
        <title>折腾openui5-sample-app之使用Yarn替换Bower</title>
        <url>https://mryqu.github.io/post/node_%E6%8A%98%E8%85%BEopenui5-sample-app%E4%B9%8B%E4%BD%BF%E7%94%A8yarn%E6%9B%BF%E6%8D%A2bower/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>build</tag><tag>nodejs</tag><tag>npm</tag><tag>yarn</tag><tag>bower</tag><tag>包管理器</tag>
        </tags>
        <content type="html">  SAP/openui5-sample-app是使用npm下载依赖的后端开发和构建模块，使用bower下载依赖的前端openui5库。 在npm install的过程中提示&amp;rdquo;npm WARN deprecated bower@1.8.4: We don&amp;rsquo;t recommend using Bower for new projects. Please consider Yarn and Webpack or Parcel. You can read how to migrate legacy project here: https://bower.io/blog/2017/how-to-migrate-away-from-bower/&amp;quot;。 对于SAP这个小示例，区分前端和后端使用包管理器有点浪费！对于所有的依赖模块，可以要么使用npm，要么使用yarn。
 删除bower_components和dist目录
 安装yarn：
npm install yarn -g  去掉bower.json 不过其中依赖的openui5/packaged-sap.ui.core、openui5/packaged-sap.m、openui5/packaged-themelib_sap_belize仅仅bower能够获取，在npm仓库里是找不到的。
 修改package.json
 去除bower模块
 去除postinstall脚本
 增加@openui5/sap.m依赖
 增加@openui5/sap.ui.core依赖
 增加@openui5/themelib_sap_belize依赖  修改Gruntfile.js npm仓库里的@openui5/sap.m、@openui5/sap.ui.core、@openui5/themelib_sap_belize仅包含openui5/packaged-sap.ui.core、openui5/packaged-sap.m、openui5/packaged-themelib_sap_belize中resources的部分，而不包含test-resources的部分。 对于openui5_connect任务，我认为无需test-resources部分即可。
 将openui5库的定位从bower_components目录下改为node_modules目录下的相应位置  构建测试
yarn grunt build grunt serve   参考 SAP/grunt-openui5
JS新包管理工具yarn和npm的对比与使用入门
</content>
    </entry>
    
     <entry>
        <title>折腾openui5-sample-app之使用npm镜像</title>
        <url>https://mryqu.github.io/post/node_%E6%8A%98%E8%85%BEopenui5-sample-app%E4%B9%8B%E4%BD%BF%E7%94%A8npm%E9%95%9C%E5%83%8F/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>build</tag><tag>nodejs</tag><tag>npm</tag>
        </tags>
        <content type="html"> 学习了一下SAP/openui5-sample-app，看看SAP是如何使用构建前端的。 SAP/openui5-sample-app中npm安装模块是从https://www.npmjs.com/下载的，不知道从[淘宝NPM镜像](https://npm.taobao.org/)下载是否会快些。
对npm使用镜像有以下几种方式，这里我使用第三种： 1. 通过config命令
npm config set registry https://registry.npm.taobao.org npm info underscore   命令行指定  npm --registry https://registry.npm.taobao.org info underscore  在.npmrc文件中指定  registry = https://registry.npm.taobao.org   结果，速度上没什么感觉，都不快！
</content>
    </entry>
    
     <entry>
        <title>[Spring] LDAP用户验证笔记</title>
        <url>https://mryqu.github.io/post/spring_ldap%E7%94%A8%E6%88%B7%E9%AA%8C%E8%AF%81%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>JavaEE</tag><tag>spring</tag><tag>security</tag><tag>ldap</tag><tag>Authentication</tag>
        </tags>
        <content type="html">  对Spring LDAP用户验证进行了学习，制作了时序图： LDAP身份验证的步骤为：
- 从客户端登录页面获得用户名和密码。
- 匿名或使用管理DN/密码绑定到LDAP服务器，通过登录用户名查询用户DN，如失败则报用户不存在。
- 使用用户DN和密码再次绑定到LDAP服务器，如果能成功绑定则验证成功，否则报用户密码错误。
参考 Spring Security Architecture
Spring Security Reference
Spring Security Project
GETTING STARTED: Authenticating a User with LDAP
GitHub: spring-guides/gs-authenticating-ldap
</content>
    </entry>
    
     <entry>
        <title>[JS] 导出数据到CSV文件</title>
        <url>https://mryqu.github.io/post/js_%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE%E5%88%B0csv%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>jquery</tag><tag>openui5</tag><tag>export</tag><tag>csv</tag>
        </tags>
        <content type="html">  项目有可能要在HTML客户端上导出数据到CSV文件，先找找方案。
JS/jQuery方案  Export to CSV using jQuery and html Demo for StackOverflow Answer to the question: Export to CSV using jQuery and html 使用javascript下载页面中的表格数据 Exporting data from a web browser to a csv file using javascript.  OpenUI5方案  GitHub: OpenUI5 Export Test Download the Model Data to a CSV/Excel file in UI5 Export sap.ui.table.Table as CSV Export To Excel customization in UI5  </content>
    </entry>
    
     <entry>
        <title>新博客mryqu.github.io开张！</title>
        <url>https://mryqu.github.io/post/readme/</url>
        <categories>
          
        </categories>
        <tags>
          
        </tags>
        <content type="html"> 我目前的博客位于新浪mryqu的博客，这个算是新窝吧。
</content>
    </entry>
    
     <entry>
        <title>新博客诞生记</title>
        <url>https://mryqu.github.io/post/%E6%96%B0%E5%8D%9A%E5%AE%A2%E8%AF%9E%E7%94%9F%E8%AE%B0/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>github</tag><tag>hexo</tag><tag>disqus</tag>
        </tags>
        <content type="html">  在新浪博客贴代码片段诸多不爽，技术文章也时常通不过莫名其妙的关键词审查。偶尔心动，就创建了https://github.com/mryqu/mryqu.github.io，立刻新博客就出来了。 感觉有点简陋，开始琢磨theme之类的东东，然后就陷入了Jekyll、Hexo、Pelican等工具的比较纠结中，最后选定了用Node.js编写的博客框架Hexo及主题hexo-theme-next。
安装Hexo 安装Hexo的前提条件是已经安装好了Node.js和NPM：
mryqu@mryqu MINGW64 /c/quTools $ node -v v6.9.5 mryqu@mryqu MINGW64 /c/quTools $ npm -v 3.10.10  安装Hexo：
mryqu@mryqu MINGW64 /c/quTools $ mkdir hexo mryqu@mryqu MINGW64 /c/quTools $ cd hexo mryqu@mryqu MINGW64 /c/quTools/hexo npm install -g hexo-cli mryqu@mryqu MINGW64 /c/quTools/hexo npm install hexo --save  创建博客 mryqu@mryqu MINGW64 /c/users/mryqu $ hexo init blog mryqu@mryqu MINGW64 /c/users/mryqu $ cd blog mryqu@mryqu MINGW64 /c/users/mryqu/blog $ git clone https://github.com/iissnan/hexo-theme-next themes/next mryqu@mryqu MINGW64 /c/users/mryqu/blog $ cd themes/next mryqu@mryqu MINGW64 /c/users/mryqu/blog/themes/next $ git tag -l mryqu@mryqu MINGW64 /c/users/mryqu/blog/themes/next $ git checkout tags/v5.1.4 mryqu@mryqu MINGW64 /c/users/mryqu/blog/themes/next $ cd ../.. mryqu@mryqu MINGW64 /c/users/mryqu/blog $ git clone https://github.com/theme-next/hexo-theme-next themes/next-reloaded mryqu@mryqu MINGW64 /c/users/mryqu/blog $ cd themes/next-reloaded mryqu@mryqu MINGW64 /c/users/mryqu/blog/themes/next-reloaded $ git tag -l mryqu@mryqu MINGW64 /c/users/mryqu/blog/themes/next-reloaded $ git checkout tags/v6.3.0 mryqu@mryqu MINGW64 /c/users/mryqu/blog/themes/next-reloaded $ cd ../.. mryqu@mryqu MINGW64 /c/users/mryqu/blog npm install hexo-deployer-git --save mryqu@mryqu MINGW64 /c/users/mryqu/blog hexo new page categories mryqu@mryqu MINGW64 /c/users/mryqu/blog hexo new page tags  修改blog/_config.yml:
...... theme: next ...... deploy: type: git repo: https://github.com/mryqu/mryqu.github.io.git branch: master name: mryqu email: quyandong@yahoo.com  注： - hexo-theme-next从6.0起从https://github.com/iissnan/hexo-theme-next移到https://github.com/theme-next/hexo-theme-next，我目前还是决定采用5.x.x版本； - deploy下面子项在冒号后必须有一个空格，否则会导致hexo-deployer-git解析失败。
修改blog/themes/next/_config.yml:
...... menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat ...... # Schemes #scheme: Muse #scheme: Mist scheme: Pisces #scheme: Gemini ...... # Disqus disqus: enable: true shortname: mryqu count: true  注： - 主题方案从默认的Muse改成Pisces； - 首页显示分类和标签； - 使用Disqus评论系统。
修改blog/source/categories/index.md:
--- title: 文章分类 date: 2018-07-30 17:20:38 type: &amp;quot;categories&amp;quot; ---  修改blog/source/tags/index.md:
--- title: 标签 date: 2018-07-30 17:21:02 type: &amp;quot;tags&amp;quot; ---  修改blog/scaffolds/post.md:
--- title: {{ title }} date: {{ date }} categories: tags: ---  配置Disqus 在https://disqus.com上注册账户，然后创建新Site。 并没有找到Hexo平台，选择Universal Code即可。而Hexo除了配置Disqus短名称，别的工作Hexo都已完成了。
更新博文 在source_posts目录下创建新Markdown博文，即可通过Hexo命令完成生成html、启动本地服务器、部署到GitHub等工作。 - hexo clean: 清除缓存文件(db.json)和生成文件(public) - hexo generate: 生成静态文件 - hexo server或hexo s: 启动本地服务器，默认为http://localhost:4000/。hexo s -p 5000即启动http://localhost:5000/ - hexo deploy或hexo d: 提交到GitHub上 - hexo s -g: 生成静态文件并启动本地服务器 - hexo d -g: 生成静态文件并提交到GitHub上
新界面如下所示，大功告成！ 参考 11个最流行的静态(博客)网站生成工具
GitHub: Hexo
Hexo Commands
使用Hexo搭建个人Github博客
Hexo-Next下添加版权声明模块
再议评论服务：从多说到 Disqus
Hexo使用攻略-添加分类及标签
</content>
    </entry>
    
     <entry>
        <title>[Vagrant]学习VBoxManage定制</title>
        <url>https://mryqu.github.io/post/vagrant_%E5%AD%A6%E4%B9%A0vboxmanage%E5%AE%9A%E5%88%B6/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>virtualbox</tag><tag>vboxmanage</tag><tag>nat</tag><tag>cpu</tag>
        </tags>
        <content type="html">  Vagrant Configuration - VBoxManage Customizations里面有讲到通过VBoxManage修改VirtualBox虚拟机。而VBoxManage modifyvm里面细致的介绍了所有设置。
VBoxManage modifyvm设置 接触过的VBoxManage modifyvm设置 下面就仔细研究一下我看到过的modifyvm设置：
# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = &amp;quot;2&amp;quot; Vagrant.require_version &amp;quot;&amp;gt;= 1.6.3&amp;quot; Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.provider &amp;quot;virtualbox&amp;quot; do |vb| # --memory设置用来指定分配的内存，单位为MB # 可简写为vb.memory=8192 vb.customize [&#39;modifyvm&#39;, :id, &#39;--memory&#39;, &#39;8192&#39;] # --cpus设置用来指定虚拟机的虚拟CPU个数 # 可简写为vb.cpus=3 vb.customize [&#39;modifyvm&#39;, :id, &#39;--cpus&#39;, &#39;3&#39;] # --cpuexecutioncap &amp;lt;1-100&amp;gt;设置用来指定虚拟CPU可用的CPU时间比例。 # 值50意味无论VM使用多少个虚拟虚拟CPU，都不会超过宿主机CPU时间的一半。 vb.customize [&#39;modifyvm&#39;, :id, &#39;--cpuexecutioncap&#39;, &#39;75&#39;] # --natdnshostresolver&amp;lt;1-N&amp;gt; on|off用来指定NAT使用宿主机的解析机制处理DNS请求。 vb.customize [&#39;modifyvm&#39;, :id, &#39;--natdnshostresolver1&#39;, &#39;on&#39;] # --natdnsproxy&amp;lt;1-N&amp;gt; on|off用来指定NAT将所有客户机DNS请求代理到宿主机的DNS服务器。 vb.customize [&#39;modifyvm&#39;, :id, &#39;--natdnsproxy1&#39;, &#39;on&#39;] # --ostype设置指定客户机OS类型 # 可用值详见下面的modifyvm --ostype选项 vb.customize [&#39;modifyvm&#39;, :id, &#39;--ostype&#39;, &#39;Ubuntu_64&#39;] # --acpi on|off设置VM是否支持ACPI # --ioapic on|off设置VM是否支持I/O ACPI vb.customize [&#39;modifyvm&#39;, :id, &#39;--ioapic&#39;, &#39;on&#39;] # Bug 51473| # --cableconnected&amp;lt;1-N&amp;gt; on|off设置可用于临时断开虚拟网接口，效果类似于从网卡上拔出 # 网线。该设置有可能用于复位VM内的某些软件模块。 # VirtualBox的某些版本 (5.1.x)似乎将NAT接口启动为断开状态，因此需要显性连接上。 # 见https://github.com/mitchellh/vagrant/issues/7648 vb.customize [&#39;modifyvm&#39;, :id, &#39;--cableconnected1&#39;, &#39;on&#39;] # 防止时钟偏移，例如Vagrant box停止15分钟后重启，VM内部时间为15分钟。 # 见http://stackoverflow.com/a/19492466/323407 vb.customize [&#39;guestproperty&#39;, &#39;set&#39;, :id, &#39;/VirtualBox/GuestAdd/VBoxService/--timesync-set-threshold&#39;, 10000] # 预以图形模式启动VM，则去掉下一行的注释： # vb.gui = true end ...... end  modifyvm &amp;ndash;ostype选项 C:&amp;gt;\quTools\oracle\VirtualBox\VBoxManage list ostypes ID: Other Description: Other/Unknown Family ID: Other Family Desc: Other 64 bit: false ID: Other_64 Description: Other/Unknown (64-bit) Family ID: Other Family Desc: Other 64 bit: true ID: Windows31 Description: Windows 3.1 Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows95 Description: Windows 95 Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows98 Description: Windows 98 Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: WindowsMe Description: Windows ME Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: WindowsNT4 Description: Windows NT 4 Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows2000 Description: Windows 2000 Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: WindowsXP Description: Windows XP (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: WindowsXP_64 Description: Windows XP (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows2003 Description: Windows 2003 (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows2003_64 Description: Windows 2003 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: WindowsVista Description: Windows Vista (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: WindowsVista_64 Description: Windows Vista (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows2008 Description: Windows 2008 (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows2008_64 Description: Windows 2008 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows7 Description: Windows 7 (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows7_64 Description: Windows 7 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows8 Description: Windows 8 (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows8_64 Description: Windows 8 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows81 Description: Windows 8.1 (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows81_64 Description: Windows 8.1 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows2012_64 Description: Windows 2012 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows10 Description: Windows 10 (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: Windows10_64 Description: Windows 10 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Windows2016_64 Description: Windows 2016 (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: WindowsNT Description: Other Windows (32-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: false ID: WindowsNT_64 Description: Other Windows (64-bit) Family ID: Windows Family Desc: Microsoft Windows 64 bit: true ID: Linux22 Description: Linux 2.2 Family ID: Linux Family Desc: Linux 64 bit: false ID: Linux24 Description: Linux 2.4 (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Linux24_64 Description: Linux 2.4 (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Linux26 Description: Linux 2.6 / 3.x / 4.x (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Linux26_64 Description: Linux 2.6 / 3.x / 4.x (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: ArchLinux Description: Arch Linux (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: ArchLinux_64 Description: Arch Linux (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Debian Description: Debian (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Debian_64 Description: Debian (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: OpenSUSE Description: openSUSE (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: OpenSUSE_64 Description: openSUSE (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Fedora Description: Fedora (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Fedora_64 Description: Fedora (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Gentoo Description: Gentoo (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Gentoo_64 Description: Gentoo (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Mandriva Description: Mandriva (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Mandriva_64 Description: Mandriva (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: RedHat Description: Red Hat (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: RedHat_64 Description: Red Hat (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Turbolinux Description: Turbolinux (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Turbolinux_64 Description: Turbolinux (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Ubuntu Description: Ubuntu (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Ubuntu_64 Description: Ubuntu (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Xandros Description: Xandros (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Xandros_64 Description: Xandros (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Oracle Description: Oracle (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Oracle_64 Description: Oracle (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Linux Description: Other Linux (32-bit) Family ID: Linux Family Desc: Linux 64 bit: false ID: Linux_64 Description: Other Linux (64-bit) Family ID: Linux Family Desc: Linux 64 bit: true ID: Solaris Description: Oracle Solaris 10 5/09 and earlier (32-bit) Family ID: Solaris Family Desc: Solaris 64 bit: false ID: Solaris_64 Description: Oracle Solaris 10 5/09 and earlier (64-bit) Family ID: Solaris Family Desc: Solaris 64 bit: true ID: OpenSolaris Description: Oracle Solaris 10 10/09 and later (32-bit) Family ID: Solaris Family Desc: Solaris 64 bit: false ID: OpenSolaris_64 Description: Oracle Solaris 10 10/09 and later (64-bit) Family ID: Solaris Family Desc: Solaris 64 bit: true ID: Solaris11_64 Description: Oracle Solaris 11 (64-bit) Family ID: Solaris Family Desc: Solaris 64 bit: true ID: FreeBSD Description: FreeBSD (32-bit) Family ID: BSD Family Desc: BSD 64 bit: false ID: FreeBSD_64 Description: FreeBSD (64-bit) Family ID: BSD Family Desc: BSD 64 bit: true ID: OpenBSD Description: OpenBSD (32-bit) Family ID: BSD Family Desc: BSD 64 bit: false ID: OpenBSD_64 Description: OpenBSD (64-bit) Family ID: BSD Family Desc: BSD 64 bit: true ID: NetBSD Description: NetBSD (32-bit) Family ID: BSD Family Desc: BSD 64 bit: false ID: NetBSD_64 Description: NetBSD (64-bit) Family ID: BSD Family Desc: BSD 64 bit: true ID: OS2Warp3 Description: OS/2 Warp 3 Family ID: OS2 Family Desc: IBM OS/2 64 bit: false ID: OS2Warp4 Description: OS/2 Warp 4 Family ID: OS2 Family Desc: IBM OS/2 64 bit: false ID: OS2Warp45 Description: OS/2 Warp 4.5 Family ID: OS2 Family Desc: IBM OS/2 64 bit: false ID: OS2eCS Description: eComStation Family ID: OS2 Family Desc: IBM OS/2 64 bit: false ID: OS21x Description: OS/2 1.x Family ID: OS2 Family Desc: IBM OS/2 64 bit: false ID: OS2 Description: Other OS/2 Family ID: OS2 Family Desc: IBM OS/2 64 bit: false ID: MacOS Description: Mac OS X (32-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: false ID: MacOS_64 Description: Mac OS X (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS106 Description: Mac OS X 10.6 Snow Leopard (32-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: false ID: MacOS106_64 Description: Mac OS X 10.6 Snow Leopard (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS107_64 Description: Mac OS X 10.7 Lion (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS108_64 Description: Mac OS X 10.8 Mountain Lion (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS109_64 Description: Mac OS X 10.9 Mavericks (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS1010_64 Description: Mac OS X 10.10 Yosemite (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS1011_64 Description: Mac OS X 10.11 El Capitan (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS1012_64 Description: macOS 10.12 Sierra (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: MacOS1013_64 Description: macOS 10.13 High Sierra (64-bit) Family ID: MacOS Family Desc: Mac OS X 64 bit: true ID: DOS Description: DOS Family ID: Other Family Desc: Other 64 bit: false ID: Netware Description: Netware Family ID: Other Family Desc: Other 64 bit: false ID: L4 Description: L4 Family ID: Other Family Desc: Other 64 bit: false ID: QNX Description: QNX Family ID: Other Family Desc: Other 64 bit: false ID: JRockitVE Description: JRockitVE Family ID: Other Family Desc: Other 64 bit: false ID: VBoxBS_64 Description: VirtualBox Bootsector Test (64-bit) Family ID: Other Family Desc: Other 64 bit: true  </content>
    </entry>
    
     <entry>
        <title>[Vagrant] 学习一下Vagrant的Provider类型</title>
        <url>https://mryqu.github.io/post/vagrant_%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%B8%8Bvagrant%E7%9A%84provider%E7%B1%BB%E5%9E%8B/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>provider</tag>
        </tags>
        <content type="html"> 偶尔动了心思，想自己装一个Wiki。就按照MediaWiki-Vagrant，装一个Vagrant版的MediaWiki玩玩。MediaWiki全球最著名的开源wiki程序，运行于PHP&#43;MySQL环境。MediaWiki从2002年2月25日被作为维基百科全书的系统软件，并有大量其他应用实例。 顺手研究了一下mediawiki/vagrant项目的Vagrantfile文件，跟我们自己项目用的Vagrantfile仅仅配置了一个VirtualBox provider不同，它配置了VirtualBox、VMWare Fusion、Microsoft Hyper-V、LXC、Parallels和libvirt (KVM/QEMU)六种provider。 在Vagrant的网站上仅仅提及了默认的VirtualBox、VMware、Docker、Hyper-V四款provider。那到底Vagrant有多少provider类型呢？ 从Vagrant Cloud上可知有22种provider类型的Vagrant box镜像可供下载： - aws - cloudstack - digitalocean - docker - google - hyperv - libvirt - lxc - openstack - parallels - qemu - rackspace - softlayer - veertu - virtualbox - vmware - vmware_desktop - vmware_fusion - vmware_ovf - vmware_workstation - vsphere - xenserver
</content>
    </entry>
    
     <entry>
        <title>[Spark] 使用Spark的REST服务Livy</title>
        <url>https://mryqu.github.io/post/spark_%E4%BD%BF%E7%94%A8spark%E7%9A%84rest%E6%9C%8D%E5%8A%A1livy/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>spark</tag><tag>livy</tag><tag>rest</tag><tag>session</tag><tag>batch</tag>
        </tags>
        <content type="html">  Apache Livy简介 Apache Livy是由Cloudera Labs贡献的基于Apache Spark的开源REST服务，它不仅以REST的方式代替了Spark传统的处理交互方式，同时也提供企业应用中不可忽视的多用户，安全，以及容错的支持。其功能如下：- 拥有可用于多Spark作业或多客户端长时间运行的SparkContext； - 同时管理多个SparkContext，并在集群（YARN / Mesos）而不是Livy服务器上运行它们，以实现良好的容错性和并发性； - 可以通过预先编译好的JAR、代码片段或是java/scala客户端API将Spark作业提交到远端的Spark集群上执行。
建立测试环境 今天在GitHub: mryqu/vagrant-hadoop-hive-spark提交了add livy support，因此可以在Vagrant搭建的Hadoop 2.7.6 &#43; Hive 2.3.3 &#43; Spark 2.3.0虚拟机环境中使用Livy 0.5.0服务。 使用Livy的REST API 创建交互式会话 curl -X POST -d &#39;{&amp;quot;kind&amp;quot;: &amp;quot;spark&amp;quot;}&#39; -H &amp;quot;Content-Type: application/json&amp;quot; http://10.211.55.101:8998/sessions { &amp;quot;id&amp;quot;:0, &amp;quot;appId&amp;quot;:null, &amp;quot;owner&amp;quot;:null, &amp;quot;proxyUser&amp;quot;:null, &amp;quot;state&amp;quot;:&amp;quot;starting&amp;quot;, &amp;quot;kind&amp;quot;:&amp;quot;spark&amp;quot;, &amp;quot;appInfo&amp;quot;:{ &amp;quot;driverLogUrl&amp;quot;:null, &amp;quot;sparkUiUrl&amp;quot;:null }, &amp;quot;log&amp;quot;:[ &amp;quot;stdout: &amp;quot;, &amp;quot; stderr: &amp;quot; ] }  成功创建会话0，kind指定为spark，如果之后提交的代码中没有指定kind，则使用此处的会话默认kind。
查询交互式会话列表 curl http://10.211.55.101:8998/sessions { &amp;quot;from&amp;quot;:0, &amp;quot;total&amp;quot;:1, &amp;quot;sessions&amp;quot;:[ { &amp;quot;id&amp;quot;:0, &amp;quot;appId&amp;quot;:null, &amp;quot;owner&amp;quot;:null, &amp;quot;proxyUser&amp;quot;:null, &amp;quot;state&amp;quot;:&amp;quot;idle&amp;quot;, &amp;quot;kind&amp;quot;:&amp;quot;spark&amp;quot;, &amp;quot;appInfo&amp;quot;:{ &amp;quot;driverLogUrl&amp;quot;:null, &amp;quot;sparkUiUrl&amp;quot;:null }, &amp;quot;log&amp;quot;:[ &amp;quot;2018-07-18 03:19:16 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy&amp;quot;, &amp;quot;2018-07-18 03:19:16 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, node1, 37891, None)&amp;quot;, &amp;quot;2018-07-18 03:19:16 INFO BlockManagerMasterEndpoint:54 - Registering block manager node1:37891 with 366.3 MB RAM, BlockManagerId(driver, node1, 37891, None)&amp;quot;, &amp;quot;2018-07-18 03:19:16 INFO BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, node1, 37891, None)&amp;quot;, &amp;quot;2018-07-18 03:19:16 INFO BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, node1, 37891, None)&amp;quot;, &amp;quot;2018-07-18 03:19:16 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6bc3c1d4{/metrics/json,null,AVAILABLE,@Spark}&amp;quot;, &amp;quot;2018-07-18 03:19:17 INFO EventLoggingListener:54 - Logging events to hdfs://node1/user/spark/applicationHistory/local-1531883956147&amp;quot;, &amp;quot;2018-07-18 03:19:17 INFO SparkEntries:53 - Spark context finished initialization in 1925ms&amp;quot;, &amp;quot;2018-07-18 03:19:17 INFO SparkEntries:87 - Created Spark session (with Hive support).&amp;quot;, &amp;quot; stderr: &amp;quot; ] } ] }  提交scala代码片段 curl -X POST -H &#39;Content-Type: application/json&#39; -d &#39;{&amp;quot;code&amp;quot;:&amp;quot;123&#43;321&amp;quot;}&#39; http://10.211.55.101:8998/sessions/0/statements { &amp;quot;id&amp;quot;:0, &amp;quot;code&amp;quot;:&amp;quot;123&#43;321&amp;quot;, &amp;quot;state&amp;quot;:&amp;quot;waiting&amp;quot;, &amp;quot;output&amp;quot;:null, &amp;quot;progress&amp;quot;:0 }  查询结果 curl http://10.211.55.101:8998/sessions/0/statements/0 { &amp;quot;id&amp;quot;:0, &amp;quot;code&amp;quot;:&amp;quot;123&#43;321&amp;quot;, &amp;quot;state&amp;quot;:&amp;quot;available&amp;quot;, &amp;quot;output&amp;quot;:{ &amp;quot;status&amp;quot;:&amp;quot;ok&amp;quot;, &amp;quot;execution_count&amp;quot;:0, &amp;quot;data&amp;quot;:{ &amp;quot;text/plain&amp;quot;:&amp;quot;res0: Int = 444&amp;quot; } }, &amp;quot;progress&amp;quot;:1 }  提交sql代码片段 curl http://10.211.55.101:8998/sessions/0/statements -X POST -H &#39;Content-Type: application/json&#39; -d &#39;{&amp;quot;code&amp;quot;:&amp;quot;show tables&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;sql&amp;quot;}&#39; { &amp;quot;id&amp;quot;:1, &amp;quot;code&amp;quot;:&amp;quot;show tables&amp;quot;, &amp;quot;state&amp;quot;:&amp;quot;waiting&amp;quot;, &amp;quot;output&amp;quot;:null, &amp;quot;progress&amp;quot;:0 }  查询结果 curl http://10.211.55.101:8998/sessions/0/statements/1 { &amp;quot;id&amp;quot;:1, &amp;quot;code&amp;quot;:&amp;quot;show tables&amp;quot;, &amp;quot;state&amp;quot;:&amp;quot;available&amp;quot;, &amp;quot;output&amp;quot;:{ &amp;quot;status&amp;quot;:&amp;quot;ok&amp;quot;, &amp;quot;execution_count&amp;quot;:1, &amp;quot;data&amp;quot;:{ &amp;quot;application/json&amp;quot;:{ &amp;quot;schema&amp;quot;:{ &amp;quot;type&amp;quot;:&amp;quot;struct&amp;quot;, &amp;quot;fields&amp;quot;:[ { &amp;quot;name&amp;quot;:&amp;quot;database&amp;quot;, &amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;, &amp;quot;nullable&amp;quot;:false, &amp;quot;metadata&amp;quot;:{ } }, { &amp;quot;name&amp;quot;:&amp;quot;tableName&amp;quot;, &amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;, &amp;quot;nullable&amp;quot;:false, &amp;quot;metadata&amp;quot;:{ } }, { &amp;quot;name&amp;quot;:&amp;quot;isTemporary&amp;quot;, &amp;quot;type&amp;quot;:&amp;quot;boolean&amp;quot;, &amp;quot;nullable&amp;quot;:false, &amp;quot;metadata&amp;quot;:{ } } ] }, &amp;quot;data&amp;quot;:[ [ &amp;quot;default&amp;quot;, &amp;quot;emp&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;emp2&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;emp3&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;t1&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;t2&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;t4&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;tv&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;yqu1&amp;quot;, false ], [ &amp;quot;default&amp;quot;, &amp;quot;yqu2&amp;quot;, false ] ] } } }, &amp;quot;progress&amp;quot;:1 }  提交批处理请求 hadoop fs -put /home/vagrant/HelloSparkHive/build/libs/hello-spark-hive-0.1.0.jar /tmp/ curl -H &amp;quot;Content-Type: application/json&amp;quot; -X POST -d &#39;{ &amp;quot;conf&amp;quot;: {&amp;quot;spark.master&amp;quot;:&amp;quot;local [2]&amp;quot;}, &amp;quot;file&amp;quot;:&amp;quot;/tmp/hello-spark-hive-0.1.0.jar&amp;quot;, &amp;quot;className&amp;quot;:&amp;quot;com.yqu.sparkhive.HelloSparkHiveDriver&amp;quot;, &amp;quot;args&amp;quot;:[&amp;quot;yqu9&amp;quot;] }&#39; http://10.211.55.101:8998/batches { &amp;quot;id&amp;quot;:3, &amp;quot;state&amp;quot;:&amp;quot;running&amp;quot;, &amp;quot;appId&amp;quot;:null, &amp;quot;appInfo&amp;quot;:{ &amp;quot;driverLogUrl&amp;quot;:null, &amp;quot;sparkUiUrl&amp;quot;:null }, &amp;quot;log&amp;quot;:[ &amp;quot;stdout: &amp;quot;, &amp;quot; stderr: &amp;quot; ] }  查询批处理会话列表 curl http://10.211.55.101:8998/batches { &amp;quot;from&amp;quot;:0, &amp;quot;total&amp;quot;:2, &amp;quot;sessions&amp;quot;:[ { &amp;quot;id&amp;quot;:1, &amp;quot;state&amp;quot;:&amp;quot;dead&amp;quot;, &amp;quot;appId&amp;quot;:null, &amp;quot;appInfo&amp;quot;:{ &amp;quot;driverLogUrl&amp;quot;:null, &amp;quot;sparkUiUrl&amp;quot;:null }, &amp;quot;log&amp;quot;:[ &amp;quot; at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:692)&amp;quot;, &amp;quot; at org.apache.spark.deploy.DependencyUtils$.downloadFile(DependencyUtils.scala:131)&amp;quot;, &amp;quot; at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:401)&amp;quot;, &amp;quot; at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:401)&amp;quot;, &amp;quot; at scala.Option.map(Option.scala:146)&amp;quot;, &amp;quot; at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:400)&amp;quot;, &amp;quot; at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:170)&amp;quot;, &amp;quot; at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)&amp;quot;, &amp;quot; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&amp;quot;, &amp;quot; stderr: &amp;quot; ] }, { &amp;quot;id&amp;quot;:3, &amp;quot;state&amp;quot;:&amp;quot;running&amp;quot;, &amp;quot;appId&amp;quot;:null, &amp;quot;appInfo&amp;quot;:{ &amp;quot;driverLogUrl&amp;quot;:null, &amp;quot;sparkUiUrl&amp;quot;:null }, &amp;quot;log&amp;quot;:[ &amp;quot;2018-07-18 03:48:45 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO MemoryStore:54 - MemoryStore cleared&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO BlockManager:54 - BlockManager stopped&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO BlockManagerMaster:54 - BlockManagerMaster stopped&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO SparkContext:54 - Successfully stopped SparkContext&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO ShutdownHookManager:54 - Shutdown hook called&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ffa75451-735a-4edc-9572-32d1a07ba748&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-7567b293-c81e-4516-a644-531e9fc584d1&amp;quot;, &amp;quot; stderr: &amp;quot; ] } ] }  曾经尝试过使用file:///home/vagrant/HelloSparkHive/build/libs/hello-spark-hive-0.1.0.jar，但是报错：&amp;rdquo;requirement failed: Local path /home/vagrant/HelloSparkHive/build/libs/hello-spark-hive-0.1.0.jar cannot be added to user sessions.&amp;ldquo;，有可能不支持本地jar文件。
查询特定批处理会话信息 curl http://10.211.55.101:8998/batches/3 { &amp;quot;id&amp;quot;:3, &amp;quot;state&amp;quot;:&amp;quot;success&amp;quot;, &amp;quot;appId&amp;quot;:null, &amp;quot;appInfo&amp;quot;:{ &amp;quot;driverLogUrl&amp;quot;:null, &amp;quot;sparkUiUrl&amp;quot;:null }, &amp;quot;log&amp;quot;:[ &amp;quot;2018-07-18 03:48:45 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO MemoryStore:54 - MemoryStore cleared&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO BlockManager:54 - BlockManager stopped&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO BlockManagerMaster:54 - BlockManagerMaster stopped&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO SparkContext:54 - Successfully stopped SparkContext&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO ShutdownHookManager:54 - Shutdown hook called&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ffa75451-735a-4edc-9572-32d1a07ba748&amp;quot;, &amp;quot;2018-07-18 03:48:45 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-7567b293-c81e-4516-a644-531e9fc584d1&amp;quot;, &amp;quot; stderr: &amp;quot; ] }  查询特定批处理会话状态 curl http://10.211.55.101:8998/batches/3/state { &amp;quot;id&amp;quot;:3, &amp;quot;state&amp;quot;:&amp;quot;success&amp;quot; }  查看WEB UI 删除回话 curl -X DELETE http://10.211.55.101:8998/sessions/0 { &amp;quot;msg&amp;quot;:&amp;quot;deleted&amp;quot; } curl -X DELETE http://10.211.55.101:8998/batches/3 { &amp;quot;msg&amp;quot;:&amp;quot;deleted&amp;quot; }  参考 Apache Livy
Livy REST API
GitHub: apache/incubator-livy
GitHub: mryqu/vagrant-hadoop-hive-spark
Livy：基于Apache Spark的REST服务
Apache Livy 实现思路及模块概述
Livy原理详解
</content>
    </entry>
    
     <entry>
        <title>[Spark] SparkCatalogAPI使用</title>
        <url>https://mryqu.github.io/post/spark_sparkcatalogapi%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>spark</tag><tag>catalog</tag><tag>hive</tag><tag>table</tag><tag>column</tag>
        </tags>
        <content type="html">  Catalog API简介 Spark中的DataSet和Dataframe API支持结构化分析。结构化分析的一个重要的方面是管理元数据。这些元数据可能是一些临时元数据（比如临时表）、SQLContext上注册的UDF以及持久化的元数据（比如Hivemeta store或者HCatalog）。 Spark2中添加了标准的API（称为catalog）来访问Spark SQL中的元数据。这个API既可以操作Spark SQL，也可以操作Hive元数据。
Catalog API使用 查询数据库 scala&amp;gt; spark.catalog.listDatabases.show(false) &#43;-------&#43;---------------------&#43;----------------------------------------&#43; |name |description |locationUri | &#43;-------&#43;---------------------&#43;----------------------------------------&#43; |default|Default Hive database|hdfs://10.211.55.101/user/hive/warehouse| &#43;-------&#43;---------------------&#43;----------------------------------------&#43; scala&amp;gt; spark.catalog.currentDatabase res4: String = default  查询表 scala&amp;gt; spark.catalog.listTables.show(false) &#43;----&#43;--------&#43;----------------------------------------&#43;---------&#43;-----------&#43; |name|database|description |tableType|isTemporary| &#43;----&#43;--------&#43;----------------------------------------&#43;---------&#43;-----------&#43; |emp |default |null |MANAGED |false | |emp2|default |Imported by sqoop on 2018/07/10 04:23:26|MANAGED |false | |emp3|default |Imported by sqoop on 2018/07/10 06:13:17|MANAGED |false | |yqu1|default |null |MANAGED |false | |yqu2|default |null |MANAGED |false | &#43;----&#43;--------&#43;----------------------------------------&#43;---------&#43;-----------&#43;  下面的示例用于创建不同TableType的表： - MANAGED: 当表被删除时，内容与元数据一同删除 - EXTERNAL: 当表被删除时，仅元数据会被删除 - VIEW: 持久视图 - TEMPORARY: 临时视图
scala&amp;gt; val df = Seq(1,2).toDF() df: org.apache.spark.sql.DataFrame = [value: int] scala&amp;gt; df.write.saveAsTable(&amp;quot;t1&amp;quot;) scala&amp;gt; df.write.option(&amp;quot;path&amp;quot;, &amp;quot;/tmp/tables/t2&amp;quot;).saveAsTable(&amp;quot;t2&amp;quot;) scala&amp;gt; df.createOrReplaceTempView(&amp;quot;t3&amp;quot;) scala&amp;gt; spark.sql(&amp;quot;CREATE VIEW tv AS SELECT * FROM t1&amp;quot;) res13: org.apache.spark.sql.DataFrame = [] scala&amp;gt; spark.catalog.listTables.filter($&amp;quot;name&amp;quot;.startsWith(&amp;quot;t&amp;quot;)).show(false) &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43; |name|database|description|tableType|isTemporary| &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43; |t1 |default |null |MANAGED |false | |t2 |default |null |EXTERNAL |false | |tv |default |null |VIEW |false | |t3 |null |null |TEMPORARY|true | &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43;  查询列 scala&amp;gt; spark.catalog.listColumns(&amp;quot;emp&amp;quot;).show(false) &#43;--------&#43;-----------&#43;--------&#43;--------&#43;-----------&#43;--------&#43; |name |description|dataType|nullable|isPartition|isBucket| &#43;--------&#43;-----------&#43;--------&#43;--------&#43;-----------&#43;--------&#43; |empno |null |int |true |false |false | |ename |null |string |true |false |false | |job |null |string |true |false |false | |mgr |null |int |true |false |false | |hiredate|null |string |true |false |false | |salary |null |double |true |false |false | |comm |null |double |true |false |false | |deptno |null |int |true |false |false | &#43;--------&#43;-----------&#43;--------&#43;--------&#43;-----------&#43;--------&#43;  查询函数 按照以前的博文[Hive] Hive Macro和UDF实践创建持久性函数my_hello和my_lower后，使用Catalog API查询：
scala&amp;gt; spark.catalog.listFunctions.filter($&amp;quot;name&amp;quot;.contains(&amp;quot;my&amp;quot;)).show(false) &#43;--------&#43;--------&#43;-----------&#43;------------------------&#43;-----------&#43; |name |database|description|className |isTemporary| &#43;--------&#43;--------&#43;-----------&#43;------------------------&#43;-----------&#43; |my_hello|default |null |com.mryqu.hive.udf.Hello|false | |my_lower|default |null |com.mryqu.hive.udf.Lower|false | &#43;--------&#43;--------&#43;-----------&#43;------------------------&#43;-----------&#43;  查询表是否缓存 scala&amp;gt; val sqlDF = spark.sql(&amp;quot;SELECT * FROM emp&amp;quot;) sqlDF: org.apache.spark.sql.DataFrame = [empno: int, ename: string ... 6 more fields] scala&amp;gt; sqlDF.createOrReplaceTempView(&amp;quot;yquTempTable&amp;quot;) scala&amp;gt; spark.catalog.listTables.show(false) &#43;------------&#43;--------&#43;----------------------------------------&#43;---------&#43;-----------&#43; |name |database|description |tableType|isTemporary| &#43;------------&#43;--------&#43;----------------------------------------&#43;---------&#43;-----------&#43; |emp |default |null |MANAGED |false | |emp2 |default |Imported by sqoop on 2018/07/10 04:23:26|MANAGED |false | |emp3 |default |Imported by sqoop on 2018/07/10 06:13:17|MANAGED |false | |yqu1 |default |null |MANAGED |false | |yqu2 |default |null |MANAGED |false | |yqutemptable|null |null |TEMPORARY|true | &#43;------------&#43;--------&#43;----------------------------------------&#43;---------&#43;-----------&#43; scala&amp;gt; spark.catalog.isCached(&amp;quot;yquTempTable&amp;quot;) res12: Boolean = false scala&amp;gt; sqlDF.cache() res14: sqlDF.type = [empno: int, ename: string ... 6 more fields] scala&amp;gt; spark.catalog.isCached(&amp;quot;yquTempTable&amp;quot;) res12: Boolean = true scala&amp;gt; spark.catalog.isCached(&amp;quot;emp&amp;quot;) res13: Boolean = false scala&amp;gt; spark.catalog.uncacheTable(&amp;quot;yqutemptable&amp;quot;) scala&amp;gt; spark.catalog.isCached(&amp;quot;yquTempTable&amp;quot;) res19: Boolean = false  创建表 scala&amp;gt; val df2=spark.catalog.createTable(&amp;quot;t4&amp;quot;,&amp;quot;/tmp/tables/t2&amp;quot;) df2: org.apache.spark.sql.DataFrame = [value: int] scala&amp;gt; spark.catalog.listTables.filter($&amp;quot;name&amp;quot;.startsWith(&amp;quot;t&amp;quot;)).show(false) &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43; |name|database|description|tableType|isTemporary| &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43; |t1 |default |null |MANAGED |false | |t2 |default |null |EXTERNAL |false | |t4 |default |null |EXTERNAL |false | |tv |default |null |VIEW |false | |t3 |null |null |TEMPORARY|true | &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43; scala&amp;gt; spark.sql(&amp;quot;select * from t4&amp;quot;).show() &#43;-----&#43; |value| &#43;-----&#43; | 1| | 2| &#43;-----&#43;  删除临时视图 scala&amp;gt; spark.catalog.dropTempView(&amp;quot;t3&amp;quot;) res18: Boolean = true scala&amp;gt; spark.catalog.listTables.filter($&amp;quot;name&amp;quot;.startsWith(&amp;quot;t&amp;quot;)).show(false) &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43; |name|database|description|tableType|isTemporary| &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43; |t1 |default |null |MANAGED |false | |t2 |default |null |EXTERNAL |false | |t4 |default |null |EXTERNAL |false | |tv |default |null |VIEW |false | &#43;----&#43;--------&#43;-----------&#43;---------&#43;-----------&#43;  dropGlobalTempView删除全局临时表跟dropTempView的操作差不多，这里就不演示了。
参考 org.apache.spark.sql.catalog
Introduction to Spark 2.0 - Part 4 : Introduction to Catalog API
Migrating to Spark 2.0 - Part 8 : Catalog API
</content>
    </entry>
    
     <entry>
        <title>[Oozie] 遭遇ShareLib无法找到的问题</title>
        <url>https://mryqu.github.io/post/oozie_%E9%81%AD%E9%81%87sharelib%E6%97%A0%E6%B3%95%E6%89%BE%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Oozie</category>
        </categories>
        <tags>
          <tag>oozie</tag><tag>setup</tag><tag>sharelib</tag><tag>hadoop</tag><tag>5.0.0</tag>
        </tags>
        <content type="html">  折腾几天，终于装好了Oozie 5.0.0，并且启动了Oozie守护进程。
vagrant@node1:~$ oozie admin -oozie http://10.211.55.101:11000/oozie -status log4j:WARN No appenders could be found for logger (org.apache.hadoop.security.authentication.client.KerberosAuthenticator). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. System mode: NORMAL  不过运行MapReduce demo总是出错，找不到Oozie的共享库，日志如下：
2018-07-12 04:45:50,228 WARN ActionStartXCommand:523 - SERVER[node1] USER[vagrant] GROUP[-] TOKEN[] APP[map-reduce-wf] JOB[0000001-XXXXXXXXXXXXXXX-oozie-root-W] ACTION[0000001-XXXXXXXXXXXXXXX-oozie-root-W@mr-node] Error starting action [mr-node]. ErrorType [FAILED], ErrorCode [It should never happen], Message [File /user/root/share/lib does not exist] org.apache.oozie.action.ActionExecutorException: File /user/root/share/lib does not exist at org.apache.oozie.action.hadoop.jaxaActionExecutor.addSystemShareLibForAction(JaxaActionExecutor.jaxa:646) at org.apache.oozie.action.hadoop.JaxaActionExecutor.addAllShareLibs(JaxaActionExecutor.jaxa:760) at org.apache.oozie.action.hadoop.JaxaActionExecutor.setLibFilesArchives(JaxaActionExecutor.jaxa:746) at org.apache.oozie.action.hadoop.JaxaActionExecutor.submitLauncher(JaxaActionExecutor.jaxa:984) at org.apache.oozie.action.hadoop.JaxaActionExecutor.start(JaxaActionExecutor.jaxa:1512) at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.jaxa:243) at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.jaxa:68) at org.apache.oozie.command.XCommand.call(XCommand.jaxa:290) at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.jaxa:334) at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.jaxa:263) at Jaxa.util.concurrent.FutureTask.run(FutureTask.jaxa:266) at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.jaxa:181) at Jaxa.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.jaxa:1149) at Jaxa.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.jaxa:624) at Jaxa.lang.Thread.run(Thread.jaxa:748)  折腾了无数次oozie-setup.sh sharelib命令，root和vagrant用户都试过。还在oozie-site.xml中指定了oozie.service.WorkflowAppService.system.libpath属性。结果统统失败。
vagrant@node1:~$ sudo -sE root@node1:~#/usr/local/oozie/bin/oozie-setup.sh sharelib create -fs hdfs://10.211.55.101 -locallib /usr/local/oozie/oozie-sharelib-5.0.0.tar.gz root@node1:~#/usr/local/oozie/bin/oozied.sh start root@node1:~#exit vagrant@node1:~$ oozie admin -configuration | grep oozie.service.WorkflowAppService.system.libpath oozie.service.WorkflowAppService.system.libpath : /user/root/share/lib vagrant@node1:~$ oozie admin -shareliblist -oozie http://localhost:11000/oozie [Available ShareLib]  之后找到了Oozie/OOZIE-3208 &amp;ldquo;It should never happen&amp;rdquo; error messages should be more specific to root cause，不过就是改善一下错误消息并且在5.1.0里发布，还是没找到问题所在。
再之后发现自Oozie4.2.0开始oozie.service.WorkflowAppService.system.libpath的默认值变了： 将oozie-site.xml中oozie.service.WorkflowAppService.system.libpath属性值改为hdfs://10.211.55.101/user/root/share/lib，结果出错信息变成了&amp;rdquo;IllegalArgumentException: Wrong FS: hdfs://10.211.55.101/user/root/share/lib, expected: file:///&amp;ldquo;。
2018-07-12 08:20:29,526 ERROR ShareLibService:517 - SERVER[node1] org.apache.oozie.service.ServiceException: E0104: Could not fully initialize service [org.apache.oozie.service.ShareLibService], Not able to cache sharelib. An Admin needs to install the sharelib with oozie-setup.sh and issue the &#39;oozie admin&#39; CLI command to update the sharelib org.apache.oozie.service.ServiceException: E0104: Could not fully initialize service [org.apache.oozie.service.ShareLibService], Not able to cache sharelib. An Admin needs to install the sharelib with oozie-setup.sh and issue the &#39;oozie admin&#39; CLI command to update the sharelib at org.apache.oozie.service.ShareLibService.init(ShareLibService.jaxa:148) at org.apache.oozie.service.Services.setServiceInternal(Services.jaxa:386) at org.apache.oozie.service.Services.setService(Services.jaxa:372) at org.apache.oozie.service.Services.loadServices(Services.jaxa:304) at org.apache.oozie.service.Services.init(Services.jaxa:212) at org.apache.oozie.server.guice.ServicesProvider.get(ServicesProvider.jaxa:31) at org.apache.oozie.server.guice.ServicesProvider.get(ServicesProvider.jaxa:25) at com.google.inject.internal.BoundProviderFactory.get(BoundProviderFactory.jaxa:55) at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.jaxa:46) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.jaxa:1031) at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.jaxa:40) at com.google.inject.Scopes$1$1.get(Scopes.jaxa:65) at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.jaxa:40) at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.jaxa:38) at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.jaxa:62) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.jaxa:84) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.jaxa:254) at com.google.inject.internal.BoundProviderFactory.get(BoundProviderFactory.jaxa:53) at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.jaxa:46) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.jaxa:1031) at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.jaxa:40) at com.google.inject.Scopes$1$1.get(Scopes.jaxa:65) at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.jaxa:40) at com.google.inject.internal.SingleParameterInjector.inject(SingleParameterInjector.jaxa:38) at com.google.inject.internal.SingleParameterInjector.getAll(SingleParameterInjector.jaxa:62) at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.jaxa:84) at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.jaxa:254) at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.jaxa:978) at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.jaxa:1024) at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.jaxa:974) at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.jaxa:1013) at org.apache.oozie.server.EmbeddedOozieServer.main(EmbeddedOozieServer.jaxa:255) Caused by: java.lang.IllegalArgumentException: Wrong FS: hdfs://10.211.55.101/user/root/share/lib, expected: file:/// at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.jaxa:648) at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.jaxa:82) at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.jaxa:427) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.jaxa:1516) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.jaxa:1556) at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.jaxa:674) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.jaxa:1516) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.jaxa:1556) at org.apache.oozie.service.ShareLibService.getLatestLibPath(ShareLibService.jaxa:734) at org.apache.oozie.service.ShareLibService.updateShareLib(ShareLibService.jaxa:597) at org.apache.oozie.service.ShareLibService.init(ShareLibService.jaxa:138) ... 31 more  最后在oozie-site.xml添加了oozie.service.HadoopAccessorService.hadoop.configurations属性值*=/usr/local/hadoop/etc/hadoop/，终于可以找到共享库了：
vagrant@node1:~$ oozie admin -shareliblist [Available ShareLib] hive distcp mapreduce-streaming spark oozie hcatalog hive2 sqoop pig  参考 Oozie Quick Start
Oozie Command Line Interface Utilities
Oozie Examples
Oozie Share Lib does not exist error
</content>
    </entry>
    
     <entry>
        <title>[Spark] Spark读取HBase</title>
        <url>https://mryqu.github.io/post/spark_spark%E8%AF%BB%E5%8F%96hbase/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>spark</tag><tag>hbase</tag><tag>newapihadooprdd</tag><tag>javasparkcontext</tag><tag>sparkcontext</tag><tag>Java</tag>
        </tags>
        <content type="html">  Spark读取Hbase有以下几张方式： - Spark的JavaSparkContext.newAPIHadoopRDD / SparkContext.newAPIHadoopRDD方法 - HBase的hbase-spark - Hortonworks的Spark HBase Connector - Cloudera labs的SparkOnHBase 本文就Spark自带的方法进行示范和演示。
HBase数据库 Spark范例 HelloSparkHBase.java import org.apache.spark.SparkContext; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; import org.apache.spark.sql.SparkSession; import org.apache.spark.api.java.function.Function; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.mapreduce.TableInputFormat; import org.apache.hadoop.hbase.io.ImmutableBytesWritable; import org.apache.hadoop.hbase.util.Bytes; import scala.Tuple2; public class HelloSparkHBase { public static void main(String[] args) { try { Configuration conf = HBaseConfiguration.create(); conf.set(TableInputFormat.INPUT_TABLE, &amp;quot;student&amp;quot;); SparkSession spark = SparkSession .builder() .appName(&amp;quot; .config(&amp;quot;spark.some.config.option&amp;quot;, &amp;quot;some-value&amp;quot;) .getOrCreate(); try (Connection connection = ConnectionFactory.createConnection(conf); Admin admin = connection.getAdmin()) { if (admin.tableExists(TableName.valueOf(&amp;quot;student&amp;quot;))) { System.out.println(&amp;quot;Table student exist at HBase!&amp;quot;); SparkContext sc = spark.sparkContext(); JavaRDD&amp;lt;Tuple2&amp;lt;ImmutableBytesWritable, Result&amp;gt;&amp;gt; studentPairRDD = sc.newAPIHadoopRDD(conf, TableInputFormat.class, ImmutableBytesWritable.class, Result.class).to; JavaRDD&amp;lt;StudentRecord&amp;gt; studentRDD = studentPairRDD.map( new Function&amp;lt;Tuple2&amp;lt;ImmutableBytesWritable,Result&amp;gt;, StudentRecord&amp;gt;() { @Override public StudentRecord call( Tuple2&amp;lt;ImmutableBytesWritable, Result&amp;gt; tuple) throws Exception { Result result = tuple._2; String rowKey = Bytes.toString(result.getRow());//row key String state = Bytes.toString(result.getValue( Bytes.toBytes(&amp;quot;addr&amp;quot;), Bytes.toBytes(&amp;quot;state&amp;quot;))); String city = Bytes.toString(result.getValue( Bytes.toBytes(&amp;quot;addr&amp;quot;), Bytes.toBytes(&amp;quot;city&amp;quot;))); String dt = Bytes.toString(result.getValue( Bytes.toBytes(&amp;quot;score&amp;quot;), Bytes.toBytes(&amp;quot;date&amp;quot;))); String numb = Bytes.toString(result.getValue( Bytes.toBytes(&amp;quot;score&amp;quot;), Bytes.toBytes(&amp;quot;numb&amp;quot;))); StudentRecord record = new StudentRecord( rowKey, state, city, dt, numb); return record; } }); Dataset&amp;lt;Row&amp;gt; studentDF = spark.createDataFrame( studentRDD, StudentRecord.class); studentDF.show(); } } spark.close(); } catch (Exception e) { e.printStackTrace(); } } }  StudentRecord.java import java.io.Serializable; public class StudentRecord implements Serializable { private String name; private String addrState; private String addrCity; private String scoreDate; private String scoreNumb; public StudentRecord() { } public StudentRecord(String name, String addrState, String addrCity, String scoreDate, String scoreNumb) { this.name = name; this.addrState = addrState; this.addrCity = addrCity; this.scoreDate = scoreDate; this.scoreNumb = scoreNumb; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getAddrState() { return addrState; } public void setAddrState(String addrState) { this.addrState = addrState; } public String getAddrCity() { return addrCity; } public void setAddrCity(String addrCity) { this.addrCity = addrCity; } public String getScoreDate() { return scoreDate; } public void setScoreDate(String scoreDate) { this.scoreDate = scoreDate; } public String getScoreNumb() { return scoreNumb; } public void setScoreNumb(String scoreNumb) { this.scoreNumb = scoreNumb; } @Override public String toString() { return &amp;quot;StudentRecord{&amp;quot; &#43; &amp;quot;name=&#39;&amp;quot; &#43; name &#43; &#39;\&#39;&#39; &#43; &amp;quot;, addrState=&#39;&amp;quot; &#43; addrState &#43; &#39;\&#39;&#39; &#43; &amp;quot;, addrCity=&#39;&amp;quot; &#43; addrCity &#43; &#39;\&#39;&#39; &#43; &amp;quot;, scoreDate=&#39;&amp;quot; &#43; scoreDate &#43; &#39;\&#39;&#39; &#43; &amp;quot;, scoreNumb=&#39;&amp;quot; &#43; scoreNumb &#43; &#39;\&#39;&#39; &#43; &#39;}&#39;; } }  run.sh #!/bin/bash HADOOP_HOME=/usr/local/hadoop HBASE_HOME=/usr/local/hbase SPARK_HOME=/usr/local/spark CLASSPATH=.:$HBASE_HOME/conf:$(hbase classpath):/usr/local/spark/jars/* rm -rf *.jar rm -rf *.class javac -cp $CLASSPATH HelloSparkHBase.java jar cvfe hello-spark-hbase.jar HelloSparkHBase *.class spark-submit --class HelloSparkHBase --deploy-mode client --master local[2] --conf spark.executor.extraLibraryPath=$(hbase classpath) --jars $(echo /usr/local/hbase/lib/*.jar | tr &#39; &#39; &#39;,&#39;) ./hello-spark-hbase.jar  spark-submit的&amp;ndash;jars选项仅接受逗号分隔的jar文件，不接受目录扩展。
测试 hadoop@node50064:~/sparkhbasetest$./run.sh ...... Table student exist at HBase! ...... &#43;--------&#43;---------&#43;-----&#43;----------&#43;---------&#43; |addrCity|addrState| name| scoreDate|scoreNumb| &#43;--------&#43;---------&#43;-----&#43;----------&#43;---------&#43; | null| HeNan| alex| null| null| | chengdu| SiChuan|jason|2013-10-01| 1236| | null| ShanXi|nancy| null| 6666| | tianjin| TianJin| tina| null| null| &#43;--------&#43;---------&#43;-----&#43;----------&#43;---------&#43; ......  参考 Spark SQL, DataFrames and Datasets Guide
StackOverflow： How to read from hbase using spark
Reading Data from HBase table using Spark
StackOverflow： Spark spark-submit &amp;ndash;jars arguments wants comma list, how to declare a directory of jars?
Use Spark to read and write HBase data
Spark On Hbase Read (
Setting Up a Sample Application in HBase, Spark, and HDFS
Spark与HBase的整合
Spark读写Hbase的二种方式对比
Spark实战之读写HBase
</content>
    </entry>
    
     <entry>
        <title>[Sqoop]尝试Sqoop</title>
        <url>https://mryqu.github.io/post/sqoop_%E5%B0%9D%E8%AF%95sqoop/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Sqoop</category>
        </categories>
        <tags>
          <tag>sqoop</tag><tag>hadoop</tag><tag>DB</tag><tag>import</tag><tag>export</tag>
        </tags>
        <content type="html">  Sqoop简介 Apache Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(HDFS、Hive、HBase、Accumulo)与关系型数据库(MySQL、PostgreSQL、Oracle、Microsoft SQL、Netezza)间进行数据传输，例如可以将一个关系型数据库中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。
测试环境 在我使用GitHub: martinprobson/vagrant-hadoop-hive-spark通过Vagrant搭建的Hadoop 2.7.6 &#43; Hive 2.3.3 &#43; Spark 2.3.0虚拟机环境中已经安装了Sqoop，正好可以尝试一下。
使用 help命令 vagrant@node1:~$ sqoop help Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 18/07/10 02:17:36 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 usage: sqoop COMMAND [ARGS] Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version information See &#39;sqoop help COMMAND&#39; for information on a specific command.  list-databases命令 vagrant@node1:~$ sqoop list-databases \ &amp;gt; --connect jdbc:mysql://10.211.55.101:3306/ \ &amp;gt; --username root --password root Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 18/07/10 02:21:50 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/07/10 02:21:50 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/07/10 02:21:50 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Thu Jul 12 02:21:50 UTC 2018 WARN: Establishing SSL connection without server&#39;s identity verification is not recommended. According to MySQL 5.5.45&#43;, 5.6.26&#43; and 5.7.6&#43; requirements SSL connection must be established by default if explicit option isn&#39;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#39;false&#39;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. information_schema hive_metastore mysql performance_schema sys test  list-tables命令 vagrant@node1:~$ sqoop list-tables \ &amp;gt; --connect jdbc:mysql://10.211.55.101:3306/test \ &amp;gt; --username root --password root Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 18/07/10 02:26:13 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/07/10 02:26:13 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/07/10 02:26:14 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Thu Jul 12 02:26:14 UTC 2018 WARN: Establishing SSL connection without server&#39;s identity verification is not recommended. According to MySQL 5.5.45&#43;, 5.6.26&#43; and 5.7.6&#43; requirements SSL connection must be established by default if explicit option isn&#39;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#39;false&#39;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. emp  import命令(MySQL-&amp;gt;HDFS) 下列导入使用&amp;ndash;table选项，仅选择emp表中的所有记录：
vagrant@node1:~$ sqoop import --connect jdbc:mysql://10.211.55.101:3306/test --username root --password root --table emp --target-dir /user/ubuntu/emp -m 1 ...... vagrant@node1:~$ hadoop fs -ls /user/ubuntu/emp SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Found 2 items -rw-r--r-- 1 vagrant supergroup 0 2018-07-10 02:59 /user/ubuntu/emp/_SUCCESS -rw-r--r-- 1 vagrant supergroup 810 2018-07-10 02:59 /user/ubuntu/emp/part-m-00000 vagrant@node1:~$ vagrant@node1:~$ hadoop fs -cat /user/ubuntu/emp/part-m-00000 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 7369,SMITH,CLERK,7902,1980-12-17,null,800.0,null 7499,ALLEN,SALESMAN,7698,1981-02-20,null,1600.0,300 7521,WARD,SALESMAN,7698,1981-02-22,null,1250.0,500 7566,JONES,MANAGER,7839,1981-04-02,null,2975.0,null 7654,MARTIN,SALESMAN,7698,1981-09-28,null,1250.0,1400 7698,BLAKE,MANAGER,7839,1981-05-01,null,2850.0,null 7782,CLARK,MANAGER,7839,1981-06-09,null,2450.0,null 7788,SCOTT,ANALYST,7566,1982-12-09,null,3000.0,null 7839,KING,PRESIDENT,null,1981-11-17,null,5000.0,null 7844,TURNER,SALESMAN,7698,1981-09-08,null,1500.0,0 7876,ADAMS,CLERK,7788,1983-01-12,null,1100.0,null  注：
* &amp;ndash;target-dir指定的HDFS目标目录必须不存在，否则导入失败，也可以使用&amp;ndash;delete-target-dir选项删除可能存在的目标目录； * 如果MySQL中待导出的表没有设定主键，提示我们使用把&amp;ndash;split-by或者把参数-m设置为1，即只有一个map运行，缺点是不能并行map录入数据。
下列导入使用&amp;ndash;query选项，仅选择job为CLERK的记录:
vagrant@node1:~$ sqoop import \ &amp;gt; --connect jdbc:mysql://10.211.55.101:3306/test \ &amp;gt; --username root \ &amp;gt; --password root \ &amp;gt; --query &amp;quot;select * from emp where job=&#39;CLERK&#39; and \$CONDITIONS&amp;quot; \ &amp;gt; --fields-terminated-by &amp;quot;,&amp;quot; \ &amp;gt; --lines-terminated-by &amp;quot;\n&amp;quot; \ &amp;gt; --delete-target-dir \ &amp;gt; --target-dir /user/ubuntu/emp1 \ &amp;gt; -m 1 vagrant@node1:~$ hadoop fs -ls /user/ubuntu/emp1 ...... Found 2 items -rw-r--r-- 1 vagrant supergroup 0 2018-07-10 06:17 /user/ubuntu/emp1/_SUCCESS -rw-r--r-- 1 vagrant supergroup 199 2018-07-10 06:17 /user/ubuntu/emp1/part-m-00000 vagrant@node1:~$ hadoop fs -cat /user/ubuntu/emp1/part-m-00000 ...... 7369,SMITH,CLERK,7902,1980-12-17,null,800.0,null 7876,ADAMS,CLERK,7788,1983-01-12,null,1100.0,null 7900,JAMES,CLERK,7698,1981-12-03,null,950.0,null 7934,MILLER,CLERK,7782,1982-01-23,null,1300.0,null  注：
* 使用&amp;ndash;query这种任意格式查询必须指定&amp;ndash;target-dir参数； * 使用&amp;ndash;query这种任意格式查询仅用于简单查询，WHERE子句中不能有OR操作符，必须有$CONDITIONS以用于给多个map任务划分任务范围。
import命令(MySQL-&amp;gt;Hive) 第一次运行import，发现ERROR exec.DDLTask: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor(LLcom/fasterxml/jackson/databind/ObjectReader;错误，根据参看三进行修复：
vagrant@node1:/usr/local/sqoop/lib$ ls jackson* jackson-annotations-2.3.1.jar jackson-core-2.3.1.jar jackson-core-asl-1.9.13.jar jackson-databind-2.3.1.jar jackson-mapper-asl-1.9.13.jar vagrant@node1:/usr/local/sqoop/lib$ rename -e &#39;s/jar/jar\.bak/&#39; jackson*.jar vagrant@node1:/usr/local/sqoop/lib$ ls -1 jackson* jackson-annotations-2.3.1.jar.bak jackson-core-2.3.1.jar.bak jackson-core-asl-1.9.13.jar.bak jackson-databind-2.3.1.jar.bak jackson-mapper-asl-1.9.13.jar.bak vagrant@node1:/usr/local/sqoop/lib$ cp /usr/local/hive/lib/jackson*.jar . vagrant@node1:/usr/local/sqoop/lib$ ls -1 jackson* jackson-annotations-2.3.1.jar.bak jackson-annotations-2.6.0.jar jackson-core-2.3.1.jar.bak jackson-core-2.6.5.jar jackson-core-asl-1.9.13.jar.bak jackson-databind-2.3.1.jar.bak jackson-databind-2.6.5.jar jackson-dataformat-smile-2.4.6.jar jackson-datatype-guava-2.4.6.jar jackson-datatype-joda-2.4.6.jar jackson-jaxrs-1.9.13.jar jackson-jaxrs-base-2.4.6.jar jackson-jaxrs-json-provider-2.4.6.jar jackson-jaxrs-smile-provider-2.4.6.jar jackson-mapper-asl-1.9.13.jar.bak jackson-module-jaxb-annotations-2.4.6.jar jackson-xc-1.9.13.jar  下列导入使用&amp;ndash;columns、&amp;ndash;where和&amp;ndash;table选项，仅选择job为MANAGER的记录的empno、ename和hiredate列。
vagrant@node1:~$ sqoop import \ &amp;gt; --connect jdbc:mysql://10.211.55.101:3306/test \ &amp;gt; --username root \ &amp;gt; --password root \ &amp;gt; --table emp \ &amp;gt; --columns &amp;quot;empno,ename,hiredate&amp;quot; \ &amp;gt; --where &amp;quot;job=&#39;MANAGER&#39;&amp;quot; \ &amp;gt; --fields-terminated-by &amp;quot;\t&amp;quot; \ &amp;gt; --lines-terminated-by &amp;quot;\n&amp;quot; \ &amp;gt; --hive-import \ &amp;gt; --hive-overwrite \ &amp;gt; --create-hive-table \ &amp;gt; --delete-target-dir \ &amp;gt; --hive-database default \ &amp;gt; --hive-table emp2 \ &amp;gt; -m 1 vagrant@node1:~$ hadoop fs -ls /user/hive/warehouse/emp2 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Found 2 items -rwxr-xr-x 1 vagrant supergroup 0 2018-07-10 04:23 /user/hive/warehouse/emp2/_SUCCESS -rwxr-xr-x 1 vagrant supergroup 66 2018-07-10 04:23 /user/hive/warehouse/emp2/part-m-00000 vagrant@node1:~$ hadoop fs -cat /user/hive/warehouse/emp2/part-m-00000 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 7566 JONES 1981-04-02 7698 BLAKE 1981-05-01 7782 CLARK 1981-06-09 vagrant@node1:~$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Connecting to jdbc:hive2:// 18/07/10 04:25:48 [main]: WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory. Connected to: Apache Hive (version 2.3.3) Driver: Hive JDBC (version 2.3.3) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 2.3.3 by Apache Hive hive&amp;gt; select * from default.emp2; OK 7566 JONES 1981-04-02 7698 BLAKE 1981-05-01 7782 CLARK 1981-06-09 3 rows selected (2.578 seconds)  注：Sqoop导入会创建不存在的表，但是不会创建不存在数据库。
eval命令 vagrant@node1:~$ sqoop eval --connect jdbc:mysql://10.211.55.101:3306/test --username root --password root --query &amp;quot;select empno, ename, job, deptno from emp&amp;quot; ...... --------------------------------------------------------------------------- | empno | ename | job | deptno | --------------------------------------------------------------------------- | 7369 | SMITH | CLERK | (null) | | 7499 | ALLEN | SALESMAN | 300 | | 7521 | WARD | SALESMAN | 500 | | 7566 | JONES | MANAGER | (null) | | 7654 | MARTIN | SALESMAN | 1400 | | 7698 | BLAKE | MANAGER | (null) | | 7782 | CLARK | MANAGER | (null) | | 7788 | SCOTT | ANALYST | (null) | | 7839 | KING | PRESIDENT | (null) | | 7844 | TURNER | SALESMAN | 0 | | 7876 | ADAMS | CLERK | (null) | | 7900 | JAMES | CLERK | (null) | | 7902 | FORD | ANALYST | (null) | | 7934 | MILLER | CLERK | (null) | | 7988 | KATY | ANALYST | (null) | | 7987 | JULIA | ANALYST | (null) | --------------------------------------------------------------------------- vagrant@node1:~$ sqoop eval --connect jdbc:mysql://10.211.55.101:3306/test --username root --password root --query &amp;quot;select empno, ename, job, deptno from emp where deptno IS NOT NULL&amp;quot; ...... --------------------------------------------------------------------------- | empno | ename | job | deptno | --------------------------------------------------------------------------- | 7499 | ALLEN | SALESMAN | 300 | | 7521 | WARD | SALESMAN | 500 | | 7654 | MARTIN | SALESMAN | 1400 | | 7844 | TURNER | SALESMAN | 0 | ---------------------------------------------------------------------------  export命令(HDFS-&amp;gt;MySQL) export执行之前必须现在MySQL中创建目标表，否则报ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &amp;lsquo;test.emp_hdfs&amp;rsquo; doesn&amp;rsquo;t exist错误。
vagrant@node1:~$ apt-get install mysql-client vagrant@node1:~$ mysql -u root -p test mysql&amp;gt; create table emp_hdfs like emp; Query OK, 0 rows affected (0.00 sec) vagrant@node1:~$ sqoop export \ &amp;gt; --connect jdbc:mysql://10.211.55.101:3306/test \ &amp;gt; --username root \ &amp;gt; --password root \ &amp;gt; --table emp_hdfs \ &amp;gt; --input-fields-terminated-by &#39;,&#39; \ &amp;gt; --export-dir /user/ubuntu/emp/ mysql&amp;gt; select * from emp_hdfs; &#43;-------&#43;--------&#43;-----------&#43;------&#43;------------&#43;--------&#43;------&#43;--------&#43; | empno | ename | job | mgr | hiredate | salary | comm | deptno | &#43;-------&#43;--------&#43;-----------&#43;------&#43;------------&#43;--------&#43;------&#43;--------&#43; | 7902 | FORD | ANALYST | 7566 | 1981-12-03 | NULL | 3000 | NULL | | 7934 | MILLER | CLERK | 7782 | 1982-01-23 | NULL | 1300 | NULL | | 7988 | KATY | ANALYST | 7566 | NULL | NULL | 1500 | NULL | | 7987 | JULIA | ANALYST | 7566 | NULL | NULL | 1500 | NULL | | 7369 | SMITH | CLERK | 7902 | 1980-12-17 | NULL | 800 | NULL | | 7499 | ALLEN | SALESMAN | 7698 | 1981-02-20 | NULL | 1600 | 300 | | 7521 | WARD | SALESMAN | 7698 | 1981-02-22 | NULL | 1250 | 500 | | 7566 | JONES | MANAGER | 7839 | 1981-04-02 | NULL | 2975 | NULL | | 7654 | MARTIN | SALESMAN | 7698 | 1981-09-28 | NULL | 1250 | 1400 | | 7698 | BLAKE | MANAGER | 7839 | 1981-05-01 | NULL | 2850 | NULL | | 7782 | CLARK | MANAGER | 7839 | 1981-06-09 | NULL | 2450 | NULL | | 7788 | SCOTT | ANALYST | 7566 | 1982-12-09 | NULL | 3000 | NULL | | 7839 | KING | PRESIDENT | NULL | 1981-11-17 | NULL | 5000 | NULL | | 7844 | TURNER | SALESMAN | 7698 | 1981-09-08 | NULL | 1500 | 0 | | 7876 | ADAMS | CLERK | 7788 | 1983-01-12 | NULL | 1100 | NULL | | 7900 | JAMES | CLERK | 7698 | 1981-12-03 | NULL | 950 | NULL | &#43;-------&#43;--------&#43;-----------&#43;------&#43;------------&#43;--------&#43;------&#43;--------&#43; 16 rows in set (0.00 sec)  Hive导出到MySQL也是指定HDFS地址的方式完成的：
mysql&amp;gt; CREATE TABLE emp_hive AS (SELECT empno,ename,hiredate FROM emp); mysql&amp;gt; truncate table emp_hive; vagrant@node1:~$ sqoop-export \ &amp;gt; --connect jdbc:mysql://10.211.55.101:3306/test \ &amp;gt; --username root \ &amp;gt; --password root \ &amp;gt; --table emp_hive \ &amp;gt; --input-fields-terminated-by &#39;\t&#39; \ &amp;gt; --lines-terminated-by &amp;quot;\n&amp;quot; \ &amp;gt; --export-dir /user/hive/warehouse/emp2 ...... mysql&amp;gt; select * from emp_hive; &#43;-------&#43;-------&#43;------------&#43; | empno | ename | hiredate | &#43;-------&#43;-------&#43;------------&#43; | 7698 | BLAKE | 1981-05-01 | | 7566 | JONES | 1981-04-02 | | 7782 | CLARK | 1981-06-09 | &#43;-------&#43;-------&#43;------------&#43; 3 rows in set (0.00 sec)  codegen命令 生成用于封装和解析导入记录的Java代码。
增量导入/导出 使用&amp;ndash;check-column (col)、&amp;ndash;incremental (mode)和&amp;ndash;last-value (value)参数完成。
参考 Sqoop User Guide
Sqoop、SQOOP-3274 hive import job throws AccessControlException for hive 2
hive&#43;sqoop jackson因版本不一致导致java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.
</content>
    </entry>
    
     <entry>
        <title>[Zeppelin] 尝试Zeppelin</title>
        <url>https://mryqu.github.io/post/zeppelin_%E5%B0%9D%E8%AF%95zeppelin/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Zeppelin</category>
        </categories>
        <tags>
          <tag>zeppelin</tag><tag>hadoop</tag><tag>spark</tag><tag>sql</tag>
        </tags>
        <content type="html">  Zeppelin简介 Apache Zeppelin是一个让交互式数据分析变得可行的基于网页的开源框架。Zeppelin提供了数据捏取、发现、分析、可视化与协作等功能。 Zeppelin 是一个提供交互数据分析且基于Web的笔记本。方便你做出可数据驱动的、可交互且可协作的精美文档，并且支持多种语言，包括 Scala(使用 Apache Spark)、Python(Apache Spark)、SparkSQL、 Hive、 Markdown、Shell等等。
试验环境搭建 跟之前的博文[Spark] 使用Spark2.30读写Hive2.3.3一样，本文的环境继续使用GitHub: martinprobson/vagrant-hadoop-hive-spark通过Vagrant搭建了一个Hadoop 2.7.6 &#43; Hive 2.3.3 &#43; Spark 2.3.0的虚拟机环境。不过当前scripts/common.sh中ZEPPELIN_VERSION=0.7.2，而Zeppelin 0.7.2已不可访问，需要改成最新版0.8.0。 按照GitHub: martinprobson/vagrant-hadoop-hive-spark说明执行zeppelin-daemon.sh start，结果说权限不足，因此我只好兜一圈开启Zeppelin守护进程。
vagrant@node1:~$ zeppelin-daemon.sh start find: File system loop detected; ‘/home/ubuntu/zeppelin/zeppelin-0.8.0-bin-netinst’ is part of the same file system loop as ‘/home/ubuntu/zeppelin’. Pid dir doesn&#39;t exist, create /home/ubuntu/zeppelin/run mkdir: cannot create directory ‘/home/ubuntu/zeppelin/run’: Permission denied /home/ubuntu/zeppelin/bin/zeppelin-daemon.sh: line 187: /home/ubuntu/zeppelin/logs/zeppelin-vagrant-node1.out: Permission denied /home/ubuntu/zeppelin/bin/zeppelin-daemon.sh: line 189: /home/ubuntu/zeppelin/logs/zeppelin-vagrant-node1.out: Permission denied /home/ubuntu/zeppelin/bin/zeppelin-daemon.sh: line 196: /home/ubuntu/zeppelin/run/zeppelin-vagrant-node1.pid: No such file or directory cat: /home/ubuntu/zeppelin/run/zeppelin-vagrant-node1.pid: No such file or directory Zeppelin process died [FAILED] vagrant@node1:~$ sudo -sE root@node1:~# su - ubuntu To run a command as administrator (user &amp;quot;root&amp;quot;), use &amp;quot;sudo &amp;quot;. See &amp;quot;man sudo_root&amp;quot; for details. ubuntu@node1:~$ pwd /home/ubuntu ubuntu@node1:~$ ls zeppelin zeppelin-0.8.0-bin-netinst ubuntu@node1:~$ zeppelin-daemon.sh start find: File system loop detected; ‘/home/ubuntu/zeppelin/zeppelin-0.8.0-bin-netinst’ is part of the same file system loop as ‘/home/ubuntu/zeppelin’. Pid dir doesn&#39;t exist, create /home/ubuntu/zeppelin/run Zeppelin start [ OK ]  Spark引擎分析HDFS文件实验  进入Zeppelin UI，点击“Create new note”创建新的笔记，名字叫Spark Sample。
 将测试文件放到hdfs集群上：
 vagrant@node1:~$ wget http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip vagrant@node1:~$ unzip bank.zip vagrant@node1:~$ rm -rf bank.zip vagrant@node1:~$ hadoop fs -mkdir /test vagrant@node1:~$ hadoop fs -put ./bank*.* hdfs://10.211.55.101/test vagrant@node1:~$ hadoop fs -ls /test SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Found 3 items -rw-r--r-- 1 vagrant supergroup 4610348 2018-07-09 05:24 /test/bank-full.csv -rw-r--r-- 1 vagrant supergroup 3864 2018-07-09 05:24 /test/bank-names.txt -rw-r--r-- 1 vagrant supergroup 461474 2018-07-09 05:24 /test/bank.csv  执行下列修改过的Zeppelin指南示例代码：
val bankText = sc.textFile(&amp;quot;hdfs://10.211.55.101/test/bank-full.csv&amp;quot;) case class Bank(age:Integer, job:String, marital : String, education : String, balance : Integer) // split each line, filter out header (starts with &amp;quot;age&amp;quot;), and map it into Bank case class val bank = bankText.map(s=&amp;gt;s.split(&amp;quot;;&amp;quot;)).filter(s=&amp;gt;s(0)!=&amp;quot;\&amp;quot;age\&amp;quot;&amp;quot;).map( s=&amp;gt;Bank(s(0).toInt, s(1).replaceAll(&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;), s(2).replaceAll(&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;), s(3).replaceAll(&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;), s(5).replaceAll(&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;).toInt ) ) // convert to DataFrame and create temporal table bank.toDF().registerTempTable(&amp;quot;bank&amp;quot;)  数据分析结果可以以表、条形图、饼图、面积图、曲线图、散点图的形式表现。
  参考 Zeppelin Tutorial
</content>
    </entry>
    
     <entry>
        <title>[Oozie] Oozie构建问题</title>
        <url>https://mryqu.github.io/post/oozie_oozie%E6%9E%84%E5%BB%BA%E9%97%AE%E9%A2%98/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Oozie</category>
        </categories>
        <tags>
          <tag>oozie</tag><tag>build</tag><tag>error</tag><tag>log4j</tag><tag>getlevel</tag>
        </tags>
        <content type="html"> 想定制Oozie构建，结果log4j总出错，移除了pig、sqoop和hive就好了。
root@node1:~# /vagrant/resources/oozie-5.0.0/bin/mkdistro.sh -DskipTests -Puber -Dhadoop.version=2.7.6 -Ptez -Dpig.version=0.17.0 -Dsqoop.version=1.4.7 -Dhive.version=2.3.3 -Dtez.version=0.9.1 ...... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.7.0:testCompile (default-testCompile) on project oozie-core: Compilation failure: Compilation failure: [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/sla/TestSLACalculatorMemory.java:[836,48] cannot find symbol [ERROR] symbol: method getLevel() [ERROR] location: variable firstLogEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/sla/TestSLACalculatorMemory.java:[837,33] cannot find symbol [ERROR] symbol: method getMessage() [ERROR] location: variable firstLogEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/sla/TestSLACalculatorMemory.java:[838,79] cannot find symbol [ERROR] symbol: method getLoggerName() [ERROR] location: variable firstLogEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/service/TestEventHandlerService.java:[213,47] cannot find symbol [ERROR] symbol: method getLevel() [ERROR] location: variable logEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/service/TestEventHandlerService.java:[214,32] cannot find symbol [ERROR] symbol: method getMessage() [ERROR] location: variable logEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/service/TestEventHandlerService.java:[215,82] cannot find symbol [ERROR] symbol: method getLoggerName() [ERROR] location: variable logEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/service/TestEventHandlerService.java:[221,47] cannot find symbol [ERROR] symbol: method getLevel() [ERROR] location: variable logEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/service/TestEventHandlerService.java:[222,32] cannot find symbol [ERROR] symbol: method getMessage() [ERROR] location: variable logEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/service/TestEventHandlerService.java:[231,32] cannot find symbol [ERROR] symbol: method getMessage() [ERROR] location: variable logEntry of type org.apache.log4j.spi.LoggingEvent [ERROR] /vagrant/resources/oozie-5.0.0/core/src/test/java/org/apache/oozie/service/TestEventHandlerService.java:[240,32] cannot find symbol [ERROR] symbol: method getMessage() [ERROR] location: variable logEntry of type org.apache.log4j.spi.LoggingEvent ...... root@node1:~# /vagrant/resources/oozie-5.0.0/bin/mkdistro.sh -Puber -Dhadoop.version=2.7.6 -Ptez -Dtez.version=0.9.1 -DskipTests ...... [INFO] Reactor Summary: [INFO] [INFO] Apache Oozie Main ........................ SUCCESS [ 1.315 s] [INFO] Apache Oozie Client ...................... SUCCESS [04:51 min] [INFO] Apache Oozie Share Lib Oozie ............. SUCCESS [ 10.467 s] [INFO] Apache Oozie Share Lib HCatalog .......... SUCCESS [ 6.158 s] [INFO] Apache Oozie Share Lib Distcp ............ SUCCESS [ 1.704 s] [INFO] Apache Oozie Core ........................ SUCCESS [03:20 min] [INFO] Apache Oozie Share Lib Streaming ......... SUCCESS [ 5.225 s] [INFO] Apache Oozie Share Lib Pig ............... SUCCESS [03:04 min] [INFO] Apache Oozie Share Lib Hive .............. SUCCESS [ 16.510 s] [INFO] Apache Oozie Share Lib Hive 2 ............ SUCCESS [ 10.609 s] [INFO] Apache Oozie Share Lib Sqoop ............. SUCCESS [ 3.858 s] [INFO] Apache Oozie Examples .................... SUCCESS [ 14.172 s] [INFO] Apache Oozie Share Lib Spark ............. SUCCESS [ 19.237 s] [INFO] Apache Oozie Share Lib ................... SUCCESS [ 54.691 s] [INFO] Apache Oozie Docs ........................ SUCCESS [09:18 min] [INFO] Apache Oozie WebApp ...................... SUCCESS [09:03 min] [INFO] Apache Oozie Tools ....................... SUCCESS [ 7.231 s] [INFO] Apache Oozie MiniOozie ................... SUCCESS [ 2.782 s] [INFO] Apache Oozie Server ...................... SUCCESS [ 17.591 s] [INFO] Apache Oozie Distro ...................... SUCCESS [13:35 min] [INFO] Apache Oozie ZooKeeper Security Tests .... SUCCESS [ 3.603 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 46:10 min [INFO] Finished at: 2018-07-04T08:44:26&#43;00:00 [INFO] Final Memory: 166M/986M [INFO] ------------------------------------------------------------------------ Oozie distro created, DATE[2018.07.04-07:58:14GMT] VC-REV[unavailable], available at [/vagrant/resources/oozie-5.0.0/distro/target]  </content>
    </entry>
    
     <entry>
        <title>[Spark] 使用Spark2.30读写MySQL</title>
        <url>https://mryqu.github.io/post/spark_%E4%BD%BF%E7%94%A8spark2.30%E8%AF%BB%E5%86%99mysql/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>spark</tag><tag>sql</tag><tag>mysql</tag><tag>hive</tag><tag>Java</tag>
        </tags>
        <content type="html">  本博文是[Spark] 使用Spark2.30读写Hive2.3.3的姊妹篇，环境及Java项目也是使用上一博文中的。
Spark项目 目录结构 vagrant@node1:~/HelloSparkHive$ ls build build.gradle src vagrant@node1:~/HelloSparkHive$ rm -rf build vagrant@node1:~/HelloSparkHive$ tree . ├── build.gradle └── src └── main └── java └── com └── yqu └── sparkhive ├── HelloSparkHiveDriver.java └── HelloSparkMysqlDriver.java 6 directories, 3 files  src/main/java/com/yqu/sparkhive/HelloSparkMysqlDriver.java 该范例加载Hive中的emp表，存储到MySQL的test数据库中，然后读取MySQL数据库加载emp表，由此完成MySQL读写示例。
package com.yqu.sparkhive; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; import org.apache.spark.sql.SparkSession; import java.io.File; import java.sql.*; public class HelloSparkMysqlDriver { private static boolean setup() { Connection conn = null; Statement stmt = null; try { Class.forName(&amp;quot;com.mysql.jdbc.Driver&amp;quot;); conn = DriverManager.getConnection( &amp;quot;jdbc:mysql://10.211.55.101:3306&amp;quot;, &amp;quot;root&amp;quot;,&amp;quot;root&amp;quot;); stmt = conn.createStatement(); stmt.executeUpdate(&amp;quot;CREATE DATABASE IF NOT EXISTS test&amp;quot;); stmt.executeUpdate(&amp;quot;DROP TABLE IF EXISTS test.emp&amp;quot;); ResultSet rs = stmt.executeQuery(&amp;quot;SHOW DATABASES&amp;quot;); while(rs.next()){ System.out.println(rs.getString(&amp;quot;Database&amp;quot;)); } rs.close(); stmt.close(); conn.close(); return true; } catch (ClassNotFoundException e) { System.out.println(&amp;quot;Can not find com.mysql.jdbc.Driver!&amp;quot;); } catch(SQLException se){ //Handle errors for JDBC se.printStackTrace(); } finally { try{ if(stmt!=null) stmt.close(); } catch(SQLException se){ se.printStackTrace(); } try{ if(conn!=null) conn.close(); } catch(SQLException se){ se.printStackTrace(); } } return false; } public static void main(String args[]) { if(setup()) { // warehouseLocation points to the default location // for managed databases and tables String warehouseLocation = new File(&amp;quot;spark-warehouse&amp;quot;). getAbsolutePath(); SparkSession spark = SparkSession .builder() .appName(&amp;quot; .config(&amp;quot;spark.sql.warehouse.dir&amp;quot;, warehouseLocation) .enableHiveSupport() .getOrCreate(); Dataset&amp;lt;Row&amp;gt; hiveDF = spark.sql(&amp;quot;SELECT * FROM emp&amp;quot;); // Saving data to a JDBC source hiveDF.write() .format(&amp;quot;jdbc&amp;quot;) .option(&amp;quot;url&amp;quot;, &amp;quot;jdbc:mysql://10.211.55.101:3306/test&amp;quot;) .option(&amp;quot;driver&amp;quot;, &amp;quot;com.mysql.jdbc.Driver&amp;quot;) .option(&amp;quot;dbtable&amp;quot;, &amp;quot;emp&amp;quot;) .option(&amp;quot;user&amp;quot;, &amp;quot;root&amp;quot;) .option(&amp;quot;password&amp;quot;, &amp;quot;root&amp;quot;) .save(); Dataset&amp;lt;Row&amp;gt; jdbcDF = spark.read() .format(&amp;quot;jdbc&amp;quot;) .option(&amp;quot;url&amp;quot;, &amp;quot;jdbc:mysql://10.211.55.101:3306/test&amp;quot;) .option(&amp;quot;driver&amp;quot;, &amp;quot;com.mysql.jdbc.Driver&amp;quot;) .option(&amp;quot;dbtable&amp;quot;, &amp;quot;emp&amp;quot;) .option(&amp;quot;user&amp;quot;, &amp;quot;root&amp;quot;) .option(&amp;quot;password&amp;quot;, &amp;quot;root&amp;quot;) .load(); jdbcDF.show(); spark.close(); } else { System.out.println(&amp;quot;MySQL database is not ready!&amp;quot;); } } }  构建及测试 vagrant@node1:~/HelloSparkHive$ gradle build jar BUILD SUCCESSFUL in 0s 2 actionable tasks: 2 executed vagrant@node1:~/HelloSparkHive$ spark-submit --class com.yqu.sparkhive.HelloSparkMysqlDriver --deploy-mode client --master local[2] --jars /usr/local/java/lib/ext/mysql-connector-java-5.1.40.jar /home/vagrant/HelloSparkHive/build/libs/hello-spark-hive-0.1.0.jar ...... Databases: information_schema hive_metastore mysql performance_schema sys test ...... 2018-07-03 10:27:49 INFO DAGScheduler:54 - Job 1 finished: show at HelloSparkMysqlDriver.java:87, took 0.078809 s &#43;-----&#43;------&#43;---------&#43;----&#43;----------&#43;------&#43;------&#43;------&#43; |empno| ename| job| mgr| hiredate|salary| comm|deptno| &#43;-----&#43;------&#43;---------&#43;----&#43;----------&#43;------&#43;------&#43;------&#43; | 7369| SMITH| CLERK|7902|1980-12-17| null| 800.0| null| | 7499| ALLEN| SALESMAN|7698|1981-02-20| null|1600.0| 300| | 7521| WARD| SALESMAN|7698|1981-02-22| null|1250.0| 500| | 7566| JONES| MANAGER|7839|1981-04-02| null|2975.0| null| | 7654|MARTIN| SALESMAN|7698|1981-09-28| null|1250.0| 1400| | 7698| BLAKE| MANAGER|7839|1981-05-01| null|2850.0| null| | 7782| CLARK| MANAGER|7839|1981-06-09| null|2450.0| null| | 7788| SCOTT| ANALYST|7566|1982-12-09| null|3000.0| null| | 7839| KING|PRESIDENT|null|1981-11-17| null|5000.0| null| | 7844|TURNER| SALESMAN|7698|1981-09-08| null|1500.0| 0| | 7876| ADAMS| CLERK|7788|1983-01-12| null|1100.0| null| | 7900| JAMES| CLERK|7698|1981-12-03| null| 950.0| null| | 7902| FORD| ANALYST|7566|1981-12-03| null|3000.0| null| | 7934|MILLER| CLERK|7782|1982-01-23| null|1300.0| null| | 7988| KATY| ANALYST|7566| NULL| null|1500.0| null| | 7987| JULIA| ANALYST|7566| NULL| null|1500.0| null| &#43;-----&#43;------&#43;---------&#43;----&#43;----------&#43;------&#43;------&#43;------&#43; ...... vagrant@node1:~/HelloSparkHive$ spark-shell --jars /usr/local/java/lib/ext/mysql-connector-java-5.1.40.jar ...... scala&amp;gt; spark. | read. | format(&amp;quot;jdbc&amp;quot;). | option(&amp;quot;url&amp;quot;, &amp;quot;jdbc:mysql://10.211.55.101:3306/test&amp;quot;). | option(&amp;quot;driver&amp;quot;, &amp;quot;com.mysql.jdbc.Driver&amp;quot;). | option(&amp;quot;dbtable&amp;quot;, &amp;quot;emp&amp;quot;). | option(&amp;quot;user&amp;quot;, &amp;quot;root&amp;quot;). | option(&amp;quot;password&amp;quot;, &amp;quot;root&amp;quot;). | load().show() ...... &#43;-----&#43;------&#43;---------&#43;----&#43;----------&#43;------&#43;------&#43;------&#43; |empno| ename| job| mgr| hiredate|salary| comm|deptno| &#43;-----&#43;------&#43;---------&#43;----&#43;----------&#43;------&#43;------&#43;------&#43; | 7369| SMITH| CLERK|7902|1980-12-17| null| 800.0| null| | 7499| ALLEN| SALESMAN|7698|1981-02-20| null|1600.0| 300| | 7521| WARD| SALESMAN|7698|1981-02-22| null|1250.0| 500| | 7566| JONES| MANAGER|7839|1981-04-02| null|2975.0| null| | 7654|MARTIN| SALESMAN|7698|1981-09-28| null|1250.0| 1400| | 7698| BLAKE| MANAGER|7839|1981-05-01| null|2850.0| null| | 7782| CLARK| MANAGER|7839|1981-06-09| null|2450.0| null| | 7788| SCOTT| ANALYST|7566|1982-12-09| null|3000.0| null| | 7839| KING|PRESIDENT|null|1981-11-17| null|5000.0| null| | 7844|TURNER| SALESMAN|7698|1981-09-08| null|1500.0| 0| | 7876| ADAMS| CLERK|7788|1983-01-12| null|1100.0| null| | 7900| JAMES| CLERK|7698|1981-12-03| null| 950.0| null| | 7902| FORD| ANALYST|7566|1981-12-03| null|3000.0| null| | 7934|MILLER| CLERK|7782|1982-01-23| null|1300.0| null| | 7988| KATY| ANALYST|7566| NULL| null|1500.0| null| | 7987| JULIA| ANALYST|7566| NULL| null|1500.0| null| &#43;-----&#43;------&#43;---------&#43;----&#43;----------&#43;------&#43;------&#43;------&#43; scala&amp;gt; :quit vagrant@node1:~/HelloSparkHive$  参考 Spark SQL, DataFrames and Datasets Guide
[Spark] 使用Spark2.30读写Hive2.3.3
</content>
    </entry>
    
     <entry>
        <title>[Spark] 使用Spark2.30读写Hive2.3.3</title>
        <url>https://mryqu.github.io/post/spark_%E4%BD%BF%E7%94%A8spark2.30%E8%AF%BB%E5%86%99hive2.3.3/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>hive</tag><tag>spark</tag><tag>sql</tag><tag>Java</tag>
        </tags>
        <content type="html">  试验环境搭建 安装Spark环境 犯懒，直接使用GitHub: martinprobson/vagrant-hadoop-hive-spark通过Vagrant搭建了一个Hadoop 2.7.6 &#43; Hive 2.3.3 &#43; Spark 2.3.0的虚拟机环境。
在Hive上加载emp表 hive&amp;gt; create table emp (empno int, ename string, job string, mgr int, hiredate string, salary double, comm double, deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;|&#39; ; hive&amp;gt; LOAD DATA LOCAL INPATH &#39;/usr/local/hive/examples/files/emp2.txt&#39; OVERWRITE INTO TABLE emp;  安装Gradle 按照Gradle用户手册中的方式手工安装Gradle：
vagrant@node1:~$ export GRADLE_HOME=/opt/gradle/gradle-4.8.1 vagrant@node1:~$ export PATH=$PATH:$GRADLE_HOME/bin vagrant@node1:~$ gradle -v Welcome to Gradle 4.8.1! Here are the highlights of this release: - Dependency locking - Maven Publish and Ivy Publish plugins improved and marked stable - Incremental annotation processing enhancements - APIs to configure tasks at creation time For more details see https://docs.gradle.org/4.8.1/release-notes.html ------------------------------------------------------------ Gradle 4.8.1 ------------------------------------------------------------ Build time: 2018-06-21 07:53:06 UTC Revision: 0abdea078047b12df42e7750ccba34d69b516a22 Groovy: 2.4.12 Ant: Apache Ant(TM) version 1.9.11 compiled on March 23 2018 JVM: 1.8.0_171 (Oracle Corporation 25.171-b11) OS: Linux 4.4.0-128-generic amd64  Spark项目 目录结构 vagrant@node1:~/HelloSparkHive$ tree . ├── build.gradle └── src └── main └── java └── com └── yqu └── sparkhive └── HelloSparkHiveDriver.java 6 directories, 2 files  build.gradle apply plugin: &#39;java-library&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; repositories { jcenter() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compileOnly &#39;org.apache.spark:spark-core_2.11:2.3.0&#39; compileOnly &#39;org.apache.spark:spark-sql_2.11:2.3.0&#39; testImplementation &#39;org.apache.spark:spark-core_2.11:2.3.0&#39;,&#39;junit:junit:4.12&#39; } jar { baseName = &#39;hello-spark-hive&#39; version = &#39;0.1.0&#39; }  src/main/java/com/yqu/sparkhive/HelloSparkHiveDriver.java 该范例仅将emp表加载成DataSet，然后由DataSet创建临时视图，由临时视图创建新表，由此完成Hive读写示例。
package com.yqu.sparkhive; import java.io.File; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; import org.apache.spark.sql.SparkSession; public class HelloSparkHiveDriver { public static void main(String args[]) { if(args.length==0) { System.out.println(&amp;quot;Please provide target hive table name!&amp;quot;); } // warehouseLocation points to the default location // for managed databases and tables String warehouseLocation = new File(&amp;quot;spark-warehouse&amp;quot;).getAbsolutePath(); SparkSession spark = SparkSession .builder() .appName(&amp;quot; .config(&amp;quot;spark.sql.warehouse.dir&amp;quot;, warehouseLocation) .enableHiveSupport() .getOrCreate(); // Queries are expressed in HiveQL Dataset sqlDF = spark.sql(&amp;quot;SELECT * FROM emp&amp;quot;); System.out.println(&amp;quot;emp content:&amp;quot;); sqlDF.show(); sqlDF.createOrReplaceTempView(&amp;quot;yquTempTable&amp;quot;); spark.sql(&amp;quot;create table &amp;quot;&#43;args[0]&#43;&amp;quot; as select * from yquTempTable&amp;quot;); System.out.println(args[0]&#43;&amp;quot; content:&amp;quot;); spark.sql(&amp;quot;SELECT * FROM &amp;quot;&#43;args[0]).show(); spark.close(); } }  构建及测试 vagrant@node1:~/HelloSparkHive$ gradle build jar BUILD SUCCESSFUL in 0s 2 actionable tasks: 2 executed vagrant@node1:~/HelloSparkHive$ spark-submit --class com.yqu.sparkhive.HelloSparkHiveDriver --deploy-mode client --master local[2] /home/vagrant/HelloSparkHive/build/libs/hello-spark-hive-0.1.0.jar yqu1 vagrant@node1:~/HelloSparkHive$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Connecting to jdbc:hive2:// 18/07/10 07:55:18 [main]: WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory. Connected to: Apache Hive (version 2.3.3) Driver: Hive JDBC (version 2.3.3) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 2.3.3 by Apache Hive hive&amp;gt; use default; OK No rows affected (0.931 seconds) hive&amp;gt; show tables; OK emp yqu1 2 rows selected (0.229 seconds) hive&amp;gt; select * from yqu1; 18/07/02 07:55:37 [XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX main]: ERROR hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !! OK 7369 SMITH CLERK 7902 1980-12-17 800.0 7499 ALLEN SALESMAN 7698 1981-02-20 1600.0 300 7521 WARD SALESMAN 7698 1981-02-22 1250.0 500 7566 JONES MANAGER 7839 1981-04-02 2975.0 7654 MARTIN SALESMAN 7698 1981-09-28 1250.0 1400 7698 BLAKE MANAGER 7839 1981-05-01 2850.0 7782 CLARK MANAGER 7839 1981-06-09 2450.0 7788 SCOTT ANALYST 7566 1982-12-09 3000.0 7839 KING PRESIDENT 1981-11-17 5000.0 7844 TURNER SALESMAN 7698 1981-09-08 1500.0 0 7876 ADAMS CLERK 7788 1983-01-12 1100.0 7900 JAMES CLERK 7698 1981-12-03 950.0 7902 FORD ANALYST 7566 1981-12-03 3000.0 7934 MILLER CLERK 7782 1982-01-23 1300.0 7988 KATY ANALYST 7566 NULL 1500.0 7987 JULIA ANALYST 7566 NULL 1500.0 16 rows selected (1.546 seconds) hive&amp;gt;  参考 Getting started Apache Spark with Java
The Java Library Plugin
geekmj/apache-spark-examples
saagie/example-spark-scala-read-and-write-from-hive
</content>
    </entry>
    
     <entry>
        <title>[AWS] 安装AWSCLI </title>
        <url>https://mryqu.github.io/post/aws_%E5%AE%89%E8%A3%85awscli/</url>
        <categories>
          <category>Tool</category><category>AWS</category>
        </categories>
        <tags>
          <tag>aws</tag><tag>cli</tag><tag>python</tag><tag>pip</tag><tag>cp65001</tag>
        </tags>
        <content type="html"> 想玩玩AWS CLI，就从https://aws.amazon.com/cli/装了一个，但是一执行就是出LookupError: unknown encoding: cp65001错误，查了一下据说是Python2.7导致的。 首先去https://www.python.org/下载了最新的Python3.7.0。然后重新安装AWS CLI，依旧出错，只好卸载。 查看是否安装pip，结果发现没有。根据https://packaging.python.org/tutorials/installing-packages/中的提示下载了get-pip.py，执行python get-pip.py，成功安装好pip。
C:\&amp;gt;pip --version pip 10.0.1 from c:\users\mryqu\appdata\local\programs\python\python37-32\lib\site-packages\pip (python 3.7  最后使用pip安装AWS CLI:
C:\&amp;gt;pip install awscli Collecting awscli Downloading https://files.pythonhosted.org/packages/1b/1b/7446d52820533164965f7e7d08cee70b170c78fbbcbd0c7a11ccb9187be6/awscli-1.15.49-py2.py3-none-any.whl (1.3MB) 100% |████████████████████████████████| 1.3MB 6.6MB/s Collecting docutils&amp;gt;=0.10 (from awscli) Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB) 100% |████████████████████████████████| 552kB 3.3MB/s Collecting s3transfer&amp;lt;0.2.0,&amp;gt;=0.1.12 (from awscli) Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB 100% |████████████████████████████████| 61kB 787kB/s Collecting botocore==1.10.48 (from awscli) Downloading https://files.pythonhosted.org/packages/0b/56/44067a8f0cae5f33007e7cbdbaac67cbd9fa598c733ad25eb8f252288fe9/botocore-1.10.48-py2.py3-none-any.whl (4.4MB 100% |████████████████████████████████| 4.4MB 6.6MB/s Collecting PyYAML&amp;lt;=3.12,&amp;gt;=3.10 (from awscli) Downloading https://files.pythonhosted.org/packages/4a/85/db5a2df477072b2902b0eb892feb37d88ac635d36245a72a6a69b23b383a/PyYAML-3.12.tar.gz (253kB) 100% |████████████████████████████████| 256kB 6.6MB/s Collecting colorama&amp;lt;=0.3.9,&amp;gt;=0.2.5 (from awscli) Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl Collecting rsa&amp;lt;=3.5.0,&amp;gt;=3.1.2 (from awscli) Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB) 100% |████████████████████████████████| 51kB 7.3MB/s Collecting jmespath&amp;lt;1.0.0,&amp;gt;=0.7.1 (from botocore==1.10.48-&amp;gt;awscli) Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl Collecting python-dateutil&amp;lt;3.0.0,&amp;gt;=2.1; python_version &amp;gt;= &amp;quot;2.7&amp;quot; (from botocore==1.10.48-&amp;gt;awscli) Downloading https://files.pythonhosted.org/packages/cf/f5/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825/python_dateutil-2.7.3-py2.py3-none-any.whl ( 100% |████████████████████████████████| 215kB 3.3MB/s Collecting pyasn1&amp;gt;=0.1.3 (from rsa&amp;lt;=3.5.0,&amp;gt;=3.1.2-&amp;gt;awscli) Downloading https://files.pythonhosted.org/packages/a0/70/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9/pyasn1-0.4.3-py2.py3-none-any.whl (72kB) 100% |████████████████████████████████| 81kB ... Collecting six&amp;gt;=1.5 (from python-dateutil&amp;lt;3.0.0,&amp;gt;=2.1; python_version &amp;gt;= &amp;quot;2.7&amp;quot;-&amp;gt;botocore==1.10.48-&amp;gt;awscli) Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl Building wheels for collected packages: PyYAML Running setup.py bdist_wheel for PyYAML ... done Stored in directory: C:\Users\mryqu\AppData\Local\pip\Cache\wheels\03\05\65\bdc14f2c6e09e82ae3e0f13d021e1b6b2481437ea2f207df3f Successfully built PyYAML Installing collected packages: docutils, jmespath, six, python-dateutil, botocore, s3transfer, PyYAML, colorama, pyasn1, rsa, awscli Successfully installed PyYAML-3.12 awscli-1.15.49 botocore-1.10.48 colorama-0.3.9 docutils-0.14 jmespath-0.9.3 pyasn1-0.4.3 python-dateutil-2.7.3 rsa-3.4.2 s3transfe  使用AWS CLI，貌似好用，就是help命令报内存不足！
C:\&amp;gt;aws --version aws-cli/1.15.49 Python/3.7.0 Windows/2008ServerR2 botocore/1.10.48 C:\&amp;gt;aws configure list Name Value Type Location ---- ----- ---- -------- profile None None access_key None None secret_key None None region None None C:\&amp;gt;aws help Not enough memory.  最后发现，我在AWS上的SAS联合账户根本没权限获得AWS Access Key ID和AWS Secret Access Key，这样就没法按照Configuring the AWS CLI进行配置了。棋差一招，只好不玩了。
</content>
    </entry>
    
     <entry>
        <title>使用Consul DNS接口</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8consuldns%E6%8E%A5%E5%8F%A3/</url>
        <categories>
          <category>Tool</category><category>Consul</category>
        </categories>
        <tags>
          <tag>consul</tag><tag>dns</tag>
        </tags>
        <content type="html">  Consul提供了两个查询接口：HTTP和DNS。DNS接口允许应用程序在没有与consul高度集成的情况下使用服务发现。 以下面这个小Consul集群为例：
root@consul:/# /usr/bin/consul members Node Address Status Type Build Protocol DC configuration 172.17.0.7:8301 alive client X.Y.Z 2 dc1 consul 172.17.0.2:8301 alive server X.Y.Z 2 dc1 httpd 172.17.0.4:8301 alive client X.Y.Z 2 dc1 logon 172.17.0.8:8301 alive client X.Y.Z 2 dc1 postgres 172.17.0.3:8301 alive client X.Y.Z 2 dc1 rabbitmq 172.17.0.6:8301 alive client X.Y.Z 2 dc1  可以通过DNS接口以&amp;lt;dnode&amp;gt;.node[.datacenter].&amp;lt;domain&amp;gt;的形式查询节点地址，也可以[tag.]&amp;lt;dservice&amp;gt;.service[.datacenter].&amp;lt;domain&amp;gt;的形式查询服务地址。
root@httpd:/# dig @127.0.0.1 -p53 postgres.node.consul ANY ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.9.5-9-Debian &amp;lt;&amp;lt;&amp;gt;&amp;gt; @127.0.0.1 -p53 postgres.node.consul ANY ; (1 server found) ;; global options: &#43;cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 57064 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;postgres.node.consul. IN ANY ;; ANSWER SECTION: postgres.node.consul. 0 IN A 172.17.0.3 ;; Query time: 0 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Wed Jun 20 03:00:29 UTC 2018 ;; MSG SIZE rcvd: 65 root@httpd:/# dig @127.0.0.1 -p53 rabbitmq.service.consul SRV ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.9.5-9-Debian &amp;lt;&amp;lt;&amp;gt;&amp;gt; @127.0.0.1 rabbitmq.service.consul SRV ; (1 server found) ;; global options: &#43;cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 45919 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 2 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;rabbitmq.service.consul. IN SRV ;; ANSWER SECTION: rabbitmq.service.consul. 0 IN SRV 1 1 5672 rabbitmq.node.dc1.consul. ;; ADDITIONAL SECTION: rabbitmq.node.dc1.consul. 0 IN A 172.17.0.6 ;; Query time: 0 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Wed Jun 20 05:03:19 UTC 2018 ;; MSG SIZE rcvd: 112 root@httpd:/# exit exit vagrant$ dig @172.17.0.2 -p53 httpd.node.consul ANY ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.9.5-3ubuntu0.8-Ubuntu &amp;lt;&amp;lt;&amp;gt;&amp;gt; @172.17.0.2 -p53 httpd.node.consul ANY ; (1 server found) ;; global options: &#43;cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 48389 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;httpd.node.consul. IN ANY ;; ANSWER SECTION: httpd.node.consul. 0 IN A 172.17.0.4 ;; Query time: 0 msec ;; SERVER: 172.17.0.2#53(172.17.0.2) ;; WHEN: Wed Jun 20 03:04:15 UTC 2018 ;; MSG SIZE rcvd: 68  Consul的DNS接口默认端口为8600，这里已通过Consul配置改为53。
{ &amp;quot;data_dir&amp;quot;: &amp;quot;/opt/XXX/config/srv/consul&amp;quot;, &amp;quot;ports&amp;quot;: { &amp;quot;dns&amp;quot;: 53 }, &amp;quot;leave_on_terminate&amp;quot;: true }  默认情况下，Consul仅对.consul域提供DNS查询服务，不支持进一步的DNS递归。可以增加上游DNS服务器已进行非Consul查询：
{ &amp;quot;data_dir&amp;quot;: &amp;quot;/opt/XXX/config/srv/consul&amp;quot;, &amp;quot;ports&amp;quot;: { &amp;quot;dns&amp;quot;: 53 }, &amp;quot;recursors&amp;quot;: [&amp;quot;AAA.BBB.CCC.DDD&amp;quot;], &amp;quot;leave_on_terminate&amp;quot;: true }  测试结果：
vagrant$ dig @172.17.0.2 -p53 www.sina.com.cn ANY ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.9.5-3ubuntu0.8-Ubuntu &amp;lt;&amp;lt;&amp;gt;&amp;gt; @172.17.0.2 -p53 www.sina.com.cn ANY ; (1 server found) ;; global options: &#43;cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 25127 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 13, ADDITIONAL: 27 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;www.sina.com.cn. IN ANY ;; ANSWER SECTION: www.sina.com.cn. 2270 IN CNAME spool.grid.sinaedge.com. ;; AUTHORITY SECTION: . 109385 IN NS i.root-servers.net. . 109385 IN NS k.root-servers.net. . 109385 IN NS c.root-servers.net. . 109385 IN NS m.root-servers.net. . 109385 IN NS l.root-servers.net. . 109385 IN NS d.root-servers.net. . 109385 IN NS g.root-servers.net. . 109385 IN NS a.root-servers.net. . 109385 IN NS b.root-servers.net. . 109385 IN NS j.root-servers.net. . 109385 IN NS h.root-servers.net. . 109385 IN NS e.root-servers.net. . 109385 IN NS f.root-servers.net. ;; ADDITIONAL SECTION: a.root-servers.net. 395644 IN A 198.41.0.4 b.root-servers.net. 395644 IN A 199.9.14.201 c.root-servers.net. 395644 IN A 192.33.4.12 d.root-servers.net. 395644 IN A 199.7.91.13 e.root-servers.net. 395644 IN A 192.203.230.10 f.root-servers.net. 395644 IN A 192.5.5.241 g.root-servers.net. 395644 IN A 192.112.36.4 h.root-servers.net. 395644 IN A 198.97.190.53 i.root-servers.net. 395644 IN A 192.36.148.17 j.root-servers.net. 395644 IN A 192.58.128.30 k.root-servers.net. 395644 IN A 193.0.14.129 l.root-servers.net. 395644 IN A 199.7.83.42 m.root-servers.net. 395644 IN A 202.12.27.33 a.root-servers.net. 395644 IN AAAA 2001:503:ba3e::2:30 b.root-servers.net. 395644 IN AAAA 2001:500:200::b c.root-servers.net. 395644 IN AAAA 2001:500:2::c d.root-servers.net. 395644 IN AAAA 2001:500:2d::d e.root-servers.net. 395644 IN AAAA 2001:500:a8::e f.root-servers.net. 395644 IN AAAA 2001:500:2f::f g.root-servers.net. 395644 IN AAAA 2001:500:12::d0d h.root-servers.net. 395644 IN AAAA 2001:500:1::53 i.root-servers.net. 395644 IN AAAA 2001:7fe::53 j.root-servers.net. 395644 IN AAAA 2001:503:c27::2:30 k.root-servers.net. 395644 IN AAAA 2001:7fd::1 l.root-servers.net. 395644 IN AAAA 2001:500:9f::42 m.root-servers.net. 395644 IN AAAA 2001:dc3::35 ;; Query time: 259 msec ;; SERVER: 172.17.0.2#53(172.17.0.2) ;; WHEN: Wed Jun 20 04:58:55 UTC 2018 ;; MSG SIZE rcvd: 1539  参考 Consul DNS Interface
Consul Configuration
dig笔记
【Consul】Consul实践指导-DNS接口
</content>
    </entry>
    
     <entry>
        <title>Vagrant折腾记</title>
        <url>https://mryqu.github.io/post/vagrant%E6%8A%98%E8%85%BE%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>virtualbox</tag><tag>win10</tag><tag>private_key</tag><tag>vagrant-vbguest</tag>
        </tags>
        <content type="html">  自从用了OpenStack，真是很久没玩Vagrant。偶尔想用用，问题不断，废了一点功夫才解决。
遭遇VBoxManage.exe: error $ vagrant up Bringing machine &#39;xdevimg&#39; up with &#39;virtualbox&#39; provider... ==&amp;gt; xdevimg: Checking if box &#39;devimage-ubuntu&#39; is up to date... ==&amp;gt; xdevimg: A newer version of the box &#39;devimage-ubuntu&#39; is available! You currently ==&amp;gt; xdevimg: have version &#39;0.13.0&#39;. The latest is version &#39;0.13.2&#39;. Run ==&amp;gt; xdevimg: `vagrant box update` to update. ==&amp;gt; xdevimg: Clearing any previously set forwarded ports... ==&amp;gt; xdevimg: Clearing any previously set network interfaces... ==&amp;gt; xdevimg: Preparing network interfaces based on configuration... xdevimg: Adapter 1: nat ==&amp;gt; xdevimg: You are trying to forward to privileged ports (ports &amp;lt;= 1024). Most ==&amp;gt; xdevimg: operating systems restrict this to only privileged process (typically ==&amp;gt; xdevimg: processes running as an administrative user). This is a warning in case ==&amp;gt; xdevimg: the port forwarding doesn&#39;t work. If any problems occur, please try a ==&amp;gt; xdevimg: port higher than 1024. ==&amp;gt; xdevimg: Forwarding ports... xdevimg: 7980 =&amp;gt; 80 (adapter 1) xdevimg: 7981 =&amp;gt; 7981 (adapter 1) xdevimg: 7982 =&amp;gt; 7982 (adapter 1) xdevimg: 8500 =&amp;gt; 8500 (adapter 1) xdevimg: 5672 =&amp;gt; 5672 (adapter 1) xdevimg: 15672 =&amp;gt; 15672 (adapter 1) xdevimg: 5577 =&amp;gt; 5577 (adapter 1) xdevimg: 5432 =&amp;gt; 5432 (adapter 1) xdevimg: 8787 =&amp;gt; 8787 (adapter 1) xdevimg: 22 =&amp;gt; 2222 (adapter 1) ==&amp;gt; xdevimg: Running &#39;pre-boot&#39; VM customizations... ==&amp;gt; xdevimg: Booting VM... There was an error while executing `VBoxManage`, a CLI used by Vagrant for controlling VirtualBox. The command and stderr is shown below. Command: [&amp;quot;startvm&amp;quot;, &amp;quot;3e7f8816-e32b-4bce-943b-a7cefc5ec43e&amp;quot;, &amp;quot;--type&amp;quot;, &amp;quot;headless&amp;quot;] Stderr: VBoxManage.exe: error: The virtual machine &#39;dev-image_xdevimg_1529568385166_16024&#39; has terminated unexpectedly during startup with exit code 1 (0x1). More details may be available in &#39;C:\Users\mryqu\VirtualBox VMs\dev-image_xdevimg_1529568385166_16024\Logs\VBoxHardening.log&#39; VBoxManage.exe: error: Details: code E_FAIL (0x80004005), component MachineWrap, interface IMachine  想一想我的操作系统都从Win7升成Win10，网上说跟Win10有关，需要升级VirtualBox。结果将VirtualBox从5.0.32升到5.2.12，结果Vagrant 1.7.4又不认新的VirtualBox，索性将Vagrant也升级到2.1.1。过关！
遭遇Warning: Remote connection disconnect. Retrying&amp;hellip; $ vagrant up Bringing machine &#39;xdevimg&#39; up with &#39;virtualbox&#39; provider... ==&amp;gt; xdevimg: Importing base box &#39;devimage-ubuntu&#39;... ==&amp;gt; xdevimg: Matching MAC address for NAT networking... ==&amp;gt; xdevimg: Checking if box &#39;devimage-ubuntu&#39; is up to date... ==&amp;gt; xdevimg: Setting the name of the VM: dev-image_xdevimg_1529637278638_25909 ==&amp;gt; xdevimg: Clearing any previously set network interfaces... ==&amp;gt; xdevimg: Preparing network interfaces based on configuration... xdevimg: Adapter 1: nat ==&amp;gt; xdevimg: You are trying to forward to privileged ports (ports &amp;lt;= 1024). Most ==&amp;gt; xdevimg: operating systems restrict this to only privileged process (typically ==&amp;gt; xdevimg: processes running as an administrative user). This is a warning in case ==&amp;gt; xdevimg: the port forwarding doesn&#39;t work. If any problems occur, please try a ==&amp;gt; xdevimg: port higher than 1024. ==&amp;gt; xdevimg: Forwarding ports... xdevimg: 7980 (guest) =&amp;gt; 80 (host) (adapter 1) xdevimg: 7981 (guest) =&amp;gt; 7981 (host) (adapter 1) xdevimg: 7982 (guest) =&amp;gt; 7982 (host) (adapter 1) xdevimg: 8500 (guest) =&amp;gt; 8500 (host) (adapter 1) xdevimg: 5672 (guest) =&amp;gt; 5672 (host) (adapter 1) xdevimg: 15672 (guest) =&amp;gt; 15672 (host) (adapter 1) xdevimg: 5577 (guest) =&amp;gt; 5577 (host) (adapter 1) xdevimg: 5432 (guest) =&amp;gt; 5432 (host) (adapter 1) xdevimg: 8787 (guest) =&amp;gt; 8787 (host) (adapter 1) xdevimg: 22 (guest) =&amp;gt; 2222 (host) (adapter 1) ==&amp;gt; xdevimg: Running &#39;pre-boot&#39; VM customizations... ==&amp;gt; xdevimg: Booting VM... ==&amp;gt; xdevimg: Waiting for machine to boot. This may take a few minutes... xdevimg: SSH address: 127.0.0.1:2222 xdevimg: SSH username: vagrant xdevimg: SSH auth method: private key xdevimg: Warning: Remote connection disconnect. Retrying... xdevimg: Warning: Remote connection disconnect. Retrying... xdevimg: Warning: Remote connection disconnect. Retrying... xdevimg: Warning: Remote connection disconnect. Retrying... xdevimg: Warning: Remote connection disconnect. Retrying... xdevimg: Warning: Remote connection disconnect. Retrying... Timed out while waiting for the machine to boot. This means that Vagrant was unable to communicate with the guest machine within the configured (&amp;quot;config.vm.boot_timeout&amp;quot; value) time period. If you look above, you should be able to see the error(s) that Vagrant had when attempting to connect to the machine. These errors are usually good hints as to what may be wrong. If you&#39;re using a custom box, make sure that networking is properly working and you&#39;re able to connect to the machine. It is a common problem that networking isn&#39;t setup properly in these boxes. Verify that authentication configurations are also setup properly, as well. If the box appears to be booting properly, you may want to increase the timeout (&amp;quot;config.vm.boot_timeout&amp;quot;) value.  Workround： 1. 执行vagrant destroy并强制删除.vagrant\machines\xdevimg\virtualbox\private_key 2. 在Vagrantfile中添加config.ssh.insert_key = false 3. 执行vagrant up 该问题将在Vagrant 2.1.2解决，细节见issue 9900。
遭遇VirtualBox Guest Addition版本与VM不一致 $ vagrant up Bringing machine &#39;xdevimg&#39; up with &#39;virtualbox&#39; provider... ==&amp;gt; xdevimg: Importing base box &#39;devimage-ubuntu&#39;... ==&amp;gt; xdevimg: Matching MAC address for NAT networking... ==&amp;gt; xdevimg: Checking if box &#39;devimage-ubuntu&#39; is up to date... ==&amp;gt; xdevimg: Setting the name of the VM: dev-image_xdevimg_1529639647353_4390 ==&amp;gt; xdevimg: Clearing any previously set network interfaces... ==&amp;gt; xdevimg: Preparing network interfaces based on configuration... xdevimg: Adapter 1: nat ==&amp;gt; xdevimg: You are trying to forward to privileged ports (ports &amp;lt;= 1024). Most ==&amp;gt; xdevimg: operating systems restrict this to only privileged process (typically ==&amp;gt; xdevimg: processes running as an administrative user). This is a warning in case ==&amp;gt; xdevimg: the port forwarding doesn&#39;t work. If any problems occur, please try a ==&amp;gt; xdevimg: port higher than 1024. ==&amp;gt; xdevimg: Forwarding ports... xdevimg: 7980 (guest) =&amp;gt; 80 (host) (adapter 1) xdevimg: 7981 (guest) =&amp;gt; 7981 (host) (adapter 1) xdevimg: 7982 (guest) =&amp;gt; 7982 (host) (adapter 1) xdevimg: 8500 (guest) =&amp;gt; 8500 (host) (adapter 1) xdevimg: 5672 (guest) =&amp;gt; 5672 (host) (adapter 1) xdevimg: 15672 (guest) =&amp;gt; 15672 (host) (adapter 1) xdevimg: 5577 (guest) =&amp;gt; 5577 (host) (adapter 1) xdevimg: 5432 (guest) =&amp;gt; 5432 (host) (adapter 1) xdevimg: 8787 (guest) =&amp;gt; 8787 (host) (adapter 1) xdevimg: 22 (guest) =&amp;gt; 2222 (host) (adapter 1) ==&amp;gt; xdevimg: Running &#39;pre-boot&#39; VM customizations... ==&amp;gt; xdevimg: Booting VM... ==&amp;gt; xdevimg: Waiting for machine to boot. This may take a few minutes... xdevimg: SSH address: 127.0.0.1:2222 xdevimg: SSH username: vagrant xdevimg: SSH auth method: private key xdevimg: xdevimg: Vagrant insecure key detected. Vagrant will automatically replace xdevimg: this with a newly generated keypair for better security. xdevimg: xdevimg: Inserting generated public key within guest... xdevimg: Removing insecure key from the guest if it&#39;s present... xdevimg: Key inserted! Disconnecting and reconnecting using new SSH key... ==&amp;gt; xdevimg: Machine booted and ready! ==&amp;gt; xdevimg: Checking for guest additions in VM... xdevimg: The guest additions on this VM do not match the installed version of xdevimg: VirtualBox! In most cases this is fine, but in rare cases it can xdevimg: prevent things such as shared folders from working properly. If you see xdevimg: shared folder errors, please make sure the guest additions within the xdevimg: virtual machine match the version of VirtualBox you have installed on xdevimg: your host and reload your VM. xdevimg: xdevimg: Guest Additions Version: 4.3.40 xdevimg: VirtualBox Version: 5.2 ==&amp;gt; xdevimg: Mounting shared folders... xdevimg: /vagrant =&amp;gt; C:/dev-image Vagrant was unable to mount VirtualBox shared folders. This is usually because the filesystem &amp;quot;vboxsf&amp;quot; is not available. This filesystem is made available via the VirtualBox Guest Additions and kernel module. Please verify that these guest additions are properly installed in the guest. This is not a bug in Vagrant and is usually caused by a faulty Vagrant box. For context, the command attempted was: mount -t vboxsf -o uid=1000,gid=1000 vagrant /vagrant The error output from the command was: /sbin/mount.vboxsf: mounting failed with the error: No such device  解决办法：
$ vagrant plugin install vagrant-vbguest Installing the &#39;vagrant-vbguest&#39; plugin. This can take a few minutes... Installed the plugin &#39;vagrant-vbguest (0.15.2)&#39;!  终于，又看到Vagrang起来了！
</content>
    </entry>
    
     <entry>
        <title>服务发现产品对比</title>
        <url>https://mryqu.github.io/post/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%BA%A7%E5%93%81%E5%AF%B9%E6%AF%94/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>micro service</tag><tag>discovery</tag><tag>registery</tag><tag>compare</tag>
        </tags>
        <content type="html">  首先就一些常见的服务发现产品进行下对比: 对比项Consul GitHubZookeeper GitHubEtcd GitHubEuerka GitHubKubernetes GitHub服务健康检查客户端可绑定 任意多个服务或 节点健康检查 (内存、磁盘)CS之间长连接 &#43;连接心跳; SS之间TCP KeepAlive连接心跳连接心跳注册、 其他可配支持多数据中心内置WAN方案支持中央Zookeeper集群, 通过Observer有限支持————KV存储限制上百MB上百MB （有时支持上GB）上GB——线性化读取 原子读取是否是——事务字段比较、 锁、读、写版本审查、 写字段比较、 读、写——多版本并发控制 MVCC————支持——watch支持/ 变化通知针对KV对、键前缀、 服务成员、服务实例、 节点列表、健康检查、 自定义用户事件等针对当前KV或目录; 单次针对过去或当前的键区间针对app、 vipAddress 或实例一致性协议RaftZab（≈Paxos）Raft——CAPCPCPCPAP成员动态更新支持&amp;gt;3.5.0支持支持支持接口HTTP和DNS客户端HTTP和gRPCHTTP (Sidecar)性能指标监控Metrics—— (可通过命令行 或JMX监控， 但没有Metrics)MetricsMetrics用户权限ACLsACLs基于Role——安全TLS / HTTPSSSLTLS / HTTPSHTTPSWeb管理界面支持支持—— （有第三方支持， 例如soyking/e3w）支持Spring Cloud集成Spring Cloud ConsulSpring Cloud ZookeeperSpring Cloud Etcd （孵化状态）Spring Cloud NetflixSpring Cloud Kubernetes （孵化状态）实现语言GoJavaGoJavaGo用户未提及用户列表用户列表除了Netflix自用 未提及其他用户合作方列表
此外，Airbnb开源了由Ruby语言实现的自动服务发现和注册框架SmartStack，它由nerve、 synapse、Zookeeper和HAProxy组成。ZooKeeper负责维护集群状态；nerve负责针对服务进行健康检查及向Zookeeper注册；synapse负责为服务提供者查询Zookeeper并动态配置HAProxy。客户端与HAProxy交互，HAProxy负责健康检查及在针对服务提供者进行负载均衡。 注：对Kubernetes的对比在后继学习之后再补上。
参考 Consul vs. ZooKeeper, doozerd, etcd
Consul vs. SmartStack
Consul vs. Eureka
etcd versus other key-value stores
服务发现比较:Consul vs Zookeeper vs Etcd vs Eureka
SmartStack: Airbnb的自动服务发现和注册框架
课程：使用Kubernetes进行可扩展微服务
Consul实践指导-DNS接口
Please stop calling databases CP or AP
</content>
    </entry>
    
     <entry>
        <title>[AWS] 学习AWS上SAS联合账户的使用</title>
        <url>https://mryqu.github.io/post/aws_%E5%AD%A6%E4%B9%A0aws%E4%B8%8Asas%E8%81%94%E5%90%88%E8%B4%A6%E6%88%B7%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>Tool</category><category>AWS</category>
        </categories>
        <tags>
          <tag>aws</tag><tag>ec2</tag>
        </tags>
        <content type="html"> 今天顺利学完《SAS Federated Accounts on Amazon Web Services》课程，测验全都答/蒙对了。 1. 创建了EC2实例。
2. 第一次用MobaXterm。相对Putty，省了将.pem SSH密钥通过PuTTYgen工具转换成.ppk格式这一步操作。 3. 在/etc/resolv.conf文件中增加了几个nameserver以指定单位的DNS，在search项增加了多个域名以解析单位的机器名。
4. 安装libXext.x86_64、libXp.x86_64、libXtst.x86_64、xorg-x11-xauth.x86_64、xorg-x11-apps等包以支持图形显示。
</content>
    </entry>
    
     <entry>
        <title>使用Fetch_API</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8fetch_api/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>fetch_api</tag><tag>chrome</tag><tag>devtool</tag><tag>javascript</tag><tag>html</tag>
        </tags>
        <content type="html"> 今天又学了一招在Chrome developer tool中通过Fetch_API发起HTTP请求。 代码示例：
fetch(&#39;https://jsonplaceholder.typicode.com/posts/1&#39;) .then(response =&amp;gt; response.json()) .then(json =&amp;gt; console.log(json))  </content>
    </entry>
    
     <entry>
        <title>安装Twurl并调试extended tweet mode</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8twurl%E8%B0%83%E8%AF%95extended_tweet_mode/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twurl</tag><tag>extended</tag><tag>tweet</tag><tag>mode</tag>
        </tags>
        <content type="html">  在研究tweet字节限制由140字节变为280字节时，想要玩一玩Twitter API控制台工具(https://dev.twitter.com/rest/tools/console)，结果dev.twitter.com跳到了developer.twitter.com，这才发现Twitter REST API按照Standard、Premium和Enterprise划分开始走上收费的道路。 Twitter API控制台工具已经找不到了，官方示例使用twurl命令工具完成的。
安装Twurl  在https://rubyinstaller.org/下载并安装RubyInstaller 执行gem install twurl  调试extended tweet mode 准备环境 认证 twurl authorize --consumer-key key --consumer-secret secret  搜索tweet twurl &amp;quot;/1.1/search/tweets.json?q=scnydq&amp;quot; { &amp;quot;statuses&amp;quot;: [ { &amp;quot;created_at&amp;quot;: &amp;quot;Thu Feb 08 02:22:11 &#43;0000 2018&amp;quot;, &amp;quot;id&amp;quot;: 9.6142489253621e&#43;17, &amp;quot;id_str&amp;quot;: &amp;quot;961424892536205313&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;scnydq.testbody280:001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032\u2026 https:\/\/t.co\/q9CyeceQku&amp;quot;, &amp;quot;truncated&amp;quot;: true, &amp;quot;entities&amp;quot;: { &amp;quot;hashtags&amp;quot;: [ ], &amp;quot;symbols&amp;quot;: [ ], &amp;quot;user_mentions&amp;quot;: [ ], &amp;quot;urls&amp;quot;: [ { &amp;quot;url&amp;quot;: &amp;quot;https:\/\/t.co\/q9CyeceQku&amp;quot;, &amp;quot;expanded_url&amp;quot;: &amp;quot;https:\/\/twitter.com\/i\/web\/status\/961424892536205313&amp;quot;, &amp;quot;display_url&amp;quot;: &amp;quot;twitter.com\/i\/web\/status\/9\u2026&amp;quot;, &amp;quot;indices&amp;quot;: [ 117, 140 ] } ] }, &amp;quot;metadata&amp;quot;: { &amp;quot;iso_language_code&amp;quot;: &amp;quot;en&amp;quot;, &amp;quot;result_type&amp;quot;: &amp;quot;recent&amp;quot; }, &amp;quot;source&amp;quot;: &amp;quot;&amp;lt;a href=\&amp;quot;http:\/\/twitter.com\&amp;quot; rel=\&amp;quot;nofollow\&amp;quot;&amp;gt;Twitter Web Client&amp;lt;\/a&amp;gt;&amp;quot;, &amp;quot;in_reply_to_status_id&amp;quot;: null, &amp;quot;in_reply_to_status_id_str&amp;quot;: null, &amp;quot;in_reply_to_user_id&amp;quot;: null, &amp;quot;in_reply_to_user_id_str&amp;quot;: null, &amp;quot;in_reply_to_screen_name&amp;quot;: null, &amp;quot;user&amp;quot;: { &amp;quot;id&amp;quot;: 4751643594, &amp;quot;id_str&amp;quot;: &amp;quot;4751643594&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;screen_name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;location&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;url&amp;quot;: null, &amp;quot;entities&amp;quot;: { &amp;quot;description&amp;quot;: { &amp;quot;urls&amp;quot;: [ ] } }, &amp;quot;protected&amp;quot;: false, &amp;quot;followers_count&amp;quot;: 0, &amp;quot;friends_count&amp;quot;: 5, &amp;quot;listed_count&amp;quot;: 0, &amp;quot;created_at&amp;quot;: &amp;quot;Wed Jan 13 06:00:44 &#43;0000 2016&amp;quot;, &amp;quot;favourites_count&amp;quot;: 1, &amp;quot;utc_offset&amp;quot;: null, &amp;quot;time_zone&amp;quot;: null, &amp;quot;geo_enabled&amp;quot;: false, &amp;quot;verified&amp;quot;: false, &amp;quot;statuses_count&amp;quot;: 4, &amp;quot;lang&amp;quot;: &amp;quot;en&amp;quot;, &amp;quot;contributors_enabled&amp;quot;: false, &amp;quot;is_translator&amp;quot;: false, &amp;quot;is_translation_enabled&amp;quot;: false, &amp;quot;profile_background_color&amp;quot;: &amp;quot;F5F8FA&amp;quot;, &amp;quot;profile_background_image_url&amp;quot;: null, &amp;quot;profile_background_image_url_https&amp;quot;: null, &amp;quot;profile_background_tile&amp;quot;: false, &amp;quot;profile_image_url&amp;quot;: &amp;quot;http:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_image_url_https&amp;quot;: &amp;quot;https:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_link_color&amp;quot;: &amp;quot;1DA1F2&amp;quot;, &amp;quot;profile_sidebar_border_color&amp;quot;: &amp;quot;C0DEED&amp;quot;, &amp;quot;profile_sidebar_fill_color&amp;quot;: &amp;quot;DDEEF6&amp;quot;, &amp;quot;profile_text_color&amp;quot;: &amp;quot;333333&amp;quot;, &amp;quot;profile_use_background_image&amp;quot;: true, &amp;quot;has_extended_profile&amp;quot;: false, &amp;quot;default_profile&amp;quot;: true, &amp;quot;default_profile_image&amp;quot;: false, &amp;quot;following&amp;quot;: false, &amp;quot;follow_request_sent&amp;quot;: false, &amp;quot;notifications&amp;quot;: false, &amp;quot;translator_type&amp;quot;: &amp;quot;none&amp;quot; }, &amp;quot;geo&amp;quot;: null, &amp;quot;coordinates&amp;quot;: null, &amp;quot;place&amp;quot;: null, &amp;quot;contributors&amp;quot;: null, &amp;quot;is_quote_status&amp;quot;: false, &amp;quot;retweet_count&amp;quot;: 0, &amp;quot;favorite_count&amp;quot;: 0, &amp;quot;favorited&amp;quot;: false, &amp;quot;retweeted&amp;quot;: false, &amp;quot;lang&amp;quot;: &amp;quot;en&amp;quot; }, { &amp;quot;created_at&amp;quot;: &amp;quot;Thu Feb 08 02:18:10 &#43;0000 2018&amp;quot;, &amp;quot;id&amp;quot;: 9.6142387998514e&#43;17, &amp;quot;id_str&amp;quot;: &amp;quot;961423879985139712&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;0010020030040050060070080090100110120130140150160170180190200210220230240250260270280290300310320330340350360370380\u2026 https:\/\/t.co\/m9tamSkv5U&amp;quot;, &amp;quot;truncated&amp;quot;: true, &amp;quot;entities&amp;quot;: { &amp;quot;hashtags&amp;quot;: [ ], &amp;quot;symbols&amp;quot;: [ ], &amp;quot;user_mentions&amp;quot;: [ ], &amp;quot;urls&amp;quot;: [ { &amp;quot;url&amp;quot;: &amp;quot;https:\/\/t.co\/m9tamSkv5U&amp;quot;, &amp;quot;expanded_url&amp;quot;: &amp;quot;https:\/\/twitter.com\/i\/web\/status\/961423879985139712&amp;quot;, &amp;quot;display_url&amp;quot;: &amp;quot;twitter.com\/i\/web\/status\/9\u2026&amp;quot;, &amp;quot;indices&amp;quot;: [ 117, 140 ] } ] }, &amp;quot;metadata&amp;quot;: { &amp;quot;iso_language_code&amp;quot;: &amp;quot;und&amp;quot;, &amp;quot;result_type&amp;quot;: &amp;quot;recent&amp;quot; }, &amp;quot;source&amp;quot;: &amp;quot;&amp;lt;a href=\&amp;quot;http:\/\/twitter.com\&amp;quot; rel=\&amp;quot;nofollow\&amp;quot;&amp;gt;Twitter Web Client&amp;lt;\/a&amp;gt;&amp;quot;, &amp;quot;in_reply_to_status_id&amp;quot;: null, &amp;quot;in_reply_to_status_id_str&amp;quot;: null, &amp;quot;in_reply_to_user_id&amp;quot;: null, &amp;quot;in_reply_to_user_id_str&amp;quot;: null, &amp;quot;in_reply_to_screen_name&amp;quot;: null, &amp;quot;user&amp;quot;: { &amp;quot;id&amp;quot;: 4751643594, &amp;quot;id_str&amp;quot;: &amp;quot;4751643594&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;screen_name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;location&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;url&amp;quot;: null, &amp;quot;entities&amp;quot;: { &amp;quot;description&amp;quot;: { &amp;quot;urls&amp;quot;: [ ] } }, &amp;quot;protected&amp;quot;: false, &amp;quot;followers_count&amp;quot;: 0, &amp;quot;friends_count&amp;quot;: 5, &amp;quot;listed_count&amp;quot;: 0, &amp;quot;created_at&amp;quot;: &amp;quot;Wed Jan 13 06:00:44 &#43;0000 2016&amp;quot;, &amp;quot;favourites_count&amp;quot;: 1, &amp;quot;utc_offset&amp;quot;: null, &amp;quot;time_zone&amp;quot;: null, &amp;quot;geo_enabled&amp;quot;: false, &amp;quot;verified&amp;quot;: false, &amp;quot;statuses_count&amp;quot;: 4, &amp;quot;lang&amp;quot;: &amp;quot;en&amp;quot;, &amp;quot;contributors_enabled&amp;quot;: false, &amp;quot;is_translator&amp;quot;: false, &amp;quot;is_translation_enabled&amp;quot;: false, &amp;quot;profile_background_color&amp;quot;: &amp;quot;F5F8FA&amp;quot;, &amp;quot;profile_background_image_url&amp;quot;: null, &amp;quot;profile_background_image_url_https&amp;quot;: null, &amp;quot;profile_background_tile&amp;quot;: false, &amp;quot;profile_image_url&amp;quot;: &amp;quot;http:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_image_url_https&amp;quot;: &amp;quot;https:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_link_color&amp;quot;: &amp;quot;1DA1F2&amp;quot;, &amp;quot;profile_sidebar_border_color&amp;quot;: &amp;quot;C0DEED&amp;quot;, &amp;quot;profile_sidebar_fill_color&amp;quot;: &amp;quot;DDEEF6&amp;quot;, &amp;quot;profile_text_color&amp;quot;: &amp;quot;333333&amp;quot;, &amp;quot;profile_use_background_image&amp;quot;: true, &amp;quot;has_extended_profile&amp;quot;: false, &amp;quot;default_profile&amp;quot;: true, &amp;quot;default_profile_image&amp;quot;: false, &amp;quot;following&amp;quot;: false, &amp;quot;follow_request_sent&amp;quot;: false, &amp;quot;notifications&amp;quot;: false, &amp;quot;translator_type&amp;quot;: &amp;quot;none&amp;quot; }, &amp;quot;geo&amp;quot;: null, &amp;quot;coordinates&amp;quot;: null, &amp;quot;place&amp;quot;: null, &amp;quot;contributors&amp;quot;: null, &amp;quot;is_quote_status&amp;quot;: false, &amp;quot;retweet_count&amp;quot;: 0, &amp;quot;favorite_count&amp;quot;: 0, &amp;quot;favorited&amp;quot;: false, &amp;quot;retweeted&amp;quot;: false, &amp;quot;lang&amp;quot;: &amp;quot;und&amp;quot; } ], &amp;quot;search_metadata&amp;quot;: { &amp;quot;completed_in&amp;quot;: 0.024, &amp;quot;max_id&amp;quot;: 9.6142489253621e&#43;17, &amp;quot;max_id_str&amp;quot;: &amp;quot;961424892536205313&amp;quot;, &amp;quot;query&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;refresh_url&amp;quot;: &amp;quot;?since_id=961424892536205313&amp;amp;q=scnydq&amp;amp;include_entities=1&amp;quot;, &amp;quot;count&amp;quot;: 15, &amp;quot;since_id&amp;quot;: 0, &amp;quot;since_id_str&amp;quot;: &amp;quot;0&amp;quot; } }  以extended tweet mode搜索tweet twurl &amp;quot;/1.1/search/tweets.json?q=scnydq&amp;amp;tweet_mode=extended&amp;quot; { &amp;quot;statuses&amp;quot;: [ { &amp;quot;created_at&amp;quot;: &amp;quot;Thu Feb 08 02:22:11 &#43;0000 2018&amp;quot;, &amp;quot;id&amp;quot;: 9.6142489253621e&#43;17, &amp;quot;id_str&amp;quot;: &amp;quot;961424892536205313&amp;quot;, &amp;quot;full_text&amp;quot;: &amp;quot;scnydq.testbody280:0010020030040050060070080090100110120130140150160170180190200210220230240250260270280290300310320330340350360370380390400410420430440450460470480490500510520530540550560570580590600610620630640650660670680690700710720730740750760770780790800810820830840850860xy&amp;quot;, &amp;quot;truncated&amp;quot;: false, &amp;quot;display_text_range&amp;quot;: [ 0, 280 ], &amp;quot;entities&amp;quot;: { &amp;quot;hashtags&amp;quot;: [ ], &amp;quot;symbols&amp;quot;: [ ], &amp;quot;user_mentions&amp;quot;: [ ], &amp;quot;urls&amp;quot;: [ ] }, &amp;quot;metadata&amp;quot;: { &amp;quot;iso_language_code&amp;quot;: &amp;quot;en&amp;quot;, &amp;quot;result_type&amp;quot;: &amp;quot;recent&amp;quot; }, &amp;quot;source&amp;quot;: &amp;quot;&amp;lt;a href=\&amp;quot;http:\/\/twitter.com\&amp;quot; rel=\&amp;quot;nofollow\&amp;quot;&amp;gt;Twitter Web Client&amp;lt;\/a&amp;gt;&amp;quot;, &amp;quot;in_reply_to_status_id&amp;quot;: null, &amp;quot;in_reply_to_status_id_str&amp;quot;: null, &amp;quot;in_reply_to_user_id&amp;quot;: null, &amp;quot;in_reply_to_user_id_str&amp;quot;: null, &amp;quot;in_reply_to_screen_name&amp;quot;: null, &amp;quot;user&amp;quot;: { &amp;quot;id&amp;quot;: 4751643594, &amp;quot;id_str&amp;quot;: &amp;quot;4751643594&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;screen_name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;location&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;url&amp;quot;: null, &amp;quot;entities&amp;quot;: { &amp;quot;description&amp;quot;: { &amp;quot;urls&amp;quot;: [ ] } }, &amp;quot;protected&amp;quot;: false, &amp;quot;followers_count&amp;quot;: 0, &amp;quot;friends_count&amp;quot;: 5, &amp;quot;listed_count&amp;quot;: 0, &amp;quot;created_at&amp;quot;: &amp;quot;Wed Jan 13 06:00:44 &#43;0000 2016&amp;quot;, &amp;quot;favourites_count&amp;quot;: 1, &amp;quot;utc_offset&amp;quot;: null, &amp;quot;time_zone&amp;quot;: null, &amp;quot;geo_enabled&amp;quot;: false, &amp;quot;verified&amp;quot;: false, &amp;quot;statuses_count&amp;quot;: 4, &amp;quot;lang&amp;quot;: &amp;quot;en&amp;quot;, &amp;quot;contributors_enabled&amp;quot;: false, &amp;quot;is_translator&amp;quot;: false, &amp;quot;is_translation_enabled&amp;quot;: false, &amp;quot;profile_background_color&amp;quot;: &amp;quot;F5F8FA&amp;quot;, &amp;quot;profile_background_image_url&amp;quot;: null, &amp;quot;profile_background_image_url_https&amp;quot;: null, &amp;quot;profile_background_tile&amp;quot;: false, &amp;quot;profile_image_url&amp;quot;: &amp;quot;http:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_image_url_https&amp;quot;: &amp;quot;https:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_link_color&amp;quot;: &amp;quot;1DA1F2&amp;quot;, &amp;quot;profile_sidebar_border_color&amp;quot;: &amp;quot;C0DEED&amp;quot;, &amp;quot;profile_sidebar_fill_color&amp;quot;: &amp;quot;DDEEF6&amp;quot;, &amp;quot;profile_text_color&amp;quot;: &amp;quot;333333&amp;quot;, &amp;quot;profile_use_background_image&amp;quot;: true, &amp;quot;has_extended_profile&amp;quot;: false, &amp;quot;default_profile&amp;quot;: true, &amp;quot;default_profile_image&amp;quot;: false, &amp;quot;following&amp;quot;: false, &amp;quot;follow_request_sent&amp;quot;: false, &amp;quot;notifications&amp;quot;: false, &amp;quot;translator_type&amp;quot;: &amp;quot;none&amp;quot; }, &amp;quot;geo&amp;quot;: null, &amp;quot;coordinates&amp;quot;: null, &amp;quot;place&amp;quot;: null, &amp;quot;contributors&amp;quot;: null, &amp;quot;is_quote_status&amp;quot;: false, &amp;quot;retweet_count&amp;quot;: 0, &amp;quot;favorite_count&amp;quot;: 0, &amp;quot;favorited&amp;quot;: false, &amp;quot;retweeted&amp;quot;: false, &amp;quot;lang&amp;quot;: &amp;quot;en&amp;quot; }, { &amp;quot;created_at&amp;quot;: &amp;quot;Thu Feb 08 02:18:10 &#43;0000 2018&amp;quot;, &amp;quot;id&amp;quot;: 9.6142387998514e&#43;17, &amp;quot;id_str&amp;quot;: &amp;quot;961423879985139712&amp;quot;, &amp;quot;full_text&amp;quot;: &amp;quot;0010020030040050060070080090100110120130140150160170180190200210220230240250260270280290300310320330340350360370380390400410420430440450460470480490500510520530540550560570580590600610620630640650660670680690700710720730740750760770780790800810820830840850860870880890900910920930&amp;quot;, &amp;quot;truncated&amp;quot;: false, &amp;quot;display_text_range&amp;quot;: [ 0, 280 ], &amp;quot;entities&amp;quot;: { &amp;quot;hashtags&amp;quot;: [ ], &amp;quot;symbols&amp;quot;: [ ], &amp;quot;user_mentions&amp;quot;: [ ], &amp;quot;urls&amp;quot;: [ ] }, &amp;quot;metadata&amp;quot;: { &amp;quot;iso_language_code&amp;quot;: &amp;quot;und&amp;quot;, &amp;quot;result_type&amp;quot;: &amp;quot;recent&amp;quot; }, &amp;quot;source&amp;quot;: &amp;quot;&amp;lt;a href=\&amp;quot;http:\/\/twitter.com\&amp;quot; rel=\&amp;quot;nofollow\&amp;quot;&amp;gt;Twitter Web Client&amp;lt;\/a&amp;gt;&amp;quot;, &amp;quot;in_reply_to_status_id&amp;quot;: null, &amp;quot;in_reply_to_status_id_str&amp;quot;: null, &amp;quot;in_reply_to_user_id&amp;quot;: null, &amp;quot;in_reply_to_user_id_str&amp;quot;: null, &amp;quot;in_reply_to_screen_name&amp;quot;: null, &amp;quot;user&amp;quot;: { &amp;quot;id&amp;quot;: 4751643594, &amp;quot;id_str&amp;quot;: &amp;quot;4751643594&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;screen_name&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;location&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;url&amp;quot;: null, &amp;quot;entities&amp;quot;: { &amp;quot;description&amp;quot;: { &amp;quot;urls&amp;quot;: [ ] } }, &amp;quot;protected&amp;quot;: false, &amp;quot;followers_count&amp;quot;: 0, &amp;quot;friends_count&amp;quot;: 5, &amp;quot;listed_count&amp;quot;: 0, &amp;quot;created_at&amp;quot;: &amp;quot;Wed Jan 13 06:00:44 &#43;0000 2016&amp;quot;, &amp;quot;favourites_count&amp;quot;: 1, &amp;quot;utc_offset&amp;quot;: null, &amp;quot;time_zone&amp;quot;: null, &amp;quot;geo_enabled&amp;quot;: false, &amp;quot;verified&amp;quot;: false, &amp;quot;statuses_count&amp;quot;: 4, &amp;quot;lang&amp;quot;: &amp;quot;en&amp;quot;, &amp;quot;contributors_enabled&amp;quot;: false, &amp;quot;is_translator&amp;quot;: false, &amp;quot;is_translation_enabled&amp;quot;: false, &amp;quot;profile_background_color&amp;quot;: &amp;quot;F5F8FA&amp;quot;, &amp;quot;profile_background_image_url&amp;quot;: null, &amp;quot;profile_background_image_url_https&amp;quot;: null, &amp;quot;profile_background_tile&amp;quot;: false, &amp;quot;profile_image_url&amp;quot;: &amp;quot;http:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_image_url_https&amp;quot;: &amp;quot;https:\/\/pbs.twimg.com\/profile_images\/861457044372471809\/C2gG1irn_normal.jpg&amp;quot;, &amp;quot;profile_link_color&amp;quot;: &amp;quot;1DA1F2&amp;quot;, &amp;quot;profile_sidebar_border_color&amp;quot;: &amp;quot;C0DEED&amp;quot;, &amp;quot;profile_sidebar_fill_color&amp;quot;: &amp;quot;DDEEF6&amp;quot;, &amp;quot;profile_text_color&amp;quot;: &amp;quot;333333&amp;quot;, &amp;quot;profile_use_background_image&amp;quot;: true, &amp;quot;has_extended_profile&amp;quot;: false, &amp;quot;default_profile&amp;quot;: true, &amp;quot;default_profile_image&amp;quot;: false, &amp;quot;following&amp;quot;: false, &amp;quot;follow_request_sent&amp;quot;: false, &amp;quot;notifications&amp;quot;: false, &amp;quot;translator_type&amp;quot;: &amp;quot;none&amp;quot; }, &amp;quot;geo&amp;quot;: null, &amp;quot;coordinates&amp;quot;: null, &amp;quot;place&amp;quot;: null, &amp;quot;contributors&amp;quot;: null, &amp;quot;is_quote_status&amp;quot;: false, &amp;quot;retweet_count&amp;quot;: 0, &amp;quot;favorite_count&amp;quot;: 0, &amp;quot;favorited&amp;quot;: false, &amp;quot;retweeted&amp;quot;: false, &amp;quot;lang&amp;quot;: &amp;quot;und&amp;quot; } ], &amp;quot;search_metadata&amp;quot;: { &amp;quot;completed_in&amp;quot;: 0.026, &amp;quot;max_id&amp;quot;: 9.6142489253621e&#43;17, &amp;quot;max_id_str&amp;quot;: &amp;quot;961424892536205313&amp;quot;, &amp;quot;query&amp;quot;: &amp;quot;scnydq&amp;quot;, &amp;quot;refresh_url&amp;quot;: &amp;quot;?since_id=961424892536205313&amp;amp;q=scnydq&amp;amp;include_entities=1&amp;quot;, &amp;quot;count&amp;quot;: 15, &amp;quot;since_id&amp;quot;: 0, &amp;quot;since_id_str&amp;quot;: &amp;quot;0&amp;quot; } }  参考 Tweet updates
</content>
    </entry>
    
     <entry>
        <title>[JS] 通过升级npm解决了error TS2322和TS2307</title>
        <url>https://mryqu.github.io/post/js_%E9%80%9A%E8%BF%87%E5%8D%87%E7%BA%A7npm%E8%A7%A3%E5%86%B3%E4%BA%86error_ts2322%E5%92%8Cts2307/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>npm</tag><tag>grunt</tag><tag>typescript</tag><tag>error</tag><tag>TS2322</tag><tag>TS2307</tag>
        </tags>
        <content type="html"> 上一个项目有个defect需要解决，很久没有动它了，所以一上来先git pull更新代码库，然后通过gradlew clean build &amp;ndash;refresh-dependencies进行构建，不料竟然碰到很多TS2322和TS2307错误，最后:grunt_build任务以失败告终。通过如下命令更新npm，问题不复重现。
npm cache clean npm install  不过具体是怎么解决的问题，还是不太清楚。有可能是因为npm升级了TypeScript，从而使问题得以解决。
C:\&amp;gt;npm list -g C:\Users\xxxxxx\AppData\Roaming\npm `-- typescript@2.1.6 C:\xxxgitws\xxxxxx-app&amp;gt;npm list typescript xxxxxx-app@3.0.0-0 C:\xxxgitws\xxxxxx-app `-- guides-buildtools-openuibundled@8.0.0 `-- guides-buildtools-openui@8.0.0 `-- typescript@2.4.1  </content>
    </entry>
    
     <entry>
        <title>[JS] 鼠标点的screenX/Y、clientX/Y、pageX/Y和offsetX/Y</title>
        <url>https://mryqu.github.io/post/js_%E9%BC%A0%E6%A0%87%E7%82%B9%E7%9A%84screenxyclientxypagexy%E5%92%8Coffsetxy/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>mousepoint</tag><tag>screen</tag><tag>client</tag><tag>page</tag>
        </tags>
        <content type="html">  理解  clientX/Y：鼠标点相对浏览器窗口内容区域（viewport）左上角的偏移量。
桌面浏览器基本支持，移动浏览器有可能不支持。 pageX/Y：鼠标点相对浏览器所有渲染内容区域（viewport）左上角的偏移量。(滚动后，文档左上角有可能不在浏览器窗口中，仍旧从文档左上角算起)
桌面浏览器基本支持，移动浏览器有可能不支持。 screenX/Y：鼠标点相对物理显示器左上角的偏移量。
当浏览器换了位置或屏幕分辨率改了，即使clientX/Y不变，screenX/Y值都有可能变动。
桌面和移动浏览器都基本支持。 offsetX/offsetY：鼠标点相对事件目标左上角的偏移量。
实验性质技术，桌面浏览器基本支持，移动浏览器有可能不支持。  代码示例 测试 做了两次测试：第一次测试没有滚动浏览器，第二次测试滚动了浏览器。
两次点击的clientY都是22；
两次点击的screenY都是225；
两次点击的pageY分别是22和428（整个文档渲染区域左上角滚动后没有出现在浏览器内）；
两次点击的offsetY分别是6和413（段落渲染区域左上角滚动后没有出现在浏览器内）。
参考 What is the difference between screenX/Y, clientX/Y and pageX/Y?
getMousePosition.js
MDN: MouseEvent.screenX
MDN: MouseEvent.clientX
MDN: MouseEvent.pageX
MDN: MouseEvent.offsetX
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] Theme加载</title>
        <url>https://mryqu.github.io/post/openui5_theme%E5%8A%A0%E8%BD%BD/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>html</tag><tag>theme</tag><tag>loading</tag>
        </tags>
        <content type="html"> 瞄了一下OpenUI5中UI主题加载，关键点在sap.ui.core.Core.includeLibraryTheme方法。其调用者主要为： - sap.ui.core.Core._boot：启动OpenUI5核心时加载必要的主题 - sap.ui.core.Core.initLibrary：加载某个库时会尝试加载其主题 假定config.js内容如下：
window[&#39;sap-ui-config&#39;] = { bindingSyntax: &#39;complex&#39;, modules: [ &amp;quot;sap.m.library&amp;quot;, &amp;quot;sap.ui.commons.library&amp;quot;, &amp;quot;sap.ui.table.library&amp;quot;, &amp;quot;sap.ui.layout.library&amp;quot;, &amp;quot;yqu.ui.kexiao.library&amp;quot; ] } };  OpenUI5在加载yqu.ui.kexiao.library库时会尝试加载其主题。
Core.includeLibraryTheme (Core.js?eval:xxxx) Core.initLibrary (Core.js?eval:xxxx) (anonymous) (Interface.js?eval:xx) (anonymous) (library.js?eval:xx) evalModuleStr (sap-ui-core-dbg.js:xxxxx) execModule (sap-ui-core-dbg.js:xxxxx) requireModule (sap-ui-core-dbg.js:xxxxx) jQuery.sap.require (sap-ui-core-dbg.js:xxxxx) Core.loadLibrary (Core.js?eval:xxxx) .............  </content>
    </entry>
    
     <entry>
        <title>Typescript类型定义文件(.d.ts)生成工具</title>
        <url>https://mryqu.github.io/post/typescript%E7%B1%BB%E5%9E%8B%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>typescript</tag><tag>type</tag><tag>definition</tag><tag>declare</tag><tag>generator</tag>
        </tags>
        <content type="html">  Microsoft/dts-gen - dts-gen creates starter TypeScript definition files for any module or library. dtsmake - d.ts file generator tool from JavaScript files. dtsgenerator - d.ts file generator tool, for only JSON Schema files. js2tsd - d.ts file generator tool, no type inferrence. JS2TSD d.ts file generator GUI tool app. Not CLI.  </content>
    </entry>
    
     <entry>
        <title>[Gradle] 遭遇 Unable to process incoming event &#39;ProgressComplete&#39; (ProgressCompleteEvent)</title>
        <url>https://mryqu.github.io/post/gradle_%E9%81%AD%E9%81%87_unable_to_process_incoming_event_progresscomplete_progresscompleteevent/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>build</tag><tag>progresscomplete</tag><tag>git</tag>
        </tags>
        <content type="html"> 最近开始玩一个项目，结果gradle build总是报错：
FAILURE: Build failed with an exception. * What went wrong: Unable to process incoming event &#39;ProgressComplete &#39; (ProgressCompleteEvent)  参考了帖子Build fails with “Unable to process incoming event ‘ProgressComplete ’ (ProgressCompleteEvent)”：
The Workaround: use the --console plain gradle command line switch The Fix: If you use git &#43; git-bash, upgrade to git 2.x.x (2.9.3 is current and works for me) If you use DOS, try increasing your screen buffer size.(mine is 1024 x 1024)  将Git Window从2.9.0升级到2.15.0，问题不复重现。
</content>
    </entry>
    
     <entry>
        <title>[GoLang]add proxy for dep</title>
        <url>https://mryqu.github.io/post/golang_add_proxy_for_dep/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>dep</tag><tag>proxy</tag>
        </tags>
        <content type="html">  玩一下goquery，dep总是报错，无法解析golang.org/x/net和cascadia，添加代理后搞定。
Windows set https_proxy=http://username:password@[proxy_host]:[proxy_port] set http_proxy=http://username:password@[proxy_host]:[proxy_port] dep init -v  Linux $https_proxy=http://username:password@[proxy_host]:[proxy_port] http_proxy=http://username:password@[proxy_host]:[proxy_port] dep init -v  </content>
    </entry>
    
     <entry>
        <title>[Golang] 折腾一下Golang项目调试</title>
        <url>https://mryqu.github.io/post/golang_%E6%8A%98%E8%85%BE%E4%B8%80%E4%B8%8Bgolang%E9%A1%B9%E7%9B%AE%E8%B0%83%E8%AF%95/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>mingw-w64</tag><tag>tdm-gcc</tag><tag>golang</tag><tag>gdb</tag><tag>delve</tag>
        </tags>
        <content type="html">  想装个MinGW-W64玩玩64位GDB，才发现继上一博文MinGW安装和使用已三年了，不经意间到看到了下列帖子： - MinGW MinGW-w64 TDM-GCC等工具链之间的区别与联系 - 发现个新环境msys2 - MinGW、MinGW-w64 与TDM-GCC 应该如何选择？
对于mingw-w64、tdm-gcc、Win-builds这三个小纠结一会，觉得自己本身也就是用一下GDB，还是去https://sourceforge.net/projects/mingw-w64/直接下载MinGW-W64好了。
MinGW-W64安装 所用MinGW-W64安装配置：
| 配置项 | 配置值 | | - | - | | Architecture | x86_64 | | Threads | posix | | Exception | seh |
Golang项目编译 编译Golang项目采用 go build -gcflags “-N -l” 关闭内联优化。
GDB调试 载入runtime-gdb.py脚本以对Go运行时结构进行完美打印和转换(例如显示slice和map值、查看goroutine等)： - 参数载入：gdb -d $GCROOT - 手工载入：source C:\Go\src\runtime\runtime-gdb.py 折腾一会，最后还是折在GDB on windows with golang: buildsym_init Assertion &amp;ldquo;free_pendings == NULL&amp;rdquo; failed.这个问题上，一调试就DDB崩了，貌似目前无解。
Delve调试 不过Debugging Go Code with GDB里还介绍了另外一个对Golang支持更好的工具Delve。上手容易，有些命令跟GDB差不多。好了，就折腾到这里了。目前不太需要的东东就不花费力气了。
</content>
    </entry>
    
     <entry>
        <title>学习一下TOML</title>
        <url>https://mryqu.github.io/post/%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%B8%8Btoml/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>toml</tag><tag>json</tag><tag>yaml</tag><tag>configuration</tag>
        </tags>
        <content type="html"> TOML（Tom&amp;rsquo;s Obvious, Minimal Language）是（GitHub的联合创始人及前CEO）Tom Preston-Werner于2013年创建的语言，其目标是成为一个小规模的易于使用的语义化配置文件格式。TOML被设计为可以无二义性的转换为一个哈希表(Hash table)。它是YAML和JSON的替代品，跟JSON相对对人类更友好，比YAML更简单。 GitHub：toml-lang/toml
Learn toml in Y minutes
[](https://www.cnblogs.com/unqiang/p/6399136.html)
TOML简介 （转）
</content>
    </entry>
    
     <entry>
        <title>[Golang] 使用dep</title>
        <url>https://mryqu.github.io/post/golang_%E4%BD%BF%E7%94%A8dep/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>dep</tag><tag>package</tag><tag>dependency</tag><tag>management</tag>
        </tags>
        <content type="html">  之前的博文[Golang]Win10下Glide的安装和使用记录了对Glide的学习，本博将记录对Golang包管理工具dep的学习使用。
dep介绍 在2012年，go get成为获取依赖包的方式。dep的FAQ中有一段描述dep是否要取代go get的解答，一句话概括就是：依赖管理工具是为应用管理代码的，go get是为GOPATH管理代码的。go get仅仅支持获取master branch上的latest代码，没有指定version、branch或revision的能力。这不符合gopher对自己项目所依赖的第三方包受控的期望。 在2015年，Russ Cox在Go 1.5发布前期以一个experiment feature身份紧急加入vendor机制，vendor标准化了项目依赖的第三方库的存放位置，隔离不同项目依赖的同一个包的不同版本。 Golang的包管理一直没有官方统一的解决方案，因此也产生了很多非官方的包管理工具。这些工具很多都很不错，但是相互兼容性差。随着Go语言在全球范围内应用的愈加广泛，缺少官方包管理工具这一问题变得日益突出。2016年GopherCon大会后，由微服务框架go-kit作者Peter Bourgon牵头， 在Go官方的组织下，Go包管理委员会经过各种头脑风暴和讨论后发布了“包管理建议书”，并启动了最有可能被接纳为官方包管理工具的项目dep的设计和开发。当前主导dep开发的是Sam Boyer，Sam也是dep底层包依赖分析引擎-gps的作者。2017年年初，dep项目正式对外开放。
安装 我的实验平台是Win10，所以无法通过brew或者shell脚本安装，只能通过go get安装：
go get -u github.com/golang/dep/cmd/dep  安装完，执行dep命令，出现帮助代表安装成功。
$ dep Dep is a tool for managing dependencies for Go projects Usage: &amp;quot;dep [command]&amp;quot; Commands: init Set up a new Go project, or migrate an existing one status Report the status of the project&#39;s dependencies ensure Ensure a dependency is safely vendored in the project prune Pruning is now performed automatically by dep ensure. version Show the dep version information Examples: dep init set up a new project dep ensure install the project&#39;s dependencies dep ensure -update update the locked versions of all dependencies dep ensure -add github.com/pkg/errors add a dependency to the project Use &amp;quot;dep help [command]&amp;quot; for more information about a command. $ dep help ensure Usage: dep ensure [-update | -add] [-no-vendor | -vendor-only] [-dry-run] [-v] [...] Project spec: [:alt source URL][@] Ensure gets a project into a complete, reproducible, and likely compilable state: * All non-stdlib imports are fulfilled * All rules in Gopkg.toml are respected * Gopkg.lock records precise versions for all dependencies * vendor/ is populated according to Gopkg.lock Ensure has fast techniques to determine that some of these steps may be unnecessary. If that determination is made, ensure may skip some steps. Flags may be passed to bypass these checks; -vendor-only will allow an out-of-date Gopkg.lock to populate vendor/, and -no-vendor will update Gopkg.lock (if needed), but never touch vendor/. The effect of passing project spec arguments varies slightly depending on the combination of flags that are passed. Examples: dep ensure Populate vendor from existing Gopkg.toml and Gopkg.lock dep ensure -add github.com/pkg/foo Introduce a named dependency at its newest version dep ensure -add github.com/pkg/foo@^1.0.1 Introduce a named dependency with a particular constraint For more detailed usage examples, see dep ensure -examples. Flags: -add add new dependencies, or populate Gopkg.toml with constraints for existing dependencies (default: false) -dry-run only report the changes that would be made (default: false) -examples print detailed usage examples (default: false) -no-vendor update Gopkg.lock (if needed), but do not update vendor/ (default: false) -update update the named dependencies (or all, if none are named) in Gopkg.lock to the latest allowed by Gopkg.toml (default: false) -v enable verbose logging (default: false) -vendor-only populate vendor/ from Gopkg.lock without updating it first (default: false)  使用 main.go 仍然使用博文[Golang]Win10下Glide的安装和使用中的源码示例：
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/pborman/uuid&amp;quot; ); func main() { id := uuid.New(); fmt.Println(&amp;quot;Hello uuid:&amp;quot;&#43;id); }  初始化 通过dep init命令创建新项目（使用了-v选项打印更多细节）：
$ dep init -v Getting direct dependencies... Checked 5 directories for packages. Found 1 direct dependencies. Root project is &amp;quot;hellodep&amp;quot; 1 transitively valid internal packages 1 external packages imported from 1 projects (0) ? select (root) (1) ? attempt github.com/pborman/uuid with 1 pkgs; 5 versions to try (1) try github.com/pborman/uuid@v1.1 (1) ? select github.com/pborman/uuid@v1.1 w/1 pkgs ? found solution with 1 packages from 1 projects Solver wall times by segment: b-list-pkgs: 5.0792684s b-gmal: 5.0078158s b-source-exists: 2.1603804s b-list-versions: 1.4574079s select-atom: 0s other: 0s select-root: 0s b-deduce-proj-root: 0s satisfy: 0s new-atom: 0s TOTAL: 13.7048725s Using ^1.1.0 as constraint for direct dep github.com/pborman/uuid Locking in v1.1 (e790cca) for direct dep github.com/pborman/uuid (1/1) Wrote github.com/pborman/uuid@v1.1  dep init大致会做这么几件事： - 利用gps分析当前代码包中的包依赖关系； - 将分析出的项目包的直接依赖(即main.go显式import的第三方包，direct dependency)约束(constraint)写入项目根目录下的Gopkg.toml文件中； - 将项目依赖的所有第三方包（包括直接依赖和传递依赖transitive dependency）在满足Gopkg.toml中约束范围内的最新version/branch/revision信息写入Gopkg.lock文件中； - 创建root vendor目录，并且以Gopkg.lock为输入，将其中的包（精确checkout 到revision）下载到项目root vendor下面。 执行完dep init后，dep会在当前目录下生成若干文件：
├── Gopkg.lock ├── Gopkg.toml ├── main.go └── vendor/ └── github.com/ └── pborman/ └── uuid/  Gopkg.toml：记录了一个直接依赖uuid，确定的uuid依赖版本约束为1.1.0
[[constraint]] name = &amp;quot;github.com/pborman/uuid&amp;quot; version = &amp;quot;1.1.0&amp;quot; [prune] go-tests = true unused-packages = true  Gopkg.lock：记录了在上述约束下所有依赖可用的最新版本
[[projects]] name = &amp;quot;github.com/pborman/uuid&amp;quot; packages = [&amp;quot;.&amp;quot;] revision = &amp;quot;e790cca94e6cc75c7064b1332e63811d4aae1a53&amp;quot; version = &amp;quot;v1.1&amp;quot; [solve-meta] analyzer-name = &amp;quot;dep&amp;quot; analyzer-version = 1 inputs-digest = &amp;quot;fbe0d2bde8a8c1659d0e965328bda4e05f246c471267358b9dfe9c95fd1025ff&amp;quot; solver-name = &amp;quot;gps-cdcl&amp;quot; solver-version = 1  至此，dep init完毕，相关依赖包也已经被vendor，可以使用go build/install进行程序构建了。 如果你对dep自动分析出来的各种约束和依赖的版本没有异议，那么这里就可以将Gopkg.toml和Gopkg.lock作为项目源码的一部分提交到代码库中了。这样其他人在下载了你的代码后，可以通过dep直接下载lock文件中的第三方包版本，并存在vendor里。这样就使得无论在何处，项目构建的依赖库理论上都是一致的，实现可重现构建。但一般不推荐将vendor目录提交到代码仓库，vendor的提交会让代码库变得异常庞大，且更新vendor时大量的diff会影响到成员对代码的review（注：对于Git代码仓库可通过.gitignore过滤掉）。
构建 这里构建是通过Makefile进行，由于其他成员将项目从代码仓库克隆到本地后并没有vendor目录，所以在setup规则里使用dep ensure -vendor-only命令通过Gopkg.lock和Gopkg.toml中的数据创建vendor目录和同步里面的包，之后就可以完成可重现构建了。
BUILD_ENV := CGO_ENABLED=0 LDFLAGS=-ldflags &amp;quot;-w -s&amp;quot; TARGET_EXEC := hellodep .PHONY: all clean setup generate build-linux build-osx build-windows all: clean setup generate build-linux build-osx build-windows clean: rm -rf build setup: mkdir -p build/linux mkdir -p build/osx mkdir -p build/windows - ${BUILD_ENV} dep ensure -vendor-only build-linux: setup ${BUILD_ENV} GOARCH=amd64 GOOS=linux go build ${LDFLAGS} -o build/linux/${TARGET_EXEC} build-osx: setup ${BUILD_ENV} GOARCH=amd64 GOOS=darwin go build ${LDFLAGS} -o build/osx/${TARGET_EXEC} build-windows: setup ${BUILD_ENV} GOARCH=amd64 GOOS=windows go build ${LDFLAGS} -o build/windows/${TARGET_EXEC}.exe  dep ensure命令的一些选项如下： - -no-vendor选项允新Gopkg.lock，但不更新vendor目录 - -vendor-only选项允许使用过时的Gopkg.lock创建vendor目录 - -update选项更新Gopkg.lock中的命名依赖到Gopkg.toml允许的最高版本
指定依赖 如果依赖包相对Gopkg.lock增加了多个版本，而项目组想要使用较新的功能时，可能会尝试用dep ensure -update命令获取最新的依赖包。 一旦发现使用最新的依赖包有问题怎么办？可以通过在Gopkg.toml指定依赖版本使用较新的可用依赖包。
dep本地缓存 dep会在$GOPATH/pkg/dep/sources下留了一块“自留地”，用于缓存所有从network上下载的依赖包。 对dep init命令使用-gopath选项会优选从$GOPATH查找依赖包。
参考 GitHub：golang/dep
dep FAQ
dep roadmap
Gopkg.toml
Gopkg.lock
dep - Go dependency management
初窥dep
Golang官方依赖管理工具：dep
Go最新的dep详解
</content>
    </entry>
    
     <entry>
        <title>[Golang] Go项目的国际化 </title>
        <url>https://mryqu.github.io/post/golang_go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%9B%BD%E9%99%85%E5%8C%96/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>i18n</tag><tag>go-bindata</tag><tag>properties</tag><tag>go-i18n</tag>
        </tags>
        <content type="html">  因为我看的那些书里都没有提及Go项目的国际化实现，但在放狗搜索前，还是觉得跟Java/JS项目中的实践应该都差不多。搜了一下才发现自己Too young too simple！ 在Internationalization plan for Go中提到了Golang曾经的I18N路线图： * 对国际化文本提供全面支持 * 对国际化日期、时间等提供支持 * 对多语言消息提供支持
这个2011年的讨论有人提及Golang已经支持UTF8/unicode，日期时间自己可以格式化，其他人纷纷表示不同意他的观点，然后就没有然后了。 Golang的标准库还有提供完整的I18N支持，所以还需要对众多的Golang I18N库进行技术选型。通过https://golanglibs.com/search?q=i18n可知go-i18n排名第一，而且使用者也比较多。i18n4go为大厂IBM的cloud CTO出品，从Cloud Foundary CLI中提取出来的，估计能遇到的坎都解决掉了，但是排名并不靠前。 最终决定开始我的go-18n学习之旅。
通过go-bindata嵌入i18n properties文件 具体细节见之前的博文Go程序内嵌I18N properties文件。
资源文件 i18n/resources/locale.properties mryqu.hello=Welcome to Golang world! mryqu.intro=This is a go-bindata-i18n example. mryqu.verinfo={{.Cli}} Version {{.Version}} (Build: {{.Build}})  i18n/resources/locale_en.properties mryqu.hello=Welcome to Golang world! mryqu.intro=This is a go-bindata-i18n example. mryqu.verinfo={{.Cli}} Version {{.Version}} (Build: {{.Build}})  i18n/resources/locale_zh-CN.properties mryqu.hello=欢迎来到Golang世界！ mryqu.intro=这是一个go-bindata-i18n示例。 mryqu.verinfo={{.Cli}} 版本 {{.Version}} (构建：{{.Build}})  其他源码 i18n/i18n.go 与之前的博文Go程序内嵌I18N properties文件相比，这里添加了下面内容： 1. 使用github.com/magiconair/properties包获取properties文件内的键值对； 2. 通过github.com/nicksnyder/go-i18n/i18n包判断properties文件语言、为文件内所有键值对创建新的翻译。（如果不用properties文件，而是json或TOML格式文件，i18n.LoadTranslationFile就可以直接完成这些事情了，见https://github.com/nicksnyder/go-i18n/blob/master/i18n/bundle/bundle.go。）
go-i18n的优点： * 实现了CLDR plural rules。 * 使用text/template处理带有变量的字符串。 * 翻译文件可以是简单的JSON、TOML和YAML。
package i18n //go:generate go-bindata -pkg i18n -o resources.go resources/ import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/magiconair/properties&amp;quot; &amp;quot;github.com/nicksnyder/go-i18n/i18n&amp;quot; &amp;quot;github.com/nicksnyder/go-i18n/i18n/language&amp;quot; &amp;quot;github.com/nicksnyder/go-i18n/i18n/translation&amp;quot; &amp;quot;strings&amp;quot; ) const ( DefaultLocale = &amp;quot;en&amp;quot; AssetSuffix = &amp;quot;.properties&amp;quot; FileNamePrefix = &amp;quot;locale_&amp;quot; ) var locale = DefaultLocale var tDefault i18n.TranslateFunc = i18n.IdentityTfunc() var T i18n.TranslateFunc func init() { assets, err := AssetDir(&amp;quot;resources&amp;quot;) if err != nil { fmt.Println(&amp;quot;Unable to read required assets:&amp;quot;&#43;err.Error()) } for _, asset := range assets { flName := strings.TrimSuffix(asset, AssetSuffix) assetLocale := strings.TrimPrefix(flName, FileNamePrefix) if &amp;quot;&amp;quot; == assetLocale { assetLocale = &amp;quot;en&amp;quot; } transFile := string(MustAsset(&amp;quot;resources/&amp;quot; &#43; asset)) p, err := properties.Load([]byte(transFile), properties.UTF8) if err != nil { fmt.Printf(&amp;quot;ERROR: during properties processing - %&#43;v\n&amp;quot;, err) } tranDocuments := []map[string]interface{}{} for _, k := range p.Keys() { tranDocuments = append(tranDocuments, map[string]interface{}{ &amp;quot;id&amp;quot;: k, &amp;quot;translation&amp;quot;: p.GetString(k, k), }) } languages := language.Parse(assetLocale) for _, lang:= range languages { for _, trans := range tranDocuments { // TODO error handling t, err := translation.NewTranslation(trans) if err != nil { fmt.Printf(&amp;quot;ERROR: during translation processing - %&#43;v\n&amp;quot;, err) } i18n.AddTranslation(lang, t) } } tDefault, _ = i18n.Tfunc(DefaultLocale) T = tDefault } } func SetLocale(l string) { locale = l T = TfuncWithFallback(l) } func TfuncWithFallback(l string) i18n.TranslateFunc { t, err := i18n.Tfunc(l) if err != nil { fmt.Println(&amp;quot;failed to set i18n.Tfunc with &amp;quot;&#43;l) } return func(translationID string, args ...interface{}) string { if translated := t(translationID, args...); translated != translationID { return translated } return tDefault(translationID, args...) } }  main.go package main import ( &amp;quot;flag&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;os&amp;quot; &amp;quot;hello-bindata-i18n/i18n&amp;quot; ) var ( Cli string = &amp;quot;hello-bindata-i18n&amp;quot; Version string Build string ) func main() { if len(os.Args) &amp;gt; 1 { version := flag.Bool(&amp;quot;version&amp;quot;, false, &amp;quot;print version information and exit&amp;quot;) locale := flag.String(&amp;quot;locale&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;locale info&amp;quot;) flag.Parse() if locale != nil { i18n.SetLocale(*locale) } if *version { fmt.Println(i18n.T(&amp;quot;mryqu.verinfo&amp;quot;, map[string]interface{}{&amp;quot;Cli&amp;quot;: Cli, &amp;quot;Version&amp;quot;: Version, &amp;quot;Build&amp;quot;: Build})) os.Exit(0) } } fmt.Println(i18n.T(&amp;quot;mryqu.hello&amp;quot;)) }  Makefile 更多细节见之前的博文使用Makefile实现Go项目的跨平台构建 。这里在setup规则里增加了glide update，增加了generate规则。
BUILD_ENV := CGO_ENABLED=0 BUILD=`date &#43;%FT%T%z` LDFLAGS=-ldflags &amp;quot;-w -s -X main.Version=${VERSION} -X main.Build=${BUILD}&amp;quot; TARGET_EXEC := helloi18n .PHONY: all clean setup generate build-linux build-osx build-windows all: clean setup generate build-linux build-osx build-windows clean: rm -rf build setup: mkdir -p build/linux mkdir -p build/osx mkdir -p build/windows - ${BUILD_ENV} glide update generate: go generate -x ./i18n build-linux: setup ${BUILD_ENV} GOARCH=amd64 GOOS=linux go build ${LDFLAGS} -o build/linux/${TARGET_EXEC} build-osx: setup ${BUILD_ENV} GOARCH=amd64 GOOS=darwin go build ${LDFLAGS} -o build/osx/${TARGET_EXEC} build-windows: setup ${BUILD_ENV} GOARCH=amd64 GOOS=windows go build ${LDFLAGS} -o build/windows/${TARGET_EXEC}.exe  测试  $ build/windows/helloi18n.exe Welcome to Golang world! $ build/windows/helloi18n.exe -version hello-bindata-i18n Version 1.2.3 (Build: 2017-10-21T20:45:26&#43;0800) $ build/windows/helloi18n.exe -locale zh-CN 欢迎来到Golang世界！ $ build/windows/helloi18n.exe -locale zh-CN -version hello-bindata-i18n 版本 1.2.3 (构建：2017-10-21T20:45:26&#43;0800) $ build/windows/helloi18n.exe -locale zh 欢迎来到Golang世界！ $ build/windows/helloi18n.exe -locale zh -version hello-bindata-i18n 版本 1.2.3 (构建：2017-10-21T20:45:26&#43;0800) $ build/windows/helloi18n.exe -locale fr failed to set i18n.Tfunc with fr Welcome to Golang world!  参考 GitHub：nicksnyder/go-i18n
GitHub：jteeuwen/go-bindata
Go 内嵌静态资源
The Go Blog - Generating code
Go Web 编程 - 10 国际化和本地化
I18n strategies for Go with App Engine?
Internationalization plan for Go
GNU gettext utilities
</content>
    </entry>
    
     <entry>
        <title>[Golang] GOPATH和包导入</title>
        <url>https://mryqu.github.io/post/golang_gopath%E5%92%8C%E5%8C%85%E5%AF%BC%E5%85%A5/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>gopath</tag><tag>package</tag><tag>import</tag>
        </tags>
        <content type="html">  才开始玩GoLang，碰到一些与包导入相关的问题： * go build没有找到vendor目录下的包 * local import &amp;ldquo;./XXX&amp;rdquo; in non-local package
GoLang自定义包的特点  Go的package不局限于一个文件，可以由多个文件组成。组成一个package的多个文件，编译后实际上和一个文件类似，组成包的不同文件相互之间可以直接引用变量和函数，不论是否导出；因此，组成包的多个文件中不能有相同的全局变量和函数（这里有一个例外就是包的初始化函数：init函数） Go不要求package的名称和所在目录名相同，但是你最好保持相同，否则容易引起歧义。因为引入包的时候，go会使用子目录名作为包的路径，而你在代码中真正使用时，却要使用你package的名称。 每个子目录中只能存在一个package，否则编译时会报错。 Go的package是以绝对路径GOPATH来寻址的，不要用相对路径来导入  包的初始化函数init 包中可以有多个初始化函数init，每个初始化函数都会被调用，且顺序固定。 1. 对同一个Go文件的init()调用顺序是从上到下的 2. 对同一个package中不同文件是按文件名字符串比较“从小到大”顺序调用各文件中的init()函数。 3. 对于对不同的package，如果不相互依赖的话，按照main包中&amp;rdquo;先import的后调用&amp;rdquo;的顺序调用其包中的init() 4. 如果package存在依赖，则先调用最早被依赖的包中的init()
GOPATH go命令依赖一个重要的环境变量：$GOPATH。从go 1.8开始，GOPATH环境变量现在有一个默认值，如果它没有被设置。 它在Unix上默认为$HOME/go,在Windows上默认为%USERPROFILE%/go。GOPATH支持多个目录。
$GOPATH src |--github.com |-mryqu |-prj1 |-vendor |--prj2 |-vendor pkg |--相应平台 |-github.com |--mryqu |-prj1 |-XXX.a |-YYY.a |-ZZZ.a |-prj2 |-AAA.a |-BBB.a |-CCC.a  Go加载包时会从vendor tree、 $GOROOT下的src目录以及$GOPATH中的多目录下的src目录查找。
相对路径导入 通过go build无法完成非本地导入（non-local imports），必须使用go build main.go。go install根本不支持非本地导入。 相对路径导入文档位于https://golang.org/cmd/go/#hdr-Relative_import_paths 更多细节见： * https://groups.google.com/forum/#!topic/golang-nuts/1XqcS8DuaNc/discussion * https://github.com/golang/go/issues/12502 * https://github.com/golang/go/issues/3515#issuecomment-66066361
参考 Build Web Application with Golang
关于golang中包（package）的二三事儿
Go: local import in non-local package
</content>
    </entry>
    
     <entry>
        <title>[Golang] Go程序内嵌I18N properties文件</title>
        <url>https://mryqu.github.io/post/golang_go%E7%A8%8B%E5%BA%8F%E5%86%85%E5%B5%8Ci18n_properties%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>go-bindata</tag><tag>generate</tag><tag>i18n</tag><tag>properties</tag>
        </tags>
        <content type="html">  本博文将介绍一下如何将I18N properties文件内嵌到Go程序中。一般来说，go-i18n等Go包官方示例使用JSON文件保存I18N消息，而我的示例还是采用properties文件。
go-bindata go-bindata包可以将任何文件转换为可管理的Go源代码，在将二进制数据嵌入Go程序时是非常有帮助的。文件数据在转换成原始字节切片之前可做选择性的gzip压缩。 在我的示例中，我选择用go-bindata将i18n/resources下的i18n properties文件嵌入Go程序。
安装 go get -u github.com/jteeuwen/go-bindata/...  地址最后的三个点 &amp;hellip;会分析所有子目录并下载依赖编译子目录内容，而go-bindata的命令行工具在子目录中。go-bindata命令行工具将被安装到$GOPATH/bin目录中。 资源文件 i18n/resources/locale.properties mryqu.hello=Welcome to Golang world! mryqu.intro=This is a go-bindata example.  i18n/resources/locale_en.properties mryqu.hello=Welcome to Golang world! mryqu.intro=This is a go-bindata example.  i18n/resources/locale_zh-Hans.properties mryqu.hello=欢迎来到Golang世界！ mryqu.intro=这是一个go-bindata示例。  操练 看了go-bindata的帮助后，感觉go-bindata简单易用。这里就探索一下nocompress选项吧。
cd {MyPrj}/i18n go-bindata -pkg i18n -o resources.go resources/ go-bindata -pkg i18n -o resources-nocompress.go -nocompress resources/  通过对比resources.go和resources-nocompress.go可以看出，resources.go里面多引入了bytes、compress/gzip和io包，多生成了一个bindataRead函数用于读取gzip压缩后的数据。 在resources.go中的内嵌数据：
var _resourcesLocaleProperties = []byte(&amp;quot;\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\xff\xca\x2d\xaa\x2c\x2c\xd5\xcb\x48\xcd\xc9\xc9\xb7\x0d\x4f\xcd\x49\xce\xcf\x4d\x55\x28\xc9\x57\x70\xcf\xcf\x49\xcc\x4b\x57\x28\xcf\x2f\xca\x49\x51\xe4\xe5\x82\xa8\xca\xcc\x2b\x29\xca\xb7\x0d\xc9\xc8\x2c\x56\xc8\x2c\x56\x48\x54\x48\xcf\xd7\x4d\xca\xcc\x4b\x49\x2c\x49\x54\x48\xad\x48\xcc\x2d\xc8\x49\xd5\x03\x04\x00\x00\xff\xff\x45\xdc\x42\x7f\x4f\x00\x00\x00&amp;quot;) var _resourcesLocale_enProperties = []byte(&amp;quot;\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\xff\xca\x2d\xaa\x2c\x2c\xd5\xcb\x48\xcd\xc9\xc9\xb7\x0d\x4f\xcd\x49\xce\xcf\x4d\x55\x28\xc9\x57\x70\xcf\xcf\x49\xcc\x4b\x57\x28\xcf\x2f\xca\x49\x51\xe4\xe5\x82\xa8\xca\xcc\x2b\x29\xca\xb7\x0d\xc9\xc8\x2c\x56\xc8\x2c\x56\x48\x54\x48\xcf\xd7\x4d\xca\xcc\x4b\x49\x2c\x49\x54\x48\xad\x48\xcc\x2d\xc8\x49\xd5\x03\x04\x00\x00\xff\xff\x45\xdc\x42\x7f\x4f\x00\x00\x00&amp;quot;) var _resourcesLocale_zhHansProperties = []byte(&amp;quot;\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\xff\xca\x2d\xaa\x2c\x2c\xd5\xcb\x48\xcd\xc9\xc9\xb7\x7d\xb6\x66\xd1\x8b\xfd\x7d\xcf\xe6\x2e\x7d\xda\xb1\xc1\x3d\x3f\x27\x31\x2f\xfd\xc9\x8e\x69\xcf\xa7\xf6\xbc\xdf\xd3\xc8\xcb\x05\x51\x98\x99\x57\x52\x94\x6f\xfb\x62\xff\xcc\x67\x33\xd6\x3f\xd9\xd1\xf0\x64\xc7\xaa\xf4\x7c\xdd\xa4\xcc\xbc\x94\xc4\x92\xc4\xe7\x4b\x76\x3d\xd9\xd7\xfd\xb8\xa1\x09\x10\x00\x00\xff\xff\xf7\xd1\x50\xc9\x54\x00\x00\x00&amp;quot;)  在resources-nocompress.go中的内嵌数据：
var _resourcesLocaleProperties = []byte(`mryqu.hello=Welcome to Golang world! mryqu.intro=This is a go-bindata example.`) var _resourcesLocale_enProperties = []byte(`mryqu.hello=Welcome to Golang world! mryqu.intro=This is a go-bindata example.`) var _resourcesLocale_zhHansProperties = []byte(`mryqu.hello=欢迎来到Golang世界！ mryqu.intro=这是一个go-bindata示例。`)  在resources-nocompress.go中的内嵌数据和properties文件中的一模一样，而resources.go中的内嵌数据经过gzip压缩后已经看不出来原貌了。
go generate 从Go1.4起包含go generate命令，可以通过扫描Go源码中的特殊注释来识别要运行的常规命令。go generate不是go build的一部分，它不包含依赖关系分析，必须在运行go build之前显式运行。 Go generate命令很容易使用。要使go generate驱动上面的go-bindata过程，在同一目录中的任何一个普通（非生成）.go文件中，将该注释添加到文件中的任何位置:
//go:generate go-bindata -pkg i18n -o resources.go resources/  注释必须从行的开始处开始，并在//和go:generate之间没有空格。在该标记之后，该行的其余部分指定go generate运行的命令。
i18n/i18n.go package i18n //go:generate go-bindata -pkg i18n -o resources.go resources/ import ( &amp;quot;fmt&amp;quot; &amp;quot;strings&amp;quot; ) var Translations = map[string]string{} const ( AssetSuffix = &amp;quot;.properties&amp;quot; FileNamePrefix = &amp;quot;locale_&amp;quot; ) func init() { assets, err := AssetDir(&amp;quot;resources&amp;quot;) if err != nil { fmt.Println(&amp;quot;Unable to read required assets:&amp;quot;&#43;err) } for _, asset := range assets { flName := strings.TrimSuffix(asset, AssetSuffix) assetLocale := strings.TrimPrefix(flName, FileNamePrefix) if &amp;quot;&amp;quot; == assetLocale { assetLocale = &amp;quot;en&amp;quot; } Translations[assetLocale] = string(MustAsset(&amp;quot;resources/&amp;quot; &#43; asset)) } }  操练 cd {MyPrj} go generate -x ./i18n  一切顺利！
参考 GitHub：jteeuwen/go-bindata
Go 内嵌静态资源
The Go Blog - Generating code
</content>
    </entry>
    
     <entry>
        <title>[Golang] 使用Makefile实现Go项目的跨平台构建</title>
        <url>https://mryqu.github.io/post/golang_%E4%BD%BF%E7%94%A8makefile%E5%AE%9E%E7%8E%B0go%E9%A1%B9%E7%9B%AE%E7%9A%84%E8%B7%A8%E5%B9%B3%E5%8F%B0%E6%9E%84%E5%BB%BA/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>makefile</tag><tag>phony</tag><tag>ldflags</tag>
        </tags>
        <content type="html">  通常编译go程序，都是用go build，对于小的示例代码还可以应付，对于稍大一点的项目就有点繁琐了。 上网调差了一下，发现很多人是通过Makefile的形式对Go项目进行可重现构建的。下面我将以hellomake项目这个小示例展示如何使用Makefile实现Go项目的跨平台构建。
hellomake/Makefile BUILD_ENV := CGO_ENABLED=0 BUILD=`date &#43;%FT%T%z` LDFLAGS=-ldflags &amp;quot;-w -s -X main.Version=${VERSION} -X main.Build=${BUILD}&amp;quot; TARGET_EXEC := hellomake .PHONY: all clean setup build-linux build-osx build-windows all: clean setup build-linux build-osx build-windows clean: rm -rf build setup: mkdir -p build/linux mkdir -p build/osx mkdir -p build/windows build-linux: setup ${BUILD_ENV} GOARCH=amd64 GOOS=linux go build ${LDFLAGS} -o build/linux/${TARGET_EXEC} build-osx: setup ${BUILD_ENV} GOARCH=amd64 GOOS=darwin go build ${LDFLAGS} -o build/osx/${TARGET_EXEC} build-windows: setup ${BUILD_ENV} GOARCH=amd64 GOOS=windows go build ${LDFLAGS} -o build/windows/${TARGET_EXEC}.exe  需要说明以下几点： * 该Makefile将构建Linux、Osx和Windows三个平台的二进制可执行文件。 * 如果代码使用了 C binding，你可能会遇到一些问题。Cgo的问题在于需要一个与给定平台兼容的gcc. 如果开发在OSX/Windows 上完成，需要有一个能够兼容Linux的gcc已进行交叉编译。如果需要Cgo, docker镜像是创建 Linux 构建的最好方式。这种方式唯一的要求就是必须安装 Docker。 * LDFLAGS中的-w选项用于抹除DWARF符号表，-s选项用于抹除符号表和调试信息。更多细节详见参考一。 * 对于Makefile中的目标，如果目录下有跟目标同名的文件，则GNU Make认为已更新则不再执行。Phony目标可以避免这一问题。更多细节详见参考二。 * 该Makefile可以进一步扩充，实现glide或dep更新、go generate过程&amp;hellip;&amp;hellip;
hellomake/main.go package main import ( &amp;quot;flag&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;os&amp;quot; ) var ( Version string Build string ) func main() { if len(os.Args) == 2 { version := flag.Bool(&amp;quot;version&amp;quot;, false, &amp;quot;print version information and exit&amp;quot;) flag.Parse() if *version { fmt.Println(&amp;quot;hellomake &amp;quot; &#43; Version &#43; &amp;quot; (&amp;quot; &#43; Build &#43; &amp;quot;)&amp;quot;) os.Exit(0) } } fmt.Println(&amp;quot;Hello World!&amp;quot;) }  通过flag包解析命令行参数是否为-version，是的话打印版本信息，否则输出Hello World! 还可以增加构建者姓名等信息。此外如果使用Git代码仓库的话，也可以通过git describe &amp;ndash;tags获取Git标签，通过git rev-parse HEAD获得提交的SHA1。
构建 我的Windows环境有安装MinGW和Git Bash，所以图省事就直接在Windows平台上直接构建了。启动Git Bash，执行下列命令：
/c/MinGW/bin/mingw32-make all VERSION=1.2.3  通过GNU make命令构建Go项目，通过命令行传递VERSION变量。
测试 $ ./build/windows/hellomake -version hellomake 1.2.3 (2017-10-17T21:15:25&#43;0800) $ ./build/windows/hellomake Hello World!  参考 go tool link
GNU make - Phony Targets
Build Golang projects properly with Makefiles
A Makefile for your Go project
Golang: Don’t afraid of makefiles
GitHub: cloudflare/hellogopher
Golang：无惧makefile
win下 golang 跨平台编译
Go与Makefile实现跨平台交叉编译
GitHub: TimothyYe/ydict
</content>
    </entry>
    
     <entry>
        <title>[Golang] UUID包</title>
        <url>https://mryqu.github.io/post/golang_uuid%E5%8C%85/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>uuid</tag><tag>package</tag><tag>rfc4122</tag>
        </tags>
        <content type="html"> 最早Google的Go UUID包位于https://code.google.com/archive/p/go-uuid/，后来移到了https://github.com/pborman/uuid。而https://github.com/google/uuid是基于pborman版的，区别于之前的实现在于用16字节数组取代了字节切片，缺点是无法表示无效UUID。上述UUID包采用BSD 3-Clause许可协议。 此外还有采用MIT协议的https://github.com/satori/go.uuid，它支持UUID版本1-5，与RFC 4122和DCE 1.1兼容。通过https://golanglibs.com/top?q=uuid可以看出，它是GitHub点星(star)最多的UUID包，远远超过了Google的。
</content>
    </entry>
    
     <entry>
        <title>[Golang] Win10下Glide的安装和使用</title>
        <url>https://mryqu.github.io/post/golang_win10%E4%B8%8Bglide%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>glide</tag><tag>windows</tag><tag>dependencies</tag><tag>package</tag>
        </tags>
        <content type="html">  不论是开发Java还是你正在学习的Golang，都会遇到依赖管理问题。Java有牛逼轰轰的Maven和Gradle。 Golang亦有Godep、Govendor、Glide、dep等等。本文主要给大家介绍Glide。 Glide是Golang的包管理工具，是为了解决Golang依赖问题的。 为什么需要Glide？ 原因很简单，Go语言原生包管理的缺陷。罗列一下Golang的get子命令管理依赖有很多大缺陷： * 能拉取源码的平台很有限，绝大多数依赖的是 github.com * 不能区分版本，以至于令开发者以最后一项包名作为版本划分 * 依赖 列表/关系 无法持久化到本地，需要找出所有依赖包然后一个个 go get * 只能依赖本地全局仓库（GOPATH/GOROOT），无法将库放置于项目局部仓库（$PROJECT_HOME/vendor）
Glide是有下列几大主要功能： * 持久化依赖列表至配置文件中，包括依赖版本（支持范围限定）以及私人仓库等 * 持久化关系树至 lock 文件中（类似于 yarn 和 cargo），以重复拉取相同版本依赖 * 兼容 go get 所支持的版本控制系统：Git, Bzr, HG, and SVN * 支持 GO15VENDOREXPERIMENT 特性，使得不同项目可以依赖相同项目的不同版本 * 可以导入其他工具配置，例如： Godep, GPM, Gom, and GB
Glide在Mac或Linux上是很容易安装的，但是在Win10 x64上据说最新版有问题。详见https://github.com/Masterminds/glide/issues/873。 想多了没用，还是实干吧。从https://github.com/Masterminds/glide/releases上下载了glide-v0.13.0-windows-amd64.zip，里面就一个glide.exe。 将glide.exe放入%GOPATH%/bin下，然后将%GOPATH%/bin加入环境变量Path中，由于我的Go版本是1.9所以GO15VENDOREXPERIMENT环境变量就不用管了。执行glide --version ，开头没问题呀！ 进入我的项目目录%GOPATH%/src/helloglide，执行下列命令：
glide create #创建新的工作空间，生成glide.yaml glide get github.com/pborman/uuid #获取uui包 glide install #建立glide.lock版本 go build #构建项目 glide list #列举项目导入的所有包 INSTALLED packages: github.com\pborman\uuid glide tree #以树的形式列举项目导入的所有包 [WARN] The tree command is deprecated and will be removed in a future version helloglide |-- github.com/pborman/uuid (XXXXXX\helloglide\vendor\github.com\pborman\uuid)  一切顺利，之前算是我庸人自扰吧。Go官方有了dep项目，Glide也呼吁切换到dep上去，所以对Glide的学习就到这里吧。
参考 GitHub：Masterminds/glide
The glide.yaml File
Semantic Versioning 2.0.0
Versions and Ranges in glide
Golang依赖管理工具：glide从入门到精通使用
</content>
    </entry>
    
     <entry>
        <title>[Golang] 解决GoClipse安装问题</title>
        <url>https://mryqu.github.io/post/golang_%E8%A7%A3%E5%86%B3goclipse%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>goclipse</tag><tag>eclipse</tag><tag>godef</tag><tag>guru</tag>
        </tags>
        <content type="html">  按照GoClipse安装文档安装Eclipse的GoClipse插件时，提示org.eclipse.platform.feature.group无法找到。 我的Eclipse还是Eclipse 4.4 (Luna)，而GoClipse要求Eclipse 4.6 (Neon)及更高版本，看来不听话是没有好下场的。接着这个机会升级到Eclipse 4.7 (Oxygen)吧。
安装gocode、guru、godef gocode在安装LiteIDE时已经安装。GoClipse下载guru会执行go get -u golang.org/x/tools/cmd/guru ，其结果是网站不响应该请求。
go get -u github.com/nsf/gocode go get -u github.com/rogpeppe/godef mkdir %GOPATH%\src\golang.org\x\ cd %GOPATH%\src\golang.org\x\ git clone https://github.com/golang/tools.git go install golang.org/x/tools/cmd/guru  配置GoClipse 测试 齐活！
</content>
    </entry>
    
     <entry>
        <title>[Golang] 安装Go开发环境</title>
        <url>https://mryqu.github.io/post/golang_%E5%AE%89%E8%A3%85go%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</url>
        <categories>
          <category>Golang</category>
        </categories>
        <tags>
          <tag>golang</tag><tag>dev</tag>
        </tags>
        <content type="html">  项目上有几个Story涉及GoLang。Go语言是谷歌2009发布的第二款开源编程语言。Go语言专门针对多处理器系统应用程序的编程进行了优化，使用Go编译的程序可以媲美C或C&#43;&#43;代码的速度，而且更加安全、支持并行进程。 学习新语言从安装环境开始。到https://golang.org/下载一个Windows版的安装程序，就可以轻松搞定。 初次上手，习惯性地选了一个IDE。没下jetbrains家的GoLand，而是拿LiteIDE试试手。感觉有点卡顿，其他还好。 链接 https://golang.org/
LiteIDE
GoClipse installation
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 优化twitcurl项目中的HMAC_SHA1</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E4%BC%98%E5%8C%96twitcurl%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84hmac_sha1/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>sha1</tag><tag>hmacsha1</tag><tag>twitcurl</tag>
        </tags>
        <content type="html">  twitcurl开源项目中包含SHA1.cpp和HMAC_SHA1.cpp用于计算Twitter认证所需的HMAC-SHA1签名。 HMAC是密钥相关的哈希运算消息认证码（Hash-based Message Authentication Code），HMAC运算利用哈希算法，以一个密钥和一个消息为输入，生成一个消息摘要作为输出。HMAC_SHA1需要一个密钥，而SHA1不需要。HMAC_SHA1的公式为：SHA1(Key XOR opad, SHA1(Key XOR ipad, text)) 通过分析oauthlib.cpp和HMAC_SHA1.cpp可知： 1. 对于HMAC_SHA1算法，请求URL及参数信息作为文本输入，ConsumerSecret和AccessTokenSecret组合作为密钥输入； 2. 第一步：如果密钥输入大小超过64字节，则先做一次SHA1获取其摘要用于后继操作；否则直接使用密钥进行后继操作； 3. 第二步：密钥输入（/密钥输入摘要）XOR ipad（即0x36）； 4. 第三步：将上一步的[Key XOR ipad]和文本输入并入缓存AppendBuf1； 5. 第四步：使用上一步生成的缓存AppendBuf1一起进行SHA1以产生内部摘要； 6. 第五步：密钥输入（/密钥输入摘要）XOR opad（即0x5C）； 7. 第六步：将上一步的[Key XOR opad]和第四步产生的内部摘要并入缓存AppendBuf2； 8. 第七步：使用上一步生成的缓存AppendBuf2一起进行SHA1以产生外部摘要。
HMAC_SHA1.h中定义的AppendBuf1和AppendBuf2都有4K大小，合计8K。能不能省点空间呢？
下面我们可以看一个小示例：
char srcTest[] = &amp;quot;abcdef&amp;quot;; int srcTestL = strlen(srcTest); char srcTest1[] = &amp;quot;abc&amp;quot;; int srcTestL1 = strlen(srcTest1); char srcTest2[] = &amp;quot;def&amp;quot;; int srcTestL2 = strlen(srcTest2); unsigned char dst1[20] = &amp;quot;&amp;quot;; unsigned char dst2[20] = &amp;quot;&amp;quot;; CSHA1 sha1A = CSHA1(); sha1A.Reset(); sha1A.Update((UINT_8 *)srcTest, srcTestL); sha1A.Final(); sha1A.GetHash((UINT_8 *)dst1); for(int i=0;i&amp;lt;20;i&#43;&#43;) { if(i&amp;lt;19) printf(&amp;quot;%x &amp;quot;, dst1[i]); else printf(&amp;quot;%x\n&amp;quot;, dst1[i]); } CSHA1 sha1B = CSHA1(); sha1B.Reset(); sha1B.Update((UINT_8 *)srcTest1, srcTestL1); sha1B.Update((UINT_8 *)srcTest2, srcTestL2); sha1B.Final(); sha1B.GetHash((UINT_8 *)dst2); for(int i=0;i&amp;lt;20;i&#43;&#43;) { if(i&amp;lt;19) printf(&amp;quot;%x &amp;quot;, dst1[i]); else printf(&amp;quot;%x\n&amp;quot;, dst1[i]); }  sha1A实例是对srcTest1和srcTest2合并的内容进行SHA1，而sha1B实例是先后对srcTest1和srcTest2进行Update操作，最终二者生成的摘要是相同的。
1f 8a c1 f 23 c5 b5 bc 11 67 bd a8 4b 83 3e 5c 5 7a 77 d2  通过学习RFC3174 US Secure Hash Algorithm 1 (SHA1)中的SHA1Input函数，可知对多个文本进行Update操作，效果等同于对合并后的大文本进行一次Update操作。 空间优化方案如下： * 第一步中密钥输入大小超过64字节，则将输出写回CHMAC_SHA1::HMAC_SHA1的key所指缓存，更新key_len。这样就省掉了4K的SHA1_Key。后继操作使用key和key_len。 * 取消第三步和第六步，原第四步和第七步改用两个Update操作，这样省掉了4K的AppendBuf1和AppendBuf2。 * 64字节的m_ipad和m_opad的使用其实没有重叠在一起，合并成一个64字节的m_block足以。
参考 RFC3174: US Secure Hash Algorithm 1 (SHA1)
RFC2104： HMAC: Keyed-Hashing for Message Authentication
</content>
    </entry>
    
     <entry>
        <title>试试swagger-codegen的invokerPackage和basePackage</title>
        <url>https://mryqu.github.io/post/%E8%AF%95%E8%AF%95swagger-codegen%E7%9A%84invokerpackage%E5%92%8Cbasepackage/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Swagger</category>
        </categories>
        <tags>
          <tag>swagger</tag><tag>codegen</tag><tag>invokerpackage</tag><tag>basepackage</tag><tag>packageprefix</tag>
        </tags>
        <content type="html">  环境 mryqu@mryqu MINGW64 /c/playground/helloSwaggerCodegen $ ls config.json swagger.json swagger.yaml swagger-codegen-cli-2.2.3.jar mryqu@mryqu MINGW64 /c/playground/helloSwaggerCodegen $ cat swagger.yaml swagger: &#39;2.0&#39; info: description: Just a SpringFox demo version: &#39;2.0&#39; title: mryqu&#39;s REST API Demo contact: name: mryqu license: name: Apache License Version 2.0 url: https://github.com/mryqu/ host: localhost:8080 basePath: &amp;quot;/hellospringfox&amp;quot; tags: - name: greeting-controller description: Greeting Controller paths: &amp;quot;/greeting{?name}&amp;quot;: get: tags: - greeting-controller summary: Get greeting information operationId: greeting consumes: - application/json produces: - application/json - &amp;quot;*/*&amp;quot; parameters: - name: name in: query description: User Name required: true type: string default: World responses: &#39;200&#39;: description: OK schema: &amp;quot;$ref&amp;quot;: &amp;quot;#/definitions/Greeting&amp;quot; &#39;204&#39;: description: No Content &#39;401&#39;: description: Unauthorized &#39;403&#39;: description: Forbidden definitions: Greeting: type: object properties: content: type: string links: type: array items: &amp;quot;$ref&amp;quot;: &amp;quot;#/definitions/Link&amp;quot; Link: type: object properties: href: type: string rel: type: string templated: type: boolean mryqu@mryqu MINGW64 /c/playground/helloSwaggerCodegen $ cat config.json { &amp;quot;sourceFolder&amp;quot; : &amp;quot;src/generated/java&amp;quot;, &amp;quot;apiPackage&amp;quot; : &amp;quot;com.yqu.swagger.api&amp;quot;, &amp;quot;modelPackage&amp;quot; : &amp;quot;com.yqu.swagger.model&amp;quot;, &amp;quot;configPackage&amp;quot; : &amp;quot;com.yqu.swagger.conf&amp;quot; }  basePackage mryqu@mryqu MINGW64 /c/playground/helloSwaggerCodegen $ java -jar swagger-codegen-cli-2.2.3.jar generate -i swagger.yaml -l spring -o ./output1 -c config.json [main] INFO io.swagger.parser.Swagger20Parser - reading from swagger.yaml [main] WARN io.swagger.codegen.ignore.CodegenIgnoreProcessor - Output directory does not exist, or is inaccessible. No file (.swager-codegen-ignore) will be evaluated. [main] INFO io.swagger.codegen.DefaultCodegen - Invoker Package Name, originally not set, is now dervied from api package name: com.yqu.swagger [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src\generated\java\com\yqu\swagger\model\Greeting.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src\generated\java\com\yqu\swagger\model\Link.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src\generated\java\com\yqu\swagger\api\GreetingnameApiController.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src\generated\java\com\yqu\swagger\api\GreetingnameApi.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\pom.xml [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\README.md [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\com\yqu\swagger\conf\HomeController.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\io\swagger\Swagger2SpringBoot.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\io\swagger\RFC3339DateFormat.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src\main\resources\application.properties [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\com\yqu\swagger\api\ApiException.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\com\yqu\swagger\api\ApiResponseMessage.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\com\yqu\swagger\api\NotFoundException.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\com\yqu\swagger\api\ApiOriginFilter.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\src/generated/java\com\yqu\swagger\conf\SwaggerDocumentationConfig.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\.swagger-codegen-ignore [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output1\.swagger-codegen\VERSION mryqu@mryqu MINGW64 /c/playground/helloSwaggerCodegen $ java -jar swagger-codegen-cli-2.2.3.jar generate -i swagger.yaml -l spring -o ./output2 -c config.json --additional-properties basePackage=com.yqu.helloswagger [main] INFO io.swagger.parser.Swagger20Parser - reading from swagger.yaml [main] WARN io.swagger.codegen.ignore.CodegenIgnoreProcessor - Output directory does not exist, or is inaccessible. No file (.swager-codegen-ignore) will be evaluated. [main] INFO io.swagger.codegen.DefaultCodegen - Invoker Package Name, originally not set, is now dervied from api package name: com.yqu.swagger [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src\generated\java\com\yqu\swagger\model\Greeting.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src\generated\java\com\yqu\swagger\model\Link.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src\generated\java\com\yqu\swagger\api\GreetingnameApiController.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src\generated\java\com\yqu\swagger\api\GreetingnameApi.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\pom.xml [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\README.md [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\swagger\conf\HomeController.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\helloswagger\Swagger2SpringBoot.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\helloswagger\RFC3339DateFormat.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src\main\resources\application.properties [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\swagger\api\ApiException.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\swagger\api\ApiResponseMessage.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\swagger\api\NotFoundException.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\swagger\api\ApiOriginFilter.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\src/generated/java\com\yqu\swagger\conf\SwaggerDocumentationConfig.java [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\.swagger-codegen-ignore [main] INFO io.swagger.codegen.AbstractGenerator - writing file C:\playground\helloSwaggerCodegen\.\output2\.swagger-codegen\VERSION  在config.json中添加basePackage或者使用&amp;ndash;additional-properties选项添加basePackage，仅影响Swagger2SpringBoot和RFC3339DateFormat的包名。
invokerPackage invokerPackage对Swagger的客户端语言有意义（例如Java），但对Spring语言没有什么影响。
packagePrefix SAS有一个swagger-codegen gradle插件，其中使用了packagePrefix，但是使用对象是gradle任务所对的project。而使用swagger-codegen-cli.jar时这个配置没有什么作用。
参考 GitHub: swagger-api/swagger-codegen
[](https://gitlab.sas.com/athens/swagger-codegen-gradle-plugin/)
</content>
    </entry>
    
     <entry>
        <title>swagger-codegen 2.2.3与2.1.5区别</title>
        <url>https://mryqu.github.io/post/swagger-codegen_2.2.3%E4%B8%8E2.1.5%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Swagger</category>
        </categories>
        <tags>
          <tag>swagger</tag><tag>codegen</tag><tag>rest</tag><tag>api</tag><tag>spring</tag>
        </tags>
        <content type="html">  支持的语法 $ java -jar swagger-codegen-cli-2.1.6.jar langs Available languages: [android, aspnet5, async-scala, csharp, dart, flash, python-flask, go, java, jaxrs, jaxrs-cxf, jaxrs-resteasy, inflector, javascript, javascript-closure-angular, jmeter, nodejs-server, objc, perl, php, python, qt5cpp, ruby, scala, scalatra, silex-PHP, sinatra, slim, spring-mvc, dynamic-html, html, swagger, swagger-yaml, swift, tizen, typescript-angular, typescript-node, akka-scala, CsharpDotNet2, clojure, haskell-servant] $ java -jar swagger-codegen-cli-2.2.3.jar langs Available languages: [akka-scala, android, apache2, apex, aspnet5, aspnetcore, async-scala, bash, csharp, clojure, cwiki, cpprest, CsharpDotNet2, dart, elixir, eiffel, erlang-server, finch, flash, python-flask, go, go-server, groovy, haskell, jmeter, jaxrs-cxf-client, jaxrs-cxf, java, inflector, jaxrs-cxf-cdi, jaxrs-spec, jaxrs, msf4j, java-play-framework, jaxrs-resteasy-eap, jaxrs-resteasy, javascript, javascript-closure-angular, java-vertx, kotlin, lumen, nancyfx, nodejs-server, objc, perl, php, php-symfony, powershell, pistache-server, python, qt5cpp, rails5, restbed, ruby, scala, scalatra, silex-PHP, sinatra, slim, spring, dynamic-html, html2, html, swagger, swagger-yaml, swift4, swift3, swift, tizen, typescript-angular2, typescript-angular, typescript-fetch, typescript-jquery, typescript-node, undertow, ze-ph]  swagger-codegen 2.2.3相较2.1.5而言，多了35种语言支持。此外我最关心的sping-mvc也换成Spring，增加了对Spring Boot和Spring Cloud的支持。 swagger-codegen 2.1.5swagger-codegen 2.2.3akka-scalaakka-scalaandroidandroidapache2apexaspnet5aspnet5aspnetcoreasync-scalaasync-scalacsharpbashcsharpclojureclojurecwikicpprestCsharpDotNet2CsharpDotNet2dartdartelixireiffelerlang-serverfinchflashflashpython-flaskpython-flaskgogogo-servergroovyhaskell-servanthaskelljmeterjmeterjaxrs-cxf-clientjaxrs-cxfjaxrs-cxfjavajavainflectorinflectorjaxrs-cxf-cdijaxrs-specjaxrsjaxrsmsf4jjava-play-frameworkjaxrs-resteasy-eapjaxrs-resteasyjaxrs-resteasyjavascriptjavascriptjavascript-closure-angularjavascript-closure-angularjava-vertxkotlinlumennancyfxnodejs-servernodejs-serverobjcobjcperlperlphpphpphp-symfonypowershellpistache-serverpythonpythonqt5cppqt5cpprails5restbedrubyrubyscalascalascalatrascalatrasilex-PHPsilex-PHPsinatrasinatraslimslimspring-mvcspringdynamic-htmldynamic-htmlhtml2htmlhtmlswaggerswaggerswagger-yamlswagger-yamlswift4swift3swiftswifttizentizentypescript-angular2typescript-angulartypescript-angulartypescript-fetchtypescript-jquerytypescript-nodetypescript-nodeundertowze-ph
Spring配置 $ java -jar swagger-codegen-cli-2.1.5.jar config-help -l spring-mvc CONFIG OPTIONS sortParamsByRequiredFlag Sort method arguments to place required parameters before optional parameters. (Default: true) ensureUniqueParams Whether to ensure parameter names are unique in an operation (rename parameters that are not). (Default: true) modelPackage package for generated models apiPackage package for generated api classes invokerPackage root package for generated code groupId groupId in generated pom.xml artifactId artifactId in generated pom.xml artifactVersion artifact version in generated pom.xml sourceFolder source folder for generated code localVariablePrefix prefix for generated code members and local variables serializableModel boolean - toggle &amp;quot;implements Serializable&amp;quot; for generated models (Default: false) bigDecimalAsString Treat BigDecimal values as Strings to avoid precision loss. (Default: false) fullJavaUtil whether to use fully qualified name for classes under library library template (sub-template) to use (Default: ) - Default Spring MVC server stub. j8-async - Use async servlet feature and Java 8&#39;s default interface. Generating interface with service declaration is useful when using Maven plugin. Just provide a implementation with @Controller to instantiate service. configPackage configuration package for generated code $ java -jar swagger-codegen-cli-2.2.3.jar config-help -l spring CONFIG OPTIONS sortParamsByRequiredFlag Sort method arguments to place required parameters before optional parameters. (Default: true) ensureUniqueParams Whether to ensure parameter names are unique in an operation (rename parameters that are not). (Default: true) allowUnicodeIdentifiers boolean, toggles whether unicode identifiers are allowed in names or not, default is false (Default: false) modelPackage package for generated models apiPackage package for generated api classes invokerPackage root package for generated code groupId groupId in generated pom.xml artifactId artifactId in generated pom.xml artifactVersion artifact version in generated pom.xml artifactUrl artifact URL in generated pom.xml artifactDescription artifact description in generated pom.xml scmConnection SCM connection in generated pom.xml scmDeveloperConnection SCM developer connection in generated pom.xml scmUrl SCM URL in generated pom.xml developerName developer name in generated pom.xml developerEmail developer email in generated pom.xml developerOrganization developer organization in generated pom.xml developerOrganizationUrl developer organization URL in generated pom.xml licenseName The name of the license licenseUrl The URL of the license sourceFolder source folder for generated code localVariablePrefix prefix for generated code members and local variables serializableModel boolean - toggle &amp;quot;implements Serializable&amp;quot; for generated models (Default: false) bigDecimalAsString Treat BigDecimal values as Strings to avoid precision loss. (Default: false) fullJavaUtil whether to use fully qualified name for classes under hideGenerationTimestamp hides the timestamp when files were generated withXml whether to include support for application/xml content type. This option only works for dateLibrary Option. Date library to use joda - Joda legacy - Legacy java.util.Date java8 - Java 8 native - note: this also sets &amp;quot;java8&amp;quot; to true java8 Option. Use Java8 classes instead of third party equivalents true - Use Java 8 classes such as Base64 false - Various third party libraries as needed title server title name or client service name configPackage configuration package for generated code basePackage base package for generated code interfaceOnly Whether to generate only API interface stubs without the server files. (Default: false) delegatePattern Whether to generate the server files using the delegate pattern (Default: false) singleContentTypes Whether to select only one produces/consumes content-type by operation. (Default: false) java8 use async use async Callable controllers (Default: false) responseWrapper wrap the responses in given type (Future,Callable,CompletableFuture,ListenableFuture,DeferredResult,HystrixCommand,RxObservable,RxSingle or fully qualified type) useTags use tags for creating interface and controller classnames (Default: false) useBeanValidation Use BeanValidation API annotations (Default: false) implicitHeaders Use of @ApiImplicitParams for headers. (Default: false) useOptional Use Optional container for optional parameters (Default: false) library library template (sub-template) to use (Default: spring-boot) spring-boot - Spring-boot Server application using the SpringFox integration. spring-mvc - Spring-MVC Server application using the SpringFox integration. spring-cloud - Spring-Cloud-Feign client with Spring-Boot auto-configured settings.  对于Spring程序，swagger-codegen 2.2.3相较2.1.5而言增加了28项配置。 swagger-codegen 2.1.5swagger-codegen 2.2.3sortParamsByRequiredFlagsortParamsByRequiredFlagensureUniqueParamsensureUniqueParamsallowUnicodeIdentifiersmodelPackagemodelPackageapiPackageapiPackageinvokerPackageinvokerPackagegroupIdgroupIdartifactIdartifactIdartifactVersionartifactVersionartifactUrlartifactDescriptionscmConnectionscmDeveloperConnectionscmUrldeveloperNamedeveloperEmaildeveloperOrganizationdeveloperOrganizationUrllicenseNamelicenseUrlsourceFoldersourceFolderlocalVariablePrefixlocalVariablePrefixserializableModelserializableModelbigDecimalAsStringbigDecimalAsStringfullJavaUtilfullJavaUtilhideGenerationTimestampwithXmldateLibraryjava8titleconfigPackageconfigPackagebasePackageinterfaceOnlydelegatePatternsingleContentTypesjava8asyncresponseWrapperuseTagsuseBeanValidationimplicitHeadersuseOptionallibrarylibrary
</content>
    </entry>
    
     <entry>
        <title>Ribbon和Spring Cloud Consul</title>
        <url>https://mryqu.github.io/post/ribbon%E5%92%8Cspring_cloud_consul/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>ribbon</tag><tag>consul</tag><tag>spring</tag><tag>loadbalance</tag><tag>负载均衡</tag>
        </tags>
        <content type="html">  学习一下Client Side Load Balancing with Ribbon and Spring Cloud快速入门指南，这里的客户端负载平衡是借助Netflix Ribbon实现的。很魔性，除了application.yml里有Ribbon的配置以及代码包含@RibbonClient、@LoadBalanced注解，应用程序几乎没什么工作要做了。
@RibbonClient和@LoadBalanced的区别 Difference between @RibbonClient and @LoadBalanced讲解了@RibbonClient和@LoadBalanced的区别。 @LoadBalanced是个标记注解，指示被注解的RestTemplate应该使用RibbonLoadBalancerClient与服务进行交互。反过来，这允许在URL除了使用物理主机名&#43;端口号组合外，还可以使用服务名的逻辑标识符。
restTemplate.getForObject(&amp;quot;http://some-service-name/user/{id}&amp;quot;, String.class, 1);  @RibbonClient是用于配置Ribbon客户端的。它不是必须的，当使用服务发现且默认Ribbon设置就可以满足需求时，无需使用@RibbonClient注解。 在下列两种情况下需要使用@RibbonClient注解： * 需要对特定Robbon客户端使用定制Ribbon设置 * 没有使用任何服务发现
定制Robbon设置：
@Configuration @RibbonClient(name = &amp;quot;foo&amp;quot;, configuration = FooConfiguration.class) public class TestConfiguration { }  注意FooConfiguration必须由@Configuration注解，但是它不能在主应用上下文@ComponentScan范围内，否则它将被所有@RibbonClient共享。如果使用@ComponentScan（或@SpringBootApplication），需要避免其被包含在内（例如放入独立不重叠的包内或显示指定@ComponentScan扫描的包）。
Spring Cloud Consul和Ribbon 在没用使用任何服务发现时，Ribbon从listOfServers配置里的服务器列表进行选择的。偶在项目中是用Consul的，它主业就是干服务发现的工作，而且还支持Netflix Ribbon。 网上有现成的Spring Cloud Consul和Ribbon示例spring-boot-consul-demo-tax和spring-boot-consul-demo-invoice。使用默认Ribbon设置，所以bootstrap.yml/application.yml里没有Ribbon设置，代码也没有使用@RibbonClient。 启动两个spring-boot-consul-demo-tax微服务实例和一个spring-boot-consul-demo-invoice微服务实例，在浏览器访问spring-boot-consul-demo-invoice微服务，就会发现spring-boot-consul-demo-invoice微服务实例以RoundRobin轮询方式调用两个spring-boot-consul-demo-tax微服务实例了。
Ribbon组件 通过BaseLoadBalancer可以看出Ribbon中的负载均衡器所包含的几个重要组件/属性，正是这几个组件为Ribbon的功能提供支持:
| 组件 | 描述 | | - | - | |Rule | 负载均衡策略，可以插件化地为Ribbon提供各种适用的负载均衡算法。| |Ping | 判断目标服务是否存活。对应不同的协议不同的方式去探测，得到后端服务是否存活。如有http的，还有对于微服务框架内的服务存活的NIWSDiscoveryPing是通过eureka client来获取的instanceinfo中的信息来获取。| |ServerList | 服务器列表，可以是静态的也可以是动态的。如果是（通过DynamicServerListLoadBalancer实现）动态的，会有后台线程以一定的间隔时间更新和过滤列表。| |LoadBalancerStats | 负载均衡器运行信息。记录负载均衡器的实时运行信息，这些运行信息可以被用来作为负载均衡器策略的输入。|
负载均衡策略 IRule | 策略名 | 策略描述 | 实现说明 | | - | - | | BestAvailableRule | 选择一个最小并发请求的服务器 | 逐个考察服务器的LoadBalancerStats信息，如果服务器被断路器断了则忽略，在其中选择ActiveRequestsCount最小的服务器。 | | AvailabilityFilteringRule | 过滤掉那些因为一直连接失败的被断路器断了的服务器，并过滤掉那些高并发的服务器（活跃连接数超过配置的阈值） | 使用一个AvailabilityPredicate来包含过滤服务器的逻辑，其实就是检查LoadBalancerStats里记录各个服务器的断路器状态和ActiveRequestsCount状态。当尝试十次后还无法选出合适的服务器，则通关过轮训策略选出一个。 | | ZoneAvoidanceRule | 通过组合判断服务器所在zone的性能和服务器可用性来选择server | 使用ZoneAvoidancePredicate和AvailabilityPredicate来判断是否选择某个服务器，前一个判断判定一个zone的运行性能是否可用，剔除不可用的zone（的所有服务器），AvailabilityPredicate用于过滤掉连接数过多的服务器。 | | RoundRobinRule | roundRobin方式轮询选择服务器 | 维护一个AtomicInteger类型的轮询下标，尝试十次从allServers选择下标对应位置的可用服务器（服务器不可用，则下标原子加一取模）。
有点疑问，allServers中前十个服务器不可用而第十一服务器可用，RoundRobinRule会取不到服务器，为什么不从reachableServers挑选呢？ | | WeightedResponseTimeRule | 根据响应时间为每个服务器分配一个权重，使用加权RoundRobin方式选择服务器。 | 一个定时器线程从ServerStats里面读取评估响应时间，为每个服务器计算一个权重。权重为所有服务器平均响应时间总和减去服务器自己的平均响应时间。当刚开始运行，没有形成统计时，使用RoundRobin策略选择服务器。 | | RandomRule | 随机选择一个服务器 | 在allServers随机选择下标，在reachableServers选择对应下标的可用服务器。
还是如上的疑问，为什么从allServers随机选择下标，而在reachableServers选择？ | | RetryRule | 对已有的负载均衡策略加上重试机制。 | 在一个配置时间段内使用subRule的方式无法选择到可用服务器时，重新尝试。 |
服务器存活检查 IPing | IPing方式 | 描述 | 实现说明 | | - | - | | PingConstant | 根据配置返回固定的服务器可用状态。 | isAlive方法仅返回的就是配置值。如果没配置，则默认为true。 | | DummyPing | 不进行任何实质操作，始终返回服务器可用。 | isAlive方法仅返回true。与NoOpPing类相比多实现了一个空的initWithNiwsConfig方法。 | | NoOpPing | 不进行任何实质操作，始终返回服务器可用。 | isAlive方法仅返回true。 | | PingUrl | Ping url以进行健康检查 | 通过对配置的服务器url进行HTTP GET操作，如果返回200 OK且HTTP响应体符合期望，则认为服务器存活，否则认为服务器有故障。 | | ConsulPing | Ping一个基于Consul服务发现的服务器 | 没有进行实质HTTP请求，通过Consul健康检查结果获取服务器是否可用信息。|
获取服务器列表 ServerList 其中ConsulServerList是通过serviceId获取某Consul服务的所有服务器实例列表。
过滤服务器 ServerListFilter 其中HealthServiceServerListFilter是用于过滤，获取通过健康检查的Consul服务器。
Ribbon配置 像上面所讲的Ribbon组件，负载均衡策略和服务器存活检查都有多种实现，所以Ribbon配置也是很丰富的。 Working with load balancers提供了一些Ribbon配置项，Client Side Load Balancer: Spring Cloud Ribbon提供了在Spring Boot环境对Ribbon进行配置的方法。 具体配置项可详见代码com.netflix.client.config.DefaultClientConfigImpl和com.netflix.client.config.CommonClientConfigKey。
| 配置项 | 配置类型 | | - | - | | AppName | String | | Version | String | | Port | Integer | | SecurePort | Integer | | VipAddress | String | | ForceClientPortConfiguration | Boolean | | DeploymentContextBasedVipAddresses | String | | MaxAutoRetries | Integer | | MaxAutoRetriesNextServer | Integer | | OkToRetryOnAllOperations | Boolean | | RequestSpecificRetryOn | Boolean | | ReceiveBufferSize | Integer | | EnablePrimeConnections | Boolean | | PrimeConnectionsClassName | String | | MaxRetriesPerServerPrimeConnection | Integer | | MaxTotalTimeToPrimeConnections | Integer | | MinPrimeConnectionsRatio | Float | | PrimeConnectionsURI | String | | PoolMaxThreads | Integer | | PoolMinThreads | Integer | | PoolKeepAliveTime | Integer | | PoolKeepAliveTimeUnits | String | | EnableConnectionPool | Boolean | | MaxHttpConnectionsPerHost | Integer | | MaxTotalHttpConnections | Integer | | MaxConnectionsPerHost | Integer | | MaxTotalConnections | Integer | | IsSecure | Boolean | | GZipPayload | Boolean | | ConnectTimeout | Integer | | BackoffTimeout | Integer | | ReadTimeout | Integer | | SendBufferSize | Integer | | StaleCheckingEnabled | Boolean | | Linger | Integer | | ConnectionManagerTimeout | Integer | | FollowRedirects | Boolean | | ConnectionPoolCleanerTaskEnabled | Boolean | | ConnIdleEvictTimeMilliSeconds | Integer | | ConnectionCleanerRepeatInterval | Integer | | EnableGZIPContentEncodingFilter | Boolean | | ProxyHost | String | | ProxyPort | Integer | | KeyStore | String | | KeyStorePassword | String | | TrustStore | String | | TrustStorePassword | String | | IsClientAuthRequired | Boolean | | CustomSSLSocketFactoryClassName | String | | IsHostnameValidationRequired | Boolean | | IgnoreUserTokenInConnectionPoolForSecureClient | Boolean | | ClientClassName | String | | InitializeNFLoadBalancer | Boolean | | NFLoadBalancerClassName | String | | NFLoadBalancerRuleClassName | String | | NFLoadBalancerPingClassName | String | | NFLoadBalancerPingInterval | Integer | | NFLoadBalancerMaxTotalPingTime | Integer | | NIWSServerListClassName | String | | ServerListUpdaterClassName | String | | NIWSServerListFilterClassName | String | | ServerListRefreshInterval | Integer | | EnableMarkingServerDownOnReachingFailureLimit | Boolean | | ServerDownFailureLimit | Integer | | ServerDownStatWindowInMillis | Integer | | EnableZoneAffinity | Boolean | | EnableZoneExclusivity | Boolean | | PrioritizeVipAddressBasedServers | Boolean | | VipAddressResolverClassName | String | | TargetRegion | String | | RulePredicateClasses | String | | RequestIdHeaderName | String | | UseIPAddrForServer | Boolean | | listOfServers | String |
Ribbon调用流 LoadBalancerAutoConfiguration类会创建了一个LoadBalancerInterceptor的Bean，用于实现对客户端发起请求时进行拦截，以实现客户端负载均衡；会创建了一个RestTemplateCustomizer的Bean，用于给RestTemplate增加LoadBalancerInterceptor拦截器；会维护了一个被@LoadBalanced注解修饰的RestTemplate对象列表，并在这里进行初始化，通过调用RestTemplateCustomizer的实例来给需要客户端负载均衡的RestTemplate增加LoadBalancerInterceptor拦截器。 RestTemplate被@LoadBalance注解后，其在ClientHttpRequestExecution调用的过程中被名为LoadBalancerInterceptor的ClientHttpRequestInterceptor拦截器截获，进而交给负载均衡器LoadBalancerClient去处理。 LoadBalancerClient只是一个抽象的负载均衡器接口，核心功能交给了ILoadBalancer实现类来处理。ILoadBalancer实现类通过配置IRule、IPing等信息，通过ConsulClient获取注册列表的信息并定时检查服务器健康状态，得到注册列表后，ILoadBalancer实现类就可以根据IRule的策略进行负载均衡。
参考 Client Side Load Balancer: Spring Cloud Ribbon
Advanced Spring Boot with Consul
SERVICE DISCOVERY USING CONSUL &amp;amp; SPRING CLOUD
GitHub: Netflix/ribbon
Netflix源码解析之Ribbon：客户端负载均衡器Ribbon的设计和实现
Netflix源码解析之Ribbon：负载均衡策略的定义和实现
Spring Cloud源码分析（二）Ribbon
深入理解Ribbon之源码解析
</content>
    </entry>
    
     <entry>
        <title>Spring Cloud Consul Config</title>
        <url>https://mryqu.github.io/post/spring_cloud_consul_config/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>cloud</tag><tag>consul</tag><tag>config</tag><tag>distrituted</tag>
        </tags>
        <content type="html">  Spring Cloud是在Spring Boot的基础上构建的，用于简化分布式系统构建的工具集，为开发人员提供快速建立分布式系统中的一些常见的模式。例如：分布式版本可控配置(Distributed/versioned configuration)，服务注册与发现(Service registration and discovery)、智能路由(intelligent routing)、服务间调用、负载均衡、断路器(circuit breakers)、微代理(micro-proxy)、控制总线(control bus)、一次性令牌(one-time tokens)、全局锁(global locks)、领导选举和集群状态(leadership election and cluster state)、分布式消息、分布式会话等。 Spring Cloud Config项目快速入门示例展示了用于分布式系统中由Git仓库支持的中央外部配置。但是偶在项目中是用Consul的，而Spring Cloud Consul项目的快速入门示例并没有展示如何使用Consul进行配置管理，所以还是自己攒一下吧。
Spring Cloud Consul简介 HashiCorp公司的Consul是用于基础架构中服务发现和配置的工具，支持服务发现、健康检查、用于不同用途的键值对存储、多数据中心支持。 Spring Cloud Consul通过自动配置及Spring环境和其他Spring编程模型进行绑定实现Cosul与Spring Boot应用的集成。通过一些简单的注释，即可激活应用内的通用模式，使用Hashicorp的Consul构建大型分布式系统。其功能如下： * 服务发现: 实例可以向Consul agent注册，客户端可以使用Spring管理的bean发现这些实例 * 支持Ribbon: 通过Spring Cloud Netflix提供的客户端负载均衡 * 支持Zuul: 通过Spring Cloud Netflix提供的动态路由和过滤 * 分布式配置: 使用Consul键值对存储 * 控制总线: 使用Consul事件的分布式控制事件
安装并启动Consul 每个集群需要最少三台Consul server，以建立仲裁(quorum)，每个机器上必须运行一个consul agent。 Consul Agent: - 健康检查
- 转发查询
Consul Server: - 存储数据
- 响应查询
- 领导选举
根据参考二Consul安装指南，可以很轻松地在本机安装Consul。用于开发环境启动本地单Consul实例的脚本可以使用参考一Spring Cloud Consul指南中所提到的src/main/bash/local_run_consul.sh。本文中采用如下命令：
consul agent -dev -ui  consul-config-demo build.gradle buildscript { repositories { mavenCentral() maven { url &amp;quot;http://repo.spring.io/libs-milestone&amp;quot; } } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.5.7.RELEASE&amp;quot;) classpath &amp;quot;io.spring.gradle:dependency-management-plugin:0.5.6.RELEASE&amp;quot; } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;org.springframework.boot&#39; apply plugin: &amp;quot;io.spring.dependency-management&amp;quot; dependencyManagement { imports { mavenBom &amp;quot;org.springframework.cloud:spring-cloud-consul-dependencies:1.2.1.RELEASE&amp;quot; } } jar { baseName = &#39;hello-spring-consul-configdemo&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() maven { url &amp;quot;http://repo.spring.io/libs-milestone&amp;quot; } } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) //compile(&amp;quot;org.springframework.cloud:spring-cloud-consul-config&amp;quot;) //compile(&amp;quot;org.springframework.cloud:spring-cloud-starter-consul-discovery&amp;quot;) compile(&#39;org.springframework.cloud:spring-cloud-starter-consul-all&#39;) }  Maven依赖管理包含&amp;rdquo;物料清单式&amp;rdquo;（BOM）概念。BOM可以控制项目依赖的版本，并可以集中定义和更新。由Better dependency management for Gradle可知，Gradle依赖管理插件可以使用Maven的BOM管理项目依赖。build.gradle中指定了spring-cloud-consul-dependencies版本，实质上是指定了下列Spring cloud consul依赖(不管直接还是传递依赖)的版本，并且在dependencies中无需指定这些依赖的version属性： src/main/java/com/yqu/consul/Application.java package com.yqu.consul; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient; import org.springframework.cloud.context.config.annotation.RefreshScope; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @EnableDiscoveryClient @SpringBootApplication @RestController @RefreshScope public class Application { @Value(&amp;quot;${greets}&amp;quot;) String name = &amp;quot;World&amp;quot;; @RequestMapping(&amp;quot;/&amp;quot;) public String home() { return &amp;quot;Hello &amp;quot; &#43; name; } public static void main(String[] args) { SpringApplication.run(Application.class, args); } }  @EnableDiscoveryClient注释用于服务自动注、客户端发现服务。具体实现可详见EnableDiscoveryClient.java、EnableDiscoveryClientImportSelector.java、AutoServiceRegistrationConfiguration.java、AutoServiceRegistrationProperties.java和spring-cloud-consul-discovery的/META-INF/spring.factories及其中所指定的RibbonConsulAutoConfiguration、ConsulConfigServerAutoConfiguration、ConsulAutoServiceRegistrationAutoConfiguration、ConsulServiceRegistryAutoConfiguration、ConsulDiscoveryClientConfigServiceBootstrapConfiguration类实现。 @Value注释用于获取配置。更多内容详见Spring Boot - Externalized Configuration和A Quick Guide to Spring @Value。 @RefreshScope注释使Spring Bean在配置变化后动态刷新。没有该注释的Bean仅在初始化时获取配置。具体实现可详见org.springframework.cloud.context.config.annotation.RefreshScope.java和org.springframework.cloud.context.scope.refresh.RefreshScope.java。
src/main/resources/bootstrap.yml spring: cloud: consul: config: watch: enabled: true port: 8500 discovery: register-health-check: true instanceId: ${spring.application.name}:${vcap.application.instance_id:${spring.application.instance_id:${random.value}}} application: name: consul-config-demo server: port: 0  spring.cloud.consul.config.watch.enabled用于激活对Consul配置变更的检查。具体实现可详见ConsulConfigAutoConfiguration.java和ConfigWatch.java。
配置Consul存储 在Consul上添加Application.java所需的配置greets： 测试 服务注册 当对consul-config-demo执行完gradle bootrun命令后，即可在Consul UI上看到注册的consul-config-demo服务了。 健康检查 Spring Cloud Consul对Spring Boot应用的健康检查是借助Spring Boot Actuator完成的。 服务测试 服务返回结果为Hello mryqu，即greets配置从Consul获取成功。当在Consul上更新greets配置，再次调用服务，会发现服务返回结果也会随greets配置而变更，这说明cloud.consul.config.watch配置和@RefreshScope注释生效。 注：Consul中的server.port配置可以覆盖掉bootstrap.yml，但是初始化后并不能更新（有次莫名其妙地可以更新，但不可重现）。
Spring Cloud Consul 优点 监控部署配置发现待监控的服务服务监控状态比单个实例健康状态更重要启动次序无关紧要部署简单环境隔离运行时更新无本地配置端点动态发现 参考 Spring Cloud Consul指南
Consul安装指南
Advanced Spring Boot with Consul
spring cloud集成 consul源码分析
</content>
    </entry>
    
     <entry>
        <title>OpenGrok使用感受</title>
        <url>https://mryqu.github.io/post/opengrok%E4%BD%BF%E7%94%A8%E6%84%9F%E5%8F%97/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>opengrok</tag><tag>source</tag><tag>code</tag><tag>search</tag><tag>index</tag>
        </tags>
        <content type="html">  之前为了学习项目涉及的C/C&#43;&#43;代码，试用过SourceInsight，后来改成Vim&#43;Exuberant Ctags&#43;Cscope。最近一个美国同事给了个链接，原来那边的兄弟是用OpenGrok搜项目代码的！ OpenGrok是一个快速、便于使用的源码搜索引擎与对照引擎，它能够帮助我们快速地搜索、定位、对照代码树。它可以理解各种程序语言和文件格式，及Mercurial、Git、SCCS、RCS、CVS、Subversion、Teamware、ClearCase、Perforce、Monotone和Bazaar等版本控制历史记录。OpenGrok是OpenSolaris操作系统源文件浏览和搜索的工具。 OpenGrok由Java语言实现，需要Java 1.8、一个Servlet容器以及Exuberant Ctags。 用完就一个字：爽！
参考 OpenGrok主页
GitHub：OpenGrok/OpenGrok
OpenGrok：Comparison with Similar Tools
OpenGrok：Supported Languages and Formats
OpenGrok：Supported Revision Control Systems
Ubuntu环境下OpenGrok的安装及使用
</content>
    </entry>
    
     <entry>
        <title>ArcGIS REST API资费</title>
        <url>https://mryqu.github.io/post/arcgis_rest_api%E8%B5%84%E8%B4%B9/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>esri</tag><tag>arcgis</tag><tag>rest</tag><tag>pricing</tag><tag>credit</tag>
        </tags>
        <content type="html">  一个ArcGIS开发者订阅每月有免费的50点积分。
GeoEnrichment API资费 在Esri GeoEnrichment Service提到了资费列表：
| GeoEnrichment服务资费 | Esri积分 | 美元 | | - | - | - | | 1 Stored GeoEnrichment Variable | 0.01 积分 (Per Feature) | $0.001 (Per Feature) | | 1,000 Stored GeoEnrichment Variables | 10 积分 (Per Feature) | $1.00 (Per Feature) | | 1 Non-Stored GeoEnrichment Request | 0.01 积分 | $0.001 | | 1000 Non-Stored GeoEnrichment Requests | 10 积分 | $1.00 | | Generated Infographic View | 0.01 积分 | $0.001 | | 1000 Generated Infographic Views | 10 积分 | $1.00 | | Generated Report | 10 积分 | $1.00 |
Geocoding API资费 在Esri World Geocoding Service提到了资费列表：
| Geocoding服务资费 | Esri积分 | 美元 | | - | - | - | | Geosearch, Individual Address - Not Stored | n/a | n/a | | Geocode, Batch or Stored | 0.04 积分 | $.004 | | Batch 1,000 Geocodes | 40 积分 | $4.00 |
</content>
    </entry>
    
     <entry>
        <title>玩玩ArcGIS REST API</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E7%8E%A9arcgis_rest_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>esri</tag><tag>arcgis</tag><tag>rest</tag><tag>api</tag><tag>geocode</tag><tag>geoenrichment</tag>
        </tags>
        <content type="html">  了解Esri和ArcGIS 美国环境系统研究所公司（Environmental Systems Research Institute, Inc. 简称Esri公司）成立于1969年，总部设在美国加利福利亚州雷德兰兹市，是世界最大的地理信息系统技术提供商。 ArcGIS是Esri公司集40余年地理信息系统（GIS）咨询和研发经验，奉献给用户的一套完整的GIS平台产品，具有强大的地图制作、空间数据管理、空间分析、空间信息整合、发布与共享的能力。
ArcGIS REST API ArcGIS REST API可用于包括ArcGIS Online在内的ArcGIS平台，包括： * Maps—随时可用的底图、参考层等。可用于快速为您的本地或全球数据添加上下文或背景。一些ArcGIS Online地图有页面主题，可以提供出你的应用所需的全部信息。 * World Geocoding Service—通过文本地址、商业名等查找位置，该服务也提供反向服务：通过地理坐标查找最近的地址。 * Directions and Routing Service (Network Analysis Service)—解决各种路线规划问题，例如简单的点到点路由、复杂的船舶航线规划、及行驶时间分析。 * GeoEnrichment Service—GeoEnrichment 服务借助本地化的人口、场所以及商业信息丰富了用户的地理数据。提交某个点、面、地址或地名后，可通过该服务了解该地区居民的生活习惯和生活方式、附近的商业类型以及区域邮政编码等信息。 * Spatial Analysis Service—各种可用于执行通用GIS分析的任务。传统上，包括查找热点和对周边汇总在内的许多任务都需要深度学习和专业知识才能运行。而这些空间分析任务仅包含少量需要研究的参数，也能获得相当不错的结果。 * Elevation Analysis service—高程分析服务允许您执行高程分析（轮廓，视角，总结高程）和水文分析（流域和跟踪下游）的各种操作。这些服务参考的数据由Esri托管和策划。
ArcGIS REST API有些是免费的，有些是付费。付费操作需要订阅ArcGIS Online，并减扣账户积分。 使用需付费API时，需要在请求中通过token参数指定访问令牌。获取ArcGIS REST API所需访问令牌的方法，请参见前一博文ArcGIS认证和登录。
响应数据格式 对于ArcGIS REST API，有些响应支持JSON、PJSON（个人理解就是完美打印版的JSON）、XML和BIN格式中的一种或多种。可在请求时通过f参数指定。
使用GeoEnrichment API GeoEnrichment服务能力、属性和限制可以通过下列请求获得：
http://geoenrich.arcgis.com/arcgis/rest/services/World/geoenrichmentserver/GeoEnrichment/?f=pjson  由响应可知，GeoEnrichment端点支持Enrich、CreateReport、Reports、Countries、DataCollections、StandardGeographyLevels这几种操作。
Countries操作 首先试一下国家列表：
http://geoenrich.arcgis.com/arcgis/rest/services/World/GeoenrichmentServer/Geoenrichment/Countries?f=pjson  当然Countries操作也可用于获取单个国家信息： StandardGeographyLevels操作 StandardGeographyLevels服务返回有效地理数据层列表。服务结果是BAIDNamePairs数组，包含数据层ID及相应名称。这些ID可被用于在其他分析中指定数据层。 下面的请求示例列举CN的有效地理数据层列表：
http://geoenrich.arcgis.com/arcgis/rest/services/World/GeoenrichmentServer/Geoenrichment/StandardGeographyLevels/CN?f=pjson  DataCollections操作 GeoEnrichment服务使用数据集合的概念定义服务返回的属性。更具体地说，数据集合是用于丰富输入特性的预先组合的属性列表。作为输入特性，集合属性可以描述所提交位置或区域的各种类型信息，例如人口特征和地理上下文。
http://geoenrich.arcgis.com/arcgis/rest/services/World/geoenrichmentserver/Geoenrichment/dataCollections?f=pjson  Reports和CreateReport操作 创建报告操作可为描述输入区域的各种用例创建各种高质量报告。如果使用一个点作为研究区域，该服务将围绕该点创建1英里的环形缓冲区以收集和附加丰富数据。或者，您可以围绕该点创建环形缓冲区或特定驾驶时间可达区域，生成包含有关人口特征、消费者支出、业务或市场潜力等相关信息的PDF或Excel报告。 报告选项可用于描述和更好地了解市场，顾客/客户以及特定领域商业竞争。 下面的请求示例列举US数据集中有效报告列表：
http://geoenrich.arcgis.com/arcgis/rest/services/World/geoenrichmentserver/Geoenrichment/Reports/US?f=pjson  下面的请求示例生成以坐标-117.1956、34.0572为中心一英里区域的人口和收入概况报告(report指定为dandi，从上一示例结果中可知为人口和收入概况报告)：
http://geoenrich.arcgis.com/arcgis/rest/services/World/geoenrichmentserver/GeoEnrichment/CreateReport?studyAreas=[{&amp;quot;geometry&amp;quot;:{&amp;quot;x&amp;quot;:-117.1956,&amp;quot;y&amp;quot;:34.0572}}]&amp;amp;report=dandi&amp;amp;f=bin&amp;amp;format=PDF&amp;amp;token={YOUR_TOKEN}  Enrich操作 提供地点或区域的事实数据。输入可为： * XY左边：例如studyAreas=[{&amp;ldquo;geometry&amp;rdquo;:{&amp;ldquo;x&amp;rdquo;:-117.1956,&amp;ldquo;y&amp;rdquo;:34.0572}}] * 多边形：例如[{&amp;ldquo;geometry&amp;rdquo;:{&amp;ldquo;rings&amp;rdquo;:[[[-117.185412,34.063170],[-122.81,37.81],[-117.200570,34.057196],[-117.185412,34.063170]]],&amp;ldquo;spatialReference&amp;rdquo;:{&amp;ldquo;wkid&amp;rdquo;:4326}},&amp;ldquo;attributes&amp;rdquo;:{&amp;ldquo;id&amp;rdquo;:&amp;ldquo;1&amp;rdquo;,&amp;ldquo;name&amp;rdquo;:&amp;ldquo;optional polygon area name&amp;rdquo;}}] * 命名统计区域：例如studyAreas=[{&amp;ldquo;sourceCountry&amp;rdquo;:&amp;ldquo;US&amp;rdquo;,&amp;ldquo;layer&amp;rdquo;:&amp;ldquo;US.ZIP5&amp;rdquo;,&amp;ldquo;ids&amp;rdquo;:[&amp;ldquo;92373&amp;rdquo;,&amp;ldquo;92129&amp;rdquo;]}] * 网络服务区域：
例如studyAreas=[{&amp;ldquo;geometry&amp;rdquo;:{&amp;ldquo;x&amp;rdquo;: -122.435, &amp;ldquo;y&amp;rdquo;: 37.785},&amp;ldquo;areaType&amp;rdquo;: &amp;ldquo;NetworkServiceArea&amp;rdquo;,&amp;ldquo;bufferUnits&amp;rdquo;: &amp;ldquo;Hours&amp;rdquo;,&amp;ldquo;bufferRadii&amp;rdquo;: [1],&amp;ldquo;travel_mode&amp;rdquo;:&amp;ldquo;Driving&amp;rdquo;}]
例如studyAreas=[{&amp;ldquo;geometry&amp;rdquo;:{&amp;ldquo;x&amp;rdquo;: -122.435, &amp;ldquo;y&amp;rdquo;: 37.785},&amp;ldquo;areaType&amp;rdquo;: &amp;ldquo;NetworkServiceArea&amp;rdquo;,&amp;ldquo;bufferUnits&amp;rdquo;: &amp;ldquo;Minutes&amp;rdquo;,&amp;ldquo;bufferRadii&amp;rdquo;: [10],&amp;ldquo;travel_mode&amp;rdquo;:&amp;ldquo;Walking&amp;rdquo;}] * 街道名称定位：例如studyAreas=[{&amp;ldquo;address&amp;rdquo;:{&amp;ldquo;text&amp;rdquo;:&amp;ldquo;380 New York St. Redlands, CA 92373&amp;rdquo;}}]
下面的请求示例请求坐标-117.1956、34.0572的事实数据：
http://geoenrich.arcgis.com/arcgis/rest/services/World/geoenrichmentserver/GeoEnrichment/enrich?studyareas=[{&amp;quot;geometry&amp;quot;:{&amp;quot;x&amp;quot;:-117.1956,&amp;quot;y&amp;quot;:34.0572}}]&amp;amp;f=pjson&amp;amp;token={YOUR_TOKEN}  使用Geocoding API Geocoding国家列表、属性和服务能力可以通过下列请求获得：
http://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/?f=pjson  由Free vs. paid operations可知，Geocoding API支持suggest（免费）、findAddressCandidates和reverseGeocode（免费或付费）、geocodeAddresses（付费）这几种操作。
suggest操作 Geocoding服务的suggest操作可为客户端应用提供输入自动完成建议。当我们不知道恰当目标名或地址时，可以通过suggest操作进行查找。findAddressCandidates操作的大部分请求也可以用suggest操作完成。 下面的请求示例查找坐标（-78.765460000000004、35.829056999999999）20公里内的宾馆，从中可以发现我住过的Embassy Suites-Raleigh Durham：
http://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/suggest?text=hotel&amp;amp;location=-78.765460000000004,35.829056999999999&amp;amp;distance=20000&amp;amp;f=pjson  findAddressCandidates操作 findAddressCandidates用于在一个请求中查找一个地址的Geo代码。地址可为街道名、名胜、官方地名、邮编或X/Y坐标。 下面的请求示例查找Cary市内的SAS，返回SAS园区信息：
http://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/findAddressCandidates?Address=SAS&amp;amp;City=Cary&amp;amp;outFields=*&amp;amp;forStorage=false&amp;amp;f=pjson  下面的示例先通过suggest获取中国的国际机场，然后显示北京首都国际机场的Geo代码：
http://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/suggest?text=国际机场&amp;amp;countryCode=CHN&amp;amp;f=pjson  http://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/findAddressCandidates?&amp;amp;singleLine=Airport&amp;amp;magicKey=GST7YMc0AM9UOsE9HhFtGTyVGST7YMc0AM9UOsE9DbTVHgA9HhB0Zcp0OhNtGMytaikZUsoAMsxGUoc4Hhp0OSTaSsEqAZ43AP9zJikuGPAmIYxQGPT-CZc0YiD7DsK7CZyAOh5-Dn47Z57ZJsF1&amp;amp;maxLocations=10&amp;amp;outFields=Match_addr,Place_addr,Type&amp;amp;f=pjson  reverseGeocode操作 reverseGeocode通过X/Y坐标判断地址信息。下面的请求示例返回坐标（-78.765460000000004、35.829056999999999）的地址信息，即SAS园区：
http://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/reverseGeocode?location=-78.765460000000004,35.829056999999999&amp;amp;distance=200&amp;amp;outSR=&amp;amp;f=pjson  geocodeAddresses操作 geocodeAddresses用于在一个请求中返回一列地址的Geo代码。下面的请求示例查找SAS园区和杜克大学的Geo代码：
http://geocode.arcgis.com/arcgis/rest/services/World/GeocodeServer/geocodeAddresses?addresses={&amp;quot;records&amp;quot;:[{&amp;quot;attributes&amp;quot;:{&amp;quot;OBJECTID&amp;quot;:1,&amp;quot;Address&amp;quot;:&amp;quot;100 SAS Campus Dr&amp;quot;,&amp;quot;City&amp;quot;:&amp;quot;Cary&amp;quot;,&amp;quot;Region&amp;quot;:&amp;quot;NC&amp;quot;,&amp;quot;Postal&amp;quot;:&amp;quot;27513&amp;quot;}},{&amp;quot;attributes&amp;quot;:{&amp;quot;OBJECTID&amp;quot;:2,&amp;quot;Address&amp;quot;:&amp;quot;Duke university&amp;quot;,&amp;quot;City&amp;quot;:&amp;quot;Durham&amp;quot;,&amp;quot;Region&amp;quot;:&amp;quot;NC&amp;quot;,&amp;quot;Postal&amp;quot;:&amp;quot;27705&amp;quot;}}]}&amp;amp;sourceCountry=USA&amp;amp;token={YOUR_TOKEN}&amp;amp;f=pjson  参考 Esri公司官网
ArcGIS产品
ArcGIS功能
GitHub: Esri/geoenrichment-samples
</content>
    </entry>
    
     <entry>
        <title>ArcGIS认证和登录</title>
        <url>https://mryqu.github.io/post/arcgis%E8%AE%A4%E8%AF%81%E5%92%8C%E7%99%BB%E5%BD%95/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>esri</tag><tag>arcgis</tag><tag>rest</tag><tag>oauth</tag><tag>login</tag>
        </tags>
        <content type="html">  申请ArcGIS Online账户 创建应用 点击创建第一个应用： 输入应用所需信息： 查看应用信息： 设置redirect URI： 获取应用访问令牌 默认情况下，访问令牌2小时过期。可在获取访问令牌的请求中加入expiration参数，指定以分钟为单位的过期间隔（响应中单位为秒），最大为14天。 应用登录具有几个内建限制： * 通过应用获取的访问令牌仅能读取公开内容和服务。 * 通过应用获取的访问令牌有可能读取Esri托管的高级内容和服务，并消费代表应用所有者的点数。 * 应用无法创建、更新、共享、修改和删除在ArcGIS Online或ArcGIS门户网站上的内容（层、文件、服务、地图）。 * 使用应用登录方式的应用无法列于ArcGIS软件商店。
获取用户访问令牌 用于用户登录的HTTP GET请求如下：
https://www.arcgis.com/sharing/rest/oauth2/authorize?client_id={YOUR_APP_CLIENT_ID}&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;response_type=code  请求用户授权： 返回地址包含code参数，内容中也有一含有code值的文本框： 获取访问令牌的HTTP GET请求包含上面获得的code参数：：
https://www.arcgis.com/sharing/rest/oauth2/token?client_id={YOUR_APP_CLIENT_ID}&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;grant_type=authorization_code&amp;amp;code={GOTTEN_CODE}  访问令牌使用 不同的ArcGIS REST API使用的访问令牌类型可能不同。例如在Accessing the GeoEnrichment service中提到使用GeoEnrichment服务需要用户访问令牌；而在 Authenticate a request to the World Geocoding Service中提到使用Geocoding服务需要应用访问令牌。 下面的示例使用用户访问令牌执行Geocoding服务的操作，结果返回403错误，提示Token is valid but access is denied，具体信息为User does not have permissions to access geocodeAddresses。 参考 ArcGIS: Implementing App Login
ArcGIS: Implementing Named User Login
ArcGIS: Mobile and Native Named User Login
ArcGIS: Limitations of App Login
</content>
    </entry>
    
     <entry>
        <title>玩一下Quandl API</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E4%B8%80%E4%B8%8Bquandl_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>quandl</tag><tag>api</tag><tag>rest</tag>
        </tags>
        <content type="html">  Quandl是为投资专业人士提供财务、经济和替代数据的平台。 Quandl来源于500多家出版商的数据。所有Quandl的数据都可通过API访问，也可以通过包含R、Python、Ruby等多种编程语言及Excel、SAS等软件进行原生访问。Quandl的来源包括联合国，世行和中央银行等提供商的公开数据、来自CLS集团，Zacks和ICE等供应商的核心财务数据、Dun＆Bradstreet的其他数据、以及许多机密来源。 什么是替代数据？替代数据的范围非常广泛，起初主要包含了未加工的、原始的公司文件、历史市场价格、投资者表现等数据，而现在替代数据已经涵盖任何从移动手机数据到职位信息再到天气预报、交通、卫星图像等能够被采集到的数据。替代数据世界由一系列模糊的数据集组成，而这些数据集可以被转换为交易信息。Quandl提供的替代数据包括企业财务压力数据、外汇数据、电子邮件收据数据、全球石油储量数据、定量股票选择数据等。 Quandl上的数据分为免费数据和高级（Premium）数据，其中高级数据只能通过订阅访问。
申请Quandl账号 除了在Quandl上注册帐号外，Quandl还支持使用GitHub、Google和LinkedIn账号进行OAuth2认证登录。登录后查看账户设置信息中的API KEY，即可用于后继API访问。 Quandl API 全部的Quandl数据产品，可通过https://www.quandl.com/search查找。Quandl的数据产品来源不同，包含时间序列和表在内的各种对象。 Guandl的大多数数据集只能以时间序列或表中的一种格式打开，其中一些则既可用时间序列格式也可用表格式访问。 * 时间序列是一段时间内观测或指标集合，以时间为索引且只包含数字数据类型字段。 * 表包含各种未排序数据类型（字符串、数字、日期等）并可用不同字段进行过滤。
Guandl可指定如下返回类型： * JSON * CSV * XML
速率限制 认证用户限制10秒300个调用、10分钟2000调用及每天50000调用。使用免费数据集的认证用户并发限制为1，即进行一个调用的同时可以在队列中有一个额外的调用。 高级数据订阅限制10分钟5000调用及每天720000调用。
访问时间序列  获取时间序列数据集数据
 GET https://www.quandl.com/api/v3/datasets/{database_code}/{dataset_code}/data.{return_format}?api_key=YOURAPIKEY   获取时间序列数据集元数据
 GET https://www.quandl.com/api/v3/datasets/{database_code}/{dataset_code}/metadata.{return_format}?api_key=YOURAPIKEY   获取时间序列数据集数据及元数据
 GET https://www.quandl.com/api/v3/datasets/{database_code}/{dataset_code}.{return_format}?api_key=YOURAPIKEY   获取时间序列数据库元数据
 GET https://www.quandl.com/api/v3/databases/{database_code}.{return_format}?api_key=YOURAPIKEY   获取整个时间序列数据库(仅能用于订阅的高级数据)
 GET https://www.quandl.com/api/v3/databases/{database_code}/data?download_type=full&amp;amp;api_key=YOURAPIKEY  查询参数  | 参数 | 必需 | 类型 | 值 | 描述 | | - | - | - | - | - | | database_code | 是 | string | | 数据库代码 | | dataset_code | 是 | string | | 数据集代码 | | limit | 否 | int | | 使用limit=n获得数据集的头n行。使用limit=1获取最新的一行。 | | column_index | 否 | int | | 指定特定列。第0列是日期列且永久返回，因此该处从第1列起。
（mryqu：不指定则显示全部列，指定就显示两列，为什么没有逗号分隔了？） | | start_date | 否 | string | yyyy-mm-dd | 用于过滤的起始日期 | | end_date | 否 | string | yyyy-mm-dd | 用于过滤的结束日期 | | order | 否 | string | asc
desc | 日期排序 | | collapse | 否 | string | none
daily
weekly
monthly
quarterly
annual | 改变返回数据的抽样频率。默认为none，即原始颗粒度。改变抽样频率后，Quandl返回给定时间段内最后一个观测值。 | | transform | 否 | string | none
diff
rdiff
rdiff_from
cumul
normalize | 在下载前对数据执行基本计算。默认为none。|
搜索FRED数据库的GDP数据集，按年抽样，取头6行，日期升序排列，显示第0和1列：
访问表  获取表元数据
 https://www.quandl.com/api/v3/datatables/{publisher_code}/{datatable_code}/metadata.{return_format}?api_key=YOURAPIKEY  通过表元数据克制列名和类型，以及那些列可用作行过滤器。 获取表数据
 https://www.quandl.com/api/v3/datatables/{publisher_code}/{datatable_code}.{return_format}?api_key=YOURAPIKEY  此API最多返回10000行数据，其中返回数据中的next-cursor-id用于指示下一页。可以通过下面提到的查询参数qopts.cursor_id进行分页查询，或者通过查询参数qopts.export=true将整表导出为压缩csv的zip文件。
  过滤器操作符 共有五种过滤器操作符：= 、.gt= 、.lt= 、.gte= 、.lte= 。
查询参数 | 参数 | 必需 | 描述 | | - | - | - | | qopts.columns | 否 | 过滤列。如果想查询多列，列名用逗号分隔。 | | qopts.export | 否 | 对大查询很有帮助。数据将导出为压缩csv的zip文件已用于下载。 | | qopts.per_page | 否 | 单页显示行数，最大为10000行。 | | qopts.cursor_id | 否 | 每个API调用将返回用于表示表下一页的游标ID。在API中包含此游标ID则可以查询表的下一页。空游标ID代码当前页为表的最后一页。 |
导出PRICES全表：
过滤mapcode为-5370、compnumber为39102且reporttype为A的行，输出表的reportdate和amount列：
参考 Quandl官网
Quandl API文档
</content>
    </entry>
    
     <entry>
        <title>Icon/logo and brand guides for social media</title>
        <url>https://mryqu.github.io/post/iconlogo_and_brand_guides_for_social_media/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>icon</tag><tag>logo</tag><tag>brand guide</tag><tag>sociamedia</tag>
        </tags>
        <content type="html">  Twitter  https://abs.twimg.com/favicons/favicon.ico https://brand.twitter.com/en.html  Facebook  https://www.facebook.com/favicon.ico https://en.facebookbrand.com/  Google https://www.google.com/favicon.ico
YouTube  https://www.youtube.com/favicon.ico https://www.youtube.com/yt/brand/using-logo.html  Google Analytics  https://analytics.google.com/analytics/web/s/analytics_suite_icon.png https://developers.google.com/analytics/terms/branding-policy  Google Drive  https://ssl.gstatic.com/docs/doclist/images/infinite_arrow_favicon_4.ico https://developers.google.com/drive/v3/web/branding  </content>
    </entry>
    
     <entry>
        <title>YouTube Analytics Dimensions And Mitrics Research</title>
        <url>https://mryqu.github.io/post/youtube_analytics_dimensions_and_mitrics_research/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>youtube</tag><tag>analytics</tag><tag>api</tag><tag>dimension</tag><tag>metrics</tag><tag>datatype</tag>
        </tags>
        <content type="html">  Dimensions GroupCore
Dim?DimensionData
TypeExampleTest
URLResourcesXvideoSTRINGNO_FORMAT
KHqrLhJPdtETestplaylistSTRINGNO_FORMAT
TestXchannelSTRINGNO_FORMAT
???group
(filter only)Time periodsXdaySTRINGYYYY-MM-DD
2016-05-03TestX7DayTotalsSTRINGYYYY-MM-DD
2014-01-01TestX30DayTotalsSTRINGYYYY-MM-DD
2014-01-01TestXmonthSTRINGYYYY-MM
2016-05TestGeographic areasXcountrySTRING2-letter ISO-3166-1 code
USTestprovince
[use country==US in filter]STRINGISO 3166-2 code
US-ZZTestcontinent
(filter only)subContinent
(filter only)Playback locationsinsightPlaybackLocationTypeSTRINGPossible Value:BROWSECHANNELEMBEDDEDEXTERNAL_APPMOBILESEARCHWATCHYT_OTHER
WATCHTestinsightPlaybackLocationDetail
[use insightPlaybackLocationType
==EMBEDDED in filter]STRINGTestPlayback detailsliveOrOnDemandSTRINGPossible Value:LIVEON_DEMAND
ON_DEMANDTestsubscribedStatusSTRINGPossible Value:SUBSCRIBEDUNSUBSCRIBEDTest?youtubeProductSTRINGPossible Values:COREGAMINGKIDSUNKNOWN
CORETestTraffic sourcesinsightTrafficSourceTypeSTRINGPossible Values:ADVERTISINGANNOTATIONCAMPAIGN_CARDEND_SCREENEXT_URLNO_LINK_EMBEDDEDNO_LINK_OTHERNOTIFICATIONPLAYLISTPROMOTEDRELATED_VIDEOSUBSCRIBERYT_CHANNELYT_OTHER_PAGEYT_PLAYLIST_PAGEYT_SEARCH
YT_CHANNELTestinsightTrafficSourceDetail
[use insightTrafficSourceType in filter]STRINGNO_FORMAT
UC-OpYDuNCwCt-AIHC6xNYdwTestDevicesdeviceTypeSTRINGPossible Values:DESKTOPGAME_CONSOLEMOBILETABLETTVUNKNOWN_PLATFORM
DESKTOPTestoperatingSystemSTRINGPossible Values:ANDROIDBADABLACKBERRYCHROMECASTDOCOMOFIREFOXHIPTOPIOSLINUXMACINTOSHMEEGONINTENDO_3DSOTHERPLAYSTATIONPLAYSTATION_VITAREALMEDIASMART_TVSYMBIANTIZENWEBOSWIIWINDOWSWINDOWS_MOBILEXBOX
WINDOWSTestDemographicsXageGroup
[use specific metric]STRINGPossible Values:age13-17age18-24age25-34age35-44age45-54age55-64age65-TestXgender
[use specific metric]STRINGPossible Values:femalemaleTestEngagement and content sharingXsharingServiceSTRINGPossible Values:AMEBAANDROID_EMAILANDROID_MESSENGERANDROID_MMSBBMBLOGGERCOPY_PASTECYWORLDDIGGDROPBOXEMBEDMAILFACEBOOKFACEBOOK_MESSENGERFACEBOOK_PAGESFOTKAGMAILGOOGOOGLEPLUSGO_SMSGROUPMEHANGOUTSHI5HTC_MMSINBOXIOS_SYSTEM_ACTIVITY_DIALOGKAKAO_STORYKIKLGE_EMAILLINELINKEDINLIVEJOURNALMENEAMEMIXIMOTOROLA_MESSAGINGMYSPACENAVERNEARBY_SHARENUJIJOTHERPINTERESTREDDITSKYPESKYBLOGSONY_CONVERSATIONSSTUMBLEUPONTELEGRAMTEXT_MESSAGETUENTITUMBLRTWITTERUNKNOWNVERIZON_MMSVIBERWECHATWEIBOWHATS_APPWYKOPYAHOOVKONTAKTEODNOKLASSNIKIRAKUTENKAKAOTestAudience retentionelapsedVideoTimeRatio
[use video in filter]FLOATrange from 0.01 to 1.0?TestaudienceType
(filter only)Ad performanceadTypeTest?PlaylistsisCurated
(filter only) Content owner dimensions (only used in content owner reports) Core Dimension?DimensionData TypeExampleTest
URLclaimedStatus???XuploaderType??? Metrics GroupCore
Metric?MetricData
TypeExampleTest
URLViewXviewsINTEGER7TestredViewsINTEGER0Testuniques
(deprecated September 27, 2016)XviewerPercentageFLOATTestWatch timeXestimatedMinutesWatchedINTEGER0TestestimatedRedMinutesWatchedINTEGER0TestXaverageViewDurationINTEGER8TestaverageViewPercentageFLOAT95.16349206349206TestEngagementXcommentsINTEGER1TestXlikesINTEGER1TestXdislikesINTEGER1TestXsharesINTEGER1TestXsubscribersGainedINTEGER0TestXsubscribersLostINTEGER0TestvideosAddedToPlaylistsINTEGER0TestvideosRemovedFromPlaylistsINTEGER0TestXfavoritesAdded
(deprecated March 31, 2015)XfavoritesRemoved
(deprecated March 31, 2015)PlaylistplaylistStartsINTEGERTestviewsPerPlaylistStartFLOATTestaverageTimeInPlaylistINTEGERTestAnnotationsannotationImpressionsINTEGERTestannotationClickableImpressionsINTEGERTestannotationClicksINTEGERTestXannotationClickThroughRateFLOATTestannotationClosableImpressionsINTEGERTestannotationClosesINTEGERTestXannotationCloseRateFLOATTestCardcardImpressionsINTEGERTestcardClicksINTEGERTestcardClickRateFLOATTestcardTeaserImpressionsINTEGERTestcardTeaserClicksINTEGERTestcardTeaserClickRateFLOATTestAudience retentionaudienceWatchRatioFLOATTestrelativeRetentionPerformanceFLOATTestEarningsXestimatedRevenue
(previously named earnings)???estimatedAdRevenue
(previously named adEarnings)???estimatedRedPartnerRevenue
(previously named redPartnerRevenue)???Ad performancegrossRevenue???cpm
(previously named impressionBasedCpm)???adImpressions
(previously named impressions)???monetizedPlaybacks???playbackBasedCpm???
参考 Youtube Analytics Reports
Youtube Analytics Dimensions
Youtube Analytics Metrics
</content>
    </entry>
    
     <entry>
        <title>Facebook根据请求API版本返回不同内容？</title>
        <url>https://mryqu.github.io/post/facebook%E6%A0%B9%E6%8D%AE%E8%AF%B7%E6%B1%82api%E7%89%88%E6%9C%AC%E8%BF%94%E5%9B%9E%E4%B8%8D%E5%90%8C%E5%86%85%E5%AE%B9/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>api</tag><tag>version</tag><tag>empty</tag><tag>depend</tag>
        </tags>
        <content type="html"> 今天跟测试组同事研究Facebook返回内容时，发现一个奇怪现象：抓取湖南卫视的Page，当API版本为2.3至2.8时，返回结果内容为空；而API版本为2.9时，返回有内容的响应。 </content>
    </entry>
    
     <entry>
        <title>处理Google Analytics数据类型</title>
        <url>https://mryqu.github.io/post/%E5%A4%84%E7%90%86google_analytics%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google analytics</tag><tag>api</tag><tag>column</tag><tag>data type</tag><tag>format</tag>
        </tags>
        <content type="html">  发送一个Google Analytics请求 分析响应中的columnHeaders 响应中每一列头都包含数据类型信息，大致包含STRING、INTEGER、FLOAT、DATE、TIME、PERCENT、CURRENCY等。
{ &amp;quot;kind&amp;quot;: &amp;quot;analytics#gaData&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;https://www.googleapis.com/analytics/v3/data/ga?ids=ga:1XXXXX0&amp;amp;dimensions=ga:campaign,ga:source,ga:medium,ga:date&amp;amp;metrics=ga:users,ga:newUsers,ga:percentNewSessions,ga:sessions,ga:bounceRate,ga:avgSessionDuration,ga:pageviewsPerSession&amp;amp;start-date=30daysAgo&amp;amp;end-date=yesterday&amp;amp;max-results=0&amp;quot;, &amp;quot;query&amp;quot;: { &amp;quot;start-date&amp;quot;: &amp;quot;30daysAgo&amp;quot;, &amp;quot;end-date&amp;quot;: &amp;quot;yesterday&amp;quot;, &amp;quot;ids&amp;quot;: &amp;quot;ga:1XXXXX0&amp;quot;, &amp;quot;dimensions&amp;quot;: &amp;quot;ga:campaign,ga:source,ga:medium,ga:date&amp;quot;, &amp;quot;metrics&amp;quot;: [ &amp;quot;ga:users&amp;quot;, &amp;quot;ga:newUsers&amp;quot;, &amp;quot;ga:percentNewSessions&amp;quot;, &amp;quot;ga:sessions&amp;quot;, &amp;quot;ga:bounceRate&amp;quot;, &amp;quot;ga:avgSessionDuration&amp;quot;, &amp;quot;ga:pageviewsPerSession&amp;quot; ], &amp;quot;start-index&amp;quot;: 1, &amp;quot;max-results&amp;quot;: 0 }, &amp;quot;itemsPerPage&amp;quot;: 0, &amp;quot;totalResults&amp;quot;: 27400, &amp;quot;selfLink&amp;quot;: &amp;quot;https://www.googleapis.com/analytics/v3/data/ga?ids=ga:1XXXXX0&amp;amp;dimensions=ga:campaign,ga:source,ga:medium,ga:date&amp;amp;metrics=ga:users,ga:newUsers,ga:percentNewSessions,ga:sessions,ga:bounceRate,ga:avgSessionDuration,ga:pageviewsPerSession&amp;amp;start-date=30daysAgo&amp;amp;end-date=yesterday&amp;amp;max-results=0&amp;quot;, &amp;quot;nextLink&amp;quot;: &amp;quot;https://www.googleapis.com/analytics/v3/data/ga?ids=ga:1XXXXX0&amp;amp;dimensions=ga:campaign,ga:source,ga:medium,ga:date&amp;amp;metrics=ga:users,ga:newUsers,ga:percentNewSessions,ga:sessions,ga:bounceRate,ga:avgSessionDuration,ga:pageviewsPerSession&amp;amp;start-date=30daysAgo&amp;amp;end-date=yesterday&amp;amp;start-index=1&amp;amp;max-results=0&amp;quot;, &amp;quot;profileInfo&amp;quot;: { &amp;quot;profileId&amp;quot;: &amp;quot;1XXXXX0&amp;quot;, &amp;quot;accountId&amp;quot;: &amp;quot;1XXXXX8&amp;quot;, &amp;quot;webPropertyId&amp;quot;: &amp;quot;UA-XXXXXXX-1&amp;quot;, &amp;quot;internalWebPropertyId&amp;quot;: &amp;quot;1XXXX1&amp;quot;, &amp;quot;profileName&amp;quot;: &amp;quot;Corporate Site (Master Profile)&amp;quot;, &amp;quot;tableId&amp;quot;: &amp;quot;ga:1XXXXX0&amp;quot; }, &amp;quot;containsSampledData&amp;quot;: true, &amp;quot;sampleSize&amp;quot;: &amp;quot;999951&amp;quot;, &amp;quot;sampleSpace&amp;quot;: &amp;quot;3174334&amp;quot;, &amp;quot;columnHeaders&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;ga:campaign&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;DIMENSION&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;STRING&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:source&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;DIMENSION&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;STRING&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:medium&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;DIMENSION&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;STRING&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:date&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;DIMENSION&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;STRING&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:users&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;METRIC&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;INTEGER&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:newUsers&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;METRIC&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;INTEGER&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:percentNewSessions&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;METRIC&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;PERCENT&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:sessions&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;METRIC&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;INTEGER&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:bounceRate&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;METRIC&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;PERCENT&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:avgSessionDuration&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;METRIC&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;TIME&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;ga:pageviewsPerSession&amp;quot;, &amp;quot;columnType&amp;quot;: &amp;quot;METRIC&amp;quot;, &amp;quot;dataType&amp;quot;: &amp;quot;FLOAT&amp;quot; } ], &amp;quot;totalsForAllResults&amp;quot;: { &amp;quot;ga:users&amp;quot;: &amp;quot;2648520&amp;quot;, &amp;quot;ga:newUsers&amp;quot;: &amp;quot;1488536&amp;quot;, &amp;quot;ga:percentNewSessions&amp;quot;: &amp;quot;46.962349316341275&amp;quot;, &amp;quot;ga:sessions&amp;quot;: &amp;quot;3169637&amp;quot;, &amp;quot;ga:bounceRate&amp;quot;: &amp;quot;64.97214034288469&amp;quot;, &amp;quot;ga:avgSessionDuration&amp;quot;: &amp;quot;152.49400514948556&amp;quot;, &amp;quot;ga:pageviewsPerSession&amp;quot;: &amp;quot;2.0496252409976283&amp;quot; } }  处理 根据Google Analytics数据类型，我们可以做相应的处理，例如设置SAS format和informat。
| GA DataType | SAS FORMAT | SAS INFORMAT | | - | - | - | | STRING | $ | | | DATE | NLDATE20. | YYMMDD8. | | TIME | TIME20.0 | | | PERCENT | PERCENT32.2 | | | CURRENCY | 32.2 | |
</content>
    </entry>
    
     <entry>
        <title>Making Nested Requests using Facebook Graph API</title>
        <url>https://mryqu.github.io/post/making_nested_requests_using_facebook_graph_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>api</tag><tag>nested request</tag><tag>fields</tag>
        </tags>
        <content type="html">  今天又玩了一把Facebook Graph API。当我们抓取Page上的帖子后，之后会发起API请求获取帖子的评论及回复。
获取Page SASsoftware（ID为193453547355388）下的帖子 https://graph.facebook.com/193453547355388/feed?fields=id,XXXX,likes.limit(0).summary(1),comments,XXXX,with_tags&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;limit=100&amp;amp;since=XXXX&amp;amp;until=XXXX&amp;amp;access_token={YOUR_TOKEN}  获取帖子193453547355388_951786161522119的评论 https://graph.facebook.com/193453547355388_951786161522119/comments?fields=id,from,message,created_time,like_count&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;limit=100&amp;amp;access_token={YOUR_TOKEN}  获取评论951786161522119_951787458188656的回复 https://graph.facebook.com/951786161522119_951787458188656/comments?fields=id,from,message,created_time,like_count&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;limit=100&amp;amp;access_token={YOUR_TOKEN}  试用嵌套请求 https://graph.facebook.com/193453547355388/feed?fields=id,XXXX,likes.limit(0).summary(1),comments{id,from,message,type,created_time,like_count,comments{id,from,message,type,created_time,like_count}},XXXX,with_tags&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;limit=100&amp;amp;since=XXXX&amp;amp;until=XXXX&amp;amp;access_token={YOUR_TOKEN}  这里的请求使用了两级嵌套请求，第一级获取帖子的评论，第二季获取评论的回复，那结果如何？ 一个API请求就能够获得了帖子、评论及回复的信息。但是，考虑到一个帖子的评论或一个评论的回复都可能很多，返回结果是第一个分页结果，还是需要通过/{object-id}/comments API 请求获取，考虑到设计复杂性和性价比，决定放弃这种方案。
</content>
    </entry>
    
     <entry>
        <title>[MySQL] 将空返回值转换成NULL</title>
        <url>https://mryqu.github.io/post/mysql_%E5%B0%86%E7%A9%BA%E8%BF%94%E5%9B%9E%E5%80%BC%E8%BD%AC%E6%8D%A2%E6%88%90null/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mysql</tag><tag>sql</tag><tag>ifnull</tag><tag>coalesce</tag><tag>empty</tag><tag></tag><tag>DB</tag>
        </tags>
        <content type="html">  当MySQL没有搜索到任何匹配行时，会返回空返回值，如何转换成NULL呢？
方法一 select (original_select_statement) as Alias  这种方法仅对一个单值有效，即： * 原语句返回单值，该值将被返回 * 原语句返回单列零行，将返回NULL * 原语句返回多列或多行，查询失败
方法二 使用IFNULL或COALESCE函数。
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] Compile JsonCpp library using CMake</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_compile_jsoncpp_library_using_cmake/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>jsoncpp</tag><tag>cmake</tag><tag>gcc</tag><tag>linux</tag><tag>windows</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html">  本文为升级JsonCpp库操作过程的备份笔记。
Linux/Unix平台 下载JsonCpp 从JsonCpp releases页面可知，当前最高版本为1.8.0。
wget https://github.com/open-source-parsers/jsoncpp/archive/1.8.0.tar.gz tar xzvf 1.8.0.tar.gz cd jsoncpp-1.8.0 mkdir build/release  升级gcc 这里我选择使用gcc 5:
sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install gcc-5 g&#43;&#43;-5 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 1 sudo update-alternatives --install /usr/bin/g&#43;&#43; g&#43;&#43; /usr/bin/g&#43;&#43;-5 1  升级cmake JsonCpp 1.8.0要求cmake&amp;gt;=3.1
sudo apt-get install software-properties-common sudo add-apt-repository ppa:george-edison55/cmake-3.x sudo apt-get update sudo apt-get upgrade cmake  编译JsonCpp cmake -DCMAKE_BUILD_TYPE=release -DBUILD_STATIC_LIBS=ON -DBUILD_SHARED_LIBS=OFF -DARCHIVE_INSTALL_DIR=. -G &amp;quot;Unix Makefiles&amp;quot; ../.. make  Windows平台 准备环境 首先下载JsonCpp 1.8.0源码，并解压缩。我的机器已经部署了VS2013和CMake。 编译JsonCpp cmake -DBUILD_STATIC_LIBS=ON -DBUILD_SHARED_LIBS=OFF -G &amp;quot;Visual Studio 12 2013 Win64&amp;quot; .. cmake --build . --config Release  注：如果编译32位库，则上述过程改用下列命令
cmake -DBUILD_STATIC_LIBS=ON -DBUILD_SHARED_LIBS=OFF -G &amp;quot;Visual Studio 12 2013&amp;quot; ..  参考 CMake WiKi
CMake command manual
CMake Useful Variables
g&#43;&#43; man page
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] Build JsonCpp library in Linux platform</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_build_jsoncpp_library_in_linux_platform/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>jsoncpp</tag><tag>g&#43;&#43;</tag><tag>ar</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html"> wget https://github.com/open-source-parsers/jsoncpp/archive/1.8.0.tar.gz tar xzvf 1.8.0.tar.gz cd jsoncpp-1.8.0/src/lib_json g&#43;&#43; -g -std=c&#43;&#43;11 -Wall -fPIC -c -I../../include json_reader.cpp json_value.cpp json_writer.cpp ar rvs libjsoncpp.a *.o g&#43;&#43; -g json_reader.o json_writer.o json_value.o -shared -o libjsoncpp.so  </content>
    </entry>
    
     <entry>
        <title>升级tsc.js解决TypeScript编译失败问题</title>
        <url>https://mryqu.github.io/post/%E5%8D%87%E7%BA%A7tsc.js%E8%A7%A3%E5%86%B3typescript%E7%BC%96%E8%AF%91%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>ts5052</tag><tag>sourcemap</tag><tag>ts5053</tag><tag>inlinesourcemap</tag><tag>typescript</tag>
        </tags>
        <content type="html"> 今天项目忽然构建失败，遭遇下列错误：
error TS5052: Option &#39;sourceRoot&#39; cannot be specified without specifying option &#39;sourceMap&#39;. error TS5053: Option &#39;sourceRoot&#39; cannot be specified with option &#39;inlineSourceMap&#39;.  查了下tsconfig.json，发现前两天&amp;rdquo;sourceMap&amp;rdquo;属性由true改为了false，又增加了值为true的&amp;rdquo;inlineSourceMap&amp;rdquo;属性。 最后只好把项目里的tsc.js从1.6.4升级成2.1.6才解决问题。
</content>
    </entry>
    
     <entry>
        <title>玩一下uptodate-gradle-plugin插件</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E4%B8%80%E4%B8%8Buptodate-gradle-plugin%E6%8F%92%E4%BB%B6/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>plugin</tag><tag>uptodate</tag><tag>maven</tag><tag>library</tag>
        </tags>
        <content type="html"> 玩了一下uptodate-gradle-plugin插件，使用这个插件后执行gradle uptodate 可以看到那些库在Maven Central仓库有新版本，用于辅助判断是否需要更新Java库。 我一般不追新库，所以这个插件对我的用处小，看一看玩一玩，仅此而已。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 在XMLView中使用带有参数的I18N消息</title>
        <url>https://mryqu.github.io/post/openui5_%E5%9C%A8xmlview%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%B8%A6%E6%9C%89%E5%8F%82%E6%95%B0%E7%9A%84i18n%E6%B6%88%E6%81%AF/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>xmlview</tag><tag>i18n</tag><tag>parameter</tag>
        </tags>
        <content type="html"> 在做的一个新项目中，美国团队那边齐刷刷地一色用XMLView而不是JSView，碰到一个小问题：那就是怎么在XMLView中设置带有参数的I18N消息。
参考Passing parameters to i18n model within XML view帖子中的方案，基本搞定：
&amp;lt;Input id=&amp;quot;myInput&amp;quot; type=&amp;quot;Text&amp;quot; required=&amp;quot;true&amp;quot; value=&amp;quot;{myyquInput}&amp;quot; placeholder=&amp;quot;{parts:[&#39;i18n&amp;gt;myKey.txt&#39;, &#39;myModel&amp;gt;myProp&#39;], formatter: &#39;jQuery.sap.formatMessage&#39;}&amp;quot; change=&amp;quot;.handleChangeForMyInput&amp;quot;&amp;gt; &amp;lt;layoutData&amp;gt; &amp;lt;l:GridData span=&amp;quot;L6 M8 S9&amp;quot; /&amp;gt; &amp;lt;/layoutData&amp;gt; &amp;lt;/Input&amp;gt;  messagebundle.properties：
myKey.txt=&amp;quot;(Example: {0})&amp;quot;  使用sap.ui.model.CompositeBinding可以通过XMLView中的parts加载多个参数，达到我的目的。缺点就是每个参数只能是model/path组合，或者省略model的path。我没有找到直接输入参数值的便捷方法。 阅读sap.ui.base.ManagedObject的bindProperty方法可知，它对part中每一元素查找是否有“&amp;gt;”，有则认为是model/path组合，否则即为path。
ManagedObject.prototype.bindProperty = function(sName, oBindingInfo, _vFormat, _sMode) { var iSeparatorPos, bAvailable = true, oProperty = this.getMetadata().getPropertyLikeSetting(sName); // check whether property or alternative type on aggregation exists if (!oProperty) { throw new Error(&amp;quot;Property \&amp;quot;&amp;quot; &#43; sName &#43; &amp;quot;\&amp;quot; does not exist in &amp;quot; &#43; this); } // old API compatibility (sName, sPath, _vFormat, _sMode) if (typeof oBindingInfo == &amp;quot;string&amp;quot;) { oBindingInfo = { parts: [ { path: oBindingInfo, type: _vFormat instanceof Type ? _vFormat : undefined, mode: _sMode } ], formatter: typeof _vFormat === &#39;function&#39; ? _vFormat : undefined }; } // only one binding object with one binding specified if (!oBindingInfo.parts) { oBindingInfo.parts = []; oBindingInfo.parts[0] = { path: oBindingInfo.path, targetType: oBindingInfo.targetType, type: oBindingInfo.type, suspended: oBindingInfo.suspended, formatOptions: oBindingInfo.formatOptions, constraints: oBindingInfo.constraints, model: oBindingInfo.model, mode: oBindingInfo.mode }; delete oBindingInfo.path; delete oBindingInfo.targetType; delete oBindingInfo.mode; delete oBindingInfo.model; } for ( var i = 0; i &amp;lt; oBindingInfo.parts.length; i&#43;&#43; ) { var oPart = oBindingInfo.parts[i]; if (typeof oPart == &amp;quot;string&amp;quot;) { oPart = { path: oPart }; oBindingInfo.parts[i] = oPart; } // if a model separator is found in the path, extract model name and path iSeparatorPos = oPart.path.indexOf(&amp;quot;&amp;gt;&amp;quot;); if (iSeparatorPos &amp;gt; 0) { oPart.model = oPart.path.substr(0, iSeparatorPos); oPart.path = oPart.path.substr(iSeparatorPos &#43; 1); } // if a formatter exists the binding mode can be one way or one time only if (oBindingInfo.formatter &amp;amp;&amp;amp; oPart.mode != BindingMode.OneWay &amp;amp;&amp;amp; oPart.mode != BindingMode.OneTime) { oPart.mode = BindingMode.OneWay; } if (!this.getModel(oPart.model)) { bAvailable = false; } } // if property is already bound, unbind it first if (this.isBound(sName)) { this.unbindProperty(sName, true); } // store binding info to create the binding, as soon as the model is available, // or when the model is changed this.mBindingInfos[sName] = oBindingInfo; // if the models are already available, create the binding if (bAvailable) { this._bindProperty(sName, oBindingInfo); } return this; };  </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 折腾了一下JSView转换XMLView</title>
        <url>https://mryqu.github.io/post/openui5_%E6%8A%98%E8%85%BE%E4%BA%86%E4%B8%80%E4%B8%8Bjsview%E8%BD%AC%E6%8D%A2xmlview/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>diagnostics</tag><tag>jsview</tag><tag>xmlview</tag><tag>conversion</tag>
        </tags>
        <content type="html"> 根据OpenUI5 Developer Guide - Diagnostics Window中的介绍，尝试一下XML View Conversion。
 Many code samples are written in JavaScript. To facilitate the conversion of these code samples into XML, OpenUI5 provides a generic conversion tool. To run the tool, proceed as follows: 1. Run the OpenUI5 app in your browser, for example, open a page in the test suite. 2. Open the support tool by choosing CTRL&#43;ALT&#43;SHIFT&#43;S. 3. Open the Control Tree panel. 4. Select the root UI area in the tree on the left hand side. 5. Open the Export tab and choose Export XML. 6. Open the ZIP archive and extract the files to your file system.
If your app does not contain views, the content is put in one view in the output. If your app contains views and all views are loaded, the content is output as separate files.
 采用我之前的博文[OpenUI5] MVC和EventBus示例中的示例做一下实验： 首先打开SAPUI5 Diagnostics，在Control Tree中选择一个JSView：看到Export to XML按钮了：悲剧了，无法导出，说什么p.indexOfContent is not a function：一开始怀疑自己的代码不规范，然后尝试很多其他JSView，甚至去转换openui5.hana.ondemand.com中的例子，结果是无一成功。 好吧，现在我开始怀疑SAPUI5这个功能了，可是找不到源码在哪里，最后凭借万能的搜索引擎，查到了sap.ui.core.support.plugins.ControlTree。 开始用Chrome Dev Tool调试，不用点击按钮就可以触发转换了：再进一步替换成如下代码：
var o = { &amp;quot;eventId&amp;quot;: &amp;quot;sapUiSupportControlTreeRequestControlTreeSerialize&amp;quot;, &amp;quot;params&amp;quot;: { controlID:&amp;quot;leftView&amp;quot;, sType:&amp;quot;XML&amp;quot;} }; var s = &amp;quot;SAPUI5SupportTool*&amp;quot; &#43; JSON.stringify(o); window.sap.ui.core.support.Support.getStub(&amp;quot;IFRAME&amp;quot;)._oRemoteWindow.postMessage(s, &amp;quot;http://null.jsbin.com&amp;quot;);  又跳到了sap.ui.core.support.plugins.ControlTree的onsapUiSupportControlTreeRequestControlTreeSerialize 方法，大致扫了一眼代码： * OpenUI5元素不限于JSView，如果元素不是sap.ui.core.mvc.View，则会在外边裹一层View
 if (oControl instanceof sap.ui.core.mvc.View) { oViewSerializer = new ViewSerializer(oControl, window, &amp;quot;sap.m&amp;quot;); } else { var oView = sap.ui.jsview(sType &#43; &amp;quot;ViewExported&amp;quot;); oView.addContent(oControl); oViewSerializer = new ViewSerializer(oView, window, &amp;quot;sap.m&amp;quot;); }   查找OpenUI5元素的父元素，调用indexOfContent去查看该元素在父元素内的顺序下标，问题就出在这里！！！  indexOfContent方法仅在sap.ui.core.mvc.View及其子类中存在，sap.ui.core.Control及sap.ui.core.Element并无该方法，所以这个出问题的概率很大呀！ 接着又尝试了一下非JSView的元素，点击按钮没反应！ 尝试失败，看来只能寄希望于SAP的改进了！
</content>
    </entry>
    
     <entry>
        <title>TypeScript初体验</title>
        <url>https://mryqu.github.io/post/typescript%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>typescript</tag><tag>javascript</tag><tag>ecmascript</tag>
        </tags>
        <content type="html">  TypeScript介绍 鉴于JavaScript这种脚本语言很难应用于大规模Web应用的开发，微软公司在2012年推出了新的开源编程语言——TypeScript。作为Object Pascal和C#之父Anders Hejisberg的又一作品，TypeScript是JavaScript的超集，但完全兼容JavaScript。相比于JavaScript，TypeScript增加了可选类型、类和模块，扩展了原有的语法，使得代码组织和复用变得更加有序，方便进行大型Web应用的开发。
安装 TypeScript可通过npm进行安装：
npm install -g typescript  查看TypeScript版本：
C:\quTemp&amp;gt;tsc -v Version 2.1.6  我开发主要使用IntelliJ IDEA，它可以很好的编辑TypeScript文件。不过对于一些小练习，还是安装Sublime的TypeScript插件好了：初体验 menu.ts源文件（来自参看一）：编译：
tsc menu.ts  执行：编译结果menu.js：测试：下一步计划 学习一下《TypeScript Essentials》和《Mastering TypeScript》这两本书。
参考 Learn TypeScript in 30 Minutes
Learn TypeScript in Y minutes
</content>
    </entry>
    
     <entry>
        <title>Upload file to Google Drive using LibCurl</title>
        <url>https://mryqu.github.io/post/upload_file_to_google_drive_using_libcurl/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>upload</tag><tag>google drive</tag><tag>api</tag><tag>libcurl</tag>
        </tags>
        <content type="html">  This is a sequel to my last blog &amp;ldquo;Upload file to Google Drive using Postman and cURL&amp;rdquo;.
I wrote C&#43;&#43; code using LibCurl to redo the three types of upload, and all of them work which shown in the below log snippet.
Simple upload =&amp;gt; Send header, 0000000309 bytes (0x00000135) 0000: 50 4f 53 54 20 2f 75 70 6c 6f 61 64 2f 64 72 69 POST /upload/dri 0010: 76 65 2f 76 33 2f 66 69 6c 65 73 3f 75 70 6c 6f ve/v3/files?uplo 0020: 61 64 54 79 70 65 3d 6d 65 64 69 61 26 61 63 63 adType=media&amp;amp;acc 0030: 65 73 73 5f 74 6f 6b 65 6e 3d 79 61 32 39 2e 47 ess_token=ya29.G ###################################################################### 00b0: 6e 6c 4c 46 5f 4e 51 78 57 32 48 20 48 54 54 50 nlLF_NQxW2H HTTP 00c0: 2f 31 2e 31 0d 0a 48 6f 73 74 3a 20 77 77 77 2e /1.1..Host: www. 00d0: 67 6f 6f 67 6c 65 61 70 69 73 2e 63 6f 6d 0d 0a googleapis.com.. 00e0: 41 63 63 65 70 74 3a 20 2a 2f 2a 0d 0a 63 61 63 Accept: **. 00f0: 0a 63 61 63 68 65 2d 63 6f 6e 74 72 6f 6c 3a 20 .cache-control: 0100: 6e 6f 2d 63 61 63 68 65 0d 0a 63 6f 6e 74 65 6e no-cache..conten 0110: 74 2d 74 79 70 65 3a 20 61 70 70 6c 69 63 61 74 t-type: applicat 0120: 69 6f 6e 2f 6a 73 6f 6e 3b 20 63 68 61 72 73 65 ion/json; charse 0130: 74 3d 55 54 46 2d 38 0d 0a 58 2d 55 70 6c 6f 61 t=UTF-8..X-Uploa 0140: 64 2d 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 74 d-Content-Type:t 0150: 65 78 74 2f 63 73 76 0d 0a 58 2d 55 70 6c 6f 61 ext/csv..X-Uploa 0160: 64 2d 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 d-Content-Length 0170: 3a 38 33 38 31 39 33 35 0d 0a 43 6f 6e 74 65 6e :8381935..Conten 0180: 74 2d 4c 65 6e 67 74 68 3a 20 33 32 0d 0a 0d 0a t-Length: 32.... =&amp;gt; Send data, 0000000032 bytes (0x00000020) 0000: 7b 22 6e 61 6d 65 22 3a 22 6d 72 79 71 75 2d 63 {&amp;quot;name&amp;quot;:&amp;quot;mryqu-c 0010: 73 76 2d 72 75 70 6c 6f 61 64 2e 63 73 76 22 7d sv-rupload.csv&amp;quot;} &amp;lt;= Recv header, 0000000017 bytes (0x00000011) 0000: 48 54 54 50 2f 31 2e 31 20 32 30 30 20 4f 4b 0d HTTP/1.1 200 OK. 0010: 0a . &amp;lt;= Recv header, 0000000122 bytes (0x0000007a) 0000: 58 2d 47 55 70 6c 6f 61 64 65 72 2d 55 70 6c 6f X-GUploader-Uplo 0010: 61 64 49 44 3a 20 41 45 6e 42 32 55 6f 71 5a 33 adID: AEnB2UoqZ3 0020: 6b 2d 35 66 2d 6a 43 55 39 68 47 59 77 47 42 79 k-5f-jCU9hGYwGBy 0030: 4c 6e 55 66 2d 61 67 42 4f 49 66 45 46 4f 76 4e LnUf-agBOIfEFOvN 0040: 6c 70 73 57 55 31 79 62 73 46 30 5f 35 64 66 45 lpsWU1ybsF0_5dfE 0050: 57 78 6e 51 69 6a 50 6b 51 71 75 4b 43 50 7a 63 WxnQijPkQquKCPzc 0060: 4d 55 35 63 70 5a 6a 44 55 36 38 51 4d 56 65 57 MU5cpZjDU68QMVeW 0070: 44 42 7a 6f 79 37 62 77 0d 0a DBzoy7bw.. &amp;lt;= Recv header, 0000000333 bytes (0x0000014d) 0000: 4c 6f 63 61 74 69 6f 6e 3a 20 68 74 74 70 73 3a Location: https: 0010: 2f 2f 77 77 77 2e 67 6f 6f 67 6c 65 61 70 69 73 //www.googleapis 0020: 2e 63 6f 6d 2f 75 70 6c 6f 61 64 2f 64 72 69 76 .com/upload/driv 0030: 65 2f 76 33 2f 66 69 6c 65 73 3f 75 70 6c 6f 61 e/v3/files?uploa 0040: 64 54 79 70 65 3d 72 65 73 75 6d 61 62 6c 65 26 dType=resumable&amp;amp; 0050: 61 63 63 65 73 73 5f 74 6f 6b 65 6e 3d 79 61 32 access_token=ya2 ###################################################################### 00d0: 6d 61 77 6e 6c 4c 46 5f 4e 51 78 57 32 48 26 75 mawnlLF_NQxW2H&amp;amp;u 00e0: 70 6c 6f 61 64 5f 69 64 3d 41 45 6e 42 32 55 6f pload_id=AEnB2Uo 00f0: 71 5a 33 6b 2d 35 66 2d 6a 43 55 39 68 47 59 77 qZ3k-5f-jCU9hGYw 0100: 47 42 79 4c 6e 55 66 2d 61 67 42 4f 49 66 45 46 GByLnUf-agBOIfEF 0110: 4f 76 4e 6c 70 73 57 55 31 79 62 73 46 30 5f 35 OvNlpsWU1ybsF0_5 0120: 64 66 45 57 78 6e 51 69 6a 50 6b 51 71 75 4b 43 dfEWxnQijPkQquKC 0130: 50 7a 63 4d 55 35 63 70 5a 6a 44 55 36 38 51 4d PzcMU5cpZjDU68QM 0140: 56 65 57 44 42 7a 6f 79 37 62 77 0d 0a VeWDBzoy7bw.. &amp;lt;= Recv header, 0000000014 bytes (0x0000000e) 0000: 56 61 72 79 3a 20 4f 72 69 67 69 6e 0d 0a Vary: Origin.. &amp;lt;= Recv header, 0000000016 bytes (0x00000010) 0000: 56 61 72 79 3a 20 58 2d 4f 72 69 67 69 6e 0d 0a Vary: X-Origin.. &amp;lt;= Recv header, 0000000063 bytes (0x0000003f) 0000: 43 61 63 68 65 2d 43 6f 6e 74 72 6f 6c 3a 20 6e Cache-Control: n 0010: 6f 2d 63 61 63 68 65 2c 20 6e 6f 2d 73 74 6f 72 o-cache, no-stor 0020: 65 2c 20 6d 61 78 2d 61 67 65 3d 30 2c 20 6d 75 e, max-age=0, mu 0030: 73 74 2d 72 65 76 61 6c 69 64 61 74 65 0d 0a st-revalidate.. &amp;lt;= Recv header, 0000000018 bytes (0x00000012) 0000: 50 72 61 67 6d 61 3a 20 6e 6f 2d 63 61 63 68 65 Pragma: no-cache 0010: 0d 0a .. &amp;lt;= Recv header, 0000000040 bytes (0x00000028) 0000: 45 78 70 69 72 65 73 3a 20 4d 6f 6e 2c 20 30 31 Expires: Mon, 01 0010: 20 4a 61 6e 20 31 39 39 30 20 30 30 3a 30 30 3a Jan 1990 00:00: 0020: 30 30 20 47 4d 54 0d 0a 00 GMT.. &amp;lt;= Recv header, 0000000037 bytes (0x00000025) 0000: 44 61 74 65 3a 20 ############################################## 0020: 47 4d 54 0d 0a GMT.. &amp;lt;= Recv header, 0000000019 bytes (0x00000013) 0000: 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20 Content-Length: 0010: 30 0d 0a 0.. &amp;lt;= Recv header, 0000000022 bytes (0x00000016) 0000: 53 65 72 76 65 72 3a 20 55 70 6c 6f 61 64 53 65 Server: UploadSe 0010: 72 76 65 72 0d 0a rver.. &amp;lt;= Recv header, 0000000040 bytes (0x00000028) 0000: 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 74 65 Content-Type: te 0010: 78 74 2f 68 74 6d 6c 3b 20 63 68 61 72 73 65 74 xt/html; charset 0020: 3d 55 54 46 2d 38 0d 0a =UTF-8.. &amp;lt;= Recv header, 0000000045 bytes (0x0000002d) 0000: 41 6c 74 2d 53 76 63 3a 20 71 75 69 63 3d 22 3a Alt-Svc: quic=&amp;quot;: 0010: 34 34 33 22 3b 20 6d 61 3d 32 35 39 32 30 30 30 443&amp;quot;; ma=2592000 0020: 3b 20 76 3d 22 33 35 2c 33 34 22 0d 0a ; v=&amp;quot;35,34&amp;quot;.. &amp;lt;= Recv header, 0000000002 bytes (0x00000002) 0000: 0d 0a .. == Info: Connection #0 to host www.googleapis.com left intact =&amp;gt; Send header, 0000000429 bytes (0x000001ad) 0000: 50 4f 53 54 20 2f 75 70 6c 6f 61 64 2f 64 72 69 POST /upload/dri 0010: 76 65 2f 76 33 2f 66 69 6c 65 73 3f 75 70 6c 6f ve/v3/files?uplo 0020: 61 64 54 79 70 65 3d 72 65 73 75 6d 61 62 6c 65 adType=resumable 0030: 26 61 63 63 65 73 73 5f 74 6f 6b 65 6e 3d 79 61 &amp;amp;access_token=ya ###################################################################### 00b0: 4e 6d 61 77 6e 6c 4c 46 5f 4e 51 78 57 32 48 26 NmawnlLF_NQxW2H&amp;amp; 00c0: 75 70 6c 6f 61 64 5f 69 64 3d 41 45 6e 42 32 55 upload_id=AEnB2U 00d0: 6f 71 5a 33 6b 2d 35 66 2d 6a 43 55 39 68 47 59 oqZ3k-5f-jCU9hGY 00e0: 77 47 42 79 4c 6e 55 66 2d 61 67 42 4f 49 66 45 wGByLnUf-agBOIfE 00f0: 46 4f 76 4e 6c 70 73 57 55 31 79 62 73 46 30 5f FOvNlpsWU1ybsF0_ 0100: 35 64 66 45 57 78 6e 51 69 6a 50 6b 51 71 75 4b 5dfEWxnQijPkQquK 0110: 43 50 7a 63 4d 55 35 63 70 5a 6a 44 55 36 38 51 CPzcMU5cpZjDU68Q 0120: 4d 56 65 57 44 42 7a 6f 79 37 62 77 20 48 54 54 MVeWDBzoy7bw HTT 0130: 50 2f 31 2e 31 0d 0a 48 6f 73 74 3a 20 77 77 77 P/1.1..Host: www 0140: 2e 67 6f 6f 67 6c 65 61 70 69 73 2e 63 6f 6d 0d .googleapis.com. 0150: 0a 41 63 63 65 70 74 3a 20 2a 2f 2a 0d 0a 54 72 .Accept: */*..Tr 0160: 61 6e 73 66 65 72 2d 45 6e 63 6f 64 69 6e 67 3a ansfer-Encoding: 0170: 20 63 68 75 6e 6b 65 64 0d 0a 63 61 63 68 65 2d chunked..cache- 0180: 63 6f 6e 74 72 6f 6c 3a 20 6e 6f 2d 63 61 63 68 control: no-cach 0190: 65 0d 0a 63 6f 6e 74 65 6e 74 2d 74 79 70 65 3a e..content-type: 01a0: 20 74 65 78 74 2f 63 73 76 0d 0a 0d 0a text/csv.... =&amp;gt; Send data, 0000016380 bytes (0x00003ffc) 0000: 33 66 66 34 0d 0a 43 4f 55 4e 54 52 59 2c 53 54 3ff4..COUNTRY,ST ###################################################################### 3fe0: 55 2e 53 2e 41 2e 2c 43 61 6c 69 66 6f 72 6e 69 U.S.A.,Californi 3ff0: 61 2c 2c 22 24 31 2c 30 33 37 0d 0a a,,&amp;quot;$1,037.. =&amp;gt; Send data, 0000016380 bytes (0x00003ffc) 0000: 33 66 66 34 0d 0a 2e 30 30 20 22 2c 22 24 31 2c 3ff4...00 &amp;quot;,&amp;quot;$1, 0010: 32 32 38 2e 30 30 20 22 2c 46 55 52 4e 49 54 55 228.00 &amp;quot;,FURNITU ###################################################################### 3fe0: 2c 37 33 34 2e 30 30 20 22 2c 46 55 52 4e 49 54 ,734.00 &amp;quot;,FURNIT 3ff0: 55 52 45 2c 42 45 44 2c 31 39 0d 0a URE,BED,19.. =&amp;gt; Send data, 0000016380 bytes (0x00003ffc) 0000: 33 66 66 34 0d 0a 39 37 2c 31 2c 46 65 62 2c 46 3ff4..97,1,Feb,F 0010: 65 62 2d 39 37 2c 55 2e 53 2e 41 2e 2c 43 61 6c eb-97,U.S.A.,Cal ###################################################################### 3fe0: 2c 43 61 6c 69 66 6f 72 6e 69 61 2c 2c 22 24 31 ,California,,&amp;quot;$1 3ff0: 2c 32 39 30 2e 33 30 20 22 2c 0d 0a ,290.30 &amp;quot;,.. ###################################################################### ###################################################################### =&amp;gt; Send data, 0000015851 bytes (0x00003deb) 0000: 33 64 65 33 0d 0a 69 6e 67 74 6f 6e 2c 2c 22 24 3de3..ington,,&amp;quot;$ 0010: 31 2c 38 33 34 2e 30 30 20 22 2c 22 24 31 2c 32 1,834.00 &amp;quot;,&amp;quot;$1,2 ###################################################################### 3dd0: 55 52 45 2c 42 45 44 2c 31 39 39 38 2c 32 2c 4d URE,BED,1998,2,M 3de0: 61 79 2c 4d 61 79 2d 39 38 0d 0a ay,May-98.. =&amp;gt; Send data, 0000000005 bytes (0x00000005) 0000: 30 0d 0a 0d 0a 0.... &amp;lt;= Recv header, 0000000017 bytes (0x00000011) 0000: 48 54 54 50 2f 31 2e 31 20 32 30 30 20 4f 4b 0d HTTP/1.1 200 OK. 0010: 0a . &amp;lt;= Recv header, 0000000122 bytes (0x0000007a) 0000: 58 2d 47 55 70 6c 6f 61 64 65 72 2d 55 70 6c 6f X-GUploader-Uplo 0010: 61 64 49 44 3a 20 41 45 6e 42 32 55 6f 71 5a 33 adID: AEnB2UoqZ3 0020: 6b 2d 35 66 2d 6a 43 55 39 68 47 59 77 47 42 79 k-5f-jCU9hGYwGBy 0030: 4c 6e 55 66 2d 61 67 42 4f 49 66 45 46 4f 76 4e LnUf-agBOIfEFOvN 0040: 6c 70 73 57 55 31 79 62 73 46 30 5f 35 64 66 45 lpsWU1ybsF0_5dfE 0050: 57 78 6e 51 69 6a 50 6b 51 71 75 4b 43 50 7a 63 WxnQijPkQquKCPzc 0060: 4d 55 35 63 70 5a 6a 44 55 36 38 51 4d 56 65 57 MU5cpZjDU68QMVeW 0070: 44 42 7a 6f 79 37 62 77 0d 0a DBzoy7bw.. &amp;lt;= Recv header, 0000000014 bytes (0x0000000e) 0000: 56 61 72 79 3a 20 4f 72 69 67 69 6e 0d 0a Vary: Origin.. &amp;lt;= Recv header, 0000000016 bytes (0x00000010) 0000: 56 61 72 79 3a 20 58 2d 4f 72 69 67 69 6e 0d 0a Vary: X-Origin.. &amp;lt;= Recv header, 0000000047 bytes (0x0000002f) 0000: 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a 20 61 70 Content-Type: ap 0010: 70 6c 69 63 61 74 69 6f 6e 2f 6a 73 6f 6e 3b 20 plication/json; 0020: 63 68 61 72 73 65 74 3d 55 54 46 2d 38 0d 0a charset=UTF-8.. &amp;lt;= Recv header, 0000000063 bytes (0x0000003f) 0000: 43 61 63 68 65 2d 43 6f 6e 74 72 6f 6c 3a 20 6e Cache-Control: n 0010: 6f 2d 63 61 63 68 65 2c 20 6e 6f 2d 73 74 6f 72 o-cache, no-stor 0020: 65 2c 20 6d 61 78 2d 61 67 65 3d 30 2c 20 6d 75 e, max-age=0, mu 0030: 73 74 2d 72 65 76 61 6c 69 64 61 74 65 0d 0a st-revalidate.. &amp;lt;= Recv header, 0000000018 bytes (0x00000012) 0000: 50 72 61 67 6d 61 3a 20 6e 6f 2d 63 61 63 68 65 Pragma: no-cache 0010: 0d 0a .. &amp;lt;= Recv header, 0000000040 bytes (0x00000028) 0000: 45 78 70 69 72 65 73 3a 20 4d 6f 6e 2c 20 30 31 Expires: Mon, 01 0010: 20 4a 61 6e 20 31 39 39 30 20 30 30 3a 30 30 3a Jan 1990 00:00: 0020: 30 30 20 47 4d 54 0d 0a 00 GMT.. &amp;lt;= Recv header, 0000000037 bytes (0x00000025) 0000: 44 61 74 65 3a 20 ############################################## 0020: 47 4d 54 0d 0a GMT.. &amp;lt;= Recv header, 0000000021 bytes (0x00000015) 0000: 43 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20 Content-Length: 0010: 31 32 34 0d 0a 124.. &amp;lt;= Recv header, 0000000022 bytes (0x00000016) 0000: 53 65 72 76 65 72 3a 20 55 70 6c 6f 61 64 53 65 Server: UploadSe 0010: 72 76 65 72 0d 0a rver.. &amp;lt;= Recv header, 0000000045 bytes (0x0000002d) 0000: 41 6c 74 2d 53 76 63 3a 20 71 75 69 63 3d 22 3a Alt-Svc: quic=&amp;quot;: 0010: 34 34 33 22 3b 20 6d 61 3d 32 35 39 32 30 30 30 443&amp;quot;; ma=2592000 0020: 3b 20 76 3d 22 33 35 2c 33 34 22 0d 0a ; v=&amp;quot;35,34&amp;quot;.. &amp;lt;= Recv header, 0000000002 bytes (0x00000002) 0000: 0d 0a .. &amp;lt;= Recv data, 0000000124 bytes (0x0000007c) 0000: 7b 0a 20 22 6b 69 6e 64 22 3a 20 22 64 72 69 76 {. &amp;quot;kind&amp;quot;: &amp;quot;driv 0010: 65 23 66 69 6c 65 22 2c 0a 20 22 69 64 22 3a 20 e#file&amp;quot;,. &amp;quot;id&amp;quot;: 0020: 22 30 42 35 56 45 56 7a 65 66 51 4c 47 78 61 33 &amp;quot;0B5VEVzefQLGxa3 0030: 68 54 4f 46 4d 77 62 30 35 4c 4c 54 41 22 2c 0a hTOFMwb05LLTA&amp;quot;,. 0040: 20 22 6e 61 6d 65 22 3a 20 22 6d 72 79 71 75 2d &amp;quot;name&amp;quot;: &amp;quot;mryqu- 0050: 63 73 76 2d 72 75 70 6c 6f 61 64 2e 63 73 76 22 csv-rupload.csv&amp;quot; 0060: 2c 0a 20 22 6d 69 6d 65 54 79 70 65 22 3a 20 22 ,. &amp;quot;mimeType&amp;quot;: &amp;quot; 0070: 74 65 78 74 2f 63 73 76 22 0a 7d 0a text/csv&amp;quot;.}.  Reference Google Drive APIs - REST - Upload Files
libcurl fileupload example
libcurl http2-upload example
libcurl postit2 example
libcurl multi-post example
libcurl ftpuploadresume example
</content>
    </entry>
    
     <entry>
        <title>Get Facebook Reaction data using Graph API</title>
        <url>https://mryqu.github.io/post/get_facebook_reaction_data_using_graph_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>api</tag><tag>reaction</tag><tag>表情</tag>
        </tags>
        <content type="html"> Facebook Enhances Everyone&amp;rsquo;s Like With Love, Haha, Wow, Sad, Angry Buttons. For example, there are two reactions in the post 778979332230227: Your can use Facebook Graph API - Reading Reactions to get the reactions data. Note: The API version must be 2.6 or higher. You also can get all reactions count of one post in single Graph API request: </content>
    </entry>
    
     <entry>
        <title>Upload file to Google Drive using Postman and cURL</title>
        <url>https://mryqu.github.io/post/upload_file_to_google_drive_using_postman_and_curl/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>upload</tag><tag>google drive</tag><tag>postman</tag><tag>curl</tag>
        </tags>
        <content type="html">  Simple upload For quick transfer of smaller files, for example, 5 MB or less.
Postman cURL curl -T mytest.csv -X POST -H &amp;quot;Content-Type: text/csv&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; &amp;quot;https://www.googleapis.com/upload/drive/v3/files?uploadType=media&amp;amp;access_token={YOUR_ACCESS_TOKEN}&amp;quot;  Multipart upload For quick transfer of smaller files and metadata; transfers the file along with metadata that describes it, all in a single request.
Postman At the beginning, I try to use form-data for body, then the second value can use file directly. But Postman can&amp;rsquo;t configure Content-Type for each part, and Google Drive return error. I use raw data for body finally.
cURL curl -X POST -H &amp;quot;Content-Type: multipart/related; boundary=foo_bar_baz&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;--foo_bar_baz Content-Type: application/json; charset=UTF-8 { &amp;quot;name&amp;quot;: &amp;quot;postman-gdmultipartupload&amp;quot; } --foo_bar_baz Content-Type: text/csv Name,Sex,Age,Height,Weight Alfred,M,14,69,112.5 Alice,F,13,56.5,84 Barbara,F,13,65.3,98 Carol,F,14,62.8,102.5 --foo_bar_baz--&#39; &amp;quot;https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart&amp;amp;access_token={YOUR_ACCESS_TOKEN}&amp;quot;  Resumable upload For reliable transfer, especially important with larger files. With this method, you use a session initiating request, which optionally can include metadata. This is a good strategy to use for most applications, since it also works for smaller files at the cost of one additional HTTP request per upload.
Postman cURL curl -X POST -H &amp;quot;Content-Type: application/json; charset=UTF-8&amp;quot; -H &amp;quot;X-Upload-Content-Type: text/csv&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;{ &amp;quot;name&amp;quot;: &amp;quot;postman-gdresumableupload&amp;quot; }&#39; &amp;quot;https://www.googleapis.com/upload/drive/v3/files?uploadType=resumable&amp;amp;access_token={YOUR_ACCESS_TOKEN}&amp;quot; curl -X PUT -H &amp;quot;Content-Type: text/csv&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;Name,Sex,Age,Height,Weight Alfred,M,14,69,112.5 Alice,F,13,56.5,84 Barbara,F,13,65.3,98 Carol,F,14,62.8,102.5&#39; &amp;quot;https://www.googleapis.com/upload/drive/v3/files?uploadType=resumable&amp;amp;access_token={YOUR_ACCESS_TOKEN}&amp;amp;upload_id={GOTTEN_UOLOAD_ID}&amp;quot;  Reference Google Drive APIs - REST - Upload Files
libcurl fileupload example
libcurl http2-upload example
libcurl postit2 example
libcurl multi-post example
libcurl ftpuploadresume example
</content>
    </entry>
    
     <entry>
        <title>在Google Drive上创建存在多个目录下的文件</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8google_drive%E4%B8%8A%E5%88%9B%E5%BB%BA%E5%AD%98%E5%9C%A8%E5%A4%9A%E4%B8%AA%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google drive</tag><tag>multiple</tag><tag>parent</tag><tag>file</tag>
        </tags>
        <content type="html">  准备环境 在Google Drive上创建两个目录: parent1和parent2 代码 package com.yqu.gd; import java.io.IOException; import java.util.ArrayList; import java.util.List; import com.google.api.client.auth.oauth2.Credential; import com.google.api.client.googleapis.auth.oauth2.GoogleCredential; import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport; import com.google.api.client.http.FileContent; import com.google.api.client.http.HttpTransport; import com.google.api.client.json.JsonFactory; import com.google.api.client.json.jackson2.JacksonFactory; import com.google.api.services.drive.Drive; import com.google.api.services.drive.model.File; import com.google.api.services.drive.model.FileList; public class FileWithMultiParents { private static final String APPLICATION_NAME = &amp;quot;Hello Google Drive API&amp;quot;; private static final JsonFactory JSON_FACTORY = JacksonFactory.getDefaultInstance(); private static HttpTransport HTTP_TRANSPORT; static { try { HTTP_TRANSPORT = GoogleNetHttpTransport.newTrustedTransport(); } catch (Throwable t) { t.printStackTrace(); System.exit(1); } } public static Credential authorize(String accessToken) throws IOException { Credential credential = new GoogleCredential() .setAccessToken(accessToken); return credential; } public static Drive getDriveService(String accessToken) throws IOException { Credential credential = authorize(accessToken); return new Drive.Builder( HTTP_TRANSPORT, JSON_FACTORY, credential) .setApplicationName(APPLICATION_NAME) .build(); } public static void listFiles(Drive service) throws IOException { // Print the names and IDs for up to 10 files. FileList result = service.files().list() .setPageSize(10) .setFields(&amp;quot;nextPageToken, files(id, name)&amp;quot;) .execute(); List files = result.getFiles(); if (files == null || files.size() == 0) { System.out.println(&amp;quot;No files found.&amp;quot;); } else { System.out.println(&amp;quot;Files:&amp;quot;); for (File file : files) { System.out.printf(&amp;quot;%s (%s)\n&amp;quot;, file.getName(), file.getId()); } } } public static void createFile(Drive service, String newFileName, List folderIds) throws IOException { File fileMetadata = new File(); fileMetadata.setName(newFileName); fileMetadata.setMimeType(&amp;quot;application/vnd.google-apps.spreadsheet&amp;quot;); if(folderIds!=null &amp;amp;&amp;amp; !folderIds.isEmpty()) { fileMetadata.setParents(folderIds); } File filePath = new File(&amp;quot;c:/gdtest/test.csv&amp;quot;); //本行File包为java.io FileContent mediaContent = new FileContent(&amp;quot;text/csv&amp;quot;, filePath); File file = service.files().create(fileMetadata, mediaContent) .setFields(&amp;quot;id, parents&amp;quot;) .execute(); System.out.println(&amp;quot;File ID: &amp;quot; &#43; file.getId()); } public static void main(String[] args) throws IOException { String accessToken = &amp;quot;ya29....&amp;quot;; // Build a new authorized API client service. Drive service = getDriveService(accessToken); List parentIds = new ArrayList(); parentIds.add(&amp;quot;0B5VEVzefQLGxZTVPZ1V1eGxrcE0&amp;quot;); parentIds.add(&amp;quot;0B5VEVzefQLGxVmpXc3diV3EyLWc&amp;quot;); createFile(service, &amp;quot;HiMultiParents&amp;quot;, parentIds); listFiles(service); } }  查看结果 在两个目录parent1和parent2下都发现所创建的HiMultiParents文件： 通过HiMultiParents属性也可知存放在两个目录下：此外，在任何一个目录下删除该文件，另一个目录下的文件也将不存在。
</content>
    </entry>
    
     <entry>
        <title>学习Java Annotation</title>
        <url>https://mryqu.github.io/post/%E5%AD%A6%E4%B9%A0java_annotation/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>spring</tag><tag>annotation</tag>
        </tags>
        <content type="html"> 以前看过Java Annotation，走马观花，现在印象已经不深刻了。这次好好看一下Java Annotation和Spring Annotation。
阅读列表： The Java Tutorials - Annotations
Java Annotation认知(包括框架图、详细介绍、示例说明)
Annotations Gotchas and Best Practices
Annotations: Don&amp;rsquo;t Mess with Java
Java Annotations Are a Big Mistake
Spring Annotation-based container configuration
Spring Framework Annotations cheat sheet
Spring Without XML: The Basics of Spring Annotations vs. Spring XML Files
Spring Annotation Tutorial
Spring Annotations [ Quick Reference ]
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 将JSON转成字符串</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E5%B0%86json%E8%BD%AC%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>jsoncpp</tag><tag>json value</tag><tag>string</tag><tag>convert</tag>
        </tags>
        <content type="html"> 需要将如下JSON字符串作为GoogleSheets API POST请求的消息体。打算使用JsonCpp实现。
{&amp;quot;majorDimension&amp;quot;:&amp;quot;ROWS&amp;quot;,&amp;quot;values&amp;quot;:[[&amp;quot;Name&amp;quot;,&amp;quot;Sex&amp;quot;,&amp;quot;Age&amp;quot;,&amp;quot;Height&amp;quot;,&amp;quot;Weight&amp;quot;],[&amp;quot;阿尔弗雷德&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;14&amp;quot;,&amp;quot;69&amp;quot;,&amp;quot;112.5&amp;quot;],[&amp;quot;爱丽丝&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;13&amp;quot;,&amp;quot;56.5&amp;quot;,&amp;quot;84&amp;quot;],[&amp;quot;芭芭拉&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;13&amp;quot;,&amp;quot;65.3&amp;quot;,&amp;quot;98&amp;quot;],[&amp;quot;凯露&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;14&amp;quot;,&amp;quot;62.8&amp;quot;,&amp;quot;102.5&amp;quot;],[&amp;quot;亨利&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;14&amp;quot;,&amp;quot;63.5&amp;quot;,&amp;quot;102.5&amp;quot;],[&amp;quot;詹姆斯&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;12&amp;quot;,&amp;quot;57.3&amp;quot;,&amp;quot;83&amp;quot;],[&amp;quot;简&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;12&amp;quot;,&amp;quot;59.8&amp;quot;,&amp;quot;84.5&amp;quot;],[&amp;quot;雅妮特&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;15&amp;quot;,&amp;quot;62.5&amp;quot;,&amp;quot;112.5&amp;quot;],[&amp;quot;杰弗瑞&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;13&amp;quot;,&amp;quot;62.5&amp;quot;,&amp;quot;84&amp;quot;],[&amp;quot;约翰&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;12&amp;quot;,&amp;quot;59&amp;quot;,&amp;quot;99.5&amp;quot;],[&amp;quot;乔伊斯&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;11&amp;quot;,&amp;quot;51.3&amp;quot;,&amp;quot;50.5&amp;quot;],[&amp;quot;茱迪&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;14&amp;quot;,&amp;quot;64.3&amp;quot;,&amp;quot;90&amp;quot;],[&amp;quot;罗伊斯&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;12&amp;quot;,&amp;quot;56.3&amp;quot;,&amp;quot;77&amp;quot;],[&amp;quot;玛丽&amp;quot;,&amp;quot;女&amp;quot;,&amp;quot;15&amp;quot;,&amp;quot;66.5&amp;quot;,&amp;quot;112&amp;quot;],[&amp;quot;菲利普&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;16&amp;quot;,&amp;quot;72&amp;quot;,&amp;quot;150&amp;quot;],[&amp;quot;罗伯特&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;12&amp;quot;,&amp;quot;64.8&amp;quot;,&amp;quot;128&amp;quot;],[&amp;quot;罗纳德&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;15&amp;quot;,&amp;quot;67&amp;quot;,&amp;quot;133&amp;quot;],[&amp;quot;托马斯&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;11&amp;quot;,&amp;quot;57.5&amp;quot;,&amp;quot;85&amp;quot;],[&amp;quot;威廉&amp;quot;,&amp;quot;男&amp;quot;,&amp;quot;15&amp;quot;,&amp;quot;66.5&amp;quot;,&amp;quot;112&amp;quot;]]}  最终代码：</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 调试libcurl程序</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E8%B0%83%E8%AF%95libcurl%E7%A8%8B%E5%BA%8F/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>libcurl</tag><tag>debug</tag><tag>dump</tag>
        </tags>
        <content type="html">  最近在调试通过libcurl发送GoogleSheets API POST请求时，增加了一点经验，特此总结。
GoogleSheets API 请求 POST /v4/spreadsheets?access_token={YOUR_ACCESSTOKEN}&amp;amp;fields=spreadsheetId HTTP/1.1 Host: sheets.googleapis.com Content-Type: application/json;charset=UTF-8 Accept: application/json Cache-Control: no-cache {&amp;quot;properties&amp;quot;:{&amp;quot;title&amp;quot;:&amp;quot;newhaha&amp;quot;},&amp;quot;sheets&amp;quot;:[{&amp;quot;properties&amp;quot;:{&amp;quot;title&amp;quot;:&amp;quot;Sheet1&amp;quot;}}]}  libcurl调试 当GoogleSheets API 请求失败时，仅能获得返回的状态码和消息。感觉没有更多信息可以研究！后来通过CURLOPT_VERBOSE和CURLOPT_DEBUGFUNCTION获得了更多调试信息。
使用CURLOPT_VERBOSE curl_easy_setopt(m_curl, CURLOPT_VERBOSE, 1L);  这样就可以看到请求报头、响应报头和消息体了。
使用CURLOPT_DEBUGFUNCTION 使用libcurl API指南中CURLOPT_DEBUGFUNCTION示例代码即可。这样就可以看到完整的请求和响应内容了。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] jQuery.sap.formatMessage的一点注意事项</title>
        <url>https://mryqu.github.io/post/openui5_jquery.sap.formatmessage%E7%9A%84%E4%B8%80%E7%82%B9%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>jquery</tag><tag>formatmessage</tag><tag>i18n</tag>
        </tags>
        <content type="html"> 今天偶然发现I18N properties文件中有字符串包含替换符，可是没起作用，还是明晃晃输出了{0}。仔细研究一下，才发现原因在于字符串中包含单个&amp;rsquo;，OpenUI5使用jQuery.sap.formatMessage替换I18N字符串，如果仅含有一个&amp;rsquo;字符的话，其/(&#39;&#39;)|&#39;([^&#39;]&#43;(?:&#39;&#39;[^&#39;]*)*)(?:&#39;|$)|\{([0-9]&#43;(?:\s*,[^{}]*)?)\}|[{}]/g 就只触发第二组替换了。如果想显示单个字符&amp;rsquo;，需要用两个字符&amp;rsquo;转义。 不过问题来了，负责翻译I18N的同事是否清楚jQuery.sap.formatMessage关于字符&amp;rsquo;的限制呢？
</content>
    </entry>
    
     <entry>
        <title>恢复误删文件内容</title>
        <url>https://mryqu.github.io/post/%E6%81%A2%E5%A4%8D%E8%AF%AF%E5%88%A0%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>restore</tag><tag>file</tag><tag>earlier version</tag>
        </tags>
        <content type="html"> 昨天鼠标有毛病，本意是选择Ultraedit中的文件，结果莫名其妙关闭了。重新打开后，内容都丢了，而且还认为是正藏保存的。傻眼了，记了一年多的内容呀！ 今天终于找到恢复方法了，右键点击文件-查看属性-选择以前版本-选择版本并恢复。我的天呀，终于不必愁眉苦脸了！
</content>
    </entry>
    
     <entry>
        <title>尝试Travis CI</title>
        <url>https://mryqu.github.io/post/%E5%B0%9D%E8%AF%95travis_ci/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>travis</tag><tag>ci</tag><tag>github</tag><tag>jenkins</tag>
        </tags>
        <content type="html">  从GitHub上下载的很多项目都包含一个.travis.yml文件，一开始不知道是什么鬼，后来才知道是Travis CI配置文件。 Travis CI是基于云的持续集成项目，供GitHub上的开源项目使用。对于GitHub上的项目来说，Travis CI无需自己部署服务器，仅需添加一个.travis.yml文件就可进行持续集成，入侵性很小，所以很多项目都纷纷采用了。
Travis CI与Jenkins的比较 对于企业开发来收，目前主流还是Jenkins/Hudson，下面可以看一下二者的功能比较。 名称平台许可构建器：Windows构建器：Java构建器：其他通知集成 IDE集成 其他Jenkins-HudsonWeb容器Creative Commons和 MITMSBuild
NAntAnt
Maven&amp;nbsp;2
KundoCmake
Gant
Gradle
Grails, Phing
Rake
Ruby
SCons
Python
shell script
command-lineAndroid
Email
Google Calendar
IRC
XMPP
RSS
Twitter
Slack
Catlight
CCMenu
CCTrayEclipse
IntelliJ IDEA
NetBeansBugzilla
Google Code
Jira
Bitbucket
Redmine
FindBugs
Checkstyle
PMD&amp;nbsp;and&amp;nbsp;Mantis
Trac
HP ALMTravis CI已在云上部署MIT无Ant
Maven
GradleC
C&#43;&#43;
Clojure
Elixir
Erlang
Go
Groovy
Haskell
Java
Node.js
Perl
PHP
Python
Ruby
Rust
Scala
SmalltalkEmail
Campfire
HipChat
IRC
Slack
Catlight
CCMenu
CCTray无GitHub
Heroku 此外Jenkins支持AccuRev、BitKeeper、CA Harvest、ClearCase、CVS、Darcs、Git、GNU Bazaar、Integrity、Mercurial、Perforce、Plastic、PVCS、StarTeam、Subversion、Surround、Synergy、Team Concert、Team Foundation Server、Vault Visual SourceSafe这些源代码控制管理系统，而Travis CI仅支持Git。
我的尝试 最终选择我的C&#43;&#43;项目mryqu/twitcurl采用Travis CI进行持续集成，测试一下整个编译是否能过。 参考 网站：travis-ci.org（免费）travis-ci.com（企业版）
GitHub: travis-ci/travis-ci
WIKI: Travis CI
WIKI: Comparison of continuous integration software
为什么国内做持续集成大家都选择 Jenkins ,用 travis-ci 的人那么少呢？
用TravisCI来做持续集成
Travis CI会替代Jenkins吗？
Travis CI，翩翩而至的CI云
免费的持续集成测试服务
</content>
    </entry>
    
     <entry>
        <title>update-alternatives与JAVA_HOME</title>
        <url>https://mryqu.github.io/post/update-alternatives%E4%B8%8Ejava_home/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>ubuntu</tag><tag>update-alternatives</tag><tag>java home</tag><tag>dynamic</tag><tag>switch</tag>
        </tags>
        <content type="html">  测试 /etc/profile配置 export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin  使用update-alternatives切换JAVA hadoop@note50064:~$ which java /usr/bin/java hadoop@note50064:~$ sudo update-alternatives --config java There are 2 choices for the alternative . Selection Path Priority Status ------------------------------------------------------------ 0 /usr/lib/jvm/java-7-oracle/jre/bin/java 1072 auto mode * 1 /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java 1071 manual mode 2 /usr/lib/jvm/java-7-oracle/jre/bin/java 1072 manual mode Press enter to keep the current choice[*], or type selection number: hadoop@note50064:~$ ls -l /usr/bin/java lrwxrwxrwx 1 root root 22 Nov 21 03:26 /usr/bin/java -&amp;gt; /etc/alternatives/java hadoop@note50064:~$ ls -l /etc/alternatives/java lrwxrwxrwx 1 root root 46 Nov 1 01:44 /etc/alternatives/java -&amp;gt; /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java hadoop@note50064:~$ java -version java version &amp;quot;1.7.0_121&amp;quot; OpenJDK Runtime Environment (IcedTea 2.6.8) (7u121-2.6.8-1ubuntu0.14.04.1) OpenJDK 64-Bit Server VM (build 24.121-b00, mixed mode) hadoop@note50064:~$ $JAVA_HOME/bin/java -version java version &amp;quot;1.7.0_121&amp;quot; OpenJDK Runtime Environment (IcedTea 2.6.8) (7u121-2.6.8-1ubuntu0.14.04.1) OpenJDK 64-Bit Server VM (build 24.121-b00, mixed mode) hadoop@note50064:~$ hadoop@note50064:~$ sudo update-alternatives --config java There are 2 choices for the alternative . Selection Path Priority Status ------------------------------------------------------------ 0 /usr/lib/jvm/java-7-oracle/jre/bin/java 1072 auto mode * 1 /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java 1071 manual mode 2 /usr/lib/jvm/java-7-oracle/jre/bin/java 1072 manual mode Press enter to keep the current choice[*], or type selection number: 2 update-alternatives: using /usr/lib/jvm/ in manual mode hadoop@note50064:~$ ls -l /etc/alternatives/java lrwxrwxrwx 1 root root 39 Nov 1 01:45 /etc/alternatives/java -&amp;gt; /usr/lib/jvm/java-7-oracle/jre/bin/java hadoop@note50064:~$ java -version java version &amp;quot;1.7.0_80&amp;quot; hadoop@note50064:~$ $JAVA_HOME/bin/java -version java version &amp;quot;1.7.0_121&amp;quot; OpenJDK Runtime Environment (IcedTea 2.6.8) (7u121-2.6.8-1ubuntu0.14.04.1) OpenJDK 64-Bit Server VM (build 24.121-b00, mixed mode)  分析 Alternatives是使用符号连接管理已装软件位置的工具。例外我们通常在命令行使用的java命令使用的是/usr/bin/java；而/usr/bin/java指向/etc/alternatives/java，/etc/alternatives/java指向的是真正某个JRE/JDK包下的java命令。 尽管可以使用Alternatives工具切换JAVA，但是由于我的JAVA_HOME始终指向OpenJDK，所以$JAVA_HOME/bin/java不会随Alternatives的指定而切换。这样使用JAVA_HOME环境变量的Hadoop等工具一直用OpenJDK。
方案 更改/etc/profile配置：
export export JDK_HOME=$(readlink -f /usr/bin/ export CLASSPATH=.:$JDK_HOME/lib/dt.jar:$JDK_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin  这样通过update-alternatives命令更改java和javac后，重启命令行后JAVA_HOME和JDK_HOME也随之更新。 这个方案自己不甚满意，有时间再继续研究吧。
参考 What is the difference between JAVA_HOME and update-alternatives?
Setting up your Linux environment to support multiple versions of Java
Using the Debian alternatives system
update-alternatives - Linux man page
</content>
    </entry>
    
     <entry>
        <title>Hello Google Drive APIs</title>
        <url>https://mryqu.github.io/post/hello_google_drive_apis/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>drive</tag><tag>api</tag><tag>crawler</tag><tag>storage</tag>
        </tags>
        <content type="html">  准备环境 当前我的Google Drive内容如下： 继续使用博文《Google Sheets API认证和鉴权》中用过的应用yquGSTest，不过需要激活Google Drive API： Google Drive API测试 方法drive.about.get测试 方法drive.about.get用于获取用户、驱动和系统容量等信息。
方法drive.files.list测试 方法drive.files.list用于列举或搜索文件。
与Microsoft OneDriveAPI仅列举请求目录下文件不同，方法drive.files.list列举文件时返回了所有目录和文件，例如子目录FolderTest1下的文件Class_cn_Tab.csv也在响应内容里面。
方法drive.files.get测试 方法drive.files.get用于通过ID获取文件元数据。下面获得Class_cn_Tab.csv文件的元数据。
方法drive.files.create测试 方法drive.files.create用于创建一个新文件。
在API Explorer中仅能指定新文件的元数据，没法上传文件内容，所以虽然测试成功且GoogleDrive里也会显示新的文件，但是打不开。这种仅指定元数据不提供内容的方式特别适合创建目录。
https://developers.google.com/drive/v3/web/manage-uploads里面说明了如何在创建或更新文件时上传文件内容。
通过Java JDK创建文件 与博文《Google Sheets API认证和鉴权》中获取访问令牌的差异如下：
GET https://accounts.google.com/o/oauth2/v2/auth? scope=**&amp;lt;font color=&amp;quot;#FF0000&amp;quot;&amp;gt;https://www.googleapis.com/auth/drive&amp;lt;/font&amp;gt;** https://www.googleapis.com/auth/drive.readonly profile&amp;amp;amp; redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;amp; response_type=code&amp;amp;amp; client_id=826380598768-5935tlo90sccvr691ofmp4nrvpthrnn6.apps.googleusercontent.com``` 代码如下：  package com.yqu.gd;
import java.io.IOException; import java.util.Collections; import java.util.List;
import com.google.api.client.auth.oauth2.Credential; import com.google.api.client.googleapis.auth.oauth2.GoogleCredential; import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport; import com.google.api.client.http.FileContent; import com.google.api.client.http.HttpTransport; import com.google.api.client.json.JsonFactory; import com.google.api.client.json.jackson2.JacksonFactory; import com.google.api.services.drive.Drive; import com.google.api.services.drive.model.File; import com.google.api.services.drive.model.FileList;
public class HelloGoogleDrive {
private static final String APPLICATION_NAME = &amp;ldquo;Hello Google Drive API&amp;rdquo;;
private static final JsonFactory JSON_FACTORY = JacksonFactory.getDefaultInstance();
private static HttpTransport HTTP_TRANSPORT;
static { try { HTTP_TRANSPORT = GoogleNetHttpTransport.newTrustedTransport(); } catch (Throwable t) { t.printStackTrace(); System.exit(1); } }
public static Credential authorize(String accessToken) throws IOException { Credential credential = new GoogleCredential() .setAccessToken(accessToken); return credential; }
public static Drive getDriveService(String accessToken) throws IOException { Credential credential = authorize(accessToken); return new Drive.Builder( HTTP_TRANSPORT, JSON_FACTORY, credential) .setApplicationName(APPLICATION_NAME) .build(); }
public static void listFiles(Drive service) throws IOException { // Print the names and IDs for up to 10 files. FileList result = service.files().list() .setPageSize(10) .setFields(&amp;ldquo;nextPageToken, files(id, name)&amp;rdquo;) .execute(); List files = result.getFiles(); if (files == null || files.size() == 0) { System.out.println(&amp;ldquo;No files found.&amp;rdquo;); } else { System.out.println(&amp;ldquo;Files:&amp;ldquo;); for (File file : files) { System.out.printf(&amp;ldquo;%s (%s)\n&amp;rdquo;, file.getName(), file.getId()); } } }
public static void createFile(Drive service, String newFileName, String folderId) throws IOException { File fileMetadata = new File(); fileMetadata.setName(newFileName); fileMetadata.setMimeType(&amp;ldquo;application/vnd.google-apps.spreadsheet&amp;rdquo;); if(folderId!=null &amp;amp;&amp;amp; !folderId.isEmpty()) { fileMetadata.setParents(Collections.singletonList(folderId)); }
File filePath = new File(&amp;quot;c:/gdtest/test.csv&amp;quot;); **&amp;lt;font color=&amp;quot;#FF0000&amp;quot;&amp;gt;//本行File包为java.io&amp;lt;/font&amp;gt;** FileContent mediaContent = new FileContent(&amp;quot;text/csv&amp;quot;, filePath); File file = service.files().create(fileMetadata, mediaContent) .setFields(&amp;quot;id, parents&amp;quot;) .execute(); System.out.println(&amp;quot;File ID: &amp;quot; &#43; file.getId());  }
public static void main(String[] args) throws IOException { String accessToken = &amp;ldquo;ya29&amp;hellip;.&amp;rdquo;;
// Build a new authorized API client service. Drive service = getDriveService(accessToken); createFile(service, &amp;quot;WhatWillBeByJava&amp;quot;, &amp;quot;0B5VEVzefQLGxX2loMGhQcEVBc00&amp;quot;; listFiles(service);  } }
 创建的新文件WhatWillBeByJava在Google Drive中显示的是GoogleSheet类型图标，可由Google Sheet打开。程序日志如下：  File ID: 1APh0cJKSMfrqrfs4AsYq4m674ZHzVLXUnSbAKdVLdgk Files: WhatWillBeBy WhatWillBe.csv (0B5VEVzefQLGxNERVZzlwa0gxQzg) 2013sales.xls (164YAJYgp5E0jp4qL-ucOV2PTOqPpoYdjFtf47tUit5U) 2013sales.xls (0B5VEVzefQLGxMkl3TldkMG5OejQ) Class_cn_Tab.csv (0B5VEVzefQLGxMVpTVzd4WF8wWnc) FolderTest1 (0B5VEVzefQLGxX2loMGhQcEVBc00) SpreadSheetCreate1 (1RRKvO7TBp1haRp4eJQU4-UX0AXXOVwxwXOXhXCHp9mM) SpreadSheetTest1 (1nAsY7MDdsuktiUHLPr82dfmXqRIqJSVriAhDKpsUMe0) To-do list (1m5YXGewJ6SCafNaALObghkfAr4WoIwSCAV0HkLp8H-A) Getting started (0B5VEVzefQLGxc3RhcnRlcl9maWxl)```
相关Scope  查看和管理Google Drive中的文件：https://www.googleapis.com/auth/drive 查看和管理Google Drive中的配置数据：https://www.googleapis.com/auth/drive.appdata 查看和管理当前应用所打开/创建Google Drive文件和目录：https://www.googleapis.com/auth/drive.file 查看和管理Google Drive中的文件元数据：https://www.googleapis.com/auth/drive.metadata 查看Google Drive中的文件元数据：https://www.googleapis.com/auth/drive.metadata.readonly 查看Google Drive中的图片、视频和相册：https://www.googleapis.com/auth/drive.photos.readonly 查看Google Drive中的文件：https://www.googleapis.com/auth/drive.readonly  学习结论 Google Drive API可以获取用户、驱动及系统容量信息，对文件及其权限、评论和版本等进行操作。
参考 Google Drive
Google Drive APIs
Google API Explorer: Drive
</content>
    </entry>
    
     <entry>
        <title>使用OneDrive的根API资源</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8onedrive%E7%9A%84%E6%A0%B9api%E8%B5%84%E6%BA%90/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>onedrive</tag><tag>root</tag><tag>api</tag><tag>resource</tag><tag>storage</tag>
        </tags>
        <content type="html">  OneDrive的根API资源 可以使用OneDrive的这些根API资源来访问一个项目或驱动。
|路径|资源 |&amp;mdash;&amp;ndash; |/drive|用户的默认驱动。 |/drives|列举对认证用户可用的驱动。 |/drives/{drive-id}|通过ID访问一个特定驱动。 |/drives/{drive-id}/root/children|列举特定驱动根路径下项目。 |/drive/items/{item-id}|通过ID访问一个元素。 |/drive/special/{special-id}|通过已知名访问一个特殊（命名）目录。ID目前可取值为：documents、photos、cameraroll、approot、music。 |/shares/{share-id}|通过共享ID或共享URL访问一个元素。
元素可由路径定位，通过在任何元素或驱动URL后加冒号。
|路径|资源 |&amp;mdash;&amp;ndash; |/drive/root:/path/to/file|通过根绝对路径访问一个元素。 |/drive/items/{item-id}:/path/to/file|通过相对路径访问一个元素。 |/drive/root:/path/to/file:/children|通过根绝对路径列举一个元素的子项。 |/drive/items/{item-id}:/path/to/file:/children|通过相对路径列举一个元素的子项。
测试 获取默认驱动 列举可用驱动 通过ID获取指定驱动 列举特定驱动根路径下项目 通过ID访问一个目录&amp;rdquo;文档&amp;rdquo; 访问特殊目录documents 通过共享ID访问文件CN_EN_JP_KO.xlsx 将文件CN_EN_JP_KO.xlsx共享，获取其共享URL： 通过共享ID使用OneDrive API访问文件CN_EN_JP_KO.xlsx： 通过根绝对路径访问文件CN_EN_JP_KO.xlsx 注意root后有冒号： 通过相对路径访问文件CN_EN_JP_KO.xlsx 712B21FCE8E08C92!442是目录&amp;rdquo;文档&amp;rdquo;的ID，注意其后有冒号： 通过根绝对路径列举目录&amp;rdquo;文档&amp;rdquo;的子元素 注意root和路径（/文档）后都有冒号： 通过相对路径列举目录&amp;rdquo;FolderTest&amp;rdquo;的子元素 为了测试，首先我在目录&amp;rdquo;文档&amp;rdquo;创建子目录&amp;rdquo;FolderTest&amp;rdquo;，然后在目录&amp;rdquo;FolderTest&amp;rdquo;中创建mryqu.txt文件。
712B21FCE8E08C92!442是目录&amp;rdquo;文档&amp;rdquo;的ID，注意其后有冒号；路径（/FolderTest）后也有冒号。 </content>
    </entry>
    
     <entry>
        <title>Microsoft OneDrive API访问速率限制</title>
        <url>https://mryqu.github.io/post/microsoft_onedrive_api%E8%AE%BF%E9%97%AE%E9%80%9F%E7%8E%87%E9%99%90%E5%88%B6/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>onedrive</tag><tag>api</tag><tag>rate</tag><tag>limit</tag><tag>quota</tag>
        </tags>
        <content type="html"> 一开始查看OneDrive文档Quota facet,，发现里面介绍的是OneDrive存储容量配额，跟API访问速率限制没有关系。除此之外，没有发现任何相关信息。 OneDrive文档Error response里面，看到如下跟访问速率限制相关的错误： - Status code: 429 (Too Many Requests)和509 (Bandwidth LimitExceeded) - The code property: activityLimitReached (The app or user hasbeen throttled) - Detailed error code: throttledRequest (Too many requests)
</content>
    </entry>
    
     <entry>
        <title>OneDrive认证时的&#34;Public clients can&#39;t send a client secret.&#34;错误</title>
        <url>https://mryqu.github.io/post/onedrive%E8%AE%A4%E8%AF%81%E6%97%B6%E7%9A%84public_clients_cant_send_a_client_secret%E9%94%99%E8%AF%AF/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>onedrive</tag><tag>authentication</tag><tag>public</tag><tag>client</tag><tag>secret</tag>
        </tags>
        <content type="html"> 在进行Microsoft OneDrive认证和登录实验的过程中，曾经用下列命令过去访问令牌：
POST https://login.live.com/oauth20_token.srf Content-Type: application/x-www-form-urlencoded client_id={client_id}&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf&amp;amp;client_secret={client_secret} &amp;amp;code={code}&amp;amp;grant_type=authorization_code  结果返回：
{&amp;quot;error&amp;quot;:&amp;quot;invalid_request&amp;quot;,&amp;quot;error_description&amp;quot;:&amp;quot;Public clients can&#39;t send a client secret.&amp;quot;}  一个&amp;rdquo;public client&amp;rdquo;指的是移动或桌面应用(web服务则是&amp;rdquo;confidentialclient&amp;rdquo;)。由于跳转URI是https://login.live.com/oauth20_desktop.srf，因而MSA返回该错误响应。这种情况下，不应该提供client_secret，使用下列请求即可。
POST https://login.live.com/oauth20_token.srf Content-Type: application/x-www-form-urlencoded client_id={client_id}&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf&amp;amp;code={code}&amp;amp;grant_type=authorization_code  </content>
    </entry>
    
     <entry>
        <title>Microsoft OneDrive认证和登录</title>
        <url>https://mryqu.github.io/post/microsoft_onedrive%E8%AE%A4%E8%AF%81%E5%92%8C%E7%99%BB%E5%BD%95/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>microsoft</tag><tag>onedrive</tag><tag>authentication</tag><tag>sign-in</tag><tag>application</tag>
        </tags>
        <content type="html">  为OneDrive注册自己的应用 Registering your app for OneDrive API里面有提到，平台支持web和移动应用两种，而默认情况下是web应用，需要一或多个跳转URI。对于原生应用，可以选择移动应用。选择移动应用后跳转URI则变成urn:ietf:wg:oauth:2.0:oob（带外认证）了，正是我想要的结果！
OneDrive认证 OneDrive authentication and sign-in有个按钮可以获得测试Token，无需注册新的应用就可以请求到与登录账户绑定的、一个有效期1小时的开发者Token。从https://dev.onedrive.com/auth/get-token.js中我们可以看到其所用的http请求为TokenFlow，其跳转URI设为https://dev.onedrive.com/auth/callback.htm。而 OneDrive authentication and sign-in 中提到对于移动应用和桌面应用，跳转URI应设为https://login.live.com/oauth20_desktop.srf （注：使用urn:ietf:wg:oauth:2.0:oob的话，MSA连响应都没有）。
Token Flow测试 HTTP GET请求如下：
https://login.live.com/oauth20_authorize.srf?client_id=b9aaf3be-6892-42a5-8a04-4a87bc28ce7b&amp;amp;scope=onedrive.readonly&#43;wl.signin&amp;amp;response_type=code&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf  响应如下，认证失败：
https://login.live.com/oauth20_desktop.srf?lc=1033#error=unsupported_response_type&amp;amp;error_description=The&#43;provided&#43;value&#43;for&#43;the&#43;input&#43;parameter&#43;&#39;response_type&#39;&#43;is&#43;not&#43;allowed&#43;for&#43;this&#43;client.&#43;Expected&#43;value&#43;is&#43;&#39;code&#39;.  找了很久微软的帖子，也没说为什么Token Flow不要使，一直纠结是微软不支持还是我配置有问题。后来，看了RFC6749 The OAuth 2.0 Authorization Framework，才明白Token Flow就是规范里的Implicit GrantFlow。如果我的应用配置为web应用，是可以看到Allow ImplicitFlow选择框的。好吧，当选择移动应用时微软不支持Token Flow，我的配置没问题！！！Code Flow测试  用于用户登录的HTTP GET请求如下：  https://login.live.com/oauth20_authorize.srf?client_id=b9aaf3be-6892-42a5-8a04-4a87bc28ce7b&amp;amp;scope=onedrive.readonly&#43;wl.signin&amp;amp;response_type=token&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf  请求用户授权：此时浏览器上地址变为：  https://account.live.com/Consent/Update?ru=https://login.live.com/oauth20_authorize.srf?lc=1033&amp;amp;client_id=b9aaf3be-6892-42a5-8a04-4a87bc28ce7b&amp;amp;scope=onedrive.readonly&#43;wl.signin&amp;amp;response_type=code&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf&amp;amp;uaid=78...e6&amp;amp;pid=...16&amp;amp;mkt=EN-US&amp;amp;scft=DSA...hfC&amp;amp;contextid=7F...D6&amp;amp;mkt=EN-US&amp;amp;uiflavor=host&amp;amp;id=27...69&amp;amp;uaid=78...e6&amp;amp;client_id=00...42&amp;amp;rd=none&amp;amp;scope=&amp;amp;cscope=onedrive.readonly&#43;wl.signin  最终跳转的地址包含了code参数： 获取访问令牌的HTTP POST请求包含上面获得的code参数：  POST https://login.live.com/oauth20_token.srf Content-Type: application/x-www-form-urlencoded client_id=b9aaf3be-6892-42a5-8a04-4a87bc28ce7b&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf&amp;amp;code=M9...5e-b...a-e...5-6685-d...06&amp;amp;grant_type=authorization_code   在OneDrive API中使用获得的访问令牌：  参考 Getting started with OneDrive API
SDKs for OneDrive integration
Registering your app for OneDrive API
OneDrive authentication and sign-in
Sign-in Microsoft Account &amp;amp; Azure AD users in a single app
Develop with the OneDrive API
getting #error=unsupported_response_type&amp;amp;error_description=AADSTS70005: with token request
</content>
    </entry>
    
     <entry>
        <title>Hello Microsoft OneDrive API</title>
        <url>https://mryqu.github.io/post/hello_microsoft_onedrive_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>microsoft</tag><tag>onedrive</tag><tag>api</tag><tag>crawler</tag><tag>storage</tag>
        </tags>
        <content type="html">  OneDriveAPI提供了一套HTTP服务用以将应用连接到OneDrive个人版、OneDrive商业版及SharePoint在线文档库上的文件和目录。OneDriveAPI使应用连接Office 365上文档及访问OneDrive和SharePoint上文件高级功能变得容易。
测试源 为了省事，就用我自己私人的OneDrive做测试吧。获取Token 最省事的方法是在OneDrive authentication and sign-in里面获得测试Token，无需注册新的应用就可以请求到与登录账户绑定的、一个有效期1小时的开发者Token。测试API 获取默认Drive 查看Drive 根目录内容 从上图可知，根目录包含一个包含&amp;rdquo;050709大同&amp;rdquo;子目录，该子目录的id为&amp;rdquo;712B21FCE8E08C92!112&amp;rdquo;。从整个响应内容可知，根目录包含&amp;rdquo;文档&amp;rdquo;子目录，其id为&amp;rdquo;712B21FCE8E08C92!442&amp;rdquo;。
查看Drive &amp;ldquo;文档&amp;rdquo;目录 该目录下有一个CN_EN_JP_KO.xlsx文件，其@content.downloadUrl属性值为下载链接。
获取CN_EN_JP_KO.xlsx文件 如果将链接直接放入浏览器，下载后将文件名变更成xlsx后缀，即可用Excel打开。
参考 Develop with the OneDrive API
</content>
    </entry>
    
     <entry>
        <title>比较OneDrive、OneDriveforBusiness和Office365</title>
        <url>https://mryqu.github.io/post/%E6%AF%94%E8%BE%83onedriveonedriveforbusiness%E5%92%8Coffice365/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>onedrive</tag><tag>business</tag><tag>office</tag><tag>365</tag><tag>比较</tag>
        </tags>
        <content type="html">  OneDrive与OneDrive for Business的区别 OneDrive与OneDrive for Business名字接近，如果认为是个人版和商业版的区别，OneDrive forBusiness在OneDrive基础上增加一些商业高级功能，那就没有正确理解二者的区别。
 OneDrive（以前称之为SkyDrive）是微软提供的云端私人存储，通过Microsoft账户或Outlook.com获得。使用OneDrive在云上存储文档、图片和其他文件，可以共享给好友，甚至内容协作。可以随意决定你自己的使用方式。 OneDrive forBusiness是用于商业目的的在线存储，它既可以在微软云上也可以在组织/企业的SharePoint服务器上。OneDrive forBusiness是Office365或SharePoint服务器的主要组成部分，提供存储、共享和同步团队或项目工作文档的地方。你的OneDrive forBusiness由你的工作组织/企业管理，以便进行工作文档协作。你的工作组织/企业的网站集管理员控制你对文档库的权限。  功能桌面同步- Windows PC- Mac OS(Soon)移动应用- Windows Tablet- Android- iOS- Windows Phone- XBox在浏览器内创建/编辑Office文档与桌面版Office 集成实时协同编辑Office文档文档版本和历史信息- 简单自动创建版本和版本恢复- 版本管理(主次版本或仅主版本，手动创建版本需要checkout选项和其他高级版本选项)多重身份认证支持审计与报告用于细颗粒度控制的高级管理功能
审批工作流使用SharePoint Designer创建定制工作流对文档创建列添加自己的元数据高级安全管理对内容创建视图(可保存视图、过滤器等等)创建列表进行数据管理(通告、任务、联系人等等)保留策略(取决于SharePoint计划)文档模板eDiscoverySharePoint提供的功能SSO/ADFS/Directory同步支持内建标准一致性存储容量5GB/15GB1TB/5TB Office 365 Office 365是一套用于个人、教育或商业活动的云服务。商业和企业计划可提供一套提高生产效率的产品。使用Office365，可以获得的Office应用(Word、Excel、PowerPoint、OneNote、Outlook和Publisher)，以及50GB邮箱容量、用于企业即时消息、音视频电话和web会议的Skypefor Business、作为企业社交网络的Yammer、用于文件管理的SharePoint及1TB的OneDrive存储。
参考 OneDrive for Business vs OneDrive – Know the difference
OneDrive, OneDrive for Business, and Office 365: What’s Best?
What is OneDrive for Business?
Ultimate Guide in choosing between OneDrive, OneDrive for Business and Office 365
</content>
    </entry>
    
     <entry>
        <title>在线文档/存储 API/SDK</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8%E7%BA%BF%E6%96%87%E6%A1%A3%E5%AD%98%E5%82%A8_apisdk/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>api</tag><tag>接口</tag><tag>编程</tag><tag>语言</tag><tag>在线存储</tag>
        </tags>
        <content type="html"> 本博文是社交媒体API/SDK的姊妹篇。
社交媒体API编程平台/语言OneDrive Filepicker SDK:
快速下载或链接在OneDrive中的文件，或将文件保存到OneDrive官方:
AndroidiOS （使用UIDocumentPicker约定）Web /JavaScriptWindows（使用FileOpenPicker或 FileSavePicker）
OneDrive APISDK:
操作OneDrive上的文件，无需处理认证、JSON解析、HTTP连接等细节官方:
Windows .NET / C#/ XamariniOSPython（目前不支持OneDrive for Business）AndroidGoogle SheetsAPI SDK:
访问和更新Google电子表格官方:
AndroidGoiOSJavaJavaScript.NETNode.jsPHPPythonRubyGoogle Drive APISDK:
在移动/web应用中读、写和同步在Google Drive上的文件官方:
AndroidGoiOS (Object-C&amp;amp; Swift)JavaJavaScript.NETNode.jsPHPPythonRubyDropbox API SDK:
操作Dropbox上的文件官方:
.NETJavaJavaScriptPythonSwiftObjective-C社区:
AndroidGoJavaJavaScriptObjective-CNode.jsPHPSwiftBox API SDK:
操作Box上的文件官方:
Java.NETNode.jsPythonRubyChromeSalesforceIOSAndroidWindows(Mobile)iCloud APISDK:
操作iCloud上的资源官方:
CloudKit JS库CloudKitSwift库CloudKitObjective-C库社区:
PythonJavaAmazonDrive API :
操作Amazon Drive上的文件 </content>
    </entry>
    
     <entry>
        <title>使用Tableau导入Google Analytics</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8tableau%E5%AF%BC%E5%85%A5google_analytics/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>tableau</tag><tag>google</tag><tag>analytics</tag><tag>import</tag>
        </tags>
        <content type="html">  配置界面 Date Range配置选项 Segment配置选项 Dimension配置选项 Measure Group配置选项 Measure配置选项 </content>
    </entry>
    
     <entry>
        <title>Dropbox API访问速率限制</title>
        <url>https://mryqu.github.io/post/dropbox_api%E8%AE%BF%E9%97%AE%E9%80%9F%E7%8E%87%E9%99%90%E5%88%B6/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>dropbox</tag><tag>api</tag><tag>rate</tag><tag>limit</tag><tag>quota</tag>
        </tags>
        <content type="html"> Dropbox的Data ingress guide介绍了关于Dropbox API访问速率限制。 错误Status code: 429 (Too ManyRequests)用于表示API访问速率超限，如果响应包内容为JSON，则包含too_many_requests或too_many_write_operations值进行更进一步说明。 关联用户的应用，访问速率限制仅适用于每用户。一个用户关联多个应用，各应用互不影响。 关联团队的应用当调用商业端点（BusinessEndpoint），访问速率限制仅适用于每个团队。如果应用有团队成员文件访问权限但是正在调用用户端点（UserEndpoint），访问速率限制仅适用于每个团队成员。这意味着，对于关联团队的应用，一个团队关联多个应用，各应用互不影响；单个应用代表多个团队成员的请求，也不会影响团队成员彼此的访问速率限制。 超过速率限制后的响应包含一个Retry-After头，提供按秒计的等待间隔值，应用在这段时间内不应重试请求以免再获得速率限制响应。 Dropbox不会公布其API速率限制值，开发时要假设Dropbox会在今后调整其API速率限制。
</content>
    </entry>
    
     <entry>
        <title>Hello Dropbox API</title>
        <url>https://mryqu.github.io/post/hello_dropbox_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>dropbox</tag><tag>api</tag><tag>crawler</tag><tag>storage</tag><tag>quickstart</tag>
        </tags>
        <content type="html">  本博文用来记录一下粗略体验Dropbox关于用户、文件、共享三方面API的过程。
准备环境 还是用我私人的Dropbox做测试，所以只显示Public目录下的东东了。 用户类API测试 方法get_current_account测试 方法get_account测试 方法get_space_usage测试 文件类API测试 方法list_folder测试 方法list_folder其实是列举文件和目录，而且是分层的。如果path没设，则显示根目录下的元素。
方法get_metadata测试 方法get_metadata用于获取一个元素（文件/目录）的元数据。
方法create_folder测试 方法get_preview测试 方法get_preview仅支持 .doc、 .docx、 .docm、 .ppt、 .pps、 .ppsx、 .ppsm、.pptx、.pptm、 .xls、 .xlsx、 .xlsm、.rtf文件类型。就我的测试而言，没看出跟下面的download方法有多大区别。
这里尝试了一下path的其他使用方式。除了最常规的文件路径外，path参数还可以使用id或rev。
方法download测试 方法search测试 方法upload测试 方法delete测试 方法delete用于删除一个元素（文件/目录）。
方法permanently_delete测试 方法permanently_delete是支持Dropbox商业应用，而我的是开发应用，因而测试失败。
共享类API测试 方法share_folder测试 方法list_folders测试 方法unshare_folder测试 方法unshare_folder使用的是异步任务的方式，需要通过下列的方法check_job_status查询任务进度及结果。
方法check_job_status测试 方法create_share_link测试 share_folder可以通过邮件或Dropbox账户的方式分享给其他Dropbox用户，而share_link甚至可以共享给没有安装Dropbox的使用者。
方法get_share_links测试 方法get_shared_link_file测试 方法revoke_shared_link测试 revoke_shared_link竟然不返回结果，查证文档后确实如此。
学习总结 Dropbox关于文件共享方面的API占比相对OneDrive、Google Drive要多一些。 Dropbox API相对OneDrive、GoogleDrive而言，成熟度更低。按照REST的Richardson成熟度模型来说仅在2-级别，它的REST资源还是动词，例如get_metadata、check_job_status。
参考 Dropbox
Dropbox API v2 for HTTP Developers
Dropbox API Explorer
</content>
    </entry>
    
     <entry>
        <title>Dropbox认证和登录</title>
        <url>https://mryqu.github.io/post/dropbox%E8%AE%A4%E8%AF%81%E5%92%8C%E7%99%BB%E5%BD%95/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>dropbox</tag><tag>authentication</tag><tag>oauth2</tag><tag>application</tag><tag>sign-in</tag>
        </tags>
        <content type="html">  为Dropbox申请自己的应用 Dropbox OAuth Guide提到：对于命令行或桌面应用，没办法让浏览器重定向回你的应用。这种情况下，你的应用无需包含redirect_uri参数。Dropbox将向用户显示认证码，以用于复制到你的应用来获得可重用的访问令牌。基于此，对于桌面应用，redirect_uri不用设置；对于web应用，我选择了http://localhost以便测试。Dropbox网站没有提及urn:ietf:wg:oauth:2.0:oob。 Dropbox认证 Token Flow测试 HTTP GET请求如下：
https://www.dropbox.com/oauth2/authorize?client_id=**3t...hi**&amp;amp;redirect_uri=http://localhost&amp;amp;response_type=token&amp;amp;state=dsxoekdmpyt  成功跳转到如下URI：
http://localhost/#access_token=  连App secret都不用，仅凭App Key就可以获得访问令牌！看来还是认证码方式更安全一些。最后把应用的Allowimplicit grant选项改成Disallow以确保安全。
Code Flow测试 HTTP GET请求如下：
https://www.dropbox.com/oauth2/authorize?client_id=3t...hi&amp;amp;response_type=code&amp;amp;state=wecidskklsxpxl123  请求用户授权：显示认证码：获取访问令牌的HTTP POST请求包含上面获得的code参数：
POST https://api.dropboxapi.com/1/oauth2/token Content-Type: application/x-www-form-urlencoded Cache-Control: no-cache code=oV...9I&amp;amp;amp;client_id=3t...hi&amp;amp;amp;client_secret=j...7&amp;amp;amp;grant_type=authorization_code  参考 Dropbox API
Dropbox OAuth Guide
Dropbox authorize API
Dropbox token API
</content>
    </entry>
    
     <entry>
        <title>Tableau不支持导入OneDrive文件？</title>
        <url>https://mryqu.github.io/post/tableau%E4%B8%8D%E6%94%AF%E6%8C%81%E5%AF%BC%E5%85%A5onedrive%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>tableau</tag><tag>microsoft</tag><tag>onedrive</tag><tag>import</tag>
        </tags>
        <content type="html"> 没有发现Tableau的Microsoft OneDrive连接器，OData连接器和WebData连接器都不成。查了一下Tableau社区，就寥寥无几的几个帖子提到OneDrive。其中一个帖子提到了Using OneDrive as a Tableau Data Source。这个博文在2015年5月提到Tableau还不支持直连OneDrive上的文件，建议用OneDrive SyncApp同步到Tableau服务器上再导入。 貌似现在还是如此！！
</content>
    </entry>
    
     <entry>
        <title>使用Tableau导入Google Sheets</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8tableau%E5%AF%BC%E5%85%A5google_sheets/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>tableau</tag><tag>google</tag><tag>sheet</tag><tag>import</tag>
        </tags>
        <content type="html">  尝试一下用Tableau导入Google Sheets，操作过程中没看到配置项，比较简洁。
用Google账户授权Tableau 显示所有电子表格 选择一个电子表格 导入一个电子表格 参考 Connect Directly to Google Sheets in Tableau 10
Tableau connector examples
</content>
    </entry>
    
     <entry>
        <title>在Google API中使用访问令牌的三种方式</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8google_api%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%AE%BF%E9%97%AE%E4%BB%A4%E7%89%8C%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>api</tag><tag>access_token</tag><tag>oauth</tag><tag>bearer</tag>
        </tags>
        <content type="html">  在Google Developers OAuth 2.0 playground 中设置OAuth2.0配置时，可以发现有一个访问令牌位置的选择框，其值为： - Authorization header w/ OAuth prefix - Authorization header w/ Bearer prefix - Access_token URL parameter
按照前面博文《Google Sheets API认证和鉴权 》中的方法生成一个访问令牌。下面我就用这个访问令牌对这三种使用方式进行一下尝试。
认证头使用OAuth前缀 认证头使用Bearer前缀 使用access_token URL参数 结论：这三种访问令牌位置的使用都工作正常，API结果相同！
</content>
    </entry>
    
     <entry>
        <title>Hello Google Sheets API</title>
        <url>https://mryqu.github.io/post/hello_google_sheets_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>sheets</tag><tag>api</tag><tag>crawler</tag><tag>storage</tag>
        </tags>
        <content type="html">  准备环境 首先在Google Sheets创建了SpreadSheetTest1和To-do list两个电子表格，以备使用。 API测试 方法spreadsheets.get测试 方法spreadsheets.get可以获得一个电子表格中所有表单的内容和元数据。 下面是用Postman进行同样操作： 方法spreadsheets.values.get测试 方法spreadsheets.values.get可以获得一个电子表格中所有表单的内容。 方法spreadsheets.create测试 方法spreadsheets.create可以创建一个新的电子表格。 查看GoogleSheets，也可以看到新创建的电子表格SpreadSheetCreate1。由于我的请求里没有数据，因此下图中数据区也是空空。 方法spreadsheets.values.append测试 方法spreadsheets.values.append可以向电子表格中添加内容。 查看Google Sheets，也可以看到刚才创建的电子表格SpreadSheetCreate1有了九个单元格新数据。 学习结论 Google Sheets API可以创建、读取和修改电子表格，但是没有找到删除电子表格的方法。 Google SheetsAPI可以创建、读取、修改和删除一个电子表格内容，例如方法spreadsheets.batchUpdate中deleteSheet就可以删除一个表单，而deleteDimension就可以删除一个表单中的行/列。
参考 Google Sheets
Google Sheets API
Google API Explorer: Sheets
</content>
    </entry>
    
     <entry>
        <title>Google Sheets API认证和鉴权</title>
        <url>https://mryqu.github.io/post/google_sheets_api%E8%AE%A4%E8%AF%81%E5%92%8C%E9%89%B4%E6%9D%83/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>sheets</tag><tag>api</tag><tag>oauth2</tag><tag>authorization</tag>
        </tags>
        <content type="html">  玩一把用于Google Sheets API的OAuth2认证，以获得用于Sheets API的访问令牌。
注册Google Sheets应用 首先在Google API Console注册一个应用： Google Sheets API鉴权  用于用户登录的HTTPGET请求如下（scope选择了profile、对文件元数据和内容只读访问、对表单和属性只读访问）：  GET https://accounts.google.com/o/oauth2/v2/auth? scope=https://www.googleapis.com/auth/spreadsheets.readonly https://www.googleapis.com/auth/drive.readonly profile&amp;amp; redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp; response_type=code&amp;amp; client_id=826380598768-5935tlo90sccvr691ofmp4nrvpthrnn6.apps.googleusercontent.com  首先要求用户登录：要求登录后用户的授权：返回页面包含授权码： 获取访问令牌的HTTPPOST请求包含上面获得的授权码（在创建Google应用时获得的client_id和client_secret）： ``` POST https://www.googleapis.com/oauth2/v4/token Content-Type: application/x-www-form-urlencoded  code=4/-qpp&amp;hellip;qA&amp;amp; client_id=826380598768-5935tlo90sccvr691ofmp4nrvpthrnn6.apps.googleusercontent.com&amp;amp; client_secret=5&amp;hellip;r&amp;amp; redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp; grant_type=authorization_code ``` 参考 Google Sheets
Google Sheets API
Authorize Google Sheets API Requests
Using OAuth 2.0 for Mobile and Desktop Applications
Using OAuth 2.0 for Web Server Applications
</content>
    </entry>
    
     <entry>
        <title>[Maven] 构建多模块项目</title>
        <url>https://mryqu.github.io/post/maven_%E6%9E%84%E5%BB%BA%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>maven</tag><tag>module</tag><tag>parent</tag><tag>multiple</tag>
        </tags>
        <content type="html">  在前一篇博文[Gradle] 将多项目转换成Maven项目中利用Gradle转换成Maven构建脚本，将朋友糊弄过去了。后来想想，还是给他做一个重头搭建多模块Maven项目的演示吧。
创建根（父）项目 下列脚本可以创建一个包含pom.xml的yqu-ts-parent目录：
mvn archetype:generate -DgroupId=com.yqu.ts -DartifactId=yqu-ts-parent -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false  测试结果： 进入yqu-ts-parent目录，删除src子目录，然后将pom.xml文件中packaging节点内容由jar改为pom。pom表示它是一个被继承的模块
创建子项目 在yqu-ts-parent目录中运行下列脚本可以创建两个包含pom.xml文件的子目录yqu-ts-service和yqu-ts-webapp：
mvn archetype:generate -DgroupId=com.yqu.ts -DartifactId=yqu-ts-service -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false mvn archetype:generate -DgroupId=com.yqu.ts -DartifactId=yqu-ts-webapp -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false  测试结果： 这两个命令会修改yqu-ts-parent项目的pom.xml，增加了两个子模块yqu-ts-service和yqu-ts-webapp。对于两个字模块的pom.xml，增加packaging节点，由于这两个子模块将用SpringBoot实现因而内容都为jar。
确认项目/模块的pom.xml yqu-ts-parent项目的pom.xml yqu-ts-service模块的pom.xml yqu-ts-webapp模块的pom.xml </content>
    </entry>
    
     <entry>
        <title>[Gradle] 将多项目转换成Maven项目</title>
        <url>https://mryqu.github.io/post/gradle_%E5%B0%86%E5%A4%9A%E9%A1%B9%E7%9B%AE%E8%BD%AC%E6%8D%A2%E6%88%90maven%E9%A1%B9%E7%9B%AE/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>maven</tag><tag>multiple-project</tag><tag>convert</tag>
        </tags>
        <content type="html">  手头有一个构建多项目的Gradle构建脚本，但是一个哥们问我能不能换成Maven的，搜到一篇gradle项目与maven项目相互转化，照着实践，证明多项目构建也是可行的。
文件结构介绍  ts-demo目录  setting.gradle build.gradle ts-service目录 build.gradle src目录 ts-webapp目录 build.gradle src目录   ts-demo/setting.gradle rootProject.name = &#39;ts-demo&#39; include &amp;quot;ts-service&amp;quot; include &amp;quot;ts-webapp&amp;quot; project(&amp;quot;:ts-service&amp;quot;).name = &amp;quot;ts-service&amp;quot; project(&amp;quot;:ts-webapp&amp;quot;).name = &amp;quot;ts-webapp&amp;quot;  ts-demo/build.gradle buildscript { repositories { mavenCentral() } } allprojects { apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;maven&#39; repositories { mavenCentral() } group = &#39;com.yqu&#39; version = &#39;0.1.0&#39; task writeNewPom { pom { project { inceptionYear &#39;2016&#39; licenses { license { name &#39;The Apache Software License, Version 2.0&#39; url &#39;http://www.apache.org/licenses/LICENSE-2.0.txt&#39; distribution &#39;repo&#39; } } } }.writeTo(&amp;quot;$buildDir/pom.xml&amp;quot;) } sourceCompatibility = 1.8 targetCompatibility = 1.8 }  ts-demo/ts-service/build.gradle jar { baseName = &#39;ts-service&#39; version = &#39;0.1.0&#39; } dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) compile(&amp;quot;com.fasterxml.jackson.core:jackson-databind:2.6.0&amp;quot;) compile(&amp;quot;org.springframework.hateoas:spring-hateoas&amp;quot;) compile(&amp;quot;org.springframework.plugin:spring-plugin-core:1.1.0.RELEASE&amp;quot;) compile(&amp;quot;com.jayway.jsonpath:json-path:0.9.1&amp;quot;) }  ts-demo/ts-webapp/build.gradle ......  转换 在ts-demo目录下执行gradlewriteNewPom，即可在父目录和两个子项目目录下的build目录找到生成的pom.xml文件了。
</content>
    </entry>
    
     <entry>
        <title>安装Tableau Public</title>
        <url>https://mryqu.github.io/post/%E5%AE%89%E8%A3%85tableau_public/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>tableau</tag><tag>public</tag><tag>安装</tag>
        </tags>
        <content type="html"> Tableau Public的安装文件可在http://public.tableau.com/s/下载。 安装后创建Tableau Public账户，并进入注册所用信箱激活账户即可。
Tableau Public可导入的服务器很有限，Tableau Desktop就丰富多了。</content>
    </entry>
    
     <entry>
        <title>在Coursera上选了几门Tableau课程</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8coursera%E4%B8%8A%E9%80%89%E4%BA%86%E5%87%A0%E9%97%A8tableau%E8%AF%BE%E7%A8%8B/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>coursera</tag><tag>tableau</tag><tag>course</tag>
        </tags>
        <content type="html"> 今天在赤兔数据挖掘群里看到有人说Coursera上有Tableau课程，有机会学习一下我司竞争对手的课程，也是不错的。
有五门课程属于加州大学的使用Tableau可视化商业数据专业课系列： - Fundamentals of Visualization with Tableau - Essential Design Principles for Tableau - Explaining Your Data Using Tableau - Creating Dashboards and Storytelling with Tableau - Data Visualization with Tableau Project
有一门课程属于杜克大学的Excel到MySQL: 用于商业的分析技术专业课系列： - Data Visualization and Communication with Tableau
当然现在这些课只能听一听，不掏钱是没有分数的
</content>
    </entry>
    
     <entry>
        <title>[Hue] 清空MySQL数据库的hue.django_content_type表时遇到由于外键约束无法删除错误!</title>
        <url>https://mryqu.github.io/post/hue_%E6%B8%85%E7%A9%BAmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84hue.django_content_type%E8%A1%A8%E6%97%B6%E9%81%87%E5%88%B0%E7%94%B1%E4%BA%8E%E5%A4%96%E9%94%AE%E7%BA%A6%E6%9D%9F%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E9%94%99%E8%AF%AF/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hue</category>
        </categories>
        <tags>
          <tag>hue</tag><tag>django_content_type</tag><tag>delete</tag><tag>foreign_key_constrai</tag><tag>mysql</tag>
        </tags>
        <content type="html">  Hue （Hadoop User Experience）是一个使用ApacheHadoop分析数据的Web界面。它可以： - 将数据加载到Hadoop - 查看数据、处理数据或准备数据 - 分析数据、搜索数据、对数据进行可视化分析
默认Hue服务器使用嵌入式数据库SQLite存储元数据和查询信息。当我将其迁移到MySQL时，按照Cloudera - Hue Installation Guide的步骤同步数据库时后清空MySQL中的django_content_type表时遭遇下列问题：
hadoop@node50064:~$ $HUE_HOME/build/env/bin/hue syncdb --noinput Syncing... Creating tables ... Creating table auth_permission Creating table auth_group_permissions Creating table auth_group Creating table auth_user_groups Creating table auth_user_user_permissions Creating table auth_user Creating table django_openid_auth_nonce Creating table django_openid_auth_association Creating table django_openid_auth_useropenid Creating table django_content_type Creating table django_session Creating table django_site Creating table django_admin_log Creating table south_migrationhistory Creating table axes_accessattempt Creating table axes_accesslog Installing custom SQL ... Installing indexes ... Installed 0 object(s) from 0 fixture(s) Synced: &amp;gt; django.contrib.auth &amp;gt; django_openid_auth &amp;gt; django.contrib.contenttypes &amp;gt; django.contrib.sessions &amp;gt; django.contrib.sites &amp;gt; django.contrib.staticfiles &amp;gt; django.contrib.admin &amp;gt; south &amp;gt; axes &amp;gt; about &amp;gt; filebrowser &amp;gt; help &amp;gt; impala &amp;gt; jobbrowser &amp;gt; metastore &amp;gt; proxy &amp;gt; rdbms &amp;gt; zookeeper &amp;gt; indexer Not synced (use migrations): - django_extensions - desktop - beeswax - hbase - jobsub - oozie - pig - search - security - spark - sqoop - useradmin - notebook (use ./manage.py migrate to migrate these) hadoop@node50064:~$ mysql -uhue -psecretpassword -e &amp;quot;DELETE FROM hue.django_content_type;&amp;quot; ERROR 1451 (23000) at line 1: Cannot delete or update a parent row: a foreign key constraint fails (`hue`.`auth_permission`, CONSTRAINT `content_type_id_refs_id_d043b34a` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`))  在网上搜了搜，最后找到Cloudera更新一点的文档Using an External Database for Hue Using the Command Line，根据其建议的下列方式解决了问题：
hadoop@node50064:~$ mysql -uhue -psecretpassword -e &amp;quot;show create table hue.auth_permission \G;&amp;quot; *************************** 1. row *************************** Table: auth_permission Create Table: CREATE TABLE `auth_permission` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) NOT NULL, `content_type_id` int(11) NOT NULL, `codename` varchar(100) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `content_type_id` (`content_type_id`,`codename`), KEY `auth_permission_37ef4eb4` (`content_type_id`), **CONSTRAINT `content_type_id_refs_id_d043b34a` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`)** ) **ENGINE=InnoDB** AUTO_INCREMENT=40 DEFAULT CHARSET=utf8 hadoop@node50064:~$ mysql -uhue -psecretpassword -e &amp;quot;alter table hue.auth_permission drop foreign key content_type_id_refs_id_d043b34a;&amp;quot; hadoop@node50064:~$ mysql -uhue -psecretpassword -e &amp;quot;DELETE FROM hue.django_content_type;&amp;quot;  参考 http://grokbase.com/t/cloudera/scm-users/12achamnx1/hue-instructions
https://groups.google.com/a/cloudera.org/forum/#!topic/hue-user/49o80Q-49CU
</content>
    </entry>
    
     <entry>
        <title>[Hue] 解决Filesystem root &#39;/&#39; should be owned by &#39;hdfs&#39;</title>
        <url>https://mryqu.github.io/post/hue_%E8%A7%A3%E5%86%B3filesystem_root_should_be_owned_by_hdfs/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hue</category>
        </categories>
        <tags>
          <tag>hue</tag><tag>hdfs</tag><tag>filesystem</tag><tag>warning</tag><tag>superuser</tag>
        </tags>
        <content type="html"> 安装Hue后，通过about页面检查配置，有一个提示：Filesystemroot &amp;lsquo;/&amp;rsquo; should be owned by &amp;lsquo;hdfs&amp;rsquo; 我在hadoop集群都使用用户hadoop，并没有创建用户hdfs。解决方案是将hue.ini中的default_hdfs_superuser设置：
# This should be the hadoop cluster admin default_hdfs_superuser=hadoop  重启Hue后警告解除，呵呵。
</content>
    </entry>
    
     <entry>
        <title>了解GSA(Google Search Appliance)</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3gsagoogle_search_appliance/</url>
        <categories>
          
        </categories>
        <tags>
          
        </tags>
        <content type="html">  今天的公司邮件有提及GSA，才了解了一下这个被谷歌停止研发的产品。 GSA(Google Search Appliance)是由Google公司出品的提供文件索引功能的一种机架设备，用于企业内部局域网、web服务、门户、文件共享服务、Sharepoint、数据库、内容管理系统、企业应用的实时数据（例如ERP）、基于云的系统（例如谷歌文档）的搜索。操作系统能够为CentOS，软件由谷歌出品，硬件由戴尔制造。当前GSA版本7.6，分两种型号：G100支持2千万文档，G500支持一亿文档。 2016年初谷歌向商业伙伴和客户发了一封密信：GSA在2018年后将被中止。自2016年不再进行三年牌照续期，2017年仅对已有硬件客户进行一年牌照续期。在此期间，谷歌将继续修复问题、提供安全更新和技术支持。 一点感言：以前是一直很酸爽，用不到后是心里狂奔“呵呵呵”，这真是一个好产品。
参考 Google Search Appliance
WIKI: Google Search Appliance&amp;rdquo;
Forbes: So Long Google Search Appliance
</content>
    </entry>
    
     <entry>
        <title>搞清楚CONSUL_PORT_8500_TCP_ADDR</title>
        <url>https://mryqu.github.io/post/%E6%90%9E%E6%B8%85%E6%A5%9Aconsul_port_8500_tcp_addr/</url>
        <categories>
          <category>Tool</category><category>Consul</category>
        </categories>
        <tags>
          <tag>consul</tag><tag>docker</tag><tag>links</tag><tag>consul_port_8500_tcp</tag><tag>join</tag>
        </tags>
        <content type="html">  Consul集群里既有Server也有Client。那除了Consul serverleader，其他节点怎么加入这个Consul集群里呢？我目前看到的Docker方案是其他节点使用consul agent -join$CONSUL_PORT_8500_TCP_ADDR加入Consul集群的。那么CONSUL_PORT_8500_TCP_ADDR是怎么设置到其他容器节点的？
root@httpd:/# env CONSUL_PORT_8300_TCP_PORT=8300 CONSUL_PORT_53_TCP_PORT=53 VAGRANT_CONSUL_1_PORT_8301_UDP_ADDR=172.17.0.2 HOSTNAME=httpd CONSUL_PORT_8301_TCP_PROTO=tcp VAGRANT_CONSUL_1_PORT_8302_TCP_PORT=8302 VAGRANT_CONSUL_1_PORT_53_UDP_PROTO=udp CONSUL_1_PORT_8300_TCP_ADDR=172.17.0.2 VAGRANT_CONSUL_1_PORT_8301_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_8400_TCP=tcp://172.17.0.2:8400 CONSUL_1_PORT_53_UDP=udp://172.17.0.2:53 CONSUL_1_PORT_8300_TCP_PROTO=tcp APACHE_RUN_USER=www-data VAGRANT_CONSUL_1_PORT_8301_TCP_PROTO=tcp CONSUL_PORT_8301_UDP=udp://172.17.0.2:8301 VAGRANT_CONSUL_1_PORT_8400_TCP_ADDR=172.17.0.2 CONSUL_PORT_53_TCP_ADDR=172.17.0.2 VAGRANT_CONSUL_1_PORT_8301_UDP_PORT=8301 CONSUL_1_PORT_53_TCP=tcp://172.17.0.2:53 CONSUL_1_PORT_8300_TCP_PORT_START=8300 CONSUL_PORT_53_TCP_PROTO=tcp VAGRANT_CONSUL_1_PORT_8302_TCP=tcp://172.17.0.2:8302 VAGRANT_CONSUL_1_PORT_8300_UDP_END=udp://172.17.0.2:8302 CONSUL_1_NAME=/vagrant_httpd_1/consul_1 CONSUL_1_PORT_8300_UDP_PORT_END=8302 VAGRANT_CONSUL_1_PORT_53_TCP_PROTO=tcp CONSUL_1_PORT_8500_TCP=tcp://172.17.0.2:8500 CONSUL_PORT_53_UDP_ADDR=172.17.0.2 CONSUL_PORT_8300_TCP_PORT_START=8300 CONSUL_1_PORT_53_TCP_PORT=53 CONSUL_1_PORT_8302_UDP_PORT=8302 VAGRANT_CONSUL_1_PORT_8400_TCP=tcp://172.17.0.2:8400 CONSUL_1_PORT_8300_UDP_END=udp://172.17.0.2:8302 CONSUL_PORT_8302_UDP_PORT=8302 CONSUL_PORT_8302_TCP_ADDR=172.17.0.2 VAGRANT_CONSUL_1_PORT_8301_UDP_PROTO=udp CONSUL_1_PORT_8302_TCP_PORT=8302 CONSUL_1_PORT_8301_TCP=tcp://172.17.0.2:8301 CONSUL_ENV_CONSUL_OPTIONS=-bootstrap -client 0.0.0.0 VAGRANT_CONSUL_1_PORT_8302_UDP_PORT=8302 CONSUL_1_PORT_8302_UDP=udp://172.17.0.2:8302 CONSUL_PORT_53_UDP_PORT=53 VAGRANT_CONSUL_1_PORT_8302_UDP_PROTO=udp VAGRANT_CONSUL_1_PORT_8300_TCP_START=tcp://172.17.0.2:8300 CONSUL_PORT_8302_UDP_ADDR=172.17.0.2 CONSUL_PORT_8300_TCP=tcp://172.17.0.2:8300 CONSUL_1_PORT_8302_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_53_TCP_PROTO=tcp VAGRANT_CONSUL_1_PORT_53_TCP=tcp://172.17.0.2:53 CONSUL_1_PORT_8302_UDP_PROTO=udp VAGRANT_CONSUL_1_PORT_8300_TCP=tcp://172.17.0.2:8300 CONSUL_1_PORT_8301_TCP_PROTO=tcp CONSUL_PORT_8400_TCP=tcp://172.17.0.2:8400 VAGRANT_CONSUL_1_PORT_8500_TCP_PROTO=tcp CONSUL_PORT_8301_TCP_PORT=8301 VAGRANT_CONSUL_1_PORT_53_UDP_PORT=53 VAGRANT_CONSUL_1_PORT_8300_TCP_PROTO=tcp CONSUL_1_PORT_8301_UDP_ADDR=172.17.0.2 CONSUL_1_PORT_8300_TCP_START=tcp://172.17.0.2:8300 APACHE_LOG_DIR=/var/log/apache2 CONSUL_PORT_53_TCP=tcp://172.17.0.2:53 VAGRANT_CONSUL_1_PORT_8301_TCP_PORT=8301 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin CONSUL_PORT_8500_TCP_ADDR=172.17.0.2 CONSUL_PORT_8301_TCP=tcp://172.17.0.2:8301 CONSUL_PORT_8300_TCP_START=tcp://172.17.0.2:8300 VAGRANT_CONSUL_1_PORT_53_UDP=udp://172.17.0.2:53 CONSUL_PORT_8500_TCP=tcp://172.17.0.2:8500 CONSUL_PORT_8300_TCP_ADDR=172.17.0.2 PWD=/ CONSUL_PORT_53_UDP=udp://172.17.0.2:53 CONSUL_1_PORT_8400_TCP_PROTO=tcp CONSUL_1_PORT_53_UDP_PROTO=udp VAGRANT_CONSUL_1_PORT_53_TCP_PORT=53 CONSUL_1_PORT_8301_UDP=udp://172.17.0.2:8301 APACHE_RUN_GROUP=www-data VAGRANT_CONSUL_1_PORT_8302_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_8300_TCP_PORT=8300 CONSUL_PORT_8400_TCP_ADDR=172.17.0.2 VAGRANT_CONSUL_1_PORT_8302_TCP_PROTO=tcp CONSUL_PORT_8400_TCP_PROTO=tcp VAGRANT_CONSUL_1_PORT_8500_TCP_PORT=8500 CONSUL_1_PORT_8301_UDP_PROTO=udp CONSUL_PORT_8500_TCP_PORT=8500 VAGRANT_CONSUL_1_ENV_CONSUL_OPTIONS=-bootstrap -client 0.0.0.0 CONSUL_1_PORT_8500_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_8300_TCP=tcp://172.17.0.2:8300 VAGRANT_CONSUL_1_PORT_8400_TCP_PORT=8400 CONSUL_1_PORT_8301_TCP_ADDR=172.17.0.2 SHLVL=1 HOME=/root CONSUL_PORT_8302_TCP_PORT=8302 CONSUL_PORT_8300_UDP_PORT_END=8302 VAGRANT_CONSUL_1_PORT_8500_TCP=tcp://172.17.0.2:8500 VAGRANT_CONSUL_1_PORT_53_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_8301_UDP_PORT=8301 CONSUL_1_ENV_CONSUL_OPTIONS=-bootstrap -client 0.0.0.0 CONSUL_PORT_8300_TCP_PROTO=tcp CONSUL_1_PORT_8400_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_53_UDP_ADDR=172.17.0.2 VAGRANT_CONSUL_1_PORT_8300_TCP_PORT=8300 CONSUL_1_PORT_8302_UDP_ADDR=172.17.0.2 CONSUL_PORT_8500_TCP_PROTO=tcp CONSUL_PORT_8301_UDP_PORT=8301 VAGRANT_CONSUL_1_PORT_8300_TCP_PORT_START=8300 CONSUL_PORT_8302_TCP=tcp://172.17.0.2:8302 CONSUL_PORT_8300_UDP_END=udp://172.17.0.2:8302 VAGRANT_CONSUL_1_PORT_8400_TCP_PROTO=tcp VAGRANT_CONSUL_1_PORT_8302_UDP_ADDR=172.17.0.2 VAGRANT_CONSUL_1_PORT_8300_UDP_PORT_END=8302 VAGRANT_CONSUL_1_PORT=tcp://172.17.0.2:53 VAGRANT_CONSUL_1_PORT_8301_UDP=udp://172.17.0.2:8301 CONSUL_1_PORT_53_TCP_ADDR=172.17.0.2 DEBIAN_FRONTEND=noninteractive CONSUL_PORT=tcp://172.17.0.2:53 VAGRANT_CONSUL_1_PORT_8300_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_8500_TCP_PORT=8500 CONSUL_PORT_8302_TCP_PROTO=tcp CONSUL_PORT_8301_TCP_ADDR=172.17.0.2 VAGRANT_CONSUL_1_PORT_53_UDP_ADDR=172.17.0.2 CONSUL_PORT_8302_UDP=udp://172.17.0.2:8302 VAGRANT_CONSUL_1_NAME=/vagrant_httpd_1/vagrant_consul_1 VAGRANT_CONSUL_1_PORT_8500_TCP_ADDR=172.17.0.2 CONSUL_1_PORT_8302_TCP=tcp://172.17.0.2:8302 VAGRANT_CONSUL_1_PORT_8301_TCP=tcp://172.17.0.2:8301 CONSUL_1_PORT_8400_TCP_PORT=8400 CONSUL_1_PORT_53_UDP_PORT=53 VAGRANT_CONSUL_1_PORT_8302_UDP=udp://172.17.0.2:8302 CONSUL_1_PORT=tcp://172.17.0.2:53 CONSUL_PORT_8302_UDP_PROTO=udp CONSUL_1_PORT_8302_TCP_PROTO=tcp CONSUL_PORT_8301_UDP_PROTO=udp CONSUL_PORT_8400_TCP_PORT=8400 CONSUL_NAME=/vagrant_httpd_1/consul CONSUL_PORT_53_UDP_PROTO=udp CONSUL_PORT_8301_UDP_ADDR=172.17.0.2 CONSUL_1_PORT_8500_TCP_PROTO=tcp CONSUL_1_PORT_8301_TCP_PORT=8301 _=/usr/bin/env``` 搜了一下[Consul代码](https://github.com/hashicorp/consul)，毛都没发现。看到了[Using the host ip in docker compose](/questions/29061026/using-the-host-ip-in-docker-compose)，才知道是Docker搞的。 在Consul镜像的Dockerfile中，通过EXPOSE命令向其他容器暴露了如下端口：  EXPOSE 53/udp 53 8300 8302 8302/udp 8400 8500
 而在Docker-Compose文件中，其他容器使用links连接第一个启动的Consul服务器容器：  httpd: image: &amp;hellip;&amp;hellip;/httpd:dev hostname: httpd dns: 127.0.0.1 restart: always &amp;hellip;&amp;hellip; links: - consul:consul
 根据Docker文档，Docker会为所连接的容器暴露出来的端口向其他容器创建上面的环境变量。此外，它还会更新其他容器的/etc/hosts中关于所连接容器的主机条目：  root@httpd:/# cat /etc/hosts 172.17.0.4 httpd 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.17.0.2 consul_1 consul vagrant_consul_1 172.17.0.2 vagrant_consul_1 consul 172.17.0.2 consul consul vagrant_consul_1 ``` 当所连接的容器重启，环境变量中的主机地址不会像/etc/hosts那样自动更新，所以Docker推荐使用/etc/hosts中的连接容器主机条目而不是环境变量。
参考 Using the host ip in docker compose
https://docs.docker.com/userguide/dockerlinks/
</content>
    </entry>
    
     <entry>
        <title>Hello UnRAVL</title>
        <url>https://mryqu.github.io/post/hello_unravl/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>unravl</tag><tag>rest</tag><tag>validation</tag><tag>sas</tag><tag>opensource</tag>
        </tags>
        <content type="html">  UnRAVL介绍 UnRAVL（Uniform REST API ValidationLanguage）是用于验证REST应用编程接口的JSON领域特定语言。UnRAVL由SAS架构师DavidBiesack实现，并作为SAS开源软件在GitHub上发布。UnRAVL脚本包含一个REST API调用的JSON描述: - HTTP方法（GET、POST、PUT、DELETE、HEAD、PATCH） - URI - (可选的)HTTP头 - (可选的)请求消息体 - (可选的)认证
对于每个API调用，一个UnRAVL脚本可以包含用于验证结果的一或多个断言。某些断言可以表达为前置条件，即在进行API调用之前必须为真。下列内容可以断言： - 结果消息体匹配期望的JSON、文本或其他数据 - 存在带有特定值的特定头 - HTTP状态码为特定值或在特定集合内 - 响应消息体匹配基准（benchmark） - 通过Groovy表达式测试响应或环境变量中的元素为真 - 一个环境变量已赋值
UnRAVL也支持从RESTAPI调用结果中抽取数据、与环境变量进行数据绑定并用于之后的API调用验证。例如，将一个创建资源的POST 调用响应的Location 头进行保存，并将此URL用于后继GET 、PUT 、DELETE 调用。 模板功能提供了可重用的API验证构建能力。
UnRAVL示例 build.gradle buildscript { repositories { mavenCentral() } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; jar { baseName = &#39;HelloUnRAVL&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.7 targetCompatibility = 1.7 ext { jacksonVersion = &amp;quot;2.5.&#43;&amp;quot; groovyVersion = &amp;quot;2.4.3&amp;quot; springVersion = &amp;quot;4.2.2.RELEASE&amp;quot; httpClientVersion = &amp;quot;4.5&amp;quot; httpCoreVersion = &amp;quot;4.4.1&amp;quot; commonsCodecVersion = &amp;quot;1.10&amp;quot; log4jVersion = &amp;quot;1.2.17&amp;quot; guavaVersion = &amp;quot;18.0&amp;quot; jsonPathVersion = &#39;2.&#43;&#39; jsonSchemaValidatorVersion = &amp;quot;2.2.6&amp;quot; jsonSchemaCoreVersion = &amp;quot;1.2.5&amp;quot; jacksonCoreUtilsVersion = &amp;quot;1.8&amp;quot; slf4jVersion = &amp;quot;1.7.16&amp;quot; } dependencies { compile files(&#39;libs/sas.unravl-1.2.2-SNAPSHOT.jar&#39;) compile group: &#39;com.fasterxml.jackson.core&#39;, name: &#39;jackson-core&#39;, version: jacksonVersion compile group: &#39;com.fasterxml.jackson.core&#39;, name: &#39;jackson-annotations&#39;, version: jacksonVersion compile group: &#39;com.fasterxml.jackson.core&#39;, name: &#39;jackson-databind&#39;, version: jacksonVersion compile group: &#39;org.codehaus.groovy&#39;, name: &#39;groovy-all&#39;, version: groovyVersion compile group: &#39;org.springframework&#39;, name: &#39;spring-core&#39;, version: springVersion compile group: &#39;org.springframework&#39;, name: &#39;spring-web&#39;, version: springVersion compile group: &#39;org.apache.httpcomponents&#39;, name: &#39;httpclient&#39;, version: httpClientVersion compile group: &#39;org.apache.httpcomponents&#39;, name: &#39;httpcore&#39;, version: httpCoreVersion compile group: &#39;commons-codec&#39;, name: &#39;commons-codec&#39;, version: commonsCodecVersion compile group: &#39;log4j&#39;, name: &#39;log4j&#39;, version: log4jVersion compile group: &#39;com.google.guava&#39;, name: &#39;guava&#39;, version: guavaVersion compile group: &#39;com.github.fge&#39;, name: &#39;json-schema-validator&#39;, version: jsonSchemaValidatorVersion compile group: &#39;com.github.fge&#39;, name: &#39;json-schema-core&#39;, version: jsonSchemaCoreVersion compile group: &#39;com.github.fge&#39;, name: &#39;jackson-coreutils&#39;, version: jacksonCoreUtilsVersion compile group: &#39;com.jayway.jsonpath&#39;, name: &#39;json-path&#39;, version: jsonPathVersion compile group: &#39;org.slf4j&#39;, name: &#39;slf4j-log4j12&#39;, version: slf4jVersion }  com/yqu/unravl/HelloUnRAVL.java package com.yqu.unravl; import java.io.IOException; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import com.fasterxml.jackson.databind.JsonNode; import com.fasterxml.jackson.databind.ObjectMapper; import com.sas.unravl.UnRAVLException; import com.sas.unravl.UnRAVLRuntime; import com.sas.unravl.util.Json; public class HelloUnRAVL { public static List&amp;lt;JsonNode&amp;gt; parseUravlScript(String content) throws IOException, UnRAVLException { JsonNode root; List&amp;lt;JsonNode&amp;gt; roots = new ArrayList&amp;lt;JsonNode&amp;gt;(); ObjectMapper mapper = new ObjectMapper(); root = mapper.readTree(content); if (root.isArray()) { for (JsonNode next : Json.array(root)) { roots.add(next); } } else { roots.add(root); } return roots; } public static void testSinaShortURL(Map&amp;lt;String, Object&amp;gt; env) { UnRAVLRuntime runtime = new UnRAVLRuntime(env); String tcnTemplate = &amp;quot;{&amp;quot; &#43; &amp;quot; \&amp;quot;name&amp;quot; : \&amp;quot;SinaShortURL\&amp;quot;,&amp;quot; &#43; &amp;quot; \&amp;quot;GET\&amp;quot; : \&amp;quot;http://t.cn/{sURL}\&amp;quot;,&amp;quot; &#43; &amp;quot; \&amp;quot;assert\&amp;quot;: [ { \&amp;quot;status\&amp;quot; : 200 } ]&amp;quot; &#43; &amp;quot;}&amp;quot;; try { List&amp;lt;JsonNode&amp;gt; unravlTemplate = parseUravlScript(tcnTemplate); runtime.execute(unravlTemplate); Map&amp;lt;String, Object&amp;gt; bindings = runtime.getBindings(); int failedAssertionCount = (int) bindings.get(&amp;quot;failedAssertionCount&amp;quot;); System.out.println(&amp;quot;failedAssertionCount:&amp;quot;&#43;failedAssertionCount); } catch (IOException e) { e.printStackTrace(); } catch (UnRAVLException e) { e.printStackTrace(); } } public static void main(String[] args) { Map&amp;lt;String, Object&amp;gt; env = new HashMap&amp;lt;String, Object&amp;gt;(); env.put(&amp;quot;sURL&amp;quot;, &amp;quot;RLoGZKa&amp;quot;); testSinaShortURL(env); } }  测试结果：参考 GitHub: sassoftware/unravl
UnRAVL脚本参考指南
UnRAVL issues
</content>
    </entry>
    
     <entry>
        <title>[RabbitMQ] AutorecoveringConnection在连接恢复后才调用ShutdownListener</title>
        <url>https://mryqu.github.io/post/rabbitmq_autorecoveringconnection%E5%9C%A8%E8%BF%9E%E6%8E%A5%E6%81%A2%E5%A4%8D%E5%90%8E%E6%89%8D%E8%B0%83%E7%94%A8shutdownlistener/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>rabbitmq</tag><tag>autorecoveringconnec</tag><tag>shutdownlistener</tag><tag>recoverylistener</tag><tag>hook</tag>
        </tags>
        <content type="html"> 想玩一玩RabbitMQ中的ShutdownListener和RecoveryListener，又不想写自己的重连接逻辑，所以使用了ConnectionFactory类的setAutomaticRecoveryEnabled方法让其自动恢复连接。代码如下：
package com.yqu.rabbitmq; import com.rabbitmq.client.*; import java.io.IOException; public class AutoRecoveryRecv { private final static String QUEUE_NAME = &amp;quot;hello&amp;quot;; public static void main(String[] argv) throws Exception { try { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(ConnectionFactoryConfiguration.HOST); factory.setUsername(ConnectionFactoryConfiguration.USERNAME); factory.setPassword(ConnectionFactoryConfiguration.PASSWORD); factory.setAutomaticRecoveryEnabled(true); factory.setNetworkRecoveryInterval(10000); Connection connection = factory.newConnection(); connection.addShutdownListener(new ShutdownListener() { @Override public void shutdownCompleted(ShutdownSignalException cause) { String hardError = &amp;quot;&amp;quot;; String applInit = &amp;quot;&amp;quot;; if (cause.isHardError()) { hardError = &amp;quot;connection&amp;quot;; } else { hardError = &amp;quot;channel&amp;quot;; } if (cause.isInitiatedByApplication()) { applInit = &amp;quot;application&amp;quot;; } else { applInit = &amp;quot;broker&amp;quot;; } System.out.println(&amp;quot;Connectivity to MQ has failed. It was caused by &amp;quot; &#43; applInit &#43; &amp;quot; at the &amp;quot; &#43; hardError &#43; &amp;quot; level. Reason received &amp;quot; &#43; cause.getReason()); } }); ((Recoverable)connection).addRecoveryListener(new RecoveryListener() { @Override public void handleRecovery(Recoverable recoverable) { if( recoverable instanceof Connection ) { System.out.println(&amp;quot;Connection was recovered.&amp;quot;); } else if ( recoverable instanceof Channel ) { int channelNumber = ((Channel) recoverable).getChannelNumber(); System.out.println( &amp;quot;Connection to channel #&amp;quot; &#43; channelNumber &#43; &amp;quot; was recovered.&amp;quot; ); } } }); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); System.out.println(&amp;quot; [*] Waiting for messages. To exit press CTRL&#43;C&amp;quot;); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &amp;quot;UTF-8&amp;quot;); System.out.println(&amp;quot; [x] Received &#39;&amp;quot; &#43; message &#43; &amp;quot;&#39;&amp;quot;); } }; channel.basicConsume(QUEUE_NAME, true, consumer); } catch(Throwable t) { t.printStackTrace(); } } }  重启了RabbitMQ所在的服务器，然后获得下列日志，显示连接已修复，然后才调用ShutdownListener显示连接关闭信息：为什么会这样？下面就开始我的解密之旅！ 首先看一下ShutdownNotifier接口的层次图，Connection下共有三个子孙类，尚不确定在我的测试里是哪一个？ 看一下com.rabbitmq.client.ConnectionFactory类newConnection方法实现，豁然开朗！对于ConnectionFactory类，如果automaticRecoveryEnabled被设置则创建AutorecoveringConnection，否则创建AMQConnection。而RecoveryAwareAMQConnectionFactory才会创建RecoveryAwareAMQConnection。 AutorecoveringConnection类init方法会调用addAutomaticRecoveryListener方法，addAutomaticRecoveryListener方法会添加一个ShutdownListener类实例automaticRecoveryListener到列表shutdownListeners，而我创建的ShutdownListener类实例是列表shutdownListeners中的第二个元素。 参看下面堆栈信息，ShutdownNotifierComponent类的notifyListeners会遍历所有shutdownListeners列表所有监听器，第一个关闭监听器实例automaticRecoveryListener会进行连接修复并通知RecoveryListener，然后才到我的关闭监听器打印关闭信息。
at AutoRecoveryRecv$2.handleRecovery(AutoRecoveryRecv. at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.notifyRecoveryListeners(AutorecoveringConnection. at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.beginAutomaticRecovery(AutorecoveringConnection. - locked &amp;lt;0x30d&amp;gt; (a com.rabbitmq.client.impl.recovery.AutorecoveringConnection) at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.access$000(AutorecoveringConnection. at com.rabbitmq.client.impl.recovery.AutorecoveringConnection$1.shutdownCompleted(AutorecoveringConnection. at com.rabbitmq.client.impl.ShutdownNotifierComponent.notifyListeners(ShutdownNotifierComponent. at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection. at  rabbitmq/rabbitmq-java-client项目在https://github.com/rabbitmq/rabbitmq-java-client/pull/136 修正了这个错误，先通知所有ShutdownListener再尝试自动修复连接。但是在我所用的amqp-client-3.6.5.jar还没有包含这一修改。
</content>
    </entry>
    
     <entry>
        <title>[RabbitMQ] 了解多个同名rabbitmq-server文件</title>
        <url>https://mryqu.github.io/post/rabbitmq_%E4%BA%86%E8%A7%A3%E5%A4%9A%E4%B8%AA%E5%90%8C%E5%90%8Drabbitmq-server%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>rabbitmq-server</tag><tag>同名</tag><tag>文件</tag><tag>介绍</tag>
        </tags>
        <content type="html"> 安装完RabbitMQ后，查了查机器中多了六个rabbitmq-server文件，除了两个位于/usr/lib/rabbitmq目录下的可以不理，其他都有什么区别呢？ 下面针对这四个文件进行一下介绍： - /etc/init.d/rabbitmq-server： RabbitMQ服务器的开机自启动脚本 - /usr/sbin/rabbitmq-server： init脚本所启动的主服务器程序脚本 - /etc/logrotate.d/rabbitmq-server：logrotate是个十分有用的工具，它可以自动对日志进行截断（或轮循）、压缩以及删除旧的日志文件。该文件是针对rabbitmq-server的logrotate配置，默认情况下logrotate每周对/var/log/rabbitmq/下的log文件进行处理。 - /usr/lib/ocf/resource.d/rabbitmq/rabbitmq-server：OCF指开放集群框架（Open Clustering Framework）。当使用pacemaker配置RabbitMQHA时，作为OCF 资源代理脚本，用于操作和监控RabbitMQ节点。OCF 规范（尤其是与资源代理相关的部分）详见在Open Clustering Framework Resource Agent API。
</content>
    </entry>
    
     <entry>
        <title>[RabbitMQ] 强制杀死RabbitMQ进程</title>
        <url>https://mryqu.github.io/post/rabbitmq_%E5%BC%BA%E5%88%B6%E6%9D%80%E6%AD%BBrabbitmq%E8%BF%9B%E7%A8%8B/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>kill</tag><tag>rabbitmq-server</tag><tag>process</tag><tag>linux</tag>
        </tags>
        <content type="html">  首先，尝试使用init.d脚本优雅关闭RabbitMQ  sudo /etc/init.d/rabbitmq-server stop  如果不成功的话，使用  ps -eaf | grep erl  查看进程及父进程ID。输出第三列为父进程ID。找到仍是erlang进程（而不是启动进程的shell脚本）的第一个祖先进程，杀死它，这会同样终止其他子进程。上述示例中进程1301为&amp;rdquo;/bin/sh -e/usr/lib/rabbitmq/bin/rabbitmq-server&amp;rdquo;，已经不是erlang进程了，所以杀死进程1587就可以了。对于目前的RabbitMQ版本，可直接使用：  sudo pkill beam.smp   </content>
    </entry>
    
     <entry>
        <title>获取Facebook User Token</title>
        <url>https://mryqu.github.io/post/%E8%8E%B7%E5%8F%96facebook_user_token/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>user</tag><tag>token</tag><tag>curl</tag><tag>分析</tag>
        </tags>
        <content type="html">  使用Facebook Graph API搜索主页数据，可以用App Token也可以用User Token。 获取Facebook App Token一贴中已经介绍了如何获取Facebook App Token，这里就介绍一下如何获取UserToken。
参考3 Facebook Login - Advance - Manually Build a Login Flow给出了如何构建一个signURL，RestFB的getLoginDialogUrl方法就实现了这样的功能。redirectUri一开始直接想用带外认证urn:ietf:wg:oauth:2.0:oob，可是Facebook不认呀。 Facebook Login - Advance - Manually Build a Login Flow已经提到了：对于桌面应用，redirectUri必须是https://www.facebook.com/connect/login_success.html 。
获取Facebook User Token步骤 生成signURL 生成signURL并进行Get请求：
curl &amp;quot;https://www.facebook.com/dialog/oauth?client_id={appId}&amp;amp;redirect_uri=https://www.facebook.com/connect/login_success.html&amp;amp;response_type=token&amp;amp;scope=public_profile&amp;quot;  可以从返回的页面中获取登录表单：认证 使用自己的Facebook账户和密码填充上一表单，使用Post请求进行认证：
curl -X POST &amp;quot;{form-action}&amp;quot; -H &amp;quot;Content-Type: application/x-www-form-urlencoded&amp;quot; --data &amp;quot;lsd={lsd_value}&amp;amp;api_key={api_key_value}&amp;amp;cancel_url={cancel_rul_value}&amp;amp;isprivate={isprivate=_value}&amp;amp;legacy_return={legacy_return_value}&amp;amp;profile_selector_ids={profile_selector_ids_value}&amp;amp;return_session={return_session_value}&amp;amp;skip_api_login={skip_api_login_value}&amp;amp;signed_next={skip_api_login_value}&amp;amp;trynum={trynum_value}&amp;amp;timezone={timezone_value}&amp;amp;lgndim={lgndim_value}&amp;amp;lgnrnd={lgnrnd_value}&amp;amp;lgnjs={lgnjs_value}&amp;amp;email={your_facebook_account}&amp;amp;pass={your_facebook_password}&amp;amp;login={login_value}&amp;amp;persistent={persistent_value}&amp;amp;default_persistent={default_persistent_value}&amp;quot;  获取User Token Facebook通过认证后返回302响应，其Location头是下面这个样子的，很好获取（也可以参考一下RestFB的fromQueryString函数实现）。
https://www.facebook.com/connect/login_success.html#access_token={userToken}&amp;amp;expires_in={expire}  参考 Facebook Login - Access Tokens
Facebook Login - Access Tokens - App Access Tokens
Facebook Login - Advance - Manually Build a Login Flow
RestFB： GET LOGIN DIALOG URL
RestFB： EXTENDING AN ACCESS TOKEN
Facebook Dialog OAuth Tutorial
</content>
    </entry>
    
     <entry>
        <title>[RabbitMQ] Hello RabbitMQ Clustering</title>
        <url>https://mryqu.github.io/post/rabbitmq_hello_rabbitmq_clustering/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>rabbitmq</tag><tag>clustering</tag><tag>集群</tag><tag>配置</tag><tag>手工</tag>
        </tags>
        <content type="html">  RabbitMQ集群文档的介绍是： 一个RabbitMQ代理（broker）是一或多个Erlang节点的逻辑组合，每个节点运行RabbitMQ应用并共享用户、虚拟主机、队列、交换器、绑定和运行时参数。有时我们将节点集合称之为集群。 对RabbitMQ代理操作所需的所有数据/状态都会在所有节点上复制。唯一的例外是消息队列，默认存在于创建队列的节点上，但是对所有其他节点可见并可访问。集群内节点通过主机名互相通信，所以这些主机名必须能被集群内所有节点解析。
在Ubuntu上安装RabbitMQ 我在三台Ubuntu服务器上安装了RabbitMQ，分别是node50064，node50069和node51054。
 执行下列命令将APT仓库添加到/etc/apt/sources.list.d:  echo &#39;deb http://www.rabbitmq.com/debian/ testing main&#39; | sudo tee /etc/apt/sources.list.d/rabbitmq.list  (可选地)为了避免未签名包告警，使用 apt-key将RabbitMQ网站的公钥添加到信赖密钥列表：  wget -O- https://www.rabbitmq.com/rabbitmq-release-signing-key.asc | sudo apt-key add -  执行下列命令更新包列表：  sudo apt-get update  安装rabbitmq-server包：  sudo apt-get install rabbitmq-server   配置RabbitMQ - 修改/etc/rabbitmq/enabled_plugins使能管理插件：
 [rabbitmq_management].   修改/etc/default/rabbitmq-server，增大每用户可打开文件数（我的系统不使用systemd，无需修改/etc/systemd/system/rabbitmq-server.service.d/limits.conf）：  ulimit -S -n 4096  修改/etc/rabbitmq/rabbitmq-env.conf，激活长主机名并使用每个主机的完整域名作为本地节点名：  USE_LONGNAME=true NODENAME=rabbit@`env hostname -f`   关闭RabbitMQ：
sudo /etc/init.d/rabbitmq-server stop  配置RabbitMQ集群 首先启动node50064上的RabbitMQ（注：会有告警，可忽略。-detached选项就会导致PID不写入PID文件。）：
sudo rabbitmq-server -detached  RabbitMQ节点和CLI工具(例如rabbitmqctl)使用cookie来判断节点间是否可以通信。两个能够通信的节点必须拥有相同的共享密文，称之为Erlangcookie。集群中所有节点必须拥有相同cookie。必须在node50069和node51054关闭RabbitMQ的情况下，从node50064将其cookie复制到node50069（对node51054也做相同操作）： 更省事的方式，是在node50069和node51054没有安装RabbitMQ之前就将node50064上的cookie复制过来，这样node50069和node51054上的erlang节点就不会自己生成cookie了。
手工配置集群：
ubuntu@node50069:~$sudo rabbitmq-server -detached ubuntu@node50069:~$sudo rabbitmqctl stop_app ubuntu@node50069:~$sudo rabbitmqctl join_cluster rabbit@node50064.mryqu.com ubuntu@node50069:~$sudo rabbitmqctl start_app ubuntu@node51054:~$sudo rabbitmq-server -detached ubuntu@node51054:~$sudo rabbitmqctl stop_app ubuntu@node51054:~$sudo rabbitmqctl join_cluster rabbit@node50064.mryqu.com ubuntu@node51054:~$sudo rabbitmqctl start_app  验证集群状态：管理UI：参考 RabbitMQ Clustering Guide
Installing RabbitMQ on Debian / Ubuntu
Hello RabbitMQ
</content>
    </entry>
    
     <entry>
        <title>[RabbitMQ] Hello RabbitMQ</title>
        <url>https://mryqu.github.io/post/rabbitmq_hello_rabbitmq/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>rabbitmq</tag><tag>queue</tag><tag>exchange</tag><tag>route</tag><tag>topic</tag>
        </tags>
        <content type="html">  为了快速入门RabbitMQ，我主要学习了下列参考中的两个链接：RabbitMQ教程和SpringAMQP范例。这里对所学教程做一个小笔记。
准备工作 由于我不打算跑本机上的RabbitMQ服务器，所有对代码稍有修改。
TutorialConfiguration.java public class TutorialConfiguration { public static final String HOST = &amp;quot;mryqu-rabbitmq-server&amp;quot;; public static final String USERNAME = &amp;quot;mryqu&amp;quot;; public static final String PASSWORD = &amp;quot;mryqu-pwd&amp;quot;; }  对原有代码进行修改 // factory.setHost(&amp;quot;localhost&amp;quot;); factory.setHost(TutorialConfiguration.HOST); factory.setUsername(TutorialConfiguration.USERNAME); factory.setPassword(TutorialConfiguration.PASSWORD); }  RabbitMQ函数 发布方和消费方首先要创建连接，通过连接创建通道。通过通道也可以声明交换器，也可以直接声明队列。 - 函数Exchange.DeclareOk exchangeDeclare(String exchange, Stringtype, boolean durable, boolean autoDelete, boolean internal, Maparguments)用于声明交换器。其中exchange为队列名；type为交换器类型，例如fanout、direct、header和topic，注意无法改变已存在交换器的类型；durable为true时为持久交换器，在服务器重启后仍将存在，默认为false；autoDelete为true时，当所有的消费者使用完交换器后，服务器会自动删除交换器。服务器必须为判断交换器未使用提供一个合理时延，起码允许客户端能够创建一个代理并将其与队列绑定。默认为false；internal为true时为内部交换器，客户端不能直接向其发布消息，默认为false。 - 函数Queue.DeclareOk queueDeclare(String queue, boolean durable,boolean exclusive, boolean autoDelete, Map arguments)用于声明队列，其中queue为队列名；durable为true时为持久队列，在服务器重启后仍将存在。默认为false；exclusive为true时为私有队列，仅在当前连接中可以访问队列，当连接关闭时删除该队列。默认为true；autoDelete为true时，当所有的消费者使用完队列后，服务器会自动删除队列。最后一个消费者可被显式取消或由于通道关闭而取消。如果队列从没有消费者，队列将不会被删除。应用可以像对普通队列一样使用Delete方法显式删除自动删除队列。默认为true。 函数Queue.BindOk queueBind(String queue, String exchange, StringroutingKey, Map arguments)用于通过路由关键字将队列与交换器进行绑定。
函数void basicPublish(String exchange, String routingKey, booleanmandatory, boolean immediate, BasicProperties props, byte[]body)用于发布者发布消息。其中exchange为交换器名；routingKey为路由关键字，使用默认交换器及命名队列时可直接设为队列名；mandatory标志告知服务器当消息无法路由到队列时如何处理。如果标志被设上，服务器通过一个Return方法返回无法路由的消息。如果标志为空，服务器就仅丢弃消息。默认为false；immediate标志告知服务器当消息无法立即路由到队列消费者时如何处理。如果标志被设上，服务器通过一个Return方法返回无法路由的消息。如果标志为空，服务器就将消息放入队列，但不保证消息最终被消费。默认为false；props可设置下列子属性（可参考com.rabbitmq.client.MessageProperties进行了解）：
public static class BasicProperties extends com.rabbitmq.client.impl.AMQBasicProperties { private String contentType;  函数String basicConsume(String queue, boolean autoAck, StringconsumerTag, boolean noLocal, boolean exclusive, Map arguments,Consumercallback)用于消费者接收消息。其中queue为队列名；autoAck为true时，服务器当传递完消息即认为消息被确认。autoAck为false时，服务器必须等待显示确认；consumerTag为客户端生成的用于建立上下文的消费者标签。消费者标签仅对一个通道局部可见，因此两个客户端可以使用相同的消费者标签。如果该字段为空，则服务器将生成一个唯一标签。如果一个标签已用于标识一个已有消费者，客户端不能在使用该函数时使用其值。标签仅对创建消费者所在的通道有效；noLocal为true的话，服务器不会将消息传递给发布消息的连接。默认为false；exclusive为true的话，请求排外消费者访问，即仅该消费者可以访问队列。客户端可能不会从已有活跃消费者的队列获得排外访问。默认为false。 消息消费者通过函数void basicQos(int prefetchSize, int prefetchCount,boolean global)指定服务质量。QoS可指定用于当前通道或连接的所有通道。机关QoS原则上施加于消息两端，当前仅对服务器有意义。客户端可以请求消息被提前发送，这样当客户端处理完一条消息，下一条消息已被收到本地，而不是此时从通道接收。预抓取可以提升性能。prefetchSize以字节为单位指定了预抓取窗口，服务器当消息大小等于小于prefetchSize（且满足其他预抓取限制）时提前发送消息。改值可以为0，即无限制。如果no-ack选项被设置时prefetchSize将被忽略。当客户端没有处理任何消息时，服务器必须忽略该选项（例如预抓取大小不限制向客户端发送单个消息的大小，仅当客户端仍有一或多个未应答消息时用于提前发送消息）。默认为0；prefetchCount为以消息个数指定预抓取窗口，可与prefetchSize联合使用。如果no-ack选项被设置时prefetchCount将被忽略。服务器可以提前发送少于prefetchCount个消息，而不能多发送；对于global，RabbitMQ重新解释了该字段。原规范指“默认QoS设置仅适用于当前通道。如果该字段被设置，将应用于整个连接”。而RabbitMQ将global=false解释为QoS设置仅应用于消费者（对通道的新消费者生效，已有消费者不受影响），global=true解释为QoS设置应用于通道。默认为false。
教程一：&amp;rdquo;Hello World!&amp;rdquo; 本教程中消息发布者和消费者直接通过一个命名队列“Hello”传递消息。
消息发布者：
private final static String QUEUE_NAME = &amp;quot;hello&amp;quot;; String message = &amp;quot;Hello World!&amp;quot;; channel.basicPublish(&amp;quot;&amp;quot;, QUEUE_NAME, null, message.getBytes(&amp;quot;UTF-8&amp;quot;));  消息消费者：
private final static String QUEUE_NAME = &amp;quot;hello&amp;quot;; Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &amp;quot;UTF-8&amp;quot;); System.out.println(&amp;quot; [x] Received &#39;&amp;quot; &#43; message &#43; &amp;quot;&#39;&amp;quot;); } }; channel.basicConsume(QUEUE_NAME, true, consumer);  教程二：工作队列 本教程跟教程一的区别主要有以下几点： - 教程一中消息发布者和消费者声明的队列为非持久性，而本教程中则为持久性； - 教程一中消息发布者发布消息时未指定属性，而本教程中指定内容为纯文本、持久的消息 - 教程一中消息消费者消息时自动应答（autoAck或no-ack），而本教程中为非自动应答 - 本教程中消息消费者通过函数void basicQos(int prefetchSize)指定服务质量。由于前一条为非自动应答，因此本设置生效。预读取字节无限制，预读取消息个数为1，仅应用于消费者级别。 - 本教程中消息消费者通过Thread.sleep函数模拟耗时操作，并通过void basicAck(longdeliveryTag, boolean multiple)确认消息。 - 本教程中起了两个消息消费者，两个消息消费者交替处理发布者迅速发出的多个消息。
教程三：订阅/发布 上一教程工作队列中每个任务仅发送给一个消费者，而本教程中一个消息可被发给多个消费者。实现主要通过一个fanout类型的交换器完成。
教程四：路由 上一教程中fanout类型的交换器将收到的消息广播给所有接收者，而本教程接收者将有选择地接收消息中感兴趣的那一部分。实现主要通过direct类型交换器完成，消费者将通道与交换器通过感兴趣的路由关键字绑定，发布者在使用basicPublish函数发布消息时需提供交换器、路由关键字和消息内容。
教程五：主题 上一教程的限制在于不能基于多个条件进行路由。本教程通过topic类型交换器完成复杂的路由。其中路由关键字为由点做分界符的词列表，*代表一个词，#代表零或多个词。 例如路由关键词中第一个代表速度、第二个代表颜色，第三个代表物种：..。这里创建三个绑定：Q1与&amp;rdquo;*.orange.*&amp;ldquo;关键词绑定，Q2与关键词&amp;rdquo;*.*.rabbit&amp;rdquo;和&amp;rdquo;lazy.#&amp;ldquo;绑定。其意义为：Q1对所有桔色动物感兴趣，Q2希望获得所有关于兔子及所有关于缓慢动物的消息。 路由关键词设为&amp;rdquo;quick.orange.rabbit&amp;rdquo;的消息将会传给两个队列。路由关键词设为&amp;rdquo;lazy.orange.elephant&amp;rdquo;的消息也将会传给两个队列。路由关键词设为&amp;rdquo;quick.orange.fox&amp;rdquo;将仅会传给第一个队列，路由关键词设为&amp;rdquo;lazy.brown.fox&amp;rdquo;将仅会传给第二个队列。路由关键词设为&amp;rdquo;lazy.pink.rabbit&amp;rdquo;将仅会传给第二个队列一次，即使它匹配了两个绑定。路由关键词设为&amp;rdquo;quick.brown.fox&amp;rdquo;的消息没有匹配任何绑定，因此会被丢弃。路由关键词设为&amp;rdquo;orange&amp;rdquo;或&amp;rdquo;quick.orange.male.rabbit&amp;rdquo;也不会匹配任何绑定，将被丢弃。而路由关键词设为&amp;rdquo;lazy.orange.male.rabbit&amp;rdquo;即使有四个词，也能匹配最后一个绑定，并传给第二个队列。 如果路由关键词不包含*和#字符，则等同direct类型。
教程六：RPC 本教程借助消息属性中的replyTo和correlationId，完成响应与请求的匹配。
参考 RabbitMQ Tutorials
Spring AMQP Samples
GETTING STARTED: Messaging with RabbitMQ
Understanding AMQP
Spring AMQP
AMQP 0-9-1 Complete Reference Guide
</content>
    </entry>
    
     <entry>
        <title>[RabbitMQ] 在Widnows平台安装配置RabbitMQ</title>
        <url>https://mryqu.github.io/post/rabbitmq_%E5%9C%A8widnows%E5%B9%B3%E5%8F%B0%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AErabbitmq/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>rabbitmq</tag><tag>windows</tag><tag>install</tag><tag>configure</tag>
        </tags>
        <content type="html">  RabbitMQ介绍 RabbitMQ是基于高级消息队列协议的消息代理软件。RabiitMQ服务器由Erlang语言开发，客户端支持多种主流编程语言。 RabbitMQ由LShift和CohesiveFT合营公司Rabbit技术有限公司开发，在2010年4月被SpringSource收购，2013年5月归入Pivotal软件。 RabbitMQ项目包括： - RabbitMQ交换服务器自身 - 用于HTTP、流文本定向消息协议(STOMP)和消息队列遥测传输协议(MQTT)的网关 - Java、.NET Framework和Erlang语言的AMQP客户端库 - 支持定制插件的插件平台，内建插件集合为: - Shovel插件，负责从一个消息代理（broker）向另一个移动/复制消息。 - Federation插件，在消息代理之间有效共享消息(基于exchange这一级) - Management插件，监控和管理消息代理 - STOMP插件，提供STOMP协议支持 - MQTT插件，提供MQTT协议支持 - LDAP插件，RabbitMQ通过外部LDAP服务器进行认证和授权
在Widnows平台安装RabbitMQ 根据http://www.rabbitmq.com/install-windows.html安装Erlang和RabbitMQ服务器，运行RabbitMQ安装程序时需要选择“Runas Administrator”，否则事后需要执行下列命令修正.erlang.cookie位置错误。
copy /Y %SystemRoot%\.erlang.cookie %HOMEDRIVE%%HOMEPATH%  设置环境变量（及安装并启动RabbitMQ服务）
SET　ERLANG_HOME=C:\tools\erl8.0 SET RABBITMQ_SERVER＝C:\tools\RabbitMQ_Server\rabbitmq_server-3.6.5 SET　RABBITMQ_BASE=C:\rabbitmq-data ECHO []. &amp;gt; C:\rabbitmq-data\rabbitmq.config %RABBITMQ_SERVER%\sbin\rabbitmq-service.bat install %RABBITMQ_SERVER%\sbin\rabbitmq-service.bat start  安装管理插件 rabbitmq-management插件提供用于管理和监控RabbitMQ服务器的基于HTTP的API，以及基于浏览器的界面和一个控制台工具rabbitmqadmin。功能包括： - 声明、列举和删除exchange、queue、binding、用户、虚拟主机和权限。 - 监控队列长度、消息总速率和每通道速率、连接数据速率等。 - 发送和接受消息。 - 监控Erlang进程、文件描述符和内存使用。 - 导出/导入对象定义到JSON格式 - 强制关闭连接、清除队列。重启RabbitMQ后登录http://guest:guest@localhost:15672/，即可见到管理页面。 rabbitmqctl 通过rabbitmqctl创建一个管理员用户admin和一个对虚拟主机有读写权限的普通用户mryqu： 自建管理员用户admin的默认用户guest的区别在于：guest仅能本机访问RabbitMQ，除非在rabbitmq.config增加loopback_users设置。
使用HTTP管理API 将配置导出成JSON格式：
curl -i -u guest:guest http://localhost:15672/api/definitions  激活其他插件 例如激活shovel插件：
%RABBITMQ_SERVER%\sbin\rabbitmq-plugins.bat enable rabbitmq_shovel %RABBITMQ_SERVER%\sbin\rabbitmq-plugins.bat enable rabbitmq_shovel_management  测试RabbitMQ 使用GETTING STARTED: Messaging with RabbitMQ中的代码即可，由于我想试验非本机访问RabbitMQ，因此添加了application.properties：
spring.rabbitmq.host=rabbitmqServer spring.rabbitmq.port=5672 spring.rabbitmq.username=mryqu spring.rabbitmq.password=XXXXXXXXXXXXXX  参考 RabbitMQ - Installing on Windows
RabbitMQ - Access Control (Authentication, Authorisation)
RabbitMQ - Windows Quirks
RabbitMQ - plugins
RabbitMQ - Management Plugin
RabbitMQ - Shovel plugin
RabbitMQ - rabbitmqctl(1) manual page
RabbitMQ Management HTTP API
GETTING STARTED: Messaging with RabbitMQ
Spring AMQP
RabbitMQ for Windows: Introduction
</content>
    </entry>
    
     <entry>
        <title>在Ubuntu中识别当前Init系统</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8ubuntu%E4%B8%AD%E8%AF%86%E5%88%AB%E5%BD%93%E5%89%8Dinit%E7%B3%BB%E7%BB%9F/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>unix</tag><tag>linux</tag><tag>init</tag>
        </tags>
        <content type="html">  Ubuntu 14.04 首先用uname命令查看一下系统信息：
ubuntu@node50069:~$ uname -a Linux node50069 3.19.0-25-generic #26~14.04.1-Ubuntu SMP Fri Jul 24 21:16:20 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux  用pstree命令查看一下ID为1的进程，原来是init：可以下面几个命令查看init进程所用的命令信息： - ps -efa|grep init - type init - sudo stat /proc/1/exe
最终通过&amp;rdquo;sudo init &amp;ndash;version&amp;rdquo;可知当前的Init系统为upstart：Ubuntu 15.04 首先查看一下系统信息：
vagrant@vagrant-ubuntu-trusty:~$ uname -a Linux vagrant-ubuntu-trusty 3.19.0-15-generic #15-Ubuntu SMP Thu Apr 16 23:32:37 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux  用pstree命令查看一下ID为1的进程，原来是systemd：参考 WIKI：init
</content>
    </entry>
    
     <entry>
        <title>[Spark]Spark和Hive集成</title>
        <url>https://mryqu.github.io/post/spark_spark%E5%92%8Chive%E9%9B%86%E6%88%90/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>spark</tag><tag>hive</tag><tag>integration</tag><tag>spark-sql</tag>
        </tags>
        <content type="html"> 在前一博文[Spark] Spark2集群安装实践中安装了Spark后，发现和Hive还没有集成在一起，此外Hive自己也不好使了。
hadoop@node50064:~$hive ls: cannot access /usr/local/spark/lib/spark-assembly-*.jar: No such file or directory .................  原来Spark assemblyjar在Spark2中已经不存在了，而Hive脚本判断系统存在Spark后仍要使用，需要将$HIVE_HOME/bin/hive中的这部分代码注释掉：
# add Spark assembly jar to the classpath #if [[ -n &amp;quot;$SPARK_HOME&amp;quot; ]] #then # sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar` # CLASSPATH=&amp;quot;${CLASSPATH}:${sparkAssemblyPath}&amp;quot; #fi  至此，Hive本身工作正常。下面开始Spark和Hive集成配置工作。 - Spark SQL CLI需要使用到Hive Metastore，因此需要在[Hive] 安装Hive 1.2.x的基础上继续修改$HIVE_HOME/conf/hive-site.xml：- 将$HIVE_HOME/conf/hive-site.xml软连接到$SPARK_HOME/conf目录中:
 cd $SPARK_HOME/conf ln -s $HIVE_HOME/conf/hive-site.xml   启动Hive Metastore和HiveServer2：  hive --service metastore &amp;amp; hive --service hiveserver2 &amp;amp;   下面进行验证工作：
hadoop@node50064:~$ hive hive&amp;gt; use default; OK Time taken: 0.942 seconds hive&amp;gt; show tables; OK apachelog b complex_datatypes_example dummy empinfo irisdata primitive_dataytpes_example testhivedrivertable Time taken: 0.288 seconds, Fetched: 8 row(s) hive&amp;gt; select * from empinfo; OK John Jones 99980 45 Payson Arizona Mary Jones 99982 25 Payson Arizona Eric Edwards 88232 32 San Diego California Mary Ann Edwards 88233 32 Phoenix Arizona Ginger Howell 98002 42 Cottonwood Arizona Sebastian Smith 92001 23 Gila Bend Arizona Gus Gray 22322 35 Bagdad Arizona Mary Ann May 32326 52 Tucson Arizona Erica Williams 32327 60 Show Low Arizona Leroy Brown 32380 22 Pinetop Arizona Elroy Cleaver 32382 22 Globe Arizona Time taken: 0.573 seconds, Fetched: 11 row(s) hive&amp;gt; quit; hadoop@node50064:~$ spark-sql spark-sql&amp;gt;use default; &amp;gt; show tables; default apachelog false default b false default complex_datatypes_example false default dummy false default empinfo false default irisdata false default primitive_dataytpes_example false default testhivedrivertable false Time taken: 0.083 seconds, Fetched 8 row(s) 16/08/02 02:47:05 INFO CliDriver: Time taken: 0.083 seconds, Fetched 8 row(s) spark-sql&amp;gt; select * from empinfo; John Jones 99980 45 Payson Arizona Mary Jones 99982 25 Payson Arizona Eric Edwards 88232 32 San Diego California Mary Ann Edwards 88233 32 Phoenix Arizona Ginger Howell 98002 42 Cottonwood Arizona Sebastian Smith 92001 23 Gila Bend Arizona Gus Gray 22322 35 Bagdad Arizona Mary Ann May 32326 52 Tucson Arizona Erica Williams 32327 60 Show Low Arizona Leroy Brown 32380 22 Pinetop Arizona Elroy Cleaver 32382 22 Globe Arizona Time taken: 0.478 seconds, Fetched 11 row(s) 16/08/02 02:48:14 INFO CliDriver: Time taken: 0.478 seconds, Fetched 11 row(s)  </content>
    </entry>
    
     <entry>
        <title>Hive与Spark的版本搭配</title>
        <url>https://mryqu.github.io/post/hive%E4%B8%8Espark%E7%9A%84%E7%89%88%E6%9C%AC%E6%90%AD%E9%85%8D/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>spark</tag><tag>version</tag><tag>compatibility</tag>
        </tags>
        <content type="html"> Hive on Spark: Getting Started里面介绍了如果Hive的查询引擎选择Spark的话，Hive所需相关配置。如果用一个不兼容的Hive和Spark版本，有潜在风险，例如Spark2就没有spark-assembly-*.jar可供低版本Hive使用。 问题来了，么查找已测试的Hive和Spark版本对呢？ 网上有人说看Hive代码根路径下的pom.xml。例如Hive branch-1.2中pom.xml包含spark.version为1.3.1，这说明官方在进行Hive1.2.0测试时用的Spark 1.3.1。 此外，也可借鉴C家、H家和MapR技术栈的版本搭配： - CDH 5 Packaging and Tarball Information中列出了CHD5中技术栈的版本情况。 - 在Hortonworks Data Platform列出了HDP所用的技术栈的版本情况。- MapR Ecosystem Support Matrix中列出了MapR中技术栈的版本情况。
</content>
    </entry>
    
     <entry>
        <title>[Spark] Set spark.yarn.archive</title>
        <url>https://mryqu.github.io/post/spark_set_spark.yarn.archive/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>spark</tag><tag>spark.yarn.jars</tag><tag>spark.yarn.archive</tag><tag>yarn</tag>
        </tags>
        <content type="html"> 提交Spark作业时，遇到没有设置spark.yarn.jars和spark.yarn.archive的告警：
16/08/01 05:01:19 INFO yarn.Client: Preparing resources for our AM container 16/08/01 05:01:20 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME. 16/08/01 05:01:23 INFO yarn.Client: Uploading resource file:/tmp/spark-AA-BB-CC-DD-EE/__spark_libs__XXX.zip -&amp;gt; hdfs://node50064.mryqu.com:9000/user/hadoop/.sparkStaging/application_1469998883123_0001/__spark_libs__XXX.zip  解决方案：
cd $SPARK_HOME zip spark-archive.zip jars/* hadoop fs -copyFromLocal spark-archive.zip echo &amp;quot;spark.yarn.archive=hdfs:///node50064.mryqu.com:9000/user/hadoop/spark-archive.zip&amp;quot; &amp;gt;&amp;gt; conf/spark-defaults.conf  如系统没有安装zip，可执行sudoapt-get install zip进行安装。 这样就不用每次上传Spark的jar文件到HDFS，YARN会找到Spark的库以用于运行作业。
</content>
    </entry>
    
     <entry>
        <title>短网址服务学习和测试</title>
        <url>https://mryqu.github.io/post/%E7%9F%AD%E7%BD%91%E5%9D%80%E6%9C%8D%E5%8A%A1%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B5%8B%E8%AF%95/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>shorturl</tag><tag>tinyurl</tag><tag>短地址</tag><tag>原理</tag><tag>测试</tag>
        </tags>
        <content type="html">  短网址服务 短网址（Short URL） ，顾名思义就是在形式上比较短的网址。通常用的是asp或者php转向，在Web2.0的今天，不得不说，这是一个潮流。目前已经有许多类似服务，借助短网址您可以用简短的网址替代原来冗长的网址，让使用者可以更容易的分享链接。 网上的短网址服务一堆堆的，例如谷歌家的goo.gl，bit.ly为推特家提供了bit.ly及为亚马逊家提供了amzn.to，推特家自己的t.co，新浪家的t.cn，百度家的dwz.cn，简直数不过来。 SearchEngineLand曾对国外市面上的URL缩短服务进行了分析总结，列出了推荐使用的和应尽力避免使用的服务。该评测从是否为301转向（永久转向，相对302为暂时转向）、是否支持Tracking（追踪链接的来源）、是否支持主流的Twitter客户端、缩短后的URL字数、是否支持自定义URL、是否支持分享等多个方面进行评价。具体结果见此表。 短网址算法 算法一  将长网址md5生成32位签名串,分为4段, 每段4个字节（即32位）; 对这四段循环处理, 取4个字节（32位）, 将他看成16进制串与0x3fffffff(30位1)与操作,即超过30位的忽略处理; 这30位分成6段, 每5位的数字作为字母表的索引取得特定字符, 依次进行获得6位字符串; 总的md5串可以获得4个6位串; 取里面的任意一个就可作为这个长url的短url地址;  算法二 把数字和字符组合做一定的映射,就可以产生唯一的字符串,如第62个组合就是aaaaa9,第63个组合就是aaaaba,再利用洗牌算法，把原字符串打乱后保存，那么对应位置的组合字符串就会是无序的组合。 把长网址存入数据库,取返回的id,找出对应的字符串,例如返回ID为1，那么对应上面的字符串组合就是bbb,同理ID为2时，字符串组合为bba,依次类推,直至到达62种组合后才会出现重复的可能，所以如果用上面的62个字符，任意取6个字符组合成字符串的话，你的数据存量达到500多亿后才会出现重复的可能。
短网址服务测试(基于新浪短地址服务) 短地址服务是否验证长URL？ 对于不存在的长地址http://www.mryqu.com/test/test.html，新浪短地址服务没有验证，直接转成了短地址http://t.cn/RtXngGQ 。如果长url其实是该短地址服务产生的短地址，如何处理？ 对于长地址http://t.cn/RtXngGQ ，新浪短地址服务做了检查识别出短地址,并直接按照http://www.mryqu.com/test/test.html返回短地址http://t.cn/RtXngGQ 。如果长url其实是其他短地址服务产生的短地址，如何处理？ 我用长地址http://blog.sina.com.cn/yandongqu 在百度短地址服务获得了短地址http://dwz.cn/3TE3bq ，将其作为新浪短地址服务的输入，新浪短地址服务识别出短地址，获得原始长地址http://blog.sina.com.cn/yandongqu ，并返回相应短地址短地址http://t.cn/RLoGZKa 。参考 短网址
短网址服务，我们该怎么选？
URL Shorteners: Which Shortening Service Should You Use?
短链接URL系统是怎么设计的？
tinyURL的设计方案与实现【一】
</content>
    </entry>
    
     <entry>
        <title>[Spark]Spark2集群安装实践</title>
        <url>https://mryqu.github.io/post/spark_spark2%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Spark</category>
        </categories>
        <tags>
          <tag>spark</tag><tag>ubuntu</tag><tag>cluster</tag><tag>yarn</tag><tag>install</tag>
        </tags>
        <content type="html">  从Spark2.0.0开始，Spark使用Scala2.11构建，不再对Java7和Python2.6进行支持。当然不编译Spark源码的话，无需安装Scala。
Spark集群模型 Spark应用作为集群上一组独立进程运行，由你的主程序（即驱动程序）的SparkContext对象管理。为了在集群上运行，SparkContext可以与若干类型集群管理器（Spark自带的独立集群管理器、Mesos、YARN）连接，集群管理器为应用分配资源。Spark需要集群节点上的执行者（executor）为应用执行计算或存储数据。接下来，它将应用代码发送给执行者，最后SparkContext将人物发往执行者进行运行。准备工作 安装Scala # Scala Installation wget www.scala-lang.org/files/archive/scala-2.11.8.deb sudo dpkg -i scala-2.11.8.deb # sbt Installation echo &amp;quot;deb https://dl.bintray.com/sbt/debian /&amp;quot; | sudo tee -a /etc/apt/sources.list.d/sbt.list sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823 sudo apt-get update sudo apt-get install sbt  安装Java8 sudo apt-add-repository ppa:webupd8team/java -y sudo apt-get update -y sudo apt-get install oracle-java8-installer -y sudo apt-get install oracle-java8-set-default  环境变量设置 在~/.bashrc中添加：
# Set SPARK_HOME export SPARK_HOME=/usr/local/spark export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin  最后通过source~/.bashrc刷新配置文件。
安装Spark （在node50064上）下载并配置Spark wget http://d3kbcqa49mib13.spark-2.X.Y-bin-hadoop2.7.tgz tar -xzf spark-2.X.Y-bin-hadoop2.7.tgz sudo mv spark-2.X.Y-bin-hadoop2.7 /usr/local/spark sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/spark hadoop fs -mkdir /user/hadoop/sparkHistory cd /usr/local/spark/conf  conf/spark-env.sh 通过cpspark-env.sh.template spark-env.sh创建spark-env.sh，并作如下修改：
# export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export SCALA_HOME=${SCALA_HOME:-/usr/share/scala} # export HADOOP_HOME=/usr/local/hadoop # export SPARK_HOME=/usr/local/spark export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-$HADOOP_HOME/etc/hadoop/conf} export YARN_CONF_DIR=${HADOOP_CONF_DIR:-$HADOOP_HOME/etc/hadoop} export HIVE_CONF_DIR=${HIVE_CONF_DIR:-$HIVE_HOME/etc/hive/conf} # Options for the daemons used in the standalone deploy mode export SPARK_MASTER_PORT=7077 export SPARK_MASTER_WEBUI_PORT=18080 export SPARK_WORKER_PORT=7078 export SPARK_WORKER_WEBUI_PORT=18081  conf/slaves 通过cpslaves.template slaves创建slaves，并作如下修改：
node50069.mryqu.com node51054.mryqu.com  conf/spark-defaults.conf 通过cpspark-defaults.conf.templatespark-defaults.conf创建spark-defaults.conf，并作如下修改：
spark.history.ui.port=18088 spark.eventLog.enabled=true spark.eventLog.dir=hdfs://node50064.mryqu.com:9000/user/hadoop/sparkHistory spark.history.fs.logDirectory=hdfs://node50064.mryqu.com:9000/user/hadoop/sparkHistory  多说一句，Spark Monitoring and Instrumentation里面提到了spark.eventLog.dir和spark.history.fs.logDirectory，但是没说二者的关系；Cloudera - Managing the Spark History Server里面就没提spark.history.fs.logDirectory；而IBM - Enabling the Spark History service里面提到spark.history.fs.logDirectory的值要跟spark.eventLog.dir一样。
从node50064复制Spark到其他机器 scp -r /usr/local/spark node50069:~/ scp -r /usr/local/spark node51054:~/  在node50069和上node51054配置Spark sudo mv spark /usr/local/spark sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/spark  （在node50064上）启动Spark （在node50064上）启动Spark Standalone custer # 启动Spark 自带的独立（Standalone）集群管理器 # 如仅使用YARN或Mesos集群管理器，无需启动 $SPARK_HOME/sbin/start-all.sh # 启动Spark History Server start-history-server.sh  检验安装结果 查看JVM进程状态 如果启动了Spark 自带的独立（Standalone）集群管理器的话，通过jps-l查看，则在node50064上有org.apache.spark.deploy.master.Master和org.apache.spark.deploy.history.HistoryServer，在node50069和node51054上有org.apache.spark.deploy.worker.Worker进程存在。
提交Spark示例作业 # 使用Spark自带的独立集群管理器 spark-submit --class org.apache.spark.examples.SparkPi \ --master spark://10.120.12.135:6066 \ --deploy-mode cluster \ --driver-memory 4g \ --executor-memory 2g \ --executor-cores 1 \ examples/jars/spark-examples*.jar \ 10 # 使用YARN集群管理器 spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ --driver-memory 4g \ --executor-memory 2g \ --executor-cores 1 \ examples/jars/spark-examples*.jar \ 10  Spark独立集群管理器的Master Web界面 Spark独立集群管理器的Worker Web界面 Spark 历史服务器Web界面 （在node50064上）关闭Spark # 关闭Spark History Server stop-history-server.sh # 关闭Spark 自带的独立（Standalone）集群管理器 $SPARK_HOME/sbin/stop-all.sh  参考 Spark官网
Scala官网
Spark Cluster Mode Overview
Spark Standalone Mode
Running Spark on YARN
Spark Monitoring and Instrumentation
Cloudera CDH - Managing Spark
</content>
    </entry>
    
     <entry>
        <title>twitcurl获取访问令牌（AccessToken和AccessTokenSecret）的实现流程</title>
        <url>https://mryqu.github.io/post/twitcurl%E8%8E%B7%E5%8F%96%E8%AE%BF%E9%97%AE%E4%BB%A4%E7%89%8Caccesstoken%E5%92%8Caccesstokensecret%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twitcurl</tag><tag>twitter</tag><tag>oauth</tag><tag>requesttoken</tag><tag>accesstoken</tag>
        </tags>
        <content type="html">  twitterClient.cpp中有一段代码是没有AccessToken和AccessTokenSecret的情况下，通过ConsumerKey、ConsumerKeySecret、UserName和UaserPassword获取AccessToken和AccessTokenSecret。 一般实现是通过重定向到Twitter页面去授权应用，通过callbackURL获得Twitter传过来的AccessToken和AccessTokenSecret信息。twitcurl既可以通过访问twitter.com获取PIN，也可以交由twitcurl自动获得。 如果通过twitter.com处理PIN，twitcurl会提供授权链接。进入链接后输入用户名和密码，会重定向到Twitter应用的callbackURL（例如http://www.mryqu.com/test.html?oauth_token={OAuthToken}&amp;amp;oauth_verifier={OAuthVerifier} ），其中oauth_verifier值即为所谓的PIN。 下面我们看一下twitcurl是如何实现不访问twitter.com获取AccessToken和AccessTokenSecret的。 - GET https://api.twitter.com/oauth/request_tokentwitCurl::oAuthRequestToken 方法实现该HTTP请求，允许消费者应用获得一个OAuth请求令牌以请求用户授权。除了HTTP OAuth头外，twitcurl实现没有其他HTTP头，也没有消息体内容。HTTP OAuth头包含如下项： - oauth_consumer_key - oauth_nonce - oauth_signature - oauth_signature_method - oauth_timestamp - oauth_version
通过如下HTTP响应可以获得oauth_token和oauth_token_secret，保存在oAuth对象的m_oAuthTokenKey和m_oAuthTokenSecret变量中：
 oauth_token=XXXXXX&amp;amp;oauth_token_secret=XXXXXX&amp;amp;oauth_callback_confirmed=true   GET https://api.twitter.com/oauth/authorize?oauth_token=XXXXXXtwitterObj.oAuthHandlePIN 方法实现该HTTP请求，获取响应页面中表单的authenticity_token和oauth_token元素的值。除了HTTP OAuth头外，twitcurl实现没有其他HTTP头，也没有消息体内容。HTTP OAuth头包含如下项：  oauth_consumer_key oauth_nonce oauth_signature oauth_signature_method oauth_timestamp oauth_token （取自上一HTTP响应） oauth_version    HTTP响应片段：- POST https://api.twitter.com/oauth/authorize?oauth_token=XXXXXXtwitterObj.oAuthHandlePIN 方法实现该HTTP请求，允许消费者应用使用OAuth请求令牌请求用户授权。HTTP OAuth头包含如下项： - oauth_consumer_key - oauth_nonce - oauth_signature - oauth_signature_method - oauth_timestamp - oauth_token （取自上一HTTP响应） - oauth_version
HTTP请求消息体内容为：
 oauth_token=XXXXXX&amp;amp;authenticity_token=XXXXXX&amp;amp;session[username_or_email]=**XXXXX**X&amp;amp;session[password]=XXXXXX  HTTP响应片段：通过如下HTTP响应可以获得oauth_verifier，保存在oAuth对象的m_oAuthPin变量中。 - GET https://api.twitter.com/oauth/access_tokentwitterObj.oAuthAccessToken 方法实现该HTTP请求，允许消费者应用使用OAuth请求令牌交换OAuth访问令牌。HTTP OAuth头包含如下项： - oauth_consumer_key - oauth_nonce - oauth_signature - oauth_signature_method - oauth_timestamp - oauth_token - oauth_verifier （取自上一HTTP响应） - oauth_version
HTTP响应：
 oauth_token=XXXXXX&amp;amp;oauth_token_secret=XXXXXX&amp;amp;user_id=XXXXXX&amp;amp;screen_name=XXXXXX&amp;amp;x_auth_expires=0  通过HTTP响应可以获得oauth_token、oauth_token_secret和user_id，保存在oAuth对象的m_oAuthTokenKey、m_oAuthTokenSecret和m_oAuthScreenName变量中，可以将此OAuth访问令牌保存下来以备之后的使用，下次就无需再次申请访问令牌了。
参考 Twitter OAuth Overview
Twitter PIN-based authorization
Github: mryqu/twitcurl
OAuth Core 1.0
Twitter：POST oauth/request_token
Twitter：GET oauth/authorize
Twitter：POST oauth/access_token
twitcurl生成HTTP OAuth头的实现流程
twitter4j/auth/OAuthAuthorization.java
</content>
    </entry>
    
     <entry>
        <title>微信语音导出和转换</title>
        <url>https://mryqu.github.io/post/%E5%BE%AE%E4%BF%A1%E8%AF%AD%E9%9F%B3%E5%AF%BC%E5%87%BA%E5%92%8C%E8%BD%AC%E6%8D%A2/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>微信</tag><tag>语音</tag><tag>导出</tag><tag>转换</tag><tag>mp3</tag>
        </tags>
        <content type="html"> 用微信语音发送了一段儿子朗诵诗，想导出来做个念想，结果发现还挺麻烦。 - 在tencent/MicsoMsg目录下有名字很长的文件夹就是微信聊天记录存放的文件夹，每登陆一个微信就会产生一个名字很长的文件夹。在这个目下下有个Voice2目录，将下面的子目录按照日期排序缩小范围，查找疑似子目录下的amr文件，想办法拷贝到计算机中。 - 一开始我按照网上的攻略用格式工厂转换成mp3，报错。用Ultraedit打开文件，头部显示有silk_v3。SILKv3编码是Skype向第三方开发人员和硬件制造商提供免版税认证(RF)的Silk宽带音频编码器，Skype后来将其开源。在Github上找到了https://github.com/kn007/silk-v3-decoder，搞定！
</content>
    </entry>
    
     <entry>
        <title>使用Jacoco</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8jacoco/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>jacoco</tag><tag>code coverage</tag><tag>gradle</tag><tag>代码覆盖率</tag>
        </tags>
        <content type="html">  在以前的项目都是用Cobertura做代码覆盖率测试的，这次有机会接触了一下另一个代码覆盖率Java库JaCoco。 JaCoCo拥有友好的授权形式。JaCoCo使用了Eclipse PublicLicense，方便个人用户和商业用户使用。JaCoCo/EclEmma项目除了提供JaCoCo库之外，还提供了Ant任务、Maven插件及EclEmmaEclipse插件，也可以使用JavaAgent技术监控Java程序。很多第三方的工具提供了对Jacoco的集成，如SonarQube、Jenkins、Netbeans、IntelliJIDEA、Gradle等。 JaCoCo包含多种级别的覆盖率计数器，包含指令级覆盖(Instructions，C0coverage)，分支（Branches，C1coverage）、圈复杂度(CyclomaticComplexity)、行覆盖(Lines)、方法覆盖(non-abstractmethods)、类覆盖(classes)。 - 指令覆盖：计数单元是单个java字节码指令，指令覆盖率提供了代码是否被执行的信息，该指标完全独立与源码格式。 - 分支覆盖率：度量if和switch语句的分支覆盖情况，计算一个方法里面的总分支数，确定执行和不执行的分支数量。 - 圈复杂度：在（线性）组合中，计算在一个方法里面所有可能路径的最小数目，缺失的复杂度同样表示测试案例没有完全覆盖到这个模块。 - 行覆盖率：度量被测程序的每行代码是否被执行，判断标准行中是否至少有一个指令被执行。 - 方法覆盖率：度量被测程序的方法执行情况，是否执行取决于方法中是否有至少一个指令被执行。 - 类覆盖率：度量计算class类文件是否被执行。
JaCoCo的一个主要优点是使用Java代理，可以动态（on-the-fly）对类进行插桩。这样代码覆盖率分析简化了预插桩过程，也无需考虑classpath的设置。但是还存在如下不适合动态插桩的情况，需要线下对字节码进行预插桩： - 运行环境不支持java agent。 - 部署环境不允许设置JVM参数。 - 字节码需要被转换成其他的虚拟机如Android Dalvik VM。 - 动态修改字节码过程中和其他agent冲突。
示例 我这个懒人还是在Building a Hypermedia-Driven RESTful Web Service示例的基础上稍作修改，熟悉一下JaCoCo的使用。
build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.X.Y.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; apply plugin: &#39;jacoco&#39; jar { baseName = &#39;hello-jacoco&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-hateoas&amp;quot;) testCompile(&amp;quot;com.jayway.jsonpath:json-path&amp;quot;) testCompile(&amp;quot;org.springframework.boot:spring-boot-starter-test&amp;quot;) }  运行 gradle test jacocoTestReport  测试结果 投入少，产出多，这种工具我喜欢。
参考 JaCoCo Java Code Coverage Library
JaCoCo Document
Gradle JaCoCo Plugin
GitHub: jacoco/jacoco
</content>
    </entry>
    
     <entry>
        <title>在Outlook 2013中查看邮件的消息头</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8outlook_2013%E4%B8%AD%E6%9F%A5%E7%9C%8B%E9%82%AE%E4%BB%B6%E7%9A%84%E6%B6%88%E6%81%AF%E5%A4%B4/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>outlook</tag><tag>2013</tag><tag>mail</tag><tag>internet</tag><tag>header</tag>
        </tags>
        <content type="html"> 双击邮件后选择File菜单，点击Porperties按钮：查看Internet headers：</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 复习Controller lifecycle</title>
        <url>https://mryqu.github.io/post/openui5_%E5%A4%8D%E4%B9%A0controller_lifecycle/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>controller</tag><tag>lifecycle</tag><tag>javascript</tag><tag>mvc</tag>
        </tags>
        <content type="html"> 昨天看到有同事添加了几个Controller的回调，对其中的onBeforeExit、beforeExit没一点印象。
sap.ui.controller(&amp;quot;kx123.foo&amp;quot;, { onInit: function () { console.info(&amp;quot;foo onInit called&amp;quot;); }, onBeforeExit: function(){ console.info(&amp;quot;foo onBeforeExit called&amp;quot;) ; } , beforeExit: function() { console.info(&amp;quot;foo beforeExit called&amp;quot;) ; }, onBeforeRendering: function() { console.info(&amp;quot;foo onBeforeRendering called&amp;quot;); }, onExit: function() { console.info(&amp;quot;foo onExit called&amp;quot;); } });  查了一下如下OpenUI5开发指南 MVC中关于Controllers的介绍。 &amp;gt; SAPUI5 provides predefined lifecycle hooks forimplementation. You can add event handlers or other functions tothe controller and the controller can fire events, for which othercontrollers or entities can register. &amp;gt; SAPUI5 provides the following lifecyclehooks: &amp;gt; - onInit(): Called when a view is instantiated and its controls(if available) have already been created; used to modify the viewbefore it is displayed to bind event handlers and do other one-timeinitialization &amp;gt; - onExit(): Called when the view is destroyed; used to freeresources and finalize activities &amp;gt; - onAfterRendering(): Called when the view has been rendered and,therefore, its HTML is part of the document; used to dopost-rendering manipulations of theHTML. SAPUI5 controls get thishook after being rendered. &amp;gt; - onBeforeRendering(): Invoked before the controller view isre-renderedand not beforethe first rendering;use onInit() for invoking thehook before the first rendering
看来sap.ui.controller确实不存在onBeforeExit、beforeExit回调。倒是sap.ui.core.mvc.View有beforeExit事件，它跟sap.ui.controller的onExit回调是捆一起的。
</content>
    </entry>
    
     <entry>
        <title>Facebook Graph API之分享数</title>
        <url>https://mryqu.github.io/post/facebook_graph_api%E4%B9%8B%E5%88%86%E4%BA%AB%E6%95%B0/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>shares</tag><tag>count</tag>
        </tags>
        <content type="html"> 现在Facebook帖子上有一个分享计数，下面这个湖南卫视的帖子可以看到总共有3个。 点进去可以查看这3个分享的具体情况： 使用Facebook Graph Explorer却可以得到分享数为7： 差4个分享，有点醉！ 其实我刚才刚刚用消息给朋友共享了这个帖子，由于是不是公开的，所以Facebook网页显示上是不计入的，但是API却是统计的。 </content>
    </entry>
    
     <entry>
        <title>Facebook Graph API之点赞数</title>
        <url>https://mryqu.github.io/post/facebook_graph_api%E4%B9%8B%E7%82%B9%E8%B5%9E%E6%95%B0/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>api</tag><tag>likes</tag>
        </tags>
        <content type="html"> 现在Facebook帖子上有一个心情计数，下面这个湖南卫视的帖子可以看到总共有293个。 通过Facebook Graph API获得的点赞数为275个。 示例里的帖子心情计数293包括点赞数275、大爱数13、笑趴数4、【未知表情计数】1。 这里可以看到点赞可以细分为点赞、大爱、笑趴、哇、心碎和怒这六种表情。 </content>
    </entry>
    
     <entry>
        <title>Facebook Graph API之主页名</title>
        <url>https://mryqu.github.io/post/facebook_graph_api%E4%B9%8B%E4%B8%BB%E9%A1%B5%E5%90%8D/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>page</tag><tag>name</tag>
        </tags>
        <content type="html"> 今天Alisa同学说搜中国湖南卫视的主页报错，我一直都只是用SASsoftware和我的YquTest做的开发测试。赶紧赶过去看了一眼，实验证明搜hntvchina好使，搜中国湖南卫视就会出错。 感觉中国湖南卫视就像是hntvchina的显示名，而SASsoftware的显示名其实是SAS software。 又和小伙伴一起创建了一个主页玩玩，让我们输入的名字最后成了显示名，而真正生成的名字将空格用&amp;rdquo;-&amp;ldquo;代替然后又加了一串数字，感觉像防止主键冲突。
</content>
    </entry>
    
     <entry>
        <title>Get comments count using Facebook Graph API</title>
        <url>https://mryqu.github.io/post/get_comments_count_using_facebook_graph_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>comments</tag><tag>summary</tag><tag>total_count</tag>
        </tags>
        <content type="html">  通过fields参数获取评论/回复个数 通过预判评论/回复个数，以决定是否发起/{object-id}/commentsAPI请求，可以显著减少API请求个数。
获取帖子的评论数 https://graph.facebook.com/{YOUR_PAGE_ID}/feed?fields=id,XXXX,likes.limit(0).summary(1),comments.limit(0).summary(1),XXXX,with_tags&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;limit=100&amp;amp;since=XXXX&amp;amp;until=XXXX&amp;amp;include_hidden=true&amp;amp;access_token={YOUR_ACCESS_TOKEN}  获取评论的回复数 https://graph.facebook.com/{YOUR_POST_ID}/comments?fields=id,from,message,created_time,like_count,comments.limit(0).summary(1)&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;limit=100&amp;amp;since=XXXX&amp;amp;until=XXXX&amp;amp;include_hidden=true&amp;amp;access_token={YOUR_ACCESS_TOKEN}  comments.summary.total_count解读 在Facebook Graph指南中/{object-id}/comments提到了total_count数值是随filter变动的。 - filter为stream，total_count为该节点下所有评论及其回复的总数； - filter为toplevel，total_count为该节点下顶层评论/回复的总数。
由于在我的使用场景中为设置filter，而其默认值为toplevel，则total_count为该节点下顶层评论/回复的总数。
</content>
    </entry>
    
     <entry>
        <title>Get likes count using fields parameters in Facebook Graph API</title>
        <url>https://mryqu.github.io/post/get_likes_count_using_fields_parameters_in_facebook_graph_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>likes</tag><tag>count</tag>
        </tags>
        <content type="html"> 原来获得Facebook帖子/评论的点赞数，需要额外单独发送一次API请求：通过对Facebook GraphAPI中fields参数添加likes.limit(0).summary(1)，仅需一次API请求就可获得帖子/评论的所有信息：</content>
    </entry>
    
     <entry>
        <title>使用SpringFox自动生成Swagger文档</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8springfox%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90swagger%E6%96%87%E6%A1%A3/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Swagger</category>
        </categories>
        <tags>
          <tag>springfox</tag><tag>swagger</tag><tag>rest</tag><tag>api</tag><tag>document</tag>
        </tags>
        <content type="html">  前面的博文Swagger实践和总结总体上探索了一下Swagger，这里着重研究Springfox。 Springfox Java库源自MartyPitt创建的swagger-springmvc项目。Swagger是一系列对RESTful接口进行规范描述和页面展示的工具，而通过Springfox将Swagger与Spring-MVC整合,可以将代码中的注解转换为符合Swagger开放API声明(OpenAPI Specification，OAS)的swagger.json文件,springfox-swagger-ui提供了将swagger.json转换为html页面的服务。
HelloSpringfox示例 尽管springfox-demos中的boot-swagger很全面了。但是对于一个写程序的人来说，不亲自写一遍，总觉得可能会有陷阱和漏洞，缺乏那么一点点自信。 我的示例是以Building a Hypermedia-Driven RESTful Web Service为基础修改的，懒人总是要找个肩膀。build.gradle jar { baseName = &#39;hello-springfox&#39; version = &#39;0.1.0&#39; } dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-hateoas&amp;quot;) compile(&amp;quot;io.springfox:springfox-swagger2:${springfoxVersion}&amp;quot;) compile(&amp;quot;io.springfox:springfox-swagger1:${springfoxVersion}&amp;quot;) compile(&amp;quot;io.springfox:springfox-swagger-ui:${springfoxVersion}&amp;quot;) testCompile(&amp;quot;com.jayway.jsonpath:json-path&amp;quot;) testCompile(&amp;quot;org.springframework.boot:spring-boot-starter-test&amp;quot;) }  Application.java package com.yqu.hellospringfox; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } }  Greeting.java package com.yqu.hellospringfox; import org.springframework.hateoas.ResourceSupport; import com.fasterxml.jackson.annotation.JsonCreator; import com.fasterxml.jackson.annotation.JsonProperty; public class Greeting extends ResourceSupport { private final String content; @JsonCreator public Greeting(@JsonProperty(&amp;quot;content&amp;quot;) String content) { this.content = content; } public String getContent() { return content; } }  GreetingController.java package com.yqu.hellospringfox; import static org.springframework.hateoas.mvc.ControllerLinkBuilder.*; import io.swagger.annotations.Api; import io.swagger.annotations.ApiOperation; import io.swagger.annotations.ApiParam; import io.swagger.annotations.ApiResponse; import org.springframework.http.HttpEntity; import org.springframework.http.HttpStatus; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; @Api(value=&amp;quot;Greeting Controller&amp;quot;) @RestController public class GreetingController { private static final String TEMPLATE = &amp;quot;Hello, %s!&amp;quot;; @ApiOperation(value = &amp;quot;Get greeting information&amp;quot;, httpMethod = &amp;quot;GET&amp;quot;, produces = &amp;quot;application/json&amp;quot;) @ApiResponse(code = 200, message = &amp;quot;OK&amp;quot;, response = Greeting.class) @RequestMapping(&amp;quot;/greeting&amp;quot;) public HttpEntity&amp;lt;Greeting&amp;gt; greeting( @ApiParam(name = &amp;quot;name&amp;quot;, required = true, value = &amp;quot;User Name&amp;quot;) @RequestParam(value = &amp;quot;name&amp;quot;, required = false, defaultValue = &amp;quot;World&amp;quot;) String name) { Greeting greeting = new Greeting(String.format(TEMPLATE, name)); greeting.add(linkTo(methodOn(GreetingController.class). greeting(name)).withSelfRel()); return new ResponseEntity(greeting, HttpStatus.OK); } }  MySwaggerConfig.java package com.yqu.hellospringfox; import com.google.common.base.Predicate; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration; import springfox.documentation.annotations.ApiIgnore; import springfox.documentation.builders.ApiInfoBuilder; import springfox.documentation.service.ApiInfo; import springfox.documentation.spi.DocumentationType; import springfox.documentation.spring.web.plugins.Docket; import springfox.documentation.swagger1.annotations.EnableSwagger; import springfox.documentation.swagger2.annotations.EnableSwagger2; import static springfox.documentation.builders.PathSelectors.regex; @Configuration @EnableSwagger //Enable swagger 1.2 spec @EnableSwagger2 //Enable swagger 2.0 spec @ComponentScan(&amp;quot;hello.GreetingController&amp;quot;) public class MySwaggerConfig { @Bean public Docket greetingApi() { return new Docket(DocumentationType.SWAGGER_2) .groupName(&amp;quot;greeting-api&amp;quot;) .apiInfo(apiInfo()) .select() .paths(greetingPaths()) .build() .ignoredParameterTypes(ApiIgnore.class) .enableUrlTemplating(true); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(&amp;quot;mryqu&#39;s REST API Demo&amp;quot;) .description(&amp;quot;Just a SpringFox demo&amp;quot;) .contact(&amp;quot;mryqu&amp;quot;) .license(&amp;quot;Apache License Version 2.0&amp;quot;) .licenseUrl(&amp;quot;https://github.com/mryqu/&amp;quot;) .version(&amp;quot;2.0&amp;quot;) .build(); } private Predicate&amp;lt;String&amp;gt; greetingPaths() { return regex(&amp;quot;/greeting&amp;quot;); } }  application.properties server.contextPath=/hellospringfox management.security.enabled=false  测试 首先测试gretting API功能：/swagger-ui.html界面：/v2/api-docs?group=greeting-api：Springfox用法 Docket对象为Springfox提供配置信息，ApiInfo为生成的文档提供了元数据信息。 常用的几个用于生成文档的注解如下: - @Api 表示该类是一个Swagger的Resource, 是对Controller进行注解的 - @ApiOperation 表示对应一个RESTful接口, 对方法进行注解 - @ApiResponse 表示对不同 HTTP 状态码的意义进行描述 - @ApiParam 表示对传入参数进行注解
参考 SpringFox
GitHub: springfox/springfox
</content>
    </entry>
    
     <entry>
        <title>了解PhantomJS</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3phantomjs/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>phantomjs</tag>
        </tags>
        <content type="html">  今天看到我们的项目依赖PhantomJS，就稍作了解。
PhantomJS 是什么? 官方介绍如下： &amp;gt; PhantomJS is a headless WebKit scriptable with a JavaScript API. It has fast and native support for various web standards: DOM handling, CSS selector, JSON, Canvas, and SVG.
PhantomJS是无需浏览器基于WebKit的全Web栈，支持JS解析引擎、渲染引擎、请求处理等，但是不包括显示和用户交互页面。
PhantomJS的使用场景  无浏览器网站测试：支持使用Jasmine、QUnit、Mocha、Capybara、WebDriver等框架进行功能测试。 页面截屏：抓取页面内容，包括SVG和Canvas。 页面自动化：使用标准DOMAPI或jQuery等通用库访问和操作网页。 网页监控：监控网页加载和导出成标准HAR文件。让使用YSlow和Jenkins的性能分析自动化。  大概可以估计出PhantomJS的作用了，应该是用于单元测试吧。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] sap.ui.model.SimpleType及其子类中的约束</title>
        <url>https://mryqu.github.io/post/openui5_sap.ui.model.simpletype%E5%8F%8A%E5%85%B6%E5%AD%90%E7%B1%BB%E4%B8%AD%E7%9A%84%E7%BA%A6%E6%9D%9F/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>model</tag><tag>type</tag><tag>constraints</tag><tag>javascript</tag>
        </tags>
        <content type="html">  对OpenUI5模型中的数据项如何设置类型，如何设置最大最小值等约束呢？这一切可以通过研究sap.ui.model.SimpleType及其子类获得答案。
sap.ui.model.SimpleType类图 SimpleType子类Integer约束测试 下面的示例中有两个sap.m.Input控件，第一个为文本类型输入没有约束，第二个整数类型输入有约束：
that.oNameInput = new Input({ id: sFormId&#43;&amp;quot;-name&amp;quot;, type: sap.m.InputType.Text, value: &amp;quot;{/name}&amp;quot;, layoutData: new GridData({span: &amp;quot;L3 M5 S6&amp;quot;}) }); that.oCountInput = new Input({ id: sFormId&#43;&amp;quot;-count&amp;quot;, type: sap.m.InputType.Number, value: { path:&#39;/count&#39;, type: &#39;sap.ui.model.type.Integer&#39;, constraints: { minimum : 1, maximum : 50 } }, placeholder: &amp;quot;(1-50)&amp;quot;, layoutData: new GridData({span: &amp;quot;L3 M5 S6&amp;quot;}) });  完整示例代码：二者调试信息的差异：一个仅指定了映射路径；另一个除了指定映射路径外，明确指定了模型数据项类型及约束。
测试结果 that.oCountInput施加了范围1到50的约束。如果输入值在范围内，则界面和模型中的count值都会改变；如果输入值不再范围内，则模型中的count值保留上一有效值，而界面发生改变且无告警。 调试堆栈如下：
PropertyBinding.setExternalValue (sap-ui-core-dbg.js:57174) ManagedObject.updateModelProperty (sap-ui-core-dbg.js:34286) ManagedObject.setProperty (sap-ui-core-dbg.js:32531) InputBase.setProperty (InputBase-dbg.js:690) InputBase.setValue (InputBase-dbg.js:1007) Input.setValue (Input-dbg.js:1792) InputBase.onChange (InputBase-dbg.js:467) InputBase.onsapfocusleave (InputBase-dbg.js:426) Input.onsapfocusleave (Input-dbg.js:827) Element._handleEvent (sap-ui-core-dbg.js:45792) UIArea._handleEvent (sap-ui-core-dbg.js:50984) triggerFocusleave (sap-ui-core-dbg.js:47117) FocusHandler.checkForLostFocus (sap-ui-core-dbg.js:47062) (anonymous function) (sap-ui-core-dbg.js:26472)  PropertyBinding的setExternalValue代码如下，它会先解析输入值，然后调用类型的validateValue函数检查输入值有效性，通过sap.ui.model.type.Integer源代码可知无效值会抛出异常，从而导致数据模型和界面都不会更新。
PropertyBinding.prototype.setExternalValue = function(oValue) { // formatter doesn&#39;t support two way binding if (this.fnFormatter) { jQuery.sap.log.warning(&amp;quot;Tried to use twoway binding, but a formatter function is used&amp;quot;); return; } var oDataState = this.getDataState(); try { if (this.oType) { oValue = this.oType.parseValue(oValue, this.sInternalType); this.oType.validateValue(oValue); } } catch (oException) { oDataState.setInvalidValue(oValue); this.checkDataState(); //data ui state is dirty inform the control throw oException; } // if no type specified set value directly oDataState.setInvalidValue(null); this.setValue(oValue); };  参考 sap.ui.model.SimpleType源代码
sap.ui.model.Type源代码
sap.ui.model.type.Integer源代码
sap.ui.model.SimpleType jsDoc
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 使用DateRangeSelection</title>
        <url>https://mryqu.github.io/post/openui5_%E4%BD%BF%E7%94%A8daterangeselection/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>daterangeselection</tag><tag>javascript</tag><tag>html5</tag><tag>web</tag>
        </tags>
        <content type="html"> 今天使用了DateRangeSelection来选择日期范围。 DateRangeSelection范例
sap.m.DateRangeSelection jsDoc
DateRangeSelection源代码
sap.ui.core.format.DateFormat jsDoc
Working with Dates in Sapui5
sap.ui.core.format.DateFormat
通过阅读上述资料，DateRangeSelection内存储的起始、结束时间为Date类。可以通过设置displayFormat和delimiter来改变界面上日期的表现形式；不支持valueFormat，因此只能通过getDateValue()、getSecondDateValue()获取Date对象，然后通过DateFormat获得相应格式化的日期字符串。 DateRangeSelection最新版代码提供了setMinDate()和setMaxDate()函数，但是jsDoc还没有体现，我司目前所用的OpenUI5版本还不支持。
</content>
    </entry>
    
     <entry>
        <title>Use proxy on RestFB</title>
        <url>https://mryqu.github.io/post/use_proxy_on_restfb/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>restfb</tag><tag>proxy</tag><tag>facebook</tag><tag>defaultfacebookclien</tag><tag>defaultwebrequestor</tag>
        </tags>
        <content type="html"> 曾经有人向RestFB开过issue（https://github.com/restfb/restfb/issues/116）询问如何给其设置代理，issue里回复扩展DefaultWebRequestor。下面的代码基于该方案并测试通过。
package com.yqu.restfb; import com.restfb.DefaultFacebookClient; import com.restfb.DefaultJsonMapper; import com.restfb.FacebookClient; import com.restfb.Version; import java.io.IOException; import java.net.HttpURLConnection; import java.net.InetSocketAddress; import java.net.Proxy; import java.net.URL; public class HelloRestFBWithProxy { private static class DefaultWebRequestor extends com.restfb.DefaultWebRequestor { protected HttpURLConnection openConnection(URL url) throws IOException { HttpURLConnection httpURLConnection = null; if (useProxyFlag.booleanValue()) { InetSocketAddress proxyLocation = new InetSocketAddress( hostName, port); Proxy proxy = new Proxy(Proxy.Type.HTTP, proxyLocation); try { httpURLConnection = (HttpURLConnection) url .openConnection(proxy); return httpURLConnection; } catch (Exception e) { return (HttpURLConnection) url.openConnection(); } } else { return (HttpURLConnection) url.openConnection(); } } private Boolean useProxyFlag; private String hostName; private int port; protected DefaultWebRequestor(Boolean useProxyFlagIn, String hostNameIn, int portIn) { useProxyFlag = useProxyFlagIn; hostName = hostNameIn; port = portIn; } } public static void main(String[] args) { String accessToken = &amp;quot;XXXX&amp;quot;; String appSecret = &amp;quot;XXXX&amp;quot;; String proxyHost = &amp;quot;XXXX&amp;quot;; int proxyPort = 80; FacebookClient facebookClient = new DefaultFacebookClient(accessToken, appSecret, new DefaultWebRequestor( true, proxyHost, proxyPort), new DefaultJsonMapper(), Version.UNVERSIONED); } }  </content>
    </entry>
    
     <entry>
        <title>在Linux终端下启动SAS管理控制台</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8linux%E7%BB%88%E7%AB%AF%E4%B8%8B%E5%90%AF%E5%8A%A8sas%E7%AE%A1%E7%90%86%E6%8E%A7%E5%88%B6%E5%8F%B0/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>sas</tag><tag>sasmanagementconsole</tag><tag>linux</tag><tag>x11forward</tag>
        </tags>
        <content type="html"> 在Linux终端下没有X11显示系统的情况下，可以通过如下命令将X11转移到其他X视窗服务器以显示SMC界面：
$ export DISPLAY=[machine]:[port] $ pwd /local/install/SASServer/SASHome/SASManagementConsole/9.4 $ ./sasmc  </content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 给twitcurl添加访问频次限制信息获取功能</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E7%BB%99twitcurl%E6%B7%BB%E5%8A%A0%E8%AE%BF%E9%97%AE%E9%A2%91%E6%AC%A1%E9%99%90%E5%88%B6%E4%BF%A1%E6%81%AF%E8%8E%B7%E5%8F%96%E5%8A%9F/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>twitcurl</tag><tag>ratelimit</tag><tag>x-rate-limit-remaining</tag><tag>x-rate-limit-limit</tag><tag>x-rate-limit-reset</tag>
        </tags>
        <content type="html"> 在我之前的博文Twitter API访问频次限制处理中，描述了Twitter API访问频次限制及Twitter4J对其处理。twitcurl项目并没有这样的功能，今天我将getLastRateLimitStatus功能添加到了twitcurl。 通过添加如下代码，我可以获取响应头信息：
curl_easy_setopt( m_curlHandle, CURLOPT_HEADERFUNCTION, curlHeaderCallback ); curl_easy_setopt( m_curlHandle, CURLOPT_HEADERDATA, this );  输出的调试信息如下：
Enter string to search: va Limit search results to: 2 twitCurl::curlHeaderCallback headers: HTTP/1.1 200 OK twitCurl::curlHeaderCallback headers: cache-control: no-cache, no-store, must-revalidate, pre-check=0, post-check=0 twitCurl::curlHeaderCallback headers: content-disposition: attachment; filename=json.json twitCurl::curlHeaderCallback headers: content-encoding: gzip twitCurl::curlHeaderCallback headers: content-length: 1301 twitCurl::curlHeaderCallback headers: content-type: application/json;charset=utf-8 twitCurl::curlHeaderCallback headers: date: Thu, 24 Mar 2016 04:59:41 GMT twitCurl::curlHeaderCallback headers: expires: Tue, 31 Mar 1981 05:00:00 GMT twitCurl::curlHeaderCallback headers: last-modified: Thu, 24 Mar 2016 04:59:41 GMT twitCurl::curlHeaderCallback headers: pragma: no-cache twitCurl::curlHeaderCallback headers: server: tsa_b twitCurl::curlHeaderCallback headers: set-cookie: guest_id=v1:145879558114535127; Domain=.twitter.com; Path=/; Expires=Sat, 24-Mar-2018 04:59:41 UTC twitCurl::curlHeaderCallback headers: status: 200 OK twitCurl::curlHeaderCallback headers: strict-transport-security: max-age=631138519 twitCurl::curlHeaderCallback headers: x-access-level: read twitCurl::curlHeaderCallback headers: x-connection-hash: 3d0de7831d3affd43bd8b05ca7af554b twitCurl::curlHeaderCallback headers: x-content-type-options: nosniff twitCurl::curlHeaderCallback headers: x-frame-options: SAMEORIGIN twitCurl::curlHeaderCallback headers: x-rate-limit-limit: 180 twitCurl::curlHeaderCallback headers: x-rate-limit-remaining: 178 twitCurl::curlHeaderCallback headers: x-rate-limit-reset: 1458796157 twitCurl::curlHeaderCallback headers: x-response-time: 64 twitCurl::curlHeaderCallback headers: x-transaction: 9a3d28454155b172 twitCurl::curlHeaderCallback headers: x-twitter-response-tags: BouncerCompliant twitCurl::curlHeaderCallback headers: x-xss-protection: 1; mode=block twitCurl::curlHeaderCallback headers: twitterClient:: twitCurl::search web response:200 {&amp;quot;statuses&amp;quot;:[{&amp;quot;created_at&amp;quot;:&amp;quot;Thu Mar 24 04:59:19 &#43;0000 2016&amp;quot;,&amp;quot;id&amp;quot;:712866360888573952,&amp;quot;id_str&amp;quot;:&amp;quot;712866360888573952&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;@laurie6805 @suzannecgordon The VA forces vets to the VA.No choice if less 40 miles??Many vets want outside docs,VAsaysNO!&amp;quot;,&amp;quot;entities&amp;quot;:{&amp;quot;hashtags&amp;quot;:[],&amp;quot;symbols&amp;quot;:[],&amp;quot;user_mentions&amp;quot;:[{&amp;quot;screen_name&amp;quot;:&amp;quot;laurie6805&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Laurie Anne&amp;quot;,&amp;quot;id&amp;quot;:3093907031,&amp;quot;id_str&amp;quot;:&amp;quot;3093907031&amp;quot;,&amp;quot;indices&amp;quot;:[0,11]},{&amp;quot;screen_name&amp;quot;:&amp;quot;suzannecgordon&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Suzanne Gordon&amp;quot;,&amp;quot;id&amp;quot;:41686321,&amp;quot;id_str&amp;quot;:&amp;quot;41686321&amp;quot;,&amp;quot;indices&amp;quot;:[12,27]}],&amp;quot;urls&amp;quot;:[]},&amp;quot;truncated&amp;quot;:false,&amp;quot;metadata&amp;quot;:{&amp;quot;iso_language_code&amp;quot;:&amp;quot;en&amp;quot;,&amp;quot;result_type&amp;quot;:&amp;quot;recent&amp;quot;},&amp;quot;source&amp;quot;:&amp;quot;\u003ca href=&amp;quot;https:\/\/mobile.twitter.com&amp;quot; rel=&amp;quot;nofollow&amp;quot;\u003eMobile Web (M2)\u003c\/a\u003e&amp;quot;,&amp;quot;in_reply_to_status_id&amp;quot;:712842279040319489,&amp;quot;in_reply_to_status_id_str&amp;quot;:&amp;quot;712842279040319489&amp;quot;,&amp;quot;in_reply_to_user_id&amp;quot;:3093907031,&amp;quot;in_reply_to_user_id_str&amp;quot;:&amp;quot;3093907031&amp;quot;,&amp;quot;in_reply_to_screen_name&amp;quot;:&amp;quot;laurie6805&amp;quot;,&amp;quot;user&amp;quot;:{&amp;quot;id&amp;quot;:3324233309,&amp;quot;id_str&amp;quot;:&amp;quot;3324233309&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;VeteransStimulusAct&amp;quot;,&amp;quot;screen_name&amp;quot;:&amp;quot;VetStimulusAct&amp;quot;,&amp;quot;location&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;description&amp;quot;:&amp;quot;https:\/\/t.co\/MMhwMHlDdU&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;https:\/\/t.co\/sasbgdAmi2&amp;quot;,&amp;quot;entities&amp;quot;:{&amp;quot;url&amp;quot;:{&amp;quot;urls&amp;quot;:[{&amp;quot;url&amp;quot;:&amp;quot;https:\/\/t.co\/sasbgdAmi2&amp;quot;,&amp;quot;expanded_url&amp;quot;:&amp;quot;https:\/\/www.facebook.com\/decreaseVAbacklognow&amp;quot;,&amp;quot;display_url&amp;quot;:&amp;quot;facebook.com\/decreaseVAback\u2026&amp;quot;,&amp;quot;indices&amp;quot;:[0,23]}]},&amp;quot;description&amp;quot;:{&amp;quot;urls&amp;quot;:[{&amp;quot;url&amp;quot;:&amp;quot;https:\/\/t.co\/MMhwMHlDdU&amp;quot;,&amp;quot;expanded_url&amp;quot;:&amp;quot;https:\/\/www.change.org\/p\/president-obama-veterans-stimulus-act-adjusted-compensation-payment-act-ii-bonus-act-ii?just_created=true&amp;quot;,&amp;quot;display_url&amp;quot;:&amp;quot;change.org\/p\/president-ob\u2026&amp;quot;,&amp;quot;indices&amp;quot;:[0,23]}]}},&amp;quot;protected&amp;quot;:false,&amp;quot;followers_count&amp;quot;:1065,&amp;quot;friends_count&amp;quot;:3236,&amp;quot;listed_count&amp;quot;:14,&amp;quot;created_at&amp;quot;:&amp;quot;Sun Jun 14 04:55:38 &#43;0000 2015&amp;quot;,&amp;quot;favourites_count&amp;quot;:653,&amp;quot;utc_offset&amp;quot;:null,&amp;quot;time_zone&amp;quot;:null,&amp;quot;geo_enabled&amp;quot;:false,&amp;quot;verified&amp;quot;:false,&amp;quot;statuses_count&amp;quot;:3326,&amp;quot;lang&amp;quot;:&amp;quot;en&amp;quot;,&amp;quot;contributors_enabled&amp;quot;:false,&amp;quot;is_translator&amp;quot;:false,&amp;quot;is_translation_enabled&amp;quot;:false,&amp;quot;profile_background_color&amp;quot;:&amp;quot;C0DEED&amp;quot;,&amp;quot;profile_background_image_url&amp;quot;:&amp;quot;http:\/\/abs.twimg.com\/images\/themes\/theme1\/bg.png&amp;quot;,&amp;quot;profile_background_image_url_https&amp;quot;:&amp;quot;https:\/\/abs.twimg.com\/images\/themes\/theme1\/bg.png&amp;quot;,&amp;quot;profile_background_tile&amp;quot;:false,&amp;quot;profile_image_url&amp;quot;:&amp;quot;http:\/\/pbs.twimg.com\/profile_images\/609994141632733184\/u3bC1h8j_normal.jpg&amp;quot;,&amp;quot;profile_image_url_https&amp;quot;:&amp;quot;https:\/\/pbs.twimg.com\/profile_images\/609994141632733184\/u3bC1h8j_normal.jpg&amp;quot;,&amp;quot;profile_banner_url&amp;quot;:&amp;quot;https:\/\/pbs.twimg.com\/profile_banners\/3324233309\/1434268816&amp;quot;,&amp;quot;profile_link_color&amp;quot;:&amp;quot;0084B4&amp;quot;,&amp;quot;profile_sidebar_border_color&amp;quot;:&amp;quot;C0DEED&amp;quot;,&amp;quot;profile_sidebar_fill_color&amp;quot;:&amp;quot;DDEEF6&amp;quot;,&amp;quot;profile_text_color&amp;quot;:&amp;quot;333333&amp;quot;,&amp;quot;profile_use_background_image&amp;quot;:true,&amp;quot;has_extended_profile&amp;quot;:false,&amp;quot;default_profile&amp;quot;:true,&amp;quot;default_profile_image&amp;quot;:false,&amp;quot;following&amp;quot;:false,&amp;quot;follow_request_sent&amp;quot;:false,&amp;quot;notifications&amp;quot;:false},&amp;quot;geo&amp;quot;:null,&amp;quot;coordinates&amp;quot;:null,&amp;quot;place&amp;quot;:null,&amp;quot;contributors&amp;quot;:null,&amp;quot;is_quote_status&amp;quot;:false,&amp;quot;retweet_count&amp;quot;:0,&amp;quot;favorite_count&amp;quot;:0,&amp;quot;favorited&amp;quot;:false,&amp;quot;retweeted&amp;quot;:false,&amp;quot;lang&amp;quot;:&amp;quot;en&amp;quot;}],&amp;quot;search_metadata&amp;quot;:{&amp;quot;completed_in&amp;quot;:0.036,&amp;quot;max_id&amp;quot;:712866360888573952,&amp;quot;max_id_str&amp;quot;:&amp;quot;712866360888573952&amp;quot;,&amp;quot;query&amp;quot;:&amp;quot;va&amp;quot;,&amp;quot;refresh_url&amp;quot;:&amp;quot;?since_id=712866360888573952&amp;amp;amp;q=va&amp;amp;amp;lang=en&amp;amp;amp;include_entities=1&amp;quot;,&amp;quot;count&amp;quot;:2,&amp;quot;since_id&amp;quot;:0,&amp;quot;since_id_str&amp;quot;:&amp;quot;0&amp;quot;}}  通过调试信息可知，curlHeaderCallback回调会被多次调用，每次仅处理一个响应头。 对于Twitter搜索来说，15时间窗允许180次调用（见x-rate-limit-limit），调用二次后剩余178次调用（见x-rate-limit-remaining），时间窗复位时间为Thu Mar 24 01:09:17 EDT 2016（通过x-rate-limit-reset由如下Java代码获得）。
long now = System.currentTimeMillis(); long reset = 1458796157*1000L; System.out.println(&amp;quot;reset:&amp;quot;&#43;reset); System.out.println(&amp;quot;now :&amp;quot;&#43;now); Date d = new Date(reset); System.out.println(&amp;quot;reset:&amp;quot;&#43;d); d = new Date(now); System.out.println(&amp;quot;now :&amp;quot;&#43;d);  经过调试，getLastRateLimitStatus功能得以实现，我的代码提交到了https://github.com/mryqu/twitcurl上。
</content>
    </entry>
    
     <entry>
        <title>遇到&#34;vagrant up&#34;无法读取box元数据问题</title>
        <url>https://mryqu.github.io/post/%E9%81%87%E5%88%B0vagrant_up%E6%97%A0%E6%B3%95%E8%AF%BB%E5%8F%96box%E5%85%83%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>version</tag><tag>constraint</tag><tag>box_url</tag><tag>issue</tag>
        </tags>
        <content type="html"> 昨天在某个Windows Server2008上升级了Vagrant (1.8.1)和VitualBox(5.0.16)，然后vagrant up就开始出问题了。
C:\foo&amp;gt;vagrant up Bringing machine &#39;xfoo&#39; up with &#39;virtualbox&#39; provider... ==&amp;gt; xfoo: Box ‘foo-ubuntu‘ could not be found. Attempting to find and install... xfoo: Box Provider: virtualbox xfoo: Box Version: &amp;gt;= 0.12.0, &amp;lt; 1.0.0 ==&amp;gt; xfoo: Box file was not detected as metadata. Adding it directly... You specified a box version constraint with a direct box file path. Box version constraints only work with boxes from Vagrant Cloud or a custom box host. Please remove the version constraint and try again.  打开调试信息，发现：
C:\foo&amp;gt;vagrant up --debug INFO global: Vagrant version: 1.8.1 INFO global: Ruby version: 2.2.3 INFO global: RubyGems version: 2.4.5.1 ...... INFO box_add: Downloading box: http://vagrant.xxx.com/xxx/foo-ubuntu.json =&amp;gt; C:/Users/xxx/.vagrant.d/tmp/box23c35c4058493b8228b6114d4cfdbbc0a5b597c2  这个问题跟下列已知问题类似，都是curl获取不到box的元数据。 - error: You specified a box version constraint with a direct box file path #165 - Vagrant cannot fetch coreos_production_vagrant.json #236 - Issue downloading json file from box_url #4647 - vagrant box add should handle 404 errors when downloading a remote box or box.json file #5332
最后，我把Vagrant降级到1.7.4，这个问题不再复现！
</content>
    </entry>
    
     <entry>
        <title>升级Windows版Git客户端</title>
        <url>https://mryqu.github.io/post/%E5%8D%87%E7%BA%A7windows%E7%89%88git%E5%AE%A2%E6%88%B7%E7%AB%AF/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>msysgit</tag><tag>gitextensions</tag><tag>漏洞</tag><tag>升级</tag>
        </tags>
        <content type="html"> 今天在公司收到通知，说Git又有安全漏洞了，需要升级到最新的2.7.3。扫了一眼Get ready to patch Git servers, clients – nasty-looking bugs surface，漏洞涉及到2.x、1.9和1.7版本。 我本机装的是git version 1.9.5.msysgit.0，这才发现MsysGit去年八月份就被Git for Windows 2.x取代了。这次顺手把Git Extensions也给升级到2.48.05了。
</content>
    </entry>
    
     <entry>
        <title>[HBase] 使用HBase Shell时遇到ZooKeeper exists failed after 4 attempts错误</title>
        <url>https://mryqu.github.io/post/hbase_%E4%BD%BF%E7%94%A8hbase_shell%E6%97%B6%E9%81%87%E5%88%B0zookeeper_exists_failed_after_4_attempts%E9%94%99%E8%AF%AF/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>shell</tag><tag>zookeeper</tag><tag>exist</tag><tag>failed</tag>
        </tags>
        <content type="html"> 今天打开HBase Shell就闪退，可是前两天还好好的。错误如下：
2016-03-05 00:32:23,597 ERROR [main] zookeeper.RecoverableZooKeeper: ZooKeeper exists failed after 4 attempts 2016-03-05 00:32:23,598 WARN [main] zookeeper.ZKUtil: hconnection-0x2dba911d0x0, quorum=node50064.mryqu.com:2181,node50069.mryqu.com:2181,node51054.mryqu.com:2181, baseZNode=/hbase Unable to set watcher on znode (/hbase/hbaseid) org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid at org.apache.zookeeper.KeeperException.create(KeeperException:99) at org.apache.zookeeper.KeeperException.create(KeeperException:51) at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper:1045) at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper:220) at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil:419) at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId:65) at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry:105) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager:905) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&amp;lt;init&amp;gt;(ConnectionManager:648) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl:45) at at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory:238) at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory:218) at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory:119) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl:43) at at org.jrubysupportMethod.invokeDirectWithExceptionHandling( at org.jrubysupportMethod.invokeStaticDirect( at org.jruby.invokers.StaticMethodInvoker.call(StaticMethodInvoker:58) at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite:312) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite:169) at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode:57) at org.jruby.ast.InstAsgnNode.interpret(InstAsgnNode:95) at org.jruby.ast.NewlineNode.interpret(NewlineNode:104) at org.jruby.ast.BlockNode.interpret(BlockNode:71) at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter:74) at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod:169) at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod:191) at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite:302) at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite:144) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite:148) at org.jruby.RubyClass.newInstance(RubyClass:822) at org.jruby.RubyClass$i$newInstance.call(RubyClass$i$newInstance.gen:65535) at org.jruby.internal.runtime.methodsMethod$ at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite:292) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite:135) at usr.local.hbase.bin.hirb.__file__(/usr/local/hbase/bin/hirb.rb:131) at usr.local.hbase.bin.hirb.load(/usr/local/hbase/bin/hirb.rb) at org.jruby.Ruby.runScript(Ruby:697) at org.jruby.Ruby.runScript(Ruby:690) at org.jruby.Ruby.runNormally(Ruby:597) at org.jruby.Ruby.runFromMain(Ruby:446) at org.jruby.Main.doRunFromMain(Main:369) at org.jruby.Main.internalRun(Main:258) at org.jruby.Main.run(Main:224) at org.jruby.Main.run(Main:208) at org.jruby.Main.main(Main:188)  首先通过jps查看JVM进程信息，没有发现QuorumPeerMain进程。看来是ZooKeeper出了问题，查看了zookeeper.out中的日志，顺着线索发现myid文件内容莫名其妙地跟我原来设定的ID不一样了。改正后，解决问题。
</content>
    </entry>
    
     <entry>
        <title>[HBase] 才发现HBase REST服务占用的是8080端口</title>
        <url>https://mryqu.github.io/post/hbase_%E6%89%8D%E5%8F%91%E7%8E%B0hbase_rest%E6%9C%8D%E5%8A%A1%E5%8D%A0%E7%94%A8%E7%9A%84%E6%98%AF8080%E7%AB%AF%E5%8F%A3/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>rest</tag><tag>service</tag><tag>port</tag><tag>8080</tag>
        </tags>
        <content type="html">  今天用一下Tomcat，结果发现8080端口还被占了，谁呀？ 竟然是HBase REST服务占用的！！看了一下Ports Used by Components of CDH 5，发现ClouderaCDH里是这么用的： - 8080：Non- Cloudera Manager - managed HBase REST Service - 20550：Cloudera Manager - managed HBase REST Service - 8085：HBase REST UI
8080端口还是留着吧，对hbase-site.xml做如下修改：重启HBase REST服务：
hbase-daemon.sh stop rest hbase-daemon.sh start rest  通过HBase REST UI检查，REST服务端口改成了20550：另一种修改REST服务端口的方式是在启动HBase REST服务命令时通过-p选项直接指定端口。例如：
hbase-daemon.sh start rest -p 20550  参考 Linux – Which application is using port 8080
Configuring and Using the HBase REST API
Ports Used by Components of CDH 5
</content>
    </entry>
    
     <entry>
        <title>[Zookeeper] 运行Zookeeper REST服务实践</title>
        <url>https://mryqu.github.io/post/zookeeper_%E8%BF%90%E8%A1%8Czookeeper_rest%E6%9C%8D%E5%8A%A1%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Zookeeper</category>
        </categories>
        <tags>
          <tag>zookeeper</tag><tag>rest</tag><tag>service</tag><tag>curl</tag><tag>zk_dump_tree.py</tag>
        </tags>
        <content type="html">  Zookeeper REST服务介绍 通常我们应该使用Java/C客户端绑定访问ZooKeeper服务器。不过由于大多数语言内建支持基于HTTP的协议，RESTZooKeeper网关还是很有用的。ZooKeeper REST实现使用Jersey JAX-RS，其REST绑定参考SPEC.txt。其中org.apache.zookeeper.server.jersey.resources.ZNodeResource是项目的核心类，提供Http请求方式对ZooKeeper节点的添加、修改、查询和删除功能，以xml方式返回数据；org.apache.zookeeper.server.jersey.RestMain提供主函数入口。
以Ant脚本方式启动 这是GitHub：apache/zookeeper - REST implementation中介绍的方式。
cd $ZOOPEEPER_HOME ant cd src/contrib/rest nohup ant run&amp;amp;  如果仅是临时运行一下REST服务，ant run即可。 通过nohug提交作业可以确保在退出控制台后ZookeeperREST服务仍在后台运行。当需要关闭时，通过jobs命令查找当前所有运行的作业，通过fg [job_spec]命令关闭作业。 以rest.sh方式启动 cd $ZOOKEEPER_HOME mkdir src/contrib/rest/lib cp build/contrib/rest/zookeeper-dev-rest.jar src/contrib/rest/lib/ cp build/contrib/rest/lib/*.jar src/contrib/rest/lib/ cp zookeeper-3.4.X.jar src/contrib/rest/lib/ cp src/java/lib/*.jar src/contrib/rest/lib/  启动
cd $ZOOKEEPER_HOME/src/contrib/rest ./rest.sh start  停止
cd $ZOOKEEPER_HOME/src/contrib/rest ./rest.sh stop  查看日志
cd $ZOOKEEPER_HOME/src/contrib/rest tail -f zkrest.log  测试 将我的Zookeeper从node50064复制到node50069和node51054上，分别在三台机器上启动Zookeeper和ZookeeperREST服务。
访问application.wadl 获取根节点数据 获取根节点的子节点 导出节点及znode层次数据 参考 GitHub：apache/zookeeper - REST implementation
Zookeeper开启Rest服务(3.4.6)
Hue（五）集成Zookeeper
New ZooKeeper Browser app!
zookeeper运维
GitHub：phunt/zookeeper_dashboard
安装HBase 1.2.x &#43; ZooKeeper 3.4.x 集群
</content>
    </entry>
    
     <entry>
        <title>[HBase] 安装HBase 1.2.x &#43; ZooKeeper 3.4.x 集群</title>
        <url>https://mryqu.github.io/post/hbase_%E5%AE%89%E8%A3%85hbase_1.2.x_&#43;_zookeeper_3.4.x_%E9%9B%86%E7%BE%A4/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>hbase</tag><tag>zookeeper</tag><tag>install</tag>
        </tags>
        <content type="html">  安装HBase，首先需要参考一下The versions of Hadoop supported with each version of HBase，以便确定Hadoop和HBase各自的版本。
集群规划 |节点|角色 |&amp;mdash;&amp;ndash; |node50064|NameNode RessourceManager QuorumPeerMain HMaster |node50069|Datanode SecondNameNode QuorumPeerMain HMasterHRegionServer |node51054|Datanade QuorumPeerMain HRegionServer
ZooKeeper在HBase集群中的作用  HBase regionserver向ZooKeeper注册，提供HBase regionserver状态信息（是否在线） HMaster启动时候会将HBase 系统表-ROOT-加载到ZooKeeper集群，通过zookeeper集群可以获取当前系统表.META.的存储所对应的regionserver信息。  HMaster主要作用在于，通过HMaster维护系统表-ROOT-,.META.，记录regionserver所对应region变化信息。此外还负责监控处理当前HBase集群中regionserver状态变化信息。
Zookeeper安装 （在node50064上）下载并配置ZooKeeper wget http://apache.mirrors.tds.net/zookeeper/zookeeper-3.4.x/zookeeper-3.4.x.tar.gz tar -xzf zookeeper-3.4.x.tar.gz sudo mv zookeeper-3.4.x /usr/local/zookeeper sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/zookeeper sudo mkdir /var/log/zookeeper sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /var/log/zookeeper sudo mkdir /var/lib/zookeeper sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /var/lib/zookeeper cd /var/lib/zookeeper touch myid echo 1 &amp;gt; myid cd /usr/local/zookeeper/conf  通过cpzoo_sample.cfg zoo.cfg，生成ZooKeeper配置文件：
# The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. **dataDir=/var/lib/zookeeper** # the port at which the clients will connect clientPort=2181 # the maximum number of client connections. # increase this if you need to handle more clients #maxClientCnxns=60 # # Be sure to read the maintenance section of the # administrator guide before turning on autopurge. # # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance # # The number of snapshots to retain in dataDir #autopurge.snapRetainCount=3 # Purge task interval in hours # Set to &amp;quot;0&amp;quot; to disable auto purge feature #autopurge.purgeInterval=1 server.1=10.120.12.135:2888:3888 server.2=10.120.11.201:2888:3888 server.3=10.120.14.226:2888:3888  参数说明： - tickTime: 单个滴答的以毫秒为单位的时间长度，zookeeper中使用的基本时间单位。 - dataDir: 数据目录。 - dataLogDir: 日志目录。 如果没有设置该参数, 将使用和dataDir相同的设置。 - clientPort: 监听客户端连接的端口号。 - initLimit:zookeeper集群中的包含多台服务器，其中一台为leader，集群中其余的服务器为follower。initLimit参数配置初始化连接时，follower和leader连接并同步的最大时间量。该参数此处设置为10，说明时间限制为10倍tickTime，即20s。 - syncLimit:该参数配置follower与leader同步所允许的最大时间间隔。如果follower太落后leader，将可能被剔除。该参数此处设置为5，说明时间限制为5倍tickTime， 即10s。 - server.x=[hostname]:mmmmm[:nnnnn] 其中x是一个数字，表示这是第几号服务器；hostname是该服务器地址；mmmmm配置该服务器和集群中的leader交换消息所使用的端口；nnnnnn配置选举leader时所使用的端口.如果配置的是同一机器上的伪集群模式，hostname相同，mmmmm和nnnnn必须不同。
修改bin/zkEnv.sh:
if [ &amp;quot;x${ZOO_LOG_DIR}&amp;quot; = &amp;quot;x&amp;quot; ] then ZOO_LOG_DIR=&amp;quot;/var/log/zookeeper&amp;quot; fi  从node50064复制ZooKeeper到其他机器 scp -r /usr/local/zookeeper node50069:~/ scp -r /usr/local/zookeeper node51054:~/  在node50069和上node51054配置ZooKeeper sudo mv zookeeper /usr/local/zookeeper sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/zookeeper sudo mkdir /var/log/zookeeper sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /var/log/zookeeper sudo mkdir /var/lib/zookeeper sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /var/lib/zookeeper cd /var/lib/zookeeper touch myid  新建的myid文件所要设的数字表示这是第几号服务器，该数字必须和zoo.cfg文件中的server.x中的x一一对应。这里node50069上myid内容设为2，node51054myid内容设为3。
HBase安装 （在node50064上）下载并解压缩HBase wget http://apache.mirrors.tds.net/hbase/1.2.x/hbase-1.2.x-bin.tar.gz wget http://apache.mirrors.tds.net/hbase/1.2.x/hbase-1.2.x-bin.tar.gz.mds md5sum hbase-1.2.x-bin.tar.gz sha1sum hbase-1.2.x-bin.tar.gz tar -xzf hbase-1.2.x-bin.tar.gz sudo mv hbase-1.2.x /usr/local/hbase sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/hbase cd /usr/local/hbase  conf/hbase-env.sh export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export HBASE_CLASSPATH=${HBASE_CLASSPATH}:${HBASE_CONF_DIR:-&amp;quot;/usr/local/hbase/conf&amp;quot;}:${HADOOP_CONF_DIR:-&amp;quot;/usr/local/hadoop/etc/hadoop&amp;quot;} export HBASE_MANAGES_ZK=false  根据Hbase/FAQ_Operations，如果要让HBase知道dfs.replication等HDFS客户端配置，需要下列途径之一： - 在hbase-env.sh中将HADOOP_CONF_DIR指給CLASSPATH，或者在HBase配置目录增加hadoop-site.xml符号链接 - 将hadoop-site.xml复制到{HBASE_HOME}/conf - 将少量HDFS客户端配置加入hbase-site.xml内
本文采用第一种方式。
conf/hbase-site.xml conf/regionservers node50069.mryqu.com node51054.mryqu.com  从node50064复制HBase到其他机器 scp -r /usr/local/hbase node50069:~/ scp -r /usr/local/hbase node51054:~/  在node50069和上node51054配置HBase sudo mv hbase /usr/local/hbase sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/hbase  （在全部机器上）环境变量设置 修改~/.bashrc：
# Set HADOOP_HOME (deprecated) export HADOOP_HOME=/usr/local/hadoop # Set HADOOP_PREFIX export HADOOP_PREFIX=$HADOOP_HOME export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_PREFIX/lib/native export HADOOP_OPTS=&amp;quot;$HADOOP_OPTS -Djava.library.path=$HADOOP_PREFIX/lib/native&amp;quot; **# Set ZOOKEEPER_HOME export ZOOKEEPER_HOME=/usr/local/zookeeper # Set HBASE_HOME export HBASE_HOME=/usr/local/hbase** export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$ZOOKEEPER_HOME/bin:$HBASE_HOME/bin  最后通过source~/.bashrc刷新配置文件。
Hadoop安装 步骤详见之前的博文[Hadoop] 安装Hadoop 2.7.x 集群。
启动 在运行HBase之前需要保证HDFS和ZooKeeper已经成功启动。
（在所有机器上）启动ZooKeeper zkServer.sh start  （在node50064上）启动Hadoop start-dfs.sh start-yarn.sh mr-jobhistory-daemon.sh start historyserver  （在node50064和node50069上）启动HBase start-hbase.sh  检验安装结果 查看JVM进程状态 测试HBase Shell 查看HBase Master web UI 查看HBase RegionServer web UI 使用命令查看ZooKeeper信息 关闭 （在node50064上）关闭HBase、Hadoopr stop-hbase.sh mr-jobhistory-daemon.sh stop historyserver stop-dfs.sh stop-yarn.sh  （在所有机器上）关闭ZooKeeper zkServer.sh stop  参考 ZooKeeper Administrator&amp;rsquo;s Guide
Apache HBase ™ Reference Guide: Advanced - Fully Distributed
ZooKeeper Getting Started Guide
ZooKeeper Administrator&amp;rsquo;s Guide
zookeeper运维
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] Hadoop3信息</title>
        <url>https://mryqu.github.io/post/hadoop_hadoop3%E4%BF%A1%E6%81%AF/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>3.0.0</tag>
        </tags>
        <content type="html"> 查看了一下Hadoop 3.0.0-alpha1，结果没查到什么太多信息。 Hadoop Roadmap - HADOOP - Move to JDK8&#43; - Classpath isolation on bydefault HADOOP-11656 - Shell scriptrewrite HADOOP-9902 - Move default ports out of ephemeralrange HDFS-9427 - HDFS - Removal of hftp in favor ofwebhdfs HDFS-5570 - Support for more than twostandby NameNodes HDFS-6440 - Support for Erasure Codes inHDFS HDFS-7285 - YARN - MAPREDUCE - Derive heap size ormapreduce.*.memory.mb automatically MAPREDUCE-5785
Apache Hadoop 3.0.0-alpha1-SNAPSHOT JIRA: Hadoop Common 3.0.0-alpha1 http://search-hadoop.com/?q=Hadoop&#43;3: 在Hadoop及其子项目中搜索Hadoop3 Hadoop 3支持最低的JDK版本是JDK8，可在hadoop-commontrunck分支（当前默认分支trunck分支为3.0.0-alpha1，master分支为2.8.0）获得其源码。
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;]获取Facebook帖子生成的SAS时间</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E8%8E%B7%E5%8F%96facebook%E5%B8%96%E5%AD%90%E7%94%9F%E6%88%90%E7%9A%84sas%E6%97%B6%E9%97%B4/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>sas</tag><tag>time</tag><tag>facebook</tag><tag>post</tag>
        </tags>
        <content type="html">  写了一个小代码分析Facebook帖子生成时间字符串，将其解析成SAS时间。 简而言之，time_t存储的是距00:00:00, Jan 1, 1970 UTC的秒数（epoch），其中tm_year存储的是当前年数减去1900；而SAS时间起始点为00:00:00, Jan 1, 1960UTC；转换主要使用difftime获取两者的时间差。
代码如下：参考 C&#43;&#43;: time_t
C&#43;&#43;: time
C&#43;&#43;: gmtime
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;]获取推文生成的SAS时间</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E8%8E%B7%E5%8F%96%E6%8E%A8%E6%96%87%E7%94%9F%E6%88%90%E7%9A%84sas%E6%97%B6%E9%97%B4/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>string</tag><tag>time</tag><tag>sas</tag><tag>tweet</tag>
        </tags>
        <content type="html">  写了一个小代码分析推文生成时间字符串，将其解析成SAS时间。 简而言之，time_t存储的是距00:00:00, Jan 1, 1970 UTC的秒数（epoch），其中tm_year存储的是当前年数减去1900；而SAS时间起始点为00:00:00, Jan 1, 1960UTC；转换主要使用difftime获取两者的时间差。 代码如下：参考 C&#43;&#43;: time_t
C&#43;&#43;: time
C&#43;&#43;: gmtime
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] YARN DistributedShell实践</title>
        <url>https://mryqu.github.io/post/hadoop_yarn_distributedshell%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>yarn</tag><tag>distributedshell</tag>
        </tags>
        <content type="html">  YARN DistributedShell介绍 Hadoop2的源代码中实现了两个基于YARN的应用，一个是MapReduce，另一个是非常简单的应用程序编程实例——DistributedShell。DistributedShell是一个构建在YARN之上的non-MapReduce应用示例。它的主要功能是在Hadoop集群中的多个节点，并行执行用户提供的shell命令或shell脚本（将用户提交的一串shell命令或者一个shell脚本，由ApplicationMaster控制，分配到不同的container中执行)。
YARN DistributedShell测试 执行下列命令进行测试：
hadoop jar /usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.X.jar -shell_command /bin/ls -shell_args /home/hadoop -jar /usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.X.jar  客户端日志显示执行成功：参考 如何运行YARN中的DistributedShell程序
YARN DistributedShell源码分析与修改
YARN Distributedshell解析
</content>
    </entry>
    
     <entry>
        <title>了解HTML5 Data Adapter for SAS®（h54s）</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3html5_data_adapter_for_sash54s/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>h54s</tag><tag>sas</tag><tag>boemska</tag><tag>html5</tag><tag>bi</tag>
        </tags>
        <content type="html">  今天在LinkedIn上看到一个消息，SAS的银牌合作伙伴Boemska开源了他们的HTML5 Data Adapter forSAS®（h54s）。 H54S是一个库，帮助和管理基于HTML5(JavaScript)的Web应用与部署在SAS企业BI平台上以SAS语言开发的后端数据服务之间的无缝双向通信。可以让Web程序员和SAS开发者协作以前所未有的速度和敏捷创建通用Web应用。 服务器端要求： - SAS® BI平台 (9.2及更高版本) - SAS® 存储过程Web应用 (集成技术)
粗略扫了一下h54s.sas、h54s.js和method.js：编写一个引入h54s.sas的SAS存储过程，接收前端的JSON数据，经过该SAS存储过程处理后返回给前端JSON结果。
参考 HTML5 Data Adapter for SAS®（h54s）
GitHub：boemska/h54s
</content>
    </entry>
    
     <entry>
        <title>[算法] 汉明重量（Hamming Weight）</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_%E6%B1%89%E6%98%8E%E9%87%8D%E9%87%8Fhamming_weight/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>algrithm</tag><tag>hamming</tag><tag>weight</tag><tag>bitcount</tag><tag>n&amp;(n-1)</tag>
        </tags>
        <content type="html">  LeetCode题191是算整数中比特1的个数，即汉明重量或汉明权重。
汉明重量 汉明重量是一串符号中非零符号的个数。因此它等同于同样长度的全零符号串的汉明距离。在最为常见的数据位符号串中，它是1的个数。 汉明重量是以理查德·卫斯里·汉明的名字命名的，它在包括信息论、编码理论、密码学等多个领域都有应用。
算法 位移实现 我自己的实现就是这种。通过判别n是否为0作为循环退出条件，如果n为0x1的话就位移一次，可是n为0x80000000还是需要位移32次。
public int hammingWeight(int n) { int res = 0; while(n!=0) { res&#43;= (n &amp;amp; 0x1); n &amp;gt;&amp;gt;&amp;gt;=1; } return res; }  n &amp;amp; (n-1)实现 public int hammingWeight(int n) { int res = 0; for(;n!=0;n = n &amp;amp; (n-1)) { res&#43;&#43;; } return res; }  减1操作将最右边的符号从0变到1，从1变到0，与操作将会移除最右端的1。如果最初n有X个1，那么经过X次这样的迭代运算，n将减到0。n&amp;amp; (n-1)实现在大多数比特为0的情况下是效率最高的。 此外n &amp;amp; (n-1)常用于判断数是否为2的幂数（LeetCode题231）：
 ----- binary ---- n n n-1 n&amp;amp;(n-1) -- ---- ---- ------- 0 0000 0111 0000 * 1 0001 0000 0000 * 2 0010 0001 0000 * 3 0011 0010 0010 4 0100 0011 0000 * 5 0101 0100 0100 6 0110 0101 0100 7 0111 0110 0110 8 1000 0111 0000 * 9 1001 1000 1000 10 1010 1001 1000 11 1011 1010 1010 12 1100 1011 1000 13 1101 1100 1100 14 1110 1101 1100 15 1111 1110 1110  JDK实现 java.lang.Integer类中bitCount函数实现：
public static int bitCount(int i) { // HD, Figure 5-2 i = i - ((i &amp;gt;&amp;gt;&amp;gt; 1) &amp;amp; 0x55555555); i = (i &amp;amp; 0x33333333) &#43; ((i &amp;gt;&amp;gt;&amp;gt; 2) &amp;amp; 0x33333333); i = (i &#43; (i &amp;gt;&amp;gt;&amp;gt; 4)) &amp;amp; 0x0f0f0f0f; i = i &#43; (i &amp;gt;&amp;gt;&amp;gt; 8); i = i &#43; (i &amp;gt;&amp;gt;&amp;gt; 16); return i &amp;amp; 0x3f; }  算法分析如下： - 对位长为2的数计算比特1的个数： nn&amp;gt;&amp;gt;&amp;gt;1比特1计数 n-(n&amp;gt;&amp;gt;&amp;gt;1)000000010001100101110110 这样，对位长为2的数进行如上计算后，2位数内存放的就是原来比特1的个数了。一个整数为32位，按2位分成16组，对于i&amp;gt;&amp;gt;&amp;gt;1操作，前一组末位为1的话就会串入后一组，所以要对每组同0x01相与，对于整个32位整数即与0x55555555相与。 - 上一步中对一个整数比特1的计数是分散到16组位长为2的数内了，这一步是收缩到8组位长为4的数内。位长为4的数，如果想计算前两位和后两位的和，可以通过(i&amp;amp;0x3) &#43;((i&amp;gt;&amp;gt;&amp;gt;2)&amp;amp;0x3)实现。0x3对应上一步的16组位长为2的数，而对整数而言即与0x33333333相与。 - 这一步是将上一步的结果收缩到4组位长为8的数内位长为8的数，如果想计算前四位和后四位的和，可以通过(i&#43;(i&amp;gt;&amp;gt;&amp;gt;4))&amp;amp;0xf实现（注：位长为8的数最多有8个比特1，所以先对两部分用0xf，还是最后对结果用0xf无所谓）。0xf对应上一步的8组位长为4的数，而对整数而言即与0x0f0f0f0f相与。 - 这一步是将上一步的结果收缩到2组位长为16的数内 - 这一步是将上一步的结果收缩到1组位长为32的数内 - 一个整数最多32个比特1，32为0b00100000，所以最后用0x3f（即0b00111111）过滤一下。而Long类中用0x7f过滤。像WIKI中popcount_1实现每一步都过滤到位，最后就无需过滤，每一步的中间结果更加清晰、易于理解。
 public static int popcount_1(int i) { i = (i &amp;amp; 0x55555555) &#43; ((i &amp;gt;&amp;gt;&amp;gt; 1) &amp;amp; 0x55555555); i = (i &amp;amp; 0x33333333) &#43; ((i &amp;gt;&amp;gt;&amp;gt; 2) &amp;amp; 0x33333333); i = (i &#43; (i &amp;gt;&amp;gt;&amp;gt; 4)) &amp;amp; 0x0f0f0f0f; i = (i &#43; (i &amp;gt;&amp;gt;&amp;gt; 8)) &amp;amp; 0x00ff00ff; i = i &#43; (i &amp;gt;&amp;gt;&amp;gt; 16) &amp;amp; 0x0000ffff; return i; }  资料 WIKI: Hamming weight
Aggregate Magic Algorithms 优化汉明重量算法及其它算法解释（附源代码）
Bit Twiddling Hacks 带有源代码的几种汉明重量计算方法
</content>
    </entry>
    
     <entry>
        <title>使用Facebook Graph API中的fields参数</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8facebook_graph_api%E4%B8%AD%E7%9A%84fields%E5%8F%82%E6%95%B0/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>api</tag><tag>fields</tag><tag>fieldexpansion</tag>
        </tags>
        <content type="html"> 使用Facebook Graph API进行查询，v2.3和v2.4版返回结果截然不同。使用v2.3之前的FacebookGraph API获得的响应信息很详细，而使用v2.4及之后的Facebook Graph API获得的响应基本没什么信息！ 这可是相同的AccessToken呀。卖萌的话，可以说句“吓死宝宝了”。
Using the Graph API提到可以使用fields参数选择所需字段，照着v2.3的加上了id、name、about、awards&amp;hellip;&amp;hellip; 失而复得，虚惊一场！！！
Using the Graph API里不仅提到了选择参数，还提到可以使用字段表达式进行嵌套查询。 likes字段使用limit(1)限定其仅返回一个点赞数据，反正我还要使用GraphAPI对帖子点赞数据进行请求，返回一个点赞知道需不需要请求就好了。
comments字段还制定了二级字段attachment、id和from。这样comments字段既不会漏了我需要的子字段，也不会多出来我不需要的字段。在上图GraphAPI Explorer中，左侧comments字段下面的”Search for afield“链接可以提示那些子字段可选，很方便。
</content>
    </entry>
    
     <entry>
        <title>[CSS] 判断一条CSS样式规则的覆盖者</title>
        <url>https://mryqu.github.io/post/css_%E5%88%A4%E6%96%AD%E4%B8%80%E6%9D%A1css%E6%A0%B7%E5%BC%8F%E8%A7%84%E5%88%99%E7%9A%84%E8%A6%86%E7%9B%96%E8%80%85/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>css</tag><tag>rule</tag><tag>override</tag><tag>chrome</tag><tag>elementinspector</tag>
        </tags>
        <content type="html"> 最近项目中有个OpenUI5控件显示缺少左填充，可它在公司的演示项目中却是正常的。接着发现.sasUiWndSectionCont是决定进行填充CSS规则。可是怎么在我的项目中就不成了呢？ 调试过程如下： - 打开Chrome开发者工具，选择元素检测器（ElementInspector），选择计算后样式（Computed）标签页，鼠标移动到感兴趣的左填充上（padding-left），点击圆圈图标查看详细内容- 跳到样式标签页后发现VDB项目下的.sasUiWndSectionCont定义覆盖了htmlcommons的。定位成功!
</content>
    </entry>
    
     <entry>
        <title>处理Twitter API访问速率超限错误</title>
        <url>https://mryqu.github.io/post/%E5%A4%84%E7%90%86twitter_api%E8%AE%BF%E9%97%AE%E9%80%9F%E7%8E%87%E8%B6%85%E9%99%90%E9%94%99%E8%AF%AF/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twitter</tag><tag>ratelimit</tag><tag>error</tag><tag>handling</tag>
        </tags>
        <content type="html"> 与处理Facebook API访问速率超限错误需要对比好几个Facebook错误代码相比，Twitter的API访问速率超限错误只需要处理HTTP响应代码429即可，很轻松。
</content>
    </entry>
    
     <entry>
        <title>处理Facebook API访问速率超限错误</title>
        <url>https://mryqu.github.io/post/%E5%A4%84%E7%90%86facebook_api%E8%AE%BF%E9%97%AE%E9%80%9F%E7%8E%87%E8%B6%85%E9%99%90%E9%94%99%E8%AF%AF/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>error</tag><tag>ratelimit</tag><tag>handling</tag><tag>quota</tag>
        </tags>
        <content type="html">  对于下列Facebook通用错误，我个人觉的#2、#4、#9、#17、#18和#32错误都可以向客户端报告FacebookAPI访问速率超限，至于#5不确定。
|Error number|PHP Constant name|Error description|Generated by methods |&amp;mdash;&amp;ndash; |2|API_EC_SERVICE|Service temporarily unavailable|(all) |4|API_EC_TOO_MANY_CALLS|Application request limit reached|(all) |5|API_EC_BAD_IP|Unauthorized source IP address|(all) |9|API_EC_RATE|User is performing too many actions| |17|API_EC_USER_TOO_MANY_CALLS|User request limit reached| |18|API_EC_REQUEST_RESOURCES_EXCEEDED|This API call could not be completed due to resourcelimits| |32||Page request limit reached|
参考 Rate Limiting on the Facebook Graph API
Facebook API Error Codes for Developers
</content>
    </entry>
    
     <entry>
        <title>SocialMedia Error Handling</title>
        <url>https://mryqu.github.io/post/socialmedia_error_handling/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>socialmedia</tag><tag>error</tag><tag>code</tag><tag>response</tag><tag>handling</tag>
        </tags>
        <content type="html">  Facebook - Handling Errors https://developers.facebook.com/docs/graph-api/using-graph-api#errors
Marketing API Error Codes https://developers.facebook.com/docs/marketing-api/error-reference
Games Payments Error Codes https://developers.facebook.com/docs/games_payments/fulfillment/errorcodes
List of Error Codes for Facebook&amp;rsquo;s API Facebook曾经有过错误代码列表，后来不提供了。这里提供完整错误代码列表。 http://www.fb-developers.info/tech/fb_dev/faq/general/gen_10.html
Twitter Error Codes &amp;amp; Responses https://dev.twitter.com/overview/api/response-codes
Google Analytics Core Reporting API - Standard Error Responses https://developers.google.com/analytics/devguides/reporting/core/v3/coreErrors#standard_errors
YouTube YouTube API v2.0 – Understanding API Error Responses https://developers.google.com/youtube/2.0/developers_guide_protocol_error_responses
Youtube Data API - Errors https://developers.google.com/youtube/v3/docs/errors
</content>
    </entry>
    
     <entry>
        <title>Facebook API Endpoint URL</title>
        <url>https://mryqu.github.io/post/facebook_api_endpoint_url/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>endpoint</tag><tag>url</tag>
        </tags>
        <content type="html"> 阅读com.restfb.DefaultFacebookClient中的createEndpointForApiCall方法，发现有四种端点URL。
端点URL地址说明FACEBOOK_READ_ONLY_ENDPOINT_URL
https://api-read.facebook.com/method从官方PHP客户端抓取的只读函数列表，当API请求为如下列表项，使用该只读端点URL。
admin.getallocationadmin.getapppropertiesadmin.getbannedusersadmin.getlivestreamvialinkadmin.getmetricsadmin.getrestrictioninfoapplication.getpublicinfoauth.getapppublickeyauth.getsessionauth.getsignedpublicsessiondatacomments.getconnect.getunconnectedfriendscountdashboard.getactivitydashboard.getcountdashboard.getglobalnewsdashboard.getnewsdashboard.multigetcountdashboard.multigetnewsdata.getcookiesevents.getevents.getmembersfbml.getcustomtagsfeed.getappfriendstoriesfeed.getregisteredtemplatebundlebyidfeed.getregisteredtemplatebundlesfql.multiqueryfql.queryfriends.arefriendsfriends.getfriends.getappusersfriends.getlistsfriends.getmutualfriendsgifts.getgroups.getgroups.getmembersintl.gettranslationslinks.getnotes.getnotifications.getpages.getinfopages.isadminpages.isappaddedpages.isfanpermissions.checkavailableapiaccesspermissions.checkgrantedapiaccessphotos.getphotos.getalbumsphotos.gettagsprofile.getinfoprofile.getinfooptionsstream.getstream.getcommentsstream.getfiltersusers.getinfousers.getloggedinuserusers.getstandardinfousers.hasapppermissionusers.isappuserusers.isverifiedvideo.getuploadlimitsFACEBOOK_GRAPH_VIDEO_ENDPOINT_URL
https://graph-video.facebook.comAPI请求以/video或/advideos结尾FACEBOOK_ENDPOINT_URL
https://www.facebook.comAPI请求以logout.php结尾FACEBOOK_GRAPH_ENDPOINT_URL
https://graph.facebook.com不满足上述三种的默认URL </content>
    </entry>
    
     <entry>
        <title>Facebook Graph API之我的常用URL笔记</title>
        <url>https://mryqu.github.io/post/facebook_graph_api%E4%B9%8B%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8url%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>page</tag><tag>post</tag><tag>comment</tag>
        </tags>
        <content type="html">  获取Facebook主页Id https://graph.facebook.com/v2.5/SasSoftware?access_token={accessToken}&amp;amp;format=json 上面示例是通过主页名SasSoftware获取其主页Id。
获取Facebook主页帖子  https://graph.facebook.com/v2.5/{pageId}/feed?limit=100&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;access_token={accessToken}&amp;amp;since=2015-01-01&amp;amp;util=2015-12-31
 https://graph.facebook.com/{pageId}/feed?limit=100&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;access_token={accessToken}&amp;amp;since=1420041660&amp;amp;util=1422634320
 https://graph.facebook.com/v2.0/{pageId}/feed?limit=100&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;access_token={accessToken}&amp;amp;since=1420041660&amp;amp;util=1422634320  通过Facebook Graph API 2.5或不带版本的API仅能获取帖子的Id、创建时间和帖子内容，而FacebookGraph API 2.0则可以获得更多内容。
获取Facebook帖子的评论信息 https://graph.facebook.com/{postId}/comments?limit=100&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;access_token={accessToken}
获取Facebook帖子的点赞信息 https://graph.facebook.com/{postId}/likes?limit=100&amp;amp;format=json&amp;amp;include_hidden=true&amp;amp;summary=true&amp;amp;access_token={accessToken}
</content>
    </entry>
    
     <entry>
        <title>cURL错误处理</title>
        <url>https://mryqu.github.io/post/curl%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>curl</tag><tag>error</tag><tag>handling</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html"> cURL执行错误分为两种： - 通过curl_easy_perform函数执行请求结果，返回值不是CURLE_OK。错误信息除了可以对照CURLcode定义查看，也可以通过设置CURLOPT_ERRORBUFFER设置错误缓存区获得人类易读的错误文字信息。范例见https://curl.haxx.se/libcurl/c/CURLOPT_ERRORBUFFER.html
 curl = curl_easy_init(); if(curl) { CURLcode res; char errbuf[CURL_ERROR_SIZE]; curl_easy_setopt(curl, CURLOPT_URL, &amp;quot;http://example.com&amp;quot;); curl_easy_setopt(curl, CURLOPT_ERRORBUFFER, errbuf); errbuf[0] = 0; res = curl_easy_perform(curl); if(res != CURLE_OK) { size_t len = strlen(errbuf); fprintf(stderr, &amp;quot;\nlibcurl: (%d) &amp;quot;, res); if(len) fprintf(stderr, &amp;quot;%s%s&amp;quot;, errbuf, ((errbuf[len - 1] != &#39;\n&#39;) ? &amp;quot;\n&amp;quot; : &amp;quot;&amp;quot;)); else fprintf(stderr, &amp;quot;%s\n&amp;quot;, curl_easy_strerror(res)); } }   另一种是curl_easy_perform返回CURLE_OK，但是HTTP响应代码为400及以上的整数。HTTP响应代码可以通过curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE,&amp;amp;httpCode)获得错误消息需要通过curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION,curlCallback)获得消息体后解析而得。  </content>
    </entry>
    
     <entry>
        <title>为cURL库设置HTTP代理的代码片段</title>
        <url>https://mryqu.github.io/post/%E4%B8%BAcurl%E5%BA%93%E8%AE%BE%E7%BD%AEhttp%E4%BB%A3%E7%90%86%E7%9A%84%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>curl</tag><tag>library</tag><tag>http</tag><tag>proxy</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html"> 在twitcurl看到cURL库设置http代理的方法，记录一下。
void twitCurl::prepareCurlProxy() { if( m_curlProxyParamsSet ) { return; } curl_easy_setopt( m_curlHandle, CURLOPT_PROXY, NULL ); curl_easy_setopt( m_curlHandle, CURLOPT_PROXYUSERPWD, NULL ); curl_easy_setopt( m_curlHandle, CURLOPT_PROXYAUTH, (long)CURLAUTH_ANY ); std::string proxyIpPort(&amp;quot;&amp;quot;); if( getProxyServerIp().size() ) { utilMakeCurlParams( proxyIpPort, getProxyServerIp(), getProxyServerPort() ); } curl_easy_setopt( m_curlHandle, CURLOPT_PROXY, proxyIpPort.c_str() ); if( m_proxyUserName.length() &amp;amp;amp;&amp;amp;amp; m_proxyPassword.length() ) { std::string proxyUserPass; utilMakeCurlParams( proxyUserPass,getProxyUserName(),getProxyPassword() ); curl_easy_setopt( m_curlHandle,CURLOPT_PROXYUSERPWD,proxyUserPass.c_str() ); } m_curlProxyParamsSet = true; }  </content>
    </entry>
    
     <entry>
        <title>Facebook Graph API之message_tags</title>
        <url>https://mryqu.github.io/post/facebook_graph_api%E4%B9%8Bmessage_tags/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>message_tags</tag><tag>post</tag><tag>comment</tag>
        </tags>
        <content type="html"> message_tag是Facebook帖子和评论中消息标记的设置档，包括标记ID、文本、类型、偏移和长度。 今天才注意到Facebook帖子（Post）中message_tag是一个JSON对象，而评论（Comment）中message_tag是一个JSON数组。
帖子中的message_tag是这个样子的：
&amp;quot;message_tags&amp;quot;: { &amp;quot;88&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;168597536563870&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;IBM&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 88, &amp;quot;length&amp;quot;: 3 } ], &amp;quot;93&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;241760048297&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Fidelity Investments&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 93, &amp;quot;length&amp;quot;: 20 } ], &amp;quot;115&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;145619362306025&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;ABB&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 115, &amp;quot;length&amp;quot;: 3 } ], &amp;quot;120&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;252467906271&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Quintiles&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 120, &amp;quot;length&amp;quot;: 9 } ], &amp;quot;131&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;193453547355388&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;SAS Software&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 131, &amp;quot;length&amp;quot;: 12 } ], &amp;quot;161&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;702317053131576&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Duke Energy&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 161, &amp;quot;length&amp;quot;: 11 } ], &amp;quot;174&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;313176732094295&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Toshiba Global Commerce Solutions&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 174, &amp;quot;length&amp;quot;: 33 } ], &amp;quot;209&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;20531316728&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Facebook&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;offset&amp;quot;: 209, &amp;quot;length&amp;quot;: 8 } ] }  评论中的message_tag是这个样子的：
&amp;quot;message_tags&amp;quot;: [ { &amp;quot;id&amp;quot;: &amp;quot;10101999889432459&amp;quot;, &amp;quot;length&amp;quot;: 16, &amp;quot;name&amp;quot;: &amp;quot;Crystal Sullivan&amp;quot;, &amp;quot;offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;user&amp;quot; }, { &amp;quot;id&amp;quot;: &amp;quot;10152592598848593&amp;quot;, &amp;quot;length&amp;quot;: 16, &amp;quot;name&amp;quot;: &amp;quot;Chris Hemedinger&amp;quot;, &amp;quot;offset&amp;quot;: 27, &amp;quot;type&amp;quot;: &amp;quot;user&amp;quot; } ]  </content>
    </entry>
    
     <entry>
        <title>Facebook Graph API获取帖子订阅信息之limit参数</title>
        <url>https://mryqu.github.io/post/facebook_graph_api%E8%8E%B7%E5%8F%96%E5%B8%96%E5%AD%90%E8%AE%A2%E9%98%85%E4%BF%A1%E6%81%AF%E4%B9%8Blimit%E5%8F%82%E6%95%B0/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>limit</tag><tag>parameter</tag>
        </tags>
        <content type="html"> 不同版本Facebook Graph API对 获取帖子订阅信息中limit参数要求不同： - v2.0及以下版本没有说明 - v2.1、v2.2和v2.3版本上限为250 - v2.4和v2.5版本上限为100</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;]用正则表达式检查日期格式yyyy-MM-dd</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%A3%80%E6%9F%A5%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8Fyyyy-mm-dd/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>regex</tag><tag>date</tag><tag>yyyy-mm-dd</tag><tag>gcc4.9.0</tag>
        </tags>
        <content type="html"> 写了一个小程序使用C&#43;&#43;的正则表达式检查日期是否符合yyyy-MM-dd格式： 结果总是抛出exception，错误代码不是error_brack就是error_escape。检查了一下代码，没觉得不符合ECMAScript语法法则。 查了一下我的环境，用的gcc 4.7.0。试了一下regex_match的例子，没问题，但是稍微改动一下用\d{4}检查4位数字就又抛exception了。
C:\&amp;gt;g&#43;&#43; -v Using built-in specs. COLLECT_GCC=C:\quTools\Anaconda\Scripts\g&#43;&#43;.bat\..\..\MinGW\bin\g&#43;&#43;.exe COLLECT_LTO_WRAPPER=c:/qutools/anaconda/mingw/bin/../libexec/gcc/x86_64-w64-mingw32/4.7.0/lto-wrapper.exe Target: x86_64-w64-mingw32 Configured with: ../../../build/gcc/src/configure --target=x86_64-w64-mingw32 --prefix=/c/bb/vista64-mingw32/mingw-x86-x86_64/build/build/root --with-sysroot=/c/bb/vista64-mingw32/mingw-x86-x86_64/build/build/root --enable-languages=all,obj-c&#43;&#43; --enable-fully-dynamic-string --disable-multilib Thread model: win32 gcc version 4.7.0 20111220 (experimental) (GCC)  搜了一下，发现C&#43;&#43;2011标准中的regex功能直到gcc 4.9.0才正式发布。啥也不说了，在Mingw-w64 Toolchains上直接下载个gcc 5.3.0试试，一切正常了
C:\ctest&amp;gt;g&#43;&#43; -v Using built-in specs. COLLECT_GCC=g&#43;&#43; COLLECT_LTO_WRAPPER=C:/tools/mingw32/bin/../libexec/gcc/i686-w64-mingw32/5.3.0/lto-wrapper.exe Target: i686-w64-mingw32 Configured with: ../../../src/gcc-5.3.0/configure --host=i686-w64-mingw32 --build=i686-w64-mingw32 --target=i686-w64-mingw32 --prefix=/mingw32 --with-sysroot=/c/mingw530/i686-530-posix-dwarf-rt_v4-rev0/mingw32 --with-gxx-include-dir=/mingw32/i686-w64-mingw32/include/c&#43;&#43; --enable-shared --enable-static --disable-multilib --enable-languages=c,c&#43;&#43;,fortran,lto --enable-libstdcxx-time=yes --enable-threads=posix --enable-libgomp --enable-libatomic --enable-lto --enable-graphite --enable-checking=release --enable-fully-dynamic-string --enable-version-specific-runtime-libs --disable-sjlj-exceptions -- with-dwarf2 --disable-isl-version-check --disable-libstdcxx-pch --disable-libstdcxx-debug --enable-bootstrap --disable-rpath --disable-win32-registry --disable-nls --disable-werror --disable-symvers --with-gnu-as --with-gnu-ld --with-arch=i686 --with-tune=generic --with-libiconv --with-system-zlib --with-gmp=/c/mingw530/prerequisites/i686-w64-mingw32-static --with-mpfr=/c/mingw530/prerequisites/i686-w64-mingw32-static --with-mpc=/c/mingw530/prerequisites/i686-w64-mingw32-static --with-isl=/c/mingw530/prerequisites/i686-w64-mingw32-static --with-pkgversion=&#39;i686-posix-dwarf-rev0, Built by MinGW -W64 project&#39; --with-bugurl=http://sourceforge.net/projects/mingw-w64 CFLAGS=&#39;-O2 -pipe -I/c/mingw530/i686-530-posix-dwarf-rt_v4-rev0/mingw32/opt/include -I/c/mingw530/prerequisites/i686-zlib-static/include -I/c/mingw530/prerequisites/i686-w64-mingw32-static/include&#39; CXXFLAGS=&#39;-O2 -pipe -I/c/mingw530/i686-530-posix-dwarf-rt_v4-rev0/mingw32/opt/include -I/c/mingw530/prerequisites/i686-zlib-static/include -I/c/mingw530/prerequisites/i686-w64-mingw32-static/include&#39; CPPFLAGS= LDFLAGS=&#39;-pipe -L/c/mingw530/i686-530-posix-dwarf-rt_v4-rev0/mingw32/opt/lib -L/c/mingw530/prerequisites/i686-zlib-static/ lib -L/c/mingw530/prerequisites/i686-w64-mingw32-static/lib -Wl,--large-address-aware&#39; Thread model: posix gcc version 5.3.0 (i686-posix-dwarf-rev0, Built by MinGW-W64 project) C:\ctest&amp;gt;testTimeRegex : 0 2015-03-01 : 1 2015-02-31 : 1 2015/02/01 : 0 15-2-1 : 0 123456789 : 0  附： - libstdc&#43;&#43;手册中列举了C&#43;&#43;2011标准支持的正则表达功能(见表1.2中28节)。 - 最强日期正则表达式博文中包含更严格的日期检查代码。
</content>
    </entry>
    
     <entry>
        <title>解决使用twitcurl.lib遇到的LNK1112和LNK2038链接错误</title>
        <url>https://mryqu.github.io/post/%E8%A7%A3%E5%86%B3%E4%BD%BF%E7%94%A8twitcurl.lib%E9%81%87%E5%88%B0%E7%9A%84lnk1112%E5%92%8Clnk2038%E9%93%BE%E6%8E%A5%E9%94%99%E8%AF%AF/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>lnk1112</tag><tag>lnk2038</tag><tag>twitcurl</tag><tag>x64</tag><tag>mt</tag>
        </tags>
        <content type="html"> 在使用twitcurl.lib时，遭遇下列链接错误：
fatal error LNK1112: module machine type &#39;X86&#39; conflicts with target machine type &#39;x64&#39; libtwitcurl.lib(twitcurl.obj) : error LNK2038: mismatch detected for &#39;RuntimeLibrary&#39;: value &#39;MD_DynamicRelease&#39; doesn&#39;t match value &#39;MT_StaticRelease&#39; in xxxxx.obj  解决方法： </content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;]玩玩Designated Initializer</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E7%8E%A9%E7%8E%A9designated_initializer/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>gcc</tag><tag>designated</tag><tag>initializer</tag>
        </tags>
        <content type="html">  玩一把gcc的Designated Initializers： 测试结果：结构体内的变量必须按照声明的顺序初始化，并且不能遗漏，否则会报“sorry, unimplemented:non-trivial designated initializers not supported”错误。
参考 C99标准
Bug 55606 - sorry, unimplemented: non-trivial designated initializers not supported
C&#43;&#43; - g&#43;&#43;: sorry, unimplemented: non-trivial designated initializers not supported - SysTutorials QA
http://stackoverflow.com/questions/31215971/non-trivial-designated-initializers-not-supported
Non-trivial designated initializers not supported · Issue #8 · couchbaselabs/cbforest · GitHub
</content>
    </entry>
    
     <entry>
        <title>twitcurl生成HTTP OAuth头的实现流程</title>
        <url>https://mryqu.github.io/post/twitcurl%E7%94%9F%E6%88%90http_oauth%E5%A4%B4%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>twitter</tag><tag>twitcurl</tag><tag>authorization</tag><tag>oauth</tag><tag>signaure</tag>
        </tags>
        <content type="html">  对twitcurl代码做了一些修改，结果遇到了认证失败的错误：
{“errors”:[{“message”:”Could not authenticate you”,”code”:32}]}  通过继续修改twitcurl代码改正问题，学习了twitcurl的认证授权部分代码。其授权部分主要在oauthlib.h和oauthlib.cpp中的oAuth类实现中。下面主要分析一下oAuth::getOAuthHeader方法。
外部数据 Http URL: https://api.twitter.com/1.1/search/tweets.json Http头参数:
|参数键|参数值 |&amp;mdash;&amp;ndash; |q|va |count|23 |result_type|recent
Http授权参数:
|参数键|参数值 |&amp;mdash;&amp;ndash; |oauth_consumer_key|xvz1evFS4wEEPTGEFPHBog |oauth_signature_method|HMAC-SHA1 |oauth_token|370773112-GmHxMAgYyLbNEtIKZeRNFsMKPR9EyMZeS9weJAEb |oauth_version|1.0
oAuth::getOAuthHeader方法  通过buildOAuthHttpParameterKeyValPairs(params, true,rawKeyValuePairs);对Http头参数中参数值进行百分号编码（URL编码），编码后结果放在哈希表rawKeyValuePairs中 rawKeyValuePairs: 键值qvacount23result_typerecent
 假定HTTP内容是经过百分号编码的，通过buildOAuthRawDataKeyValPairs( rawData,false, rawKeyValuePairs );找到内容中的键值对，放入哈希表rawKeyValuePairs中 rawKeyValuePairs: 键值qvacount23result_typerecent 通过buildOAuthTokenKeyValuePairs( includeOAuthVerifierPin,std::string( &amp;ldquo;&amp;rdquo; ), rawKeyValuePairs, true );创建认授权证： rawKeyValuePairs: 键值说明qvacount23result_typerecentoauth_consumer_keyxvz1evFS4wEEPTGEFPHBogoauth_nonce131862295819ctwitcurl实现就是时戳项加一个随机数oauth_signature_methodHMAC-SHA1固定值oauth_timestamp1318622958oauth_token370773112-GmHxMAgYyLbNEtIKZeRNFsMKPR9EyMZeS9weJAEboauth_version1.0固定值 通过getSignature( eType, pureUrl, rawKeyValuePairs,oauthSignature );获得签名  生成 sigBase： 使用consumer_secret和token_secret组成signing_key，使用HMAC_SHA1算法通过sigBase和signing_key生成摘要strDigest：B6 79 C0 AF 18 F4 E9 C5 87 AB 8E 20 0A CD 4E 48 A9 3F 8C B6(非真实计算而得数据) 通过base64_encode进行编码：tnnArxj06cWHq44gCs1OSKk/jLY= (非真实计算而得数据) 通过百分比编码获得最终签名：(非真实计算而得数据)  通过rawKeyValuePairs.clear();清除OAuth不需要的键值对 通过buildOAuthTokenKeyValuePairs( includeOAuthVerifierPin, oauthSignature, rawKeyValuePairs, false );重新创建认授权证： rawKeyValuePairs: 键值说明oauth_consumer_keyxvz1evFS4wEEPTGEFPHBogoauth_nonce131862295819ctwitcurl实现就是时戳项加一个随机数oauth_signature_methodHMAC-SHA1固定值oauth_timestamp1318622958oauth_token370773112-GmHxMAgYyLbNEtIKZeRNFsMKPR9EyMZeS9weJAEboauth_version1.0固定值oauth_signature 最终生成HTTP授权头：  参考 Twitter OAuth Overview
Twitter sign in implementation
Creating a signature
Authorizing a Twitter request
Percent encoding parameters for Twitter
Github: mryqu/twitcurl
</content>
    </entry>
    
     <entry>
        <title>表情符号之Unicode和UTF-8编码</title>
        <url>https://mryqu.github.io/post/%E8%A1%A8%E6%83%85%E7%AC%A6%E5%8F%B7%E4%B9%8Bunicode%E5%92%8Cutf-8%E7%BC%96%E7%A0%81/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>emoji</tag><tag>unicode</tag><tag>utf-8</tag><tag>绘文字</tag>
        </tags>
        <content type="html"> 最近在玩表情符号，这个表情符号Unicode表格，还挺全： http://apps.timwhitlock.info/emoji/tables/unicode 此外也常用这个在线编码转换工具进行验证：http://tool.oschina.net/encode 表情符号的Unicode范围在Android - How to filter emoji (emoticons) from a string?有提到过： - U&#43;2190 to U&#43;21FF - U&#43;2600 to U&#43;26FF - U&#43;2700 to U&#43;27BF - U&#43;3000 to U&#43;303F - U&#43;1F300 to U&#43;1F64F - U&#43;1F680 to U&#43;1F6FF
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 我服务器上的GCC版本不支持C&#43;&#43;11特性</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E6%88%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84gcc%E7%89%88%E6%9C%AC%E4%B8%8D%E6%94%AF%E6%8C%81c&#43;&#43;11%E7%89%B9%E6%80%A7/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>c11</tag><tag>-stdc0x</tag><tag>gcc</tag><tag>support</tag>
        </tags>
        <content type="html"> 用了点C&#43;&#43;11特性，结果编译失败，编译参数加&amp;rdquo;-std=c&#43;&#43;0x&amp;rdquo;，结果识别不出来。
$ g&#43;&#43; -v Using built-in specs. Target: amd64-undermydesk-freebsd Configured with: FreeBSD/amd64 system compiler Thread model: posix gcc version 4.2.1 20070719 [FreeBSD]  C&#43;&#43;0x/C&#43;&#43;11 Support in GCC提到GCC 4.3版本之后才支持C&#43;&#43;11特性，白折腾一把！ 好吧，我用gcc docker!
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;]遭遇error C2039: &#39;min&#39; : is not a member of &#39;std&#39;</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E9%81%AD%E9%81%87error_c2039_min__is_not_a_member_of_std/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>visual studio</tag><tag>min</tag><tag>std</tag><tag>algorithm</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html"> 使用Visual Studio2013编译twitcurl遭遇下列错误：
error C2039: &#39;min&#39; : is not a member of &#39;std&#39;  解决方法：
#include &amp;lt;algorithm&amp;gt;  </content>
    </entry>
    
     <entry>
        <title>Visual Studio: 使用简体中文（GB2312）编码加载文件, 有些字节已用Unicode替换字符更换</title>
        <url>https://mryqu.github.io/post/visual_studio_%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87gb2312%E7%BC%96%E7%A0%81%E5%8A%A0%E8%BD%BD%E6%96%87%E4%BB%B6_%E6%9C%89%E4%BA%9B%E5%AD%97%E8%8A%82%E5%B7%B2%E7%94%A8unicode%E6%9B%BF%E6%8D%A2%E5%AD%97%E7%AC%A6%E6%9B%B4%E6%8D%A2/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>visual studio</tag><tag>2010</tag><tag>unicode</tag><tag>replaced</tag>
        </tags>
        <content type="html"> 遭遇下列VS2010错误: Some bytes have been replaced with the Unicodesubstitution character while loading file base64.cpp with ChineseSimplified (GB2312) encoding. Saving the file will not preserve theoriginal file contents. 我的Visual Studio已经勾选了Auto-detect UTF-8 encoding withoutsignature:Ultraedit不能正常显示base64.cpp，但Sublime Text能正常显示。 原因是base64.cpp并不是UTF-8编码，但是包含一个十六进制为&amp;rdquo;E9&amp;rdquo;的字符，即带重音符的e。由于我的系统locale是RPC，所以显示不正常，估计系统locale改成CP1252 - Windows 拉丁语1代码页就可以了。用Sublime Text将其转存为UTF-8编码当然也可以。
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] Building twitcurl Library in Unix platform</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_building_twitcurl_library_in_unix_platform/</url>
        <categories>
          <category>DataBuilder</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>build</tag><tag>twitcuil</tag><tag>library</tag><tag>unix</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html">  Download twitcurl source from https://github.com/swatkat/twitcurl using Git client.  git clone https://github.com/swatkat/twitcurl.git  In Unix shell, cd into libtwitcurl directory. Compile all of the twitcurlsource files into object files.  g&#43;&#43; -Wall -fPIC -c -I. twitcurl.cpp oauthlib.cpp urlencode.cpp base64.cpp HMAC_SHA1.cpp SHA1.cpp  Building twitcurl asstatic library: Use the archive commandto build twitcurl library from object files.  ar rvs libtwitcurl.a *.o   </content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 使用NM查看目标文件的符号列表</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E4%BD%BF%E7%94%A8nm%E6%9F%A5%E7%9C%8B%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E7%9A%84%E7%AC%A6%E5%8F%B7%E5%88%97%E8%A1%A8/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>nm</tag><tag>cpp</tag><tag>gcc</tag>
        </tags>
        <content type="html">  练习使用nm查看目标文件的符号列表。此外发现G&#43;&#43;竟然创建了两套构造函数和析构函数。
nm命令  -a或&amp;ndash;debug-syms：显示调试符号。 -B：等同于&amp;ndash;format=bsd，用来兼容MIPS的nm。 -C或&amp;ndash;demangle：将低级符号名解码(demangle)成用户级名字。这样可以使得C&#43;&#43;函数名具有可读性。 -D或&amp;ndash;dynamic：显示动态符号。该任选项仅对于动态目标(例如特定类型的共享库)有意义。 -fformat：使用format格式输出。format可以选取bsd、sysv或posix，该选项在GNU的nm中有用。默认为bsd。 -g或&amp;ndash;extern-only：仅显示外部符号。 -n、-v或&amp;ndash;numeric-sort：按符号对应地址的顺序排序，而非按符号名的字符顺序。 -p或&amp;ndash;no-sort：按目标文件中遇到的符号顺序显示，不排序。 -P或&amp;ndash;portability：使用POSIX.2标准输出格式代替默认的输出格式。等同于使用任选项-fposix。 -s或&amp;ndash;print-armap：当列出库中成员的符号时，包含索引。索引的内容包含：哪些模块包含哪些名字的映射。 -r或&amp;ndash;reverse-sort：反转排序的顺序(例如，升序变为降序)。 &amp;ndash;size-sort：按大小排列符号顺序。该大小是按照一个符号的值与它下一个符号的值进行计算的。 -tradix或&amp;ndash;radix=radix：使用radix进制显示符号值。radix只能为&amp;rdquo;d&amp;rdquo;表示十进制、&amp;rdquo;o&amp;rdquo;表示八进制或&amp;rdquo;x&amp;rdquo;表示十六进制。 &amp;ndash;target=bfdname：指定一个目标代码的格式，而非使用系统的默认格式。 -u或&amp;ndash;undefined-only：仅显示没有定义的符号(那些外部符号)。 -l或&amp;ndash;line-numbers：对每个符号，使用调试信息来试图找到文件名和行号。对于已定义的符号，查找符号地址的行号。对于未定义符号，查找指向符号重定位入口的行号。如果可以找到行号信息，显示在符号信息之后。 -V或&amp;ndash;version：显示nm的版本号。 &amp;ndash;help：显示nm的任选项。  练习 symtest.hpp #include &amp;lt;iostream&amp;gt; class SymTest { SymTest(); SymTest(int x); ~SymTest(); void foo(); };  symtest.cpp #include &amp;quot;symtest.hpp&amp;quot; SymTest::SymTest() { printf(&amp;quot;SymTest::SymTest\n&amp;quot;); } SymTest::SymTest(int x) { printf(&amp;quot;SymTest::SymTest(int)\n&amp;quot;); } SymTest::~SymTest() { printf(&amp;quot;SymTest::~SymTest\n&amp;quot;); } void SymTest::foo() { printf(&amp;quot;SymTest::foo\n&amp;quot;); }  编译 NM: -g 仅显示外部符号 -C 显示用户级名字 学习了StackOverflow上的帖子Dual emission of constructor symbols，才了解这是G&#43;&#43;的一个已知问题，两套构造函数分别是complete objectconstructor和base object constructor。
NM: -A 在每个符号前显示文件名 NM: -l 在每个符号前显示行号 mymachine&amp;gt; g&#43;&#43; -c symtest.cpp -g -o symtest.dbg.o mymachine&amp;gt; nm -l symtest.dbg.o 0000000000000170 t _GLOBAL__I__ZN7SymTestC2Ev /u/mryqu/nmtest/symtest.cpp:10 0000000000000000 t _Z17__gthread_triggerv 0000000000000130 t _Z41__static_initialization_and_destruction_0ii /u/mryqu/nmtest/symtest.cpp:9 00000000000001b0 T _ZN7SymTest3fooEv /u/mryqu/nmtest/symtest.cpp:9 0000000000000210 T _ZN7SymTestC1Ei /u/mryqu/nmtest/symtest.cpp:5 0000000000000250 T _ZN7SymTestC1Ev /u/mryqu/nmtest/symtest.cpp:3 0000000000000230 T _ZN7SymTestC2Ei /u/mryqu/nmtest/symtest.cpp:5 0000000000000270 T _ZN7SymTestC2Ev /u/mryqu/nmtest/symtest.cpp:3 00000000000001d0 T _ZN7SymTestD1Ev /u/mryqu/nmtest/symtest.cpp:7 00000000000001f0 T _ZN7SymTestD2Ev /u/mryqu/nmtest/symtest.cpp:7 U _ZNKSs4sizeEv /usr/include/c&#43;&#43;/4.2/bits/stl_algobase.h:189 U _ZNKSsixEm /usr/include/c&#43;&#43;/4.2/bits/locale_facets.tcc:2569 U _ZNSt8ios_base4InitC1Ev /usr/include/c&#43;&#43;/4.2/iostream:77 U _ZNSt8ios_base4InitD1Ev /usr/include/c&#43;&#43;/4.2/iostream:77 0000000000000010 t _ZSt17__verify_groupingPKcmRKSs /usr/include/c&#43;&#43;/4.2/bits/locale_facets.tcc:2558 0000000000000000 W _ZSt3minImERKT_S2_S2_ 0000000000000000 b _ZSt8__ioinit U __cxa_atexit /usr/include/c&#43;&#43;/4.2/iostream:77 U __dso_handle /usr/include/c&#43;&#43;/4.2/iostream:77 0000000000000000 d __gthread_active U __gxx_personality_v0 /usr/include/c&#43;&#43;/4.2/bits/locale_facets.tcc:2558 0000000000000190 t __tcf_0 /usr/include/c&#43;&#43;/4.2/iostream:77 U puts /u/mryqu/nmtest/symtest.cpp:9  符号类型 符号类型一列有两个字母时，小写字母代表局部符号，大写则为全局/外部符号。 &amp;ldquo;A&amp;rdquo;The symbol&amp;rsquo;s value is absolute, and will not be changed byfurther linking.&amp;ldquo;B&amp;rdquo;
&amp;ldquo;b&amp;rdquo;The symbol is in the uninitialized data section (known as BSS).&amp;ldquo;C&amp;rdquo;The symbol is common. Common symbols are uninitialized data.When linking, multiple common symbols may appear with the samename. If the symbol is defined anywhere, the common symbols aretreated as undefined references.&amp;ldquo;D&amp;rdquo;
&amp;ldquo;d&amp;rdquo;The symbol is in the initialized data section.&amp;ldquo;G&amp;rdquo;
&amp;ldquo;g&amp;rdquo;The symbol is in an initialized data section for small objects.Some object file formats permit more efficient access to small dataobjects, such as a global int variable as opposed to a large globalarray.&amp;ldquo;i&amp;rdquo;For PE format files this indicates that the symbol is in asection specific to the implementation of DLLs. For ELF formatfiles this indicates that the symbol is an indirect function. Thisis a GNU extension to the standard set of ELF symbol types. Itindicates a symbol which if referenced by a relocation does notevaluate to its address, but instead must be invoked at runtime.The runtime execution will then return the value to be used in therelocation.&amp;ldquo;N&amp;rdquo;The symbol is a debugging symbol.&amp;ldquo;p&amp;rdquo;The symbols is in a stack unwind section.&amp;ldquo;R&amp;rdquo;
&amp;ldquo;r&amp;rdquo;The symbol is in a read only data section.&amp;ldquo;S&amp;rdquo;
&amp;ldquo;s&amp;rdquo;The symbol is in an uninitialized data section for smallobjects.&amp;ldquo;T&amp;rdquo;
&amp;ldquo;t&amp;rdquo;The symbol is in the text (code) section.&amp;ldquo;U&amp;rdquo;The symbol is undefined.&amp;ldquo;u&amp;rdquo;The symbol is a unique global symbol. This is a GNU extensionto the standard set of ELF symbol bindings. For such a symbol thedynamic linker will make sure that in the entire process there isjust one symbol with this name and type in use.&amp;ldquo;V&amp;rdquo;
&amp;ldquo;v&amp;rdquo;The symbol is a weak object. When a weak defined symbol islinked with a normal defined symbol, the normal defined symbol isused with no error. When a weak undefined symbol is linked and thesymbol is not defined, the value of the weak symbol becomes zerowith no error. On some systems, uppercase indicates that a defaultvalue has been specified.&amp;ldquo;W&amp;rdquo;
&amp;ldquo;w&amp;rdquo;The symbol is a weak symbol that has not been specificallytagged as a weak object symbol. When a weak defined symbol islinked with a normal defined symbol, the normal defined symbol isused with no error. When a weak undefined symbol is linked and thesymbol is not defined, the value of the symbol is determined in asystem-specific manner without error. On some systems, uppercaseindicates that a default value has been specified.&amp;rdquo;-&amp;rdquo;The symbol is a stabs symbol in an a.out object file. In thiscase, the next values printed are the stabs other field, the stabsdesc field, and the stab type. Stabs symbols are used to holddebugging information.&amp;rdquo;?&amp;rdquo;The symbol type is unknown, or object file formatspecific.
</content>
    </entry>
    
     <entry>
        <title>FileUpload Streaming</title>
        <url>https://mryqu.github.io/post/fileupload_streaming/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>fileupload</tag><tag>streaming</tag>
        </tags>
        <content type="html"> 最近看一下org.apache.tomcat.util.http.fileupload，这个包是从commons-fileupload和commons-io复制而来，为了避免冲突而改名。 Apache Commons FileUpload是用于servlet和web应用的健壮、高性能文件上传库，它支持 RFC 1867和RFC2047。 传统的文件上传API假设文件在被用户访问前必须存储在某处，这种途径便捷、易于访问，但是消耗内存和耗时。流处理API允许在高性能和低内存配置之间做一点折中。 首先，需要确保请求是一个文件上传请求。这通过与传统API相同的静态方法实现：
// Check that we have a file upload request boolean isMultipart = ServletFileUpload.isMultipartContent(request);  现在需要解析请求获取成分项： ``` // Create a new file upload handler ServletFileUpload upload = new ServletFileUpload();
// Parse the request FileItemIterator iter = upload.getItemIterator(request); while (iter.hasNext()) { FileItemStream item = iter.next(); if (!item.isFormField()) { String name = item.getFieldName(); if(name==null) continue; InputStream stream = item.openStream(); System.out.println(&amp;ldquo;File field &amp;rdquo; &#43; name &#43; &amp;ldquo; with file name &amp;ldquo; &#43; item.getName() &#43; &amp;ldquo; detected.&amp;rdquo;); // Process the input stream &amp;hellip; } }```
这就是全部所需工作！
</content>
    </entry>
    
     <entry>
        <title>Sublime Text2&#43;Ctags&#43;Cscope使用实践</title>
        <url>https://mryqu.github.io/post/sublime_text2&#43;ctags&#43;cscope%E4%BD%BF%E7%94%A8%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>sublime</tag><tag>ctags</tag><tag>cscope</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html">  安装 安装Package Control 步骤见https://packagecontrol.io/installation#st2
安装CTags插件  通过 Preference -&amp;gt; Package Control -&amp;gt; InstallPackage安装Ctags插件（快捷键Ctrl&#43;Shift&#43;P，输入install） 打开Preference -&amp;gt; Package Settings -&amp;gt; Ctags -&amp;gt;Settings-Default和Setting-User，将Settings-Default中的内容拷贝到Setting-User中，将&amp;rdquo;command&amp;rdquo;: &amp;ldquo;&amp;rdquo; 中的 &amp;ldquo;&amp;rdquo; 填入Ctags.exe的路径位置 打开C工程根目录，在上点击右键，选择Ctags:Rebuild tags  安装Cscope插件  同样通过 Preference -&amp;gt; Package Control -&amp;gt; InstallPackage安装Cscope插件（快捷键Ctrl&#43;Shift&#43;P，输入install） 通过cscope –Rb在C工程根目录创建cscope.out文件 Cscope在ST2上没有包配置菜单，需要打开CscopeSublime.sublime-settings文件(我的机器在C:/Users/yqu/AppData/Roaming/SublimeText 2/Packages/Cscope目录下)，将 &amp;ldquo;executable&amp;rdquo;: &amp;ldquo;&amp;rdquo; 中的 &amp;ldquo;&amp;ldquo;填入Cscope.exe的路径位置,将 &amp;ldquo;database_location&amp;rdquo;: &amp;ldquo;&amp;rdquo; 中的&amp;rdquo;&amp;ldquo;填入cscope.out的路径位置。  使用 CTags命令 |Command|Key Binding|Alt Binding|Mouse Binding |&amp;mdash;&amp;ndash; |rebuild_ctags|ctrl&#43;t, ctrl&#43;r| &amp;nbsp; | &amp;nbsp; |navigate_to_definition|ctrl&#43;t, ctrl&#43;t|ctrl&#43;&amp;gt;|ctrl&#43;shift&#43;left_click |jump_prev|ctrl&#43;t, ctrl&#43;b|ctrl&#43;&amp;lt;|ctrl&#43;shift&#43;right_click |show_symbols|alt&#43;s| &amp;nbsp;| &amp;nbsp; |show_symbols (all files)|alt&#43;shift&#43;s| &amp;nbsp;| &amp;nbsp; |show_symbols (suffix)|ctrl&#43;alt&#43;shift&#43;s| &amp;nbsp;| &amp;nbsp;
Cscope命令  Ctrl &#43; \ - Show Cscope options Ctrl &#43; L , Ctrl &#43; S - Look up symbol undercursor Ctrl &#43; L , Ctrl &#43; D - Look up definition undercursor Ctrl &#43; L , Ctrl &#43; E - Look up functions calledby the function under the cursor Ctrl &#43; L , Ctrl &#43; R - Look up functionscalling the function under the cursor Ctrl &#43; Shift &#43; [ - Jump back Ctrl &#43; Shift &#43; ] - Jump forward  其他快捷键  Ctrl &#43; p - 快速定位项目中的文件 Ctrl &#43; R - 获取当前文件中的函数列表（# 和 @分别为变量和函数），这个功能也使得ST2不需要taglist插件了。  参考 使用Sublime Text3&#43;Ctags&#43;Cscope替代Source Insight
Exuberant Ctags笔记
Cscope笔记
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 使用readelf</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E4%BD%BF%E7%94%A8readelf/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>readelf</tag><tag>cpp</tag><tag>gcc</tag>
        </tags>
        <content type="html">  在计算机科学中，ELF文件（Executableand LinkableFormat）是一种用于二进制文件、可执行文件、目标代码、共享库和核心转储格式文件，是UNIX系统实验室（USL）作为应用程序二进制接口（ApplicationBinaryInterface，ABI）而开发和发布的，也是Linux的主要可执行文件格式。1999年，被86open项目选为x86架构上的类Unix操作系统的二进制文件标准格式，用来取代COFF。因其可扩展性与灵活性，也可应用在其它处理器、计算机系统架构的操作系统上。 ELF文件由4部分组成，分别是ELF头（ELF header）、程序头表（Program headertable）、节（Section）和节头表（Section headertable）。实际上，一个文件中不一定包含全部内容，而且他们的位置也未必如同所示这样安排，只有ELF头的位置是固定的，其余各部分的位置、大小等信息由ELF头中的各项值来决定。 而readelf用于显示ELF文件的信息。 Usage: readelf &amp;lt;option(s)&amp;gt; elf-file(s) Display information about the contents of ELF format files Options are: -a --all 全部 Equivalent to: -h -l -S -s -r -d -V -A -I -h --file-header ELF头 Display the ELF file header -l --program-headers 程序头表 Display the program headers --segments An alias for --program-headers -S --section-headers 节头 Display the sections&#39; header --sections An alias for --section-headers -g --section-groups 节组 Display the section groups -t --section-details 节细节 Display the section details -e --headers 全部头 Equivalent to: -h -l -S -s --syms 符号表 Display the symbol table --symbols An alias for --syms --dyn-syms 动态符号表 Display the dynamic symbol table -n --notes 核心注释 Display the core notes (if present) -r --relocs 重定位 Display the relocations (if present) -u --unwind Display the unwind info (if present) -d --dynamic 动态节 Display the dynamic section (if present) -V --version-info 版本节 Display the version sections (if present) -A --arch-specific 架构信息 Display architecture specific information (if any) -c --archive-index 该架构下符号/文件索引 Display the symbol/file index in an archive -D --use-dynamic Use the dynamic section info when displaying symbols -x --hex-dump=&amp;lt;number|name&amp;gt; Dump the contents of section &amp;lt;number|name&amp;gt; as bytes -p --string-dump=&amp;lt;number|name&amp;gt; Dump the contents of section &amp;lt;number|name&amp;gt; as strings -R --relocated-dump=&amp;lt;number|name&amp;gt; Dump the contents of section &amp;lt;number|name&amp;gt; as relocated bytes -w[lLiaprmfFsoRt] or --debug-dump[=rawline,=decodedline,=info,=abbrev,=pubnames,=aranges,=macro,=frames, =frames-interp,=str,=loc,=Ranges,=pubtypes, =gdb_index,=trace_info,=trace_abbrev,=trace_aranges, =addr,=cu_index] 显示DWARF2调试节 Display the contents of DWARF2 debug sections --dwarf-depth=N Do not display DIEs at depth N or greater --dwarf-start=N Display DIEs starting with N, at the same depth or deeper -I --histogram 柱状图 Display histogram of bucket list lengths -W --wide 输出宽度 Allow output width to exceed 80 characters @&amp;lt;file&amp;gt; Read options from &amp;lt;file&amp;gt; -H --help 帮助 Display this information -v --version 版本 Display the version number of readelf  练习 - 查看ELF文件头 hadoop@node51054:/usr/bin$ readelf -h curl ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2&#39;s complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: EXEC (Executable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x4023b1 Start of program headers: 64 (bytes into file) Start of section headers: 152600 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 9 Size of section headers: 64 (bytes) Number of section headers: 27  练习 - 查看符号表 hadoop@node51054:/usr/bin$ readelf -s curl Symbol table &#39;.dynsym&#39; contains 108 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __errno_location@GLIBC_2.2.5 (2) 2: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_global_cleanup@CURL_OPENSSL_3 (3) 3: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __strdup@GLIBC_2.2.5 (4) 4: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strstr@GLIBC_2.2.5 (4) 5: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strtoul@GLIBC_2.2.5 (4) 6: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strerror@GLIBC_2.2.5 (4) 7: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_getinfo@CURL_OPENSSL_3 (3) 8: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strchr@GLIBC_2.2.5 (4) 9: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strlen@GLIBC_2.2.5 (4) 10: 0000000000000000 0 FUNC GLOBAL DEFAULT UND memcmp@GLIBC_2.2.5 (4) 11: 0000000000000000 0 FUNC GLOBAL DEFAULT UND mkdir@GLIBC_2.2.5 (4) 12: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strncpy@GLIBC_2.2.5 (4) 13: 0000000000000000 0 FUNC GLOBAL DEFAULT UND utime@GLIBC_2.2.5 (4) 14: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_slist_free_all@CURL_OPENSSL_3 (3) 15: 0000000000000000 0 FUNC GLOBAL DEFAULT UND memset@GLIBC_2.2.5 (4) 16: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_version_info@CURL_OPENSSL_3 (3) 17: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fcntl@GLIBC_2.2.5 (2) 18: 0000000000000000 0 FUNC GLOBAL DEFAULT UND geteuid@GLIBC_2.2.5 (4) 19: 0000000000000000 0 FUNC GLOBAL DEFAULT UND calloc@GLIBC_2.2.5 (4) 20: 0000000000000000 0 FUNC GLOBAL DEFAULT UND open@GLIBC_2.2.5 (2) 21: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_mvsnprintf@CURL_OPENSSL_3 (3) 22: 0000000000000000 0 FUNC GLOBAL DEFAULT UND localtime@GLIBC_2.2.5 (4) 23: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_getenv@CURL_OPENSSL_3 (3) 24: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_mvaprintf@CURL_OPENSSL_3 (3) 25: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strtod@GLIBC_2.2.5 (4) 26: 0000000000000000 0 FUNC GLOBAL DEFAULT UND inflate 27: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_formfree@CURL_OPENSSL_3 (3) 28: 0000000000000000 0 FUNC GLOBAL DEFAULT UND memcpy@GLIBC_2.14 (5) 29: 0000000000000000 0 FUNC GLOBAL DEFAULT UND inflateInit2_ 30: 0000000000000000 0 FUNC GLOBAL DEFAULT UND setlocale@GLIBC_2.2.5 (4) 31: 0000000000000000 0 FUNC GLOBAL DEFAULT UND clock_gettime@GLIBC_2.17 (6) 32: 0000000000000000 0 FUNC GLOBAL DEFAULT UND time@GLIBC_2.2.5 (4) 33: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strcpy@GLIBC_2.2.5 (4) 34: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __isoc99_sscanf@GLIBC_2.7 (7) 35: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fclose@GLIBC_2.2.5 (4) 36: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __xstat@GLIBC_2.2.5 (4) 37: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_mvfprintf@CURL_OPENSSL_3 (3) 38: 0000000000000000 0 FUNC GLOBAL DEFAULT UND inflateEnd 39: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fileno@GLIBC_2.2.5 (4) 40: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __fxstat@GLIBC_2.2.5 (4) 41: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_init@CURL_OPENSSL_3 (3) 42: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __ctype_b_loc@GLIBC_2.3 (8) 43: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strrchr@GLIBC_2.2.5 (4) 44: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fseek@GLIBC_2.2.5 (4) 45: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __stack_chk_fail@GLIBC_2.4 (9) 46: 0000000000000000 0 FUNC GLOBAL DEFAULT UND tcgetattr@GLIBC_2.2.5 (4) 47: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fputs@GLIBC_2.2.5 (4) 48: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_strequal@CURL_OPENSSL_3 (3) 49: 0000000000000000 0 FUNC GLOBAL DEFAULT UND getpwuid@GLIBC_2.2.5 (4) 50: 0000000000000000 0 NOTYPE WEAK DEFAULT UND _Jv_RegisterClasses 51: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fflush@GLIBC_2.2.5 (4) 52: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fopen@GLIBC_2.2.5 (4) 53: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_getdate@CURL_OPENSSL_3 (3) 54: 0000000000000000 0 FUNC GLOBAL DEFAULT UND signal@GLIBC_2.2.5 (4) 55: 0000000000000000 0 FUNC GLOBAL DEFAULT UND free@GLIBC_2.2.5 (4) 56: 0000000000000000 0 FUNC GLOBAL DEFAULT UND getenv@GLIBC_2.2.5 (4) 57: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fputc@GLIBC_2.2.5 (4) 58: 0000000000000000 0 FUNC GLOBAL DEFAULT UND tcsetattr@GLIBC_2.2.5 (4) 59: 0000000000000000 0 FUNC GLOBAL DEFAULT UND malloc@GLIBC_2.2.5 (4) 60: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_msnprintf@CURL_OPENSSL_3 (3) 61: 0000000000000000 0 FUNC GLOBAL DEFAULT UND pipe@GLIBC_2.2.5 (4) 62: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_version@CURL_OPENSSL_3 (3) 63: 0000000000000000 0 FUNC GLOBAL DEFAULT UND ftruncate@GLIBC_2.2.5 (4) 64: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __memset_chk@GLIBC_2.3.4 (10) 65: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strcmp@GLIBC_2.2.5 (4) 66: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strtol@GLIBC_2.2.5 (4) 67: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_setopt@CURL_OPENSSL_3 (3) 68: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_free@CURL_OPENSSL_3 (3) 69: 0000000000000000 0 FUNC GLOBAL DEFAULT UND read@GLIBC_2.2.5 (2) 70: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_strnequal@CURL_OPENSSL_3 (3) 71: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_cleanup@CURL_OPENSSL_3 (3) 72: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fread@GLIBC_2.2.5 (4) 73: 0000000000000000 0 FUNC GLOBAL DEFAULT UND poll@GLIBC_2.2.5 (4) 74: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_slist_append@CURL_OPENSSL_3 (3) 75: 0000000000000000 0 FUNC GLOBAL DEFAULT UND puts@GLIBC_2.2.5 (4) 76: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_perform@CURL_OPENSSL_3 (3) 77: 0000000000000000 0 FUNC GLOBAL DEFAULT UND strtok@GLIBC_2.2.5 (4) 78: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fgets@GLIBC_2.2.5 (4) 79: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_maprintf@CURL_OPENSSL_3 (3) 80: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_mprintf@CURL_OPENSSL_3 (3) 81: 0000000000000000 0 FUNC GLOBAL DEFAULT UND gettimeofday@GLIBC_2.2.5 (4) 82: 0000000000000000 0 FUNC GLOBAL DEFAULT UND memmove@GLIBC_2.2.5 (4) 83: 0000000000000000 0 FUNC GLOBAL DEFAULT UND realloc@GLIBC_2.2.5 (4) 84: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_formadd@CURL_OPENSSL_3 (3) 85: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_pause@CURL_OPENSSL_3 (3) 86: 0000000000000000 0 FUNC GLOBAL DEFAULT UND access@GLIBC_2.2.5 (4) 87: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_strerror@CURL_OPENSSL_3 (3) 88: 0000000000000000 0 FUNC GLOBAL DEFAULT UND isatty@GLIBC_2.2.5 (4) 89: 0000000000000000 0 NOTYPE WEAK DEFAULT UND _ITM_deregisterTMCloneTab 90: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_mfprintf@CURL_OPENSSL_3 (3) 91: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fsetxattr@GLIBC_2.3 (8) 92: 0000000000000000 0 FUNC GLOBAL DEFAULT UND lseek@GLIBC_2.2.5 (2) 93: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_main@GLIBC_2.2.5 (4) 94: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ 95: 0000000000000000 0 NOTYPE WEAK DEFAULT UND _ITM_registerTMCloneTable 96: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_global_init@CURL_OPENSSL_3 (3) 97: 0000000000000000 0 FUNC GLOBAL DEFAULT UND fwrite@GLIBC_2.2.5 (4) 98: 0000000000000000 0 FUNC GLOBAL DEFAULT UND close@GLIBC_2.2.5 (2) 99: 0000000000000000 0 FUNC GLOBAL DEFAULT UND curl_easy_escape@CURL_OPENSSL_3 (3) 100: 0000000000625340 8 OBJECT GLOBAL DEFAULT 25 stdout@GLIBC_2.2.5 (4) 101: 0000000000625328 0 NOTYPE GLOBAL DEFAULT 24 _edata 102: 0000000000625450 0 NOTYPE GLOBAL DEFAULT 25 _end 103: 0000000000625348 8 OBJECT GLOBAL DEFAULT 25 stdin@GLIBC_2.2.5 (4) 104: 0000000000401cc8 0 FUNC GLOBAL DEFAULT 11 _init 105: 0000000000625350 8 OBJECT GLOBAL DEFAULT 25 stderr@GLIBC_2.2.5 (4) 106: 0000000000625328 0 NOTYPE GLOBAL DEFAULT 25 __bss_start 107: 000000000040f314 0 FUNC GLOBAL DEFAULT 14 _fini  </content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 使用ldd</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E4%BD%BF%E7%94%A8ldd/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>ldd</tag><tag>gcc</tag><tag>cpp</tag>
        </tags>
        <content type="html">  ldd是用来查看共享库依赖的Shell脚本命令。下面来看一下ldd命令的参数。 - -v：显示所有信息，例如包括符号版本信息。 - -u：显示没有使用的直接依赖。 - -d：执行重新定位，报告任何缺失对象（仅针对ELF） - -r：对数据对象和函数执行重新定位，报告任何缺失对象或函数（仅针对ELF）
Oracle - Linker and Libraries Guide - Relocations介绍了重新定位技术的来龙去脉，也介绍了ldd命令的相关使用。
ldd练习 hadoop@node51054:/usr/bin$ ldd curl linux-vdso.so.1 =&amp;gt; (0x00007ffefe989000) libcurl.so.4 =&amp;gt; /usr/lib/x86_64-linux-gnu/libcurl.so.4 (0x00007fb86cf8f000) libz.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libz.so.1 (0x00007fb86cd76000) libpthread.so.0 =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fb86cb58000) libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fb86c790000) libidn.so.11 =&amp;gt; /usr/lib/x86_64-linux-gnu/libidn.so.11 (0x00007fb86c55d000) librtmp.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/librtmp.so.0 (0x00007fb86c343000) libssl.so.1.0.0 =&amp;gt; /lib/x86_64-linux-gnu/libssl.so.1.0.0 (0x00007fb86c0e4000) libcrypto.so.1.0.0 =&amp;gt; /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 (0x00007fb86bd08000) libgssapi_krb5.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2 (0x00007fb86bac1000) liblber-2.4.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/liblber-2.4.so.2 (0x00007fb86b8b2000) libldap_r-2.4.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/libldap_r-2.4.so.2 (0x00007fb86b661000) /lib64/ld-linux-x86-64.so.2 (0x00007fb86d1f6000) libgnutls.so.26 =&amp;gt; /usr/lib/x86_64-linux-gnu/libgnutls.so.26 (0x00007fb86b3a2000) libgcrypt.so.11 =&amp;gt; /lib/x86_64-linux-gnu/libgcrypt.so.11 (0x00007fb86b122000) libdl.so.2 =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fb86af1e000) libkrb5.so.3 =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.3 (0x00007fb86ac53000) libk5crypto.so.3 =&amp;gt; /usr/lib/x86_64-linux-gnu/libk5crypto.so.3 (0x00007fb86aa24000) libcom_err.so.2 =&amp;gt; /lib/x86_64-linux-gnu/libcom_err.so.2 (0x00007fb86a820000) libkrb5support.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5support.so.0 (0x00007fb86a615000) libresolv.so.2 =&amp;gt; /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007fb86a3fa000) libsasl2.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/libsasl2.so.2 (0x00007fb86a1df000) libgssapi.so.3 =&amp;gt; /usr/lib/x86_64-linux-gnu/libgssapi.so.3 (0x00007fb869fa1000) libtasn1.so.6 =&amp;gt; /usr/lib/x86_64-linux-gnu/libtasn1.so.6 (0x00007fb869d8d000) libp11-kit.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libp11-kit.so.0 (0x00007fb869b4b000) libgpg-error.so.0 =&amp;gt; /lib/x86_64-linux-gnu/libgpg-error.so.0 (0x00007fb869946000) libkeyutils.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libkeyutils.so.1 (0x00007fb869742000) libheimntlm.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libheimntlm.so.0 (0x00007fb869539000) libkrb5.so.26 =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.26 (0x00007fb8692b1000) libasn1.so.8 =&amp;gt; /usr/lib/x86_64-linux-gnu/libasn1.so.8 (0x00007fb869010000) libhcrypto.so.4 =&amp;gt; /usr/lib/x86_64-linux-gnu/libhcrypto.so.4 (0x00007fb868ddd000) libroken.so.18 =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 (0x00007fb868bc8000) libffi.so.6 =&amp;gt; /usr/lib/x86_64-linux-gnu/libffi.so.6 (0x00007fb8689c0000) libwind.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libwind.so.0 (0x00007fb868797000) libheimbase.so.1 =&amp;gt; /usr/lib/x86_64-linux-gnu/libheimbase.so.1 (0x00007fb868589000) libhx509.so.5 =&amp;gt; /usr/lib/x86_64-linux-gnu/libhx509.so.5 (0x00007fb868340000) libsqlite3.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libsqlite3.so.0 (0x00007fb868087000) libcrypt.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libcrypt.so.1 (0x00007fb867e4e000)  ldd -u练习 ldd -v练习 hadoop@node51054:/usr/bin$ ldd -v curl linux-vdso.so.1 =&amp;gt; (0x00007ffef2be1000) libcurl.so.4 =&amp;gt; /usr/lib/x86_64-linux-gnu/libcurl.so.4 (0x00007fe38e414000) libz.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libz.so.1 (0x00007fe38e1fb000) libpthread.so.0 =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fe38dfdd000) libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe38dc15000) libidn.so.11 =&amp;gt; /usr/lib/x86_64-linux-gnu/libidn.so.11 (0x00007fe38d9e2000) librtmp.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/librtmp.so.0 (0x00007fe38d7c8000) libssl.so.1.0.0 =&amp;gt; /lib/x86_64-linux-gnu/libssl.so.1.0.0 (0x00007fe38d569000) libcrypto.so.1.0.0 =&amp;gt; /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 (0x00007fe38d18d000) libgssapi_krb5.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2 (0x00007fe38cf46000) liblber-2.4.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/liblber-2.4.so.2 (0x00007fe38cd37000) libldap_r-2.4.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/libldap_r-2.4.so.2 (0x00007fe38cae6000) /lib64/ld-linux-x86-64.so.2 (0x00007fe38e67b000) libgnutls.so.26 =&amp;gt; /usr/lib/x86_64-linux-gnu/libgnutls.so.26 (0x00007fe38c827000) libgcrypt.so.11 =&amp;gt; /lib/x86_64-linux-gnu/libgcrypt.so.11 (0x00007fe38c5a7000) libdl.so.2 =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fe38c3a3000) libkrb5.so.3 =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.3 (0x00007fe38c0d8000) libk5crypto.so.3 =&amp;gt; /usr/lib/x86_64-linux-gnu/libk5crypto.so.3 (0x00007fe38bea9000) libcom_err.so.2 =&amp;gt; /lib/x86_64-linux-gnu/libcom_err.so.2 (0x00007fe38bca5000) libkrb5support.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5support.so.0 (0x00007fe38ba9a000) libresolv.so.2 =&amp;gt; /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007fe38b87f000) libsasl2.so.2 =&amp;gt; /usr/lib/x86_64-linux-gnu/libsasl2.so.2 (0x00007fe38b664000) libgssapi.so.3 =&amp;gt; /usr/lib/x86_64-linux-gnu/libgssapi.so.3 (0x00007fe38b426000) libtasn1.so.6 =&amp;gt; /usr/lib/x86_64-linux-gnu/libtasn1.so.6 (0x00007fe38b212000) libp11-kit.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libp11-kit.so.0 (0x00007fe38afd0000) libgpg-error.so.0 =&amp;gt; /lib/x86_64-linux-gnu/libgpg-error.so.0 (0x00007fe38adcb000) libkeyutils.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libkeyutils.so.1 (0x00007fe38abc7000) libheimntlm.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libheimntlm.so.0 (0x00007fe38a9be000) libkrb5.so.26 =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.26 (0x00007fe38a736000) libasn1.so.8 =&amp;gt; /usr/lib/x86_64-linux-gnu/libasn1.so.8 (0x00007fe38a495000) libhcrypto.so.4 =&amp;gt; /usr/lib/x86_64-linux-gnu/libhcrypto.so.4 (0x00007fe38a262000) libroken.so.18 =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 (0x00007fe38a04d000) libffi.so.6 =&amp;gt; /usr/lib/x86_64-linux-gnu/libffi.so.6 (0x00007fe389e45000) libwind.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libwind.so.0 (0x00007fe389c1c000) libheimbase.so.1 =&amp;gt; /usr/lib/x86_64-linux-gnu/libheimbase.so.1 (0x00007fe389a0e000) libhx509.so.5 =&amp;gt; /usr/lib/x86_64-linux-gnu/libhx509.so.5 (0x00007fe3897c5000) libsqlite3.so.0 =&amp;gt; /usr/lib/x86_64-linux-gnu/libsqlite3.so.0 (0x00007fe38950c000) libcrypt.so.1 =&amp;gt; /lib/x86_64-linux-gnu/libcrypt.so.1 (0x00007fe3892d3000) Version information: ./curl: libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.7) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.17) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libcurl.so.4 (CURL_OPENSSL_3) =&amp;gt; /usr/lib/x86_64-linux-gnu/libcurl.so.4 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 /usr/lib/x86_64-linux-gnu/libcurl.so.4: libldap_r-2.4.so.2 (OPENLDAP_2.4_2) =&amp;gt; /usr/lib/x86_64-linux-gnu/libldap_r-2.4.so.2 libgssapi_krb5.so.2 (gssapi_krb5_2_MIT) =&amp;gt; /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2 liblber-2.4.so.2 (OPENLDAP_2.4_2) =&amp;gt; /usr/lib/x86_64-linux-gnu/liblber-2.4.so.2 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libidn.so.11 (LIBIDN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libidn.so.11 libssl.so.1.0.0 (OPENSSL_1.0.1) =&amp;gt; /lib/x86_64-linux-gnu/libssl.so.1.0.0 libssl.so.1.0.0 (OPENSSL_1.0.0) =&amp;gt; /lib/x86_64-linux-gnu/libssl.so.1.0.0 libc.so.6 (GLIBC_2.15) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.16) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.7) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.17) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libcrypto.so.1.0.0 (OPENSSL_1.0.0) =&amp;gt; /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 /lib/x86_64-linux-gnu/libz.so.1: libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libpthread.so.0: ld-linux-x86-64.so.2 (GLIBC_2.2.5) =&amp;gt; /lib64/ld-linux-x86-64.so.2 ld-linux-x86-64.so.2 (GLIBC_2.3) =&amp;gt; /lib64/ld-linux-x86-64.so.2 ld-linux-x86-64.so.2 (GLIBC_PRIVATE) =&amp;gt; /lib64/ld-linux-x86-64.so.2 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.2) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_PRIVATE) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libc.so.6: ld-linux-x86-64.so.2 (GLIBC_2.3) =&amp;gt; /lib64/ld-linux-x86-64.so.2 ld-linux-x86-64.so.2 (GLIBC_PRIVATE) =&amp;gt; /lib64/ld-linux-x86-64.so.2 /usr/lib/x86_64-linux-gnu/libidn.so.11: libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/librtmp.so.0: libgnutls.so.26 (GNUTLS_1_4) =&amp;gt; /usr/lib/x86_64-linux-gnu/libgnutls.so.26 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.7) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libgcrypt.so.11 (GCRYPT_1.2) =&amp;gt; /lib/x86_64-linux-gnu/libgcrypt.so.11 /lib/x86_64-linux-gnu/libssl.so.1.0.0: libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libcrypto.so.1.0.0 (OPENSSL_1.0.1d) =&amp;gt; /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 libcrypto.so.1.0.0 (OPENSSL_1.0.1) =&amp;gt; /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 libcrypto.so.1.0.0 (OPENSSL_1.0.0) =&amp;gt; /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0: libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.7) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2: libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libk5crypto.so.3 (k5crypto_3_MIT) =&amp;gt; /usr/lib/x86_64-linux-gnu/libk5crypto.so.3 libkrb5support.so.0 (krb5support_0_MIT) =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5support.so.0 libkrb5.so.3 (krb5_3_MIT) =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.3 /usr/lib/x86_64-linux-gnu/liblber-2.4.so.2: libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libldap_r-2.4.so.2: libgcrypt.so.11 (GCRYPT_1.2) =&amp;gt; /lib/x86_64-linux-gnu/libgcrypt.so.11 libresolv.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libresolv.so.2 libgssapi.so.3 (HEIMDAL_GSS_2.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libgssapi.so.3 libgnutls.so.26 (GNUTLS_1_4) =&amp;gt; /usr/lib/x86_64-linux-gnu/libgnutls.so.26 libpthread.so.0 (GLIBC_2.3.2) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libsasl2.so.2 (SASL2) =&amp;gt; /usr/lib/x86_64-linux-gnu/libsasl2.so.2 libc.so.6 (GLIBC_2.12) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 liblber-2.4.so.2 (OPENLDAP_2.4_2) =&amp;gt; /usr/lib/x86_64-linux-gnu/liblber-2.4.so.2 /usr/lib/x86_64-linux-gnu/libgnutls.so.26: libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libtasn1.so.6 (LIBTASN1_0_3) =&amp;gt; /usr/lib/x86_64-linux-gnu/libtasn1.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.8) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.2) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libgcrypt.so.11 (GCRYPT_1.2) =&amp;gt; /lib/x86_64-linux-gnu/libgcrypt.so.11 /lib/x86_64-linux-gnu/libgcrypt.so.11: libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.15) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libdl.so.2: ld-linux-x86-64.so.2 (GLIBC_PRIVATE) =&amp;gt; /lib64/ld-linux-x86-64.so.2 libc.so.6 (GLIBC_PRIVATE) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libkrb5.so.3: libresolv.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libresolv.so.2 libresolv.so.2 (GLIBC_2.9) =&amp;gt; /lib/x86_64-linux-gnu/libresolv.so.2 libk5crypto.so.3 (k5crypto_3_MIT) =&amp;gt; /usr/lib/x86_64-linux-gnu/libk5crypto.so.3 libkrb5support.so.0 (krb5support_0_MIT) =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5support.so.0 libkeyutils.so.1 (KEYUTILS_1.0) =&amp;gt; /lib/x86_64-linux-gnu/libkeyutils.so.1 libkeyutils.so.1 (KEYUTILS_0.3) =&amp;gt; /lib/x86_64-linux-gnu/libkeyutils.so.1 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.16) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libk5crypto.so.3: libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libkrb5support.so.0 (krb5support_0_MIT) =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5support.so.0 /lib/x86_64-linux-gnu/libcom_err.so.2: ld-linux-x86-64.so.2 (GLIBC_2.3) =&amp;gt; /lib64/ld-linux-x86-64.so.2 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.17) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libkrb5support.so.0: libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libresolv.so.2: libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_PRIVATE) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libsasl2.so.2: libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.15) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libgssapi.so.3: libroken.so.18 (HEIMDAL_ROKEN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.8) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libheimntlm.so.0 (HEIMDAL_NTLM_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libheimntlm.so.0 libasn1.so.8 (HEIMDAL_ASN1_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libasn1.so.8 libhcrypto.so.4 (HEIMDAL_CRYPTO_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libhcrypto.so.4 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libkrb5.so.26 (HEIMDAL_KRB5_2.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.26 /usr/lib/x86_64-linux-gnu/libtasn1.so.6: libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libp11-kit.so.0: libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.16) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.2) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libgpg-error.so.0: libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libkeyutils.so.1: libc.so.6 (GLIBC_2.7) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libheimntlm.so.0: libroken.so.18 (HEIMDAL_ROKEN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 libkrb5.so.26 (HEIMDAL_KRB5_2.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.26 libhcrypto.so.4 (HEIMDAL_CRYPTO_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libhcrypto.so.4 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libkrb5.so.26: libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libcrypt.so.1 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libcrypt.so.1 libhcrypto.so.4 (HEIMDAL_CRYPTO_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libhcrypto.so.4 libwind.so.0 (HEIMDAL_WIND_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libwind.so.0 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libpthread.so.0 (GLIBC_2.3.2) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libroken.so.18 (HEIMDAL_ROKEN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 libheimbase.so.1 (HEIMDAL_BASE_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libheimbase.so.1 libc.so.6 (GLIBC_2.15) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.7) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.8) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libasn1.so.8 (HEIMDAL_ASN1_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libasn1.so.8 libhx509.so.5 (HEIMDAL_X509_1.2) =&amp;gt; /usr/lib/x86_64-linux-gnu/libhx509.so.5 /usr/lib/x86_64-linux-gnu/libasn1.so.8: libroken.so.18 (HEIMDAL_ROKEN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.8) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libhcrypto.so.4: libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libasn1.so.8 (HEIMDAL_ASN1_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libasn1.so.8 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libroken.so.18 (HEIMDAL_ROKEN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 libc.so.6 (GLIBC_2.7) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libroken.so.18: libresolv.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libresolv.so.2 libcrypt.so.1 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libcrypt.so.1 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.8) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.15) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libffi.so.6: libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libwind.so.0: libroken.so.18 (HEIMDAL_ROKEN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libheimbase.so.1: libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.8) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /usr/lib/x86_64-linux-gnu/libhx509.so.5: libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libheimbase.so.1 (HEIMDAL_BASE_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libheimbase.so.1 libwind.so.0 (HEIMDAL_WIND_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libwind.so.0 libroken.so.18 (HEIMDAL_ROKEN_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libroken.so.18 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.8) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libasn1.so.8 (HEIMDAL_ASN1_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libasn1.so.8 libhcrypto.so.4 (HEIMDAL_CRYPTO_1.0) =&amp;gt; /usr/lib/x86_64-linux-gnu/libhcrypto.so.4 /usr/lib/x86_64-linux-gnu/libsqlite3.so.0: libdl.so.2 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libdl.so.2 libpthread.so.0 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libpthread.so.0 libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.3.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.4) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libcrypt.so.1: libc.so.6 (GLIBC_2.14) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_PRIVATE) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 libc.so.6 (GLIBC_2.2.5) =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6  ld-Linux.so ldd显示可执行模块的dependency的工作原理，其实质是通过ld-Linux.so（elf动态库的装载器）来实现的。我们知道，ld-linux.so模块会先于executable模块程序工作，并获得控制权，因此当上述的那些环境变量被设置时，ld-linux.so选择了显示可执行模块的dependency。 直接执行ld-Linux.so命令也可获得共享库的依赖关系：
hadoop@node51054:/usr/bin$ /lib64/ld-linux-x86-64.so.2 --list ./curl  </content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] GNU Binutils之ar和ranlib</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_gnu_binutils%E4%B9%8Bar%E5%92%8Cranlib/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>ar</tag><tag>ranlib</tag><tag>binutils</tag><tag>gcc</tag>
        </tags>
        <content type="html">  GNU binutils是一组二进制工具集。包括：ld、as、addr2line、ar、gprof、nm、objcopy、objdump、ranlib、size、strings、strip等。本文重点学习一下其中的ar和ranlib。
ar命令 ar用于建立、修改、提取档案文件(archive)。archive是一个包含多个被包含文件的单一文件（也称之为库文件），其结构保证了可以从中检索并得到原始的被包含文件（称之为archive中的member）。member的原始文件内容、模式（权限）、时间戳、所有者和组等属性都被保存在archive中。member被提取后，他们的属性被恢复到初始状态。 ar命令第一个参数可混合指令代码（operationcode p）和修饰符标志（modifier flags mod），可按意愿添加一个折线。
ar [--plugin name] [-X32_64] [-]p[mod [relpos] [count]] archive [member...]  指令参数  d：删除档案文件中的成员文件。 m：移动在档案文件中的成员文件，改变次序。可以借助修饰符标志a、b、i移动到指定位置。 p：显示档案文件中的成员文件内容。 q：将文件快速附加在档案文件末端。不检查、不替换已有同名成员文件，也不更新档案文件的符号表索引，修饰符标志a、b、i无效。然而很多不同系统都假设q指令重建档案文件的符号表索引，因此GNU将其按照r指令进行相同实现。 r：将文件插入档案文件中。检查并替换已有同名成员文件，重建档案文件的符号表索引，借助修饰符标志a、b、i将文件插入到指定位置。 t：显示档案文件中所包含的文件。 x：自档案文件中取出成员文件。  修饰符标志  a &amp;lt;成员文件&amp;gt;：将文件插入档案文件中指定的成员文件之后。 b&amp;lt;成员文件&amp;gt;：将文件插入档案文件中指定的成员文件之前。 c：建立档案文件。当更新档案文件时，档案文件不存在则创建档案文件，但会告警。此标志可抑制告警。 D：以确定模式工作。 f：为避免过长的文件名不兼容于其他系统的ar指令指令，可利用此参数，截掉要放入档案文件中过长的成员文件名称。 i &amp;lt;成员文件&amp;gt;：将文件插入档案文件中指定的成员文件之前。（等同标志b） I：可接受但不使用。 N：使用count参数。当档案文件中存在多个同名成员，用于指定提取/删除的个数。 o：保留档案文件中文件的日期。如无此参数，则输出文件的修改时间为提取时间。 s：若档案文件中包含了对象模式，可利用此参数建立档案文件的符号表。 S：不产生符号表。 T：使指定档案文件成为瘦档案文件。例如将多个档案文件加入目标档案文件，目标档案文件可以包含符号索引及对源档案文件中成员文件的引用。 u：只将日期较新文件插入档案文件中。 v：程序执行时显示详细的信息。 V：显示版本信息。  练习 # 将当前目录下所有.o打包成libyqutest.a档案文件：r插入，v显示操作信息，s生成符号表。 ar rvs libyqutest.a *.o # 制作瘦档案文件：r插入，c建立档案文件，T指定为瘦档案文件。 ar -rcT libkx.a libke.a libxiao.a  ranlib命令 为档案文件创建符号索引。
ranlib [-vVt] archive  选项： - -v、-V或&amp;ndash;version：显示版本 - -t：更新档案文件符号映射的时戳。
最早在Unix系统上ar程序是单纯用来打包多个.o到.a（类似于tar做的事情），而不处理.o里的符号表。Linker程序则需要.a文件提供一个完整的符号表，所以当时就写了单独的ranlib程序用来产生linker所需要的符号信息，也就是说那时，产生一个对linker合格的的.a文件需要做ar和ranlib两步 。如今，ar-s就做了ranlib的工作，但为了保证这些早期Makefile文件的兼容性，ranlib被保留下来了。
</content>
    </entry>
    
     <entry>
        <title>[MapR培训笔记] Hadoop生态系统</title>
        <url>https://mryqu.github.io/post/mapr%E5%9F%B9%E8%AE%AD%E7%AC%94%E8%AE%B0_hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>ecosystem</tag><tag>mapr</tag>
        </tags>
        <content type="html"> - SQL: Language designed for querying &amp;amp; transformingdata held in a relational database management system. - NoSQL: Database model that&amp;rsquo;s not necessaryly held intabular format. Often, NoSQL refers to data that is flat or nestedin format. - Log Data: Information captured about organization&amp;rsquo;sinternal system, external customer interactions &amp;amp; how they&amp;rsquo;reused - Streaming Data: Twitter, Facebook, Web click data, Webform data. - Flume: Reliable, scalable service used to collectstreaming data in Hadoop cluster.- Sqoop: Transfers data between external data store &amp;amp;Hadoop cluster. Command line tool used with both RDBMS &amp;amp; NoSQLdata.- NFS: Distributed file system protocal allowing computerto access files over network as through they were locally mountedstorage. - Kafka: Fast, scalable open source messaging servicedistributing log data over a cluster. - Map Reduce: Multi step process splitting large data setinto parts, &amp;amp; performing functions to produce finalresult. - Spark: General data analytucs on distributed computingcluster. Provides in-memory computation, enhances speed of dataprocessing over MapReduce.- Drill: Query engine for Big Data exploration. Performsdynamic queries on self-describing data in files (JSON, parquest,text, MapR-DB and HBase tables).- Mahout: Scable machine learning libraries. Corealgorithm supporting data clustering, classfication,categorization, collaborative filter (recommendation engine).- Oozie: Scalable &amp;amp; extensible scheduling systemcreating complex Hadoop workflows. Manages multiple job types inworkflow (MapReduce, Pig, Hive, Sqoop, distcp, script &amp;amp; Javaprogram).- Hive: Data warehouse instructure. SQL-like syntax forqueries using MapReduce. HiveQL provides operations similar to SQL,transformed into MapREduce program for query results.- Pig: Platform for analyzing data. Consists of data flowscripting language called &amp;lsquo;Pig Latin&amp;rsquo; &amp;amp; Instructure forconverting scripts into sequence of MapReduce program.- HBase: Hadoop database, provided random, realtime,read/write access to very large data.- Elatic search: Scalable search server build on top ofLucene that support real-time or near real time search. - Solr: Open source search platform, part of the ApacheLucene project. - SiLK: Interface for Lucene based Solr search platform.Provides tool to perform searches &amp;amp; visualize results in adashboard with reports. - Web Tier: Web site delivery of big data processingresults directly to consumer (new song recommendation base on whatthe consumer hsa listened to previsouly). - Banana: Based on Kibana. Used to create dashboards forvisualizing data stored in Solr. - Kibana: Browser based dashboard for search &amp;amp; dataanalytics. But with HTM: &amp;amp; JavaScript. Easy to set up &amp;amp;used with just a web server. - Data Warehouse: Centralized repository for storing datafrom multiple sources (marketing, finance, human resources, websites, external data stores).
</content>
    </entry>
    
     <entry>
        <title>[Spring Boot] 让非Spring管理的类获得一个Bean</title>
        <url>https://mryqu.github.io/post/spring_boot_%E8%AE%A9%E9%9D%9Espring%E7%AE%A1%E7%90%86%E7%9A%84%E7%B1%BB%E8%8E%B7%E5%BE%97%E4%B8%80%E4%B8%AAbean/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>get</tag><tag>bean</tag>
        </tags>
        <content type="html">  我有一个工具类，它既会被SpringBean调用，也会被非Spring管理的类调用。我想在这个工具类里获得Spring注入了拦截器的RestTemplate。一开始考虑了ApplicationContextAware、ContextLoaderListener和ContextLoaderServlet，最后采用了下面这种改动最小的解决方案。
示例代码 Application.java @SpringBootApplication public class Application{ public static void main(String[] args) { final ApplicationContext applicationContext = SpringApplication.run(Application.class, args); MyUtil.setApplicationContext(applicationContext); } @Bean public RestTemplate restTemplate() { return new RestTemplate(); } }  MyUtil.java public class MyUtil { private static ApplicationContext applicationContext; public static void setApplicationContext(ApplicationContext context) { applicationContext = context; } public static void doSomething() { RestTemplate _restTemplate = applicationContext.getBean(RestTemplate.class); ........ } }  </content>
    </entry>
    
     <entry>
        <title>[Spring Boot] Use alwaysUseFullPath for Spring MVC URL mapping</title>
        <url>https://mryqu.github.io/post/spring_boot_use_alwaysusefullpath_for_spring_mvc_url_mapping/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>url</tag><tag>mapping</tag><tag>alwaysusefullpath</tag>
        </tags>
        <content type="html">  简介 SpringMVC的URL映射有一个控制路径匹配的参数alwaysUseFullPath。当它被设置为true后，总是使用当前servlet上下文中的全路径进行URL查找，否则使用当前servlet映射内的路径。默认为false。下面示例一下当一个请求的全路径通过servlet映射找到所服务的RequestDispatcherservelet后alwaysUseFullPath为false时URL映射表现：
         servlet mapping = &amp;ldquo;/*&amp;ldquo;; request URI = &amp;ldquo;/test/a&amp;rdquo; -&amp;gt; &amp;ldquo;/test/a&amp;rdquo;   servlet mapping = &amp;ldquo;/&amp;rdquo;; request URI = &amp;ldquo;/test/a&amp;rdquo; -&amp;gt; &amp;ldquo;/test/a&amp;rdquo;   servlet mapping = &amp;ldquo;/test/*&amp;ldquo;; request URI = &amp;ldquo;/test/a&amp;rdquo; -&amp;gt; &amp;ldquo;/a&amp;rdquo;   servlet mapping = &amp;ldquo;/test&amp;rdquo;; request URI = &amp;ldquo;/test&amp;rdquo; -&amp;gt; &amp;ldquo;&amp;rdquo;   servlet mapping = &amp;ldquo;/*.test&amp;rdquo;; request URI = &amp;ldquo;/a.test&amp;rdquo; -&amp;gt; &amp;ldquo;&amp;rdquo;    从org.springframework.web.util.UrlPathHelper的getLookupPathForRequest方法可知，当alwaysUseFullPath为true时使用getPathWithinApplication获得待查找的全路径，否则使用getPathWithinServletMapping获得待查找的剩余路径。 如果对alwaysUseFullPath的设置进行修改，对RestController的请求映射也要做相应的设置修改。
@RequestMapping(value = {&amp;quot;**/test/dosomething**&amp;quot;}, method = RequestMethod.POST, produces = { MediaType.APPLICATION_JSON_VALUE })  假设servlet映射为&amp;rdquo;/test/*&amp;ldquo;且RestControoler仅在方法级别进行请求映射，如果alwaysUseFullPath为true时请求映射为上面的&amp;rdquo;/test/dosomething&amp;rdquo;。则在alwaysUseFullPath改为false后，请求映射相应改为&amp;rdquo;/dosomething&amp;rdquo;即可。
alwaysUseFullPath设置范例 想在SpringBoot应用中设定alwaysUseFullPath为true，可通过BeanPostProcessor完成其设置。
@SpringBootApplication public class Application implements BeanPostProcessor { public static void main(String[] args) { final ApplicationContext applicationContext = SpringApplication.run(Application.class, args); } @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof RequestMappingHandlerMapping) { ((RequestMappingHandlerMapping) bean).setAlwaysUseFullPath(true); } return bean; } @Overrid public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { return bean; } }  在我的范例中是使用了RestController和RequestMapping，如果使用simple URLmapping的话则需要将RequestMappingHandlerMapping相应替换为SimpleUrlHandlerMapping。
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] YARN中Application Manager和Application Master区别</title>
        <url>https://mryqu.github.io/post/hadoop_yarn%E4%B8%ADapplication_manager%E5%92%8Capplication_master%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>yarn</tag><tag>hadoop</tag><tag>application</tag><tag>manager</tag><tag>master</tag>
        </tags>
        <content type="html">  术语Application Master和Application Manager经常被交换使用。其实，ApplicationMaster是一个主要的容器，用于请求、启动和监控应用特定资源；而ApplicationManager是资源管理器中的一个部件。 一个作业在YARN中启动流程如下： - 首先客户端向YARN资源管理器提交应用，包括请求容器启动上下文所需的信息。 - 接着资源管理器中的应用管理器协商好一个容器，为应用引导一个Application Master实例。 - 之后Application Master向资源管理器注册并请求容器。 - 当ApplicationMaster同节点管理器进行通信启动所授予的容器之后，为每个容器指定容器启动上下文描述（CLC，包括执行启动的命令、安全令牌、依赖[可执行文件、压缩包]、环境变量等等）。 - Application Master管理应用执行。在执行期间，应用向ApplicationMaster提供进度和状态信息。客户端通过查询资源管理器或直接与ApplicationMaster联系，可以监控应用的状态。 - Application Master向资源管理器报告应用结束。
应用管理器负责维护一系列已提交的应用。当应用提交后，它首先验证应用规格，为ApplicationMaster拒绝任何请求无法满足资源的应用（例如，集群中没有节点有足够资源运行ApplicationMaster自身）。之后确保没有已经运行的使用相同应用ID的其他应用，错误的客户端或恶意客户端有可能导致此类问题。最后，将提交的应用转给调度器。已结束应用从资源管理器内存完全清除之前，此部件也负责记录和管理这些已结束应用。当应用结束，它将应用汇总信息放在守护进程的日志文件。最后，应用管理器在应用完成用户请求后很久都会在缓存中保留该已结束应用。配置参数yarn.resourcemanager.max-completed-applications控制资源管理器在任意时刻可以记住的已结束应用的最大数量。该缓存是先入先出队列，为了存放最新的已结束应用，最老的应用将被移出。 参考 Difference between Application Manager and Application Master in YARN?
Application Master 启动流程与服务简介
</content>
    </entry>
    
     <entry>
        <title>[MapR培训笔记]Hadoop基础</title>
        <url>https://mryqu.github.io/post/mapr%E5%9F%B9%E8%AE%AD%E7%AC%94%E8%AE%B0_hadoop%E5%9F%BA%E7%A1%80/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapr-fs</tag>
        </tags>
        <content type="html">  学习目标  大数据介绍: 理解大数据定义，判断是否存在大数据问题，描述如何用Hadoop解决大数据问题； Hadoop核心元素:描述分布式文件系统如何工作，map/reduce如何在分布式文件系统上处理数据； Hadoop工具生态系统: 了解Hadoop相关工具及其作用； 解决大数据用例: 描述Hadoop生态系统如何协同解决各种大数据用例，如何在不同场景下选择工具。  第一课 数据如何变大 学习目标  定义大数据及大数据问题 描述Hadoop主要部件  大数据 差不多是4V中的前三个,以及使数据难于描述、存储或处理的一些其他特性 - Volume（大量）:太大以至于系统无法处理 - Variety（多样）: 太多不同种类的数据，无法简单描述 - Velocity（高速）: 数据产生太快以至于系统无法处理 - Value（价值）:
Hadoop主要部件 Google收到大数据挑战后，认识到无法用传统关系型数据库解决，创建了GFS&#43;BigTabel&#43;Map/Reduce。 - GFS将文件分割成块，分布在集群的节点上。文件块在不同节点进行复制，以防止节点故障导致的数据丢失。 - BigTable是使用GFS存储和获取数据的（分布式、多级）数据库系统。BigTable使用行键、列键和时戳映射到所存储的数据，可以不重写已有数据在不同时间对相同信息进行采集。行被分片为称之为Tablet的子表，分布到集群内。BigTable被设计成可以处理大量数据，可以无需重新配置已有文件的情况下向集群添加新的节点。 - 并行处理范式map/reduce被用于处理存储在GPS上的数据。map/reduce代表处理的两个步骤。在mapping阶段，所有数据在逻辑上分割，map函数应用于所有割片以生成键值对。框架对所有来自mapper的键值对进行排序然后在reducer之间分片。在reduce阶段，reduce函数应用于所有分片。map/reduce是一种分而治之的方式，将单个庞大的作业分解成一系列小的可管理的任务。
Google实验多年后发表了论文阐述了他们的大数据解决方案。DougCutting以此在Yahoo开发一个项目，后来开源成Apache基金会项目下的Hadoop（Cutting儿子玩具象的名字）。 Mapr利用Hadoop理念，开发了更快、更稳定的企业版Hadoop。
第二课 Hadoop核心 学习目标  本地&amp;amp;分布式文件系统 MapR-FS中的数据管理 （使用命令行）执行数据管理 Map Reduce范式  本地文件系统 HFS/NTFS（读写）和CDFS（只读）都是本地文件系统。文件系统中每个文件都由一个iNode和一系列数据块构成。iNode存储文件元数据，例如文件类型、权限、所有者、文件名和最后一次修改时间等信息，它还存储文件所存储的数据块的指针。数据块用于存储文件的实际内容。 本地文件系统的常见问题 - 硬盘故障：本地硬盘镜像（RAID-1） - 丢失：云镜像 - 人为失误 - 误删：定期增量备份 - 空间不足：增加硬盘、硬盘阵列（RAID-0）
分布式文件系统 分布式文件系统行为与RAID-0类似，但硬盘分布在多个服务器上。由Sun微系统公司开发的网络文件系统NFS仍广泛用于在网络内存储和获取数据。 分布式文件系统当处理数据时致力于透明性，即对于客户端程序来说，文件系统与本地文件系统类似，分布式文件系统不可见。由于数据在网络内多个机器内分布，分布式文件系统使用数据定位器存储数据位置信息。与本地文件系统中的iNode类似，数据定位器只想数据在分布式文件系统中存储的位置。
MapR-FS存储 MapRFS是分布式文件系统，是Hadoop的MapR分发版的底层文件系统。它支持所有前面提到的本地文件系统特性，包括读写访问、本地或远程镜像，及在联机时对文件系统扩容的能力。此外，MapR文件系统可以加载并直接处理已有HDFS或NFS文件系统中的数据。
物理存储 集群是一组使用分布文件系统（例如MapR-FS）的计算机。集群中每个计算机称为一个节点，每个节点有一或多个物理硬盘。 在MapR-FS中，硬盘组合成组，称为存储池（storagepool）。默认一个存储池有三块硬盘组成。当数据写往存储池，数据在三个磁盘拆分写入，增加写速度。每个节点包含一或多个存储池，所有节点上的所有存储池构成了MapR-FS上的全部可用存储。
逻辑存储 MapR-FS将数据写入称之为容器（container）的逻辑单元。一个容器默认大小32GB，一个存储池通常有多个容器。容器内的数据在集群内节点间复制以防止单点故障或硬盘故障。 容器组合成卷（volume）。卷是跨集群内一组节点的数据资源逻辑抽象概念。所有容器及其副本在卷拓扑内的节点上分布。MapR-FS使用称之为容器定位数据库（CDLB）的特定服务存储容器及其副本的位置。CLDB为MapR-FS执行数据定位功能。它为数据存储在那个容器及找到集群内容器及其副本提供查找信息。 许多集群存储策略定义在卷这一级。
定义 拓扑（Topology）、快照（Snapshots）、配额（Quotas）、复制（Replication）、镜像（Mirrors）、权限（Permissions）、压缩（Compression）。 - 拓扑:可以配置不同的拓扑以实施某些数据存放在某些物理位置，例如机架和节点。通过这种方式，你可以设计数据来开发引用局部性以获得性能、可用性和其他适用于你的任意标准。之后可以将这些拓扑与卷进行关联。 - 压缩: 卷内数据当被写入硬盘时会被自动压缩。 - 镜像:可以配置策略集群内本地镜像或远程另一集群镜像。镜像对磁盘故障济公一定程度的保护，远程镜像当整个集群故障时可用于灾难恢复。 - 卷快照:在维护活动卷的持续一致性的同时，员徐创建数据的时间点版本。快照对用户错误提供一定程度的保护，也可以让你在任意时间返回某个数据集版本。快照可以手工创建，也可以基于计划自动定期执行。 - 配额:磁盘空间使用的上限。每个卷、用户或组都可以有一个配额。用户和组配额适用于对该用户和组所负责的所有卷的大小之和。硬配额在达到配额后会阻止写入，软配额则是发送告警邮件。 - 权限: 卷级别的权限可被设置为dump、restore、modify、delete或fullcontrol。卷内的文件和目录可以有标准的UNIX权限。 - 复制:包含卷内容的容器可被冗余复制。默认的复制因子为3，原始数据加上两个备份，但是每个卷都可以设置自己的复制因子。卷内所有容器具有相同的复制设置。
汇总 MapR文件系统支持POSIX语义及NFS导出。 MapR文件系统被写入到一个卷，卷是复制、镜像、快照、拓扑和使用权限等数据管理功能的一个管理抽象。 卷数据保存在容器内，卷用于定义数据位置及数据复制。 容器被写入到存储池，存储池是一个或多个物理硬盘。
知识检查 答案D
执行数据管理 HDFS和MapR-FS命令： - hadoop fs -mkdir mydir: 创建新目录 - haoop fs -copyFromLocal /etc/hosts mydir:从本地文件系统复制文件到MapR-FS - hadoop fs -lsr mydir: 递归列举目录内容 - hadoop fs -cat mydir/hosts: 显示一个文件的内容 - hadoop fs -rm mydir/h: 删除指定文件
MapR-FS特有命令： - hadoop mfs -ln mydir yourdir: 在两个目录之间建立链接 - hadoop mfs -setcompression off mydir: 关闭目录下的文件压缩 - hadoop mfs -setchunksize 65536 mydir: 设置目录下文件的块大小
可在MapR-FS下工作的标准POSIX命令： - mkdir: 创建目录 - cp: 复制文件 - ls: 列举目录内容 - ln: 在两个目录之间建立链接 - rm: 删除文件 - tail: 选定文件指定部分（显示或复制） - grep和awk: 按模式搜索输入 - sed: 流式编辑文件
Map Reduce历史 map reduce过程由于Hadoop而出名，但是它不是一个新概念，早在70年代的Lisp编程就可以看到。 Google使用map reduce以各种方式增强web搜索经验，例如用于帮助索引网页内容以及PageRank算法。 Map Reduce过程 MapReduce过程有三个阶段：map、shuffle和reduce。 map阶段输入在TaskTracker节点中分割，任务被分配给数据所在的节点。map阶段的节点基于输入中的记录发出键值对。 shuffle阶段由Hadoop框架处理，mapper的输出被分片发往reducer。 在reducer阶段，每个reducer接受一个或多个分片，读取键并遍历与键关联的值列表。Reducer基于应用逻辑返回零或多个键值对。
word count算法示例： 第三课 Hadoop生态系统 学习目标  Flume: 将流数据收集到集群的工具 Sqoop: 在外部数据存储（RDBMS、NoSQL）和Hadoop集群之间传输数据的工具 Hbase: Hadoop上的表数据库 Hive: 对Hadoop集群上存储的数据进行SQL相似的查询 Oozie: Hadoop任务管理 Pig: 分析非结构化数据的数据流脚本语言 Mahout: 使用Hadoop进行机器学习 Drill: 用于结构化、半结构化和嵌套数据的低延迟查询引擎 Spark: 用于集群计算的数据处理引擎  Hadoop生态系统 Hadoop的两个核心要素，map-r文件系统和map-reduce，用于存储和处理数据，但是这仅是Hadoop成为平台的开端。 进入的数据是各种结构化和非结构化的格式，Hadoop能用于处理许多不同数据源的数据。为了处理不同数据源，几个应用被开发出来将不同类型的数据收集到Hadoop分布式文件系统。除了map-reduce作为Hadoop中最初的处理范式，各种额外的分析和处理工具被开发出来增加Hadoop的使用范围。最后，各种工具被开发出来对Hadoop分布式处理的结果进行可视化、再挖掘或进行其他处理。 所有的这些城区一起构成了Hadoop生态系统。当学习Hadoop生态系统后，你会注意到一些工具提供类似的功能，其中一些甚至替代核心功能。Hadoop生态系统的一个优点就是允许用户选择最适合自己需求的工具。
Flume Flume是一个可靠的、可伸缩的服务，可用于将流数据收集到Hadoop集群。Flume作为一个代理执行，包括三个关键元素： - 源：从系统日志、web服务器日志、Facebook数据或Twitter订阅等外部源消费事件 - 通道：临时保留流事件 - 目的：从通道删除事件并发送到外部仓库，例如MapR-FS集群。
处理流可被用作单过程，将数据从单个流源取到集群内。大多数对Flume的使用以更复杂的配置使用这些元素的某些组合。常见的用例为多跳、扇入和复用。 - 多跳（Multi-hop）:在多跳用例中，多个源-通道-目的配置连接成一系列，第一个流的目的成为第一个流的源，直到整个流结束。 - 扇入（Fan-in）：在扇入用例中，多个流数据源合并成一个源。: - 复用（Multiplex）：在复用用例中，输入数据根据用户输入被送往不同的通道，之后发往不同的仓库或同一仓库的不同分区。:
无需写与数据源API通信的特定脚本，Flume可以从不同的数据源收集数据，并发往多个仓库。此外，Flume可以配置为在导入数据的同时识别错误并进行反应，使你无需自己实现错误逻辑。
Sqoop Sqoop是一个在外部数据存储和Hadoop集群之间传输数据的工具。Sqoop是一个命令行工具，可用于RDBMS和NoSQL数据。 可以任何客户端运行sqoop，包括你的笔记本。Sqoop将只调用仅含map的MapReduce作业在外部数据存储和Hadoop集群之间传输数据。使用MapReduce对处理提供了并行和容错能力，在集群上的数据可被存储在MapR-FS、Hive表、或HBase表。 Sqoop可在Hadoop集群和外部数据存储之间执行一个表或多个表的导入/导出。，也可以用作执行增量导入或导出，即发生变化的数据会被传输。这些选项通常一起使用，先执行批量导入/导出，然后在此基础上执行增量导入/导出。
HBase Apache HBase是一个开源、分布式、版本化的非关系数据库。HBase被设计成处理大量不一致的数据，尤其适合： - 从系统度量或用户点击等数据源中收集上百万甚至上亿行和列有价值的数据 - 存储聊天或邮件等列之间有不一致值的稀疏数据 - 连续捕获数据并需要随机读写访问，例如web应用的后端或搜索索引HBase由三类服务器组成，以主从架构创建，但所有服务器一起工作提供表的存储和查找服务。 - RegionServer为读写的数据服务。RegionServer与MapR-FS数据节点搭配，这使得ReginServer服务的数据具有数据本地性。 - Master server负责监控集群内所有RegionServer的健康情况，并为数据定位提供元数据服务。 - HBase客户请求数据的一个子集，定位服务所关注行的RegionServer。客户端直接向RegionServer发送读/写请求，不会访问Masterserver来服务请求。 - Zookeeper服务协调场景内的全部服务器。Zookeeper是集中式服务，为所有服务器维护配置信息、命名、分布式同步、心跳和组服务。
Hive Hive是构建于Hadoop之上的数据仓库基础架构，使用MapReduce提供SQL相似语法进行查询。HiveQL语言提供与SQL机构类似的一系列操作，用于转化成MapReduce程序来产生所期望的查询结果。对Hadoop的访问更加简单，Hive查询可以各种方式提交，包括通过CLI、REST之类的web接口，以及通过ODBC或JDBC。 Hive的核心是驱动。当查询提交给Hive，驱动负责执行查询并返回接口。驱动解析查询并转化为一或多个mapreduce作业。对于查询中每个复杂自居，例如JOIN或过滤，额外的mapreduce作业将串行执行，第一个mapreduce作业的结果导入第二个作业，以此类推。因此，Hive适合对非常大或复杂的完全数据集进行查询，其中mapreduce可以利用并行处理和容错能力。
Oozie OOzie是用于创建复杂Hadoop工作流的可伸缩、可扩展的调度程序系统。Oozie可用于管理工作流内多种类型的作业，包括MapReduce、Pig、Hive、Sqoop、distcp以及任意脚本和Java程序。 Hadoop工作流一般分厂复杂，包括以串行、并行方式运行的多个应用程序。Oozie中的工作流以有向无环图（directeda-cyclic graph，DAG）的形式表现。 当在Oozie内创建工作流，客户端可以使用CLI、Java API或RESTAPI之类的web服务编写并编码到一个XML文件。客户端之后向Oozie服务器提交工作流。OOzie服务器联系Hadoop集群内适当的端点以在适当的时间启动每一部分作业。 使用Oozie管理复杂工作流细节，Hadoop应用开发人员无需人工查看作业就可以提交一系列作业，并在转到下一步之前做决定。除了复杂的工作流，Oozie可以显著减少时间相关、定时执行或需要等待数据采集的作业的人工投入量
Pig Pig是一个分析数据的平台。Pig由称之为“PigLatin”的数据流脚本语言及将这些脚本转化成一些列MapReduce程序的基础设施组成。通过使用MapReduce，PigLatin脚本可用于Hadoop平台对大而复杂的数据集执行转换。 Pig的一个常见用途是数据流水线。例如，你想要从web服务器向数据仓库导入日志。使用Pig，你可以从多个服务器加载日志文件，对一或多个文件实行过滤。可以将这些源连接（JOIN）到一起，在最终排序到期望位置之前执行其他转换。对PigLatin中执行数据转换的每一行代码，一个新的MapReduce程序将被创建用于执行转换。 Pig也常用于对非结构化数据的即席（adhoc）查询，无需使用模式（schema）。Pig通过用户定义函数实现高度可扩展，这些函数可以用Java、Python、JavaScript、Ruby或Groovy编写。为了查询和转换非常大的、复杂的或非结构化的数据集，Pig提供了一个非常简单的平台来访问MapReduce的平行和容错功能。
Mahout Mahout是一套可扩展机器学习库。它包含一套核心算法，支持数据聚类（clustering，基于相似的主题收集新的数据项）、分类（classification或categorization，创建一套标签，将新文章打上最适合的标签）或协同过滤（collaborativefiltering，通常指推荐系统）。Mahout也支持底层矩阵数学库，以支持自有算法。Mahout传统使用次序算法和MapReduce算法，但是对ApacheSpark和H20（原名oxdata）的支持已经加入Mahout。 推荐系统引擎工作在装满用户（user）、物品（item）及用户对物品的评分（rating）的数据库上。例如，我们想基于具有类似排名历史的其他用户所购买的物品向一个用户推荐一个新物品。当我们的服务准备好一个新的推荐，首先将用户、物品和评分数据加载并存储到DataModel。 一旦具有自己的DataModel，我们需要定义如何比较我们的用户。首先我们需要提供一个UserSimilarity模型。Mahout具有多个UserSimilarity选项使用不同的度量来比较用户。我们可以选择最适合我们要比较数据的相似性度量。 下一步我们定义UserNeighborhood，这是用于比较的相似用户范围。例如，UserNeighborhood为5将给推荐引擎与所分析用户最相似的5个用户。推荐系统运行所选择的UserSimilarity模型通过DataModel查看每个用户对每个物品的评分。对于我们想要推荐新物品的用户，它会选出5个最接近的用户。这些用户评分醉方的物品将会推荐给我们的用户。 推荐引擎能获得非常复杂的结果、以多种方式过滤推荐物品，可离线执行计算。
Drill Drill是用于大数据探索的查询引擎。在提供工业标准ANSISQL熟悉感和生态系统的同时，Drill从一开始就设计成支持半结构化数据高性能分析，可以快速处理现代大数据应用产生的数据。Drill能对文件中的自描述数据，例如JSON、Parquet、文本，以及MapR-DB和HBase表，执行动态查询，无需请求Hive元数据存储中的元数据定义。Drill通过对所有Hive文件格式和HiveUDF的支持，可以在Hive表和视图上执行查询。 Drill中的核心元素是Drillbit。Drillbit是一个守护进程服务，可以安装在集群的所有节点上，任意一个Drillbit能偶接收并处理查询请求。 当一个Drill查询请求到达集群，它被传给某一节点上的drillbit。该drillbit变成了驱动drillbit，负责管理解析查询，为快速有效执行生成优化后的分布式查询计划。驱动drillbit从ZooKeeper获得集群内有效drillbit节点列表，为最大数据本地化选择执行各种查询片段的合适节点。驱动drillbit根据执行计划，在单个节点上调度查询片段的执行。单个节点执行完查询片段后将数据返回给驱动drillbit，然后由驱动drillbit返回给客户端。 drill对数据源支持的灵活性使其可以足够有效，可为在多个数据仓库上执行合并查询（consolidatingquery）或探索未知数据仅使用drill一个工具就能执行所有查询。
Spark Spark是在例如Hadoop之类的分布式计算集群上执行通用数据分析的框架。Spark提供了内存内计算，极大增强了MapReduce之上的数据处理速度。Spark运行在已有Hadoop集群之上，可以访问存储在MapR-FS之类的分布式文件系统或Hbase中的Hadoop数据。 Spark也可以通过Spark SQL进行直接Hive查询处理Hive中的结构化数据，可以通过SparkStraming处理HDFS、Flume、Kafka、Twitter或自定义数据源上的流数据。 Spark应用可用Java、Scala或Python编写，这让Spark便于用于数据分析。 Spark在用于迭代作业时能特别有效地优化结果，即对相同数据集执行多次机器学习算法。在这种情况下，Spark将数据装载为内存中只读的对象弹性分布式数据集（resilientdistributeddataset，RDD）。在RDD中的数据在集群内节点上分片。如果分片丢失，它可以在内存内重建，提供与Hadoop共同的容错能力。 在机器学习算法的第一个迭代内，Spark和MapReduce具有相似的速度。然而，MapReduce需要从磁盘重新加载数据并为每个迭代启动一个新的作业，而RDD能够缓存在内存中重复使用无需重新加载数据，这使得处理速度显著增加。 通过Spark应用开发的灵活性，RDD的速度和容错能力，流数据、图计算和机器学习的内建库支持，Spark为下一代并行处理分析提供了一个易于使用的平台。
第四课 用例 学习目标  数据仓库优化 大规模日志分析 创建一个推荐引擎  Hadoop生态系统回顾 在第三课里，我们介绍了Hadoop生态系统里的几个部分。这里我们将它们归为如下几类，数据源、数据获取（Ingestion）、数据处理和数据处理结果使用。除了归类，这些话题也组成了经典的Hadoop工作流： - 我们拥有某种或多种数据需要进行信息探究 - 该数据可以使用一种或多种生态系统组件获取并存入Hadoop集群 - 之后使用一种或多种生态系统组件处理数据 - 这些数据处理结果可被我们的组织查看和使用，并可能被再次处理用于进一步研究
数据源 大数据始于数据源，可能是传统结构化数据或新的复杂类型数据，Hadoop对所有数据源提供了单个仓库。 - SQL - 结构化查询语言SQL是用于查询和转换存储在关系型数据库管理系统中的数据。 - NoSQL - NoSQL是无需以表格格式存储的数据模型统称。通常，NoSQL指扁平或嵌套格式的数据。 - 日志数据 - 计算机系统日志抓取了各种各样关于组织内部系统、外部客户交互的信息及其使用方式。 - 流数据 -很多基于web源的流数据，例如Twitter订阅、Facebook帖子、web点击或表单数据都是流数据示例。
数据获取 为了处理多种数据源，几种应用被开发用于获取不同类型数据到Hadoop分布式文件系统。 - Flume - Flume是一个可靠的、可伸缩的将流数据收集到Hadoop集群的服务。 - Sqoop -Sqoop是一个在外部数据存储和Hadoop集群之间传输数据的工具。Sqoop是对RDBMS和NoSQL数据都可使用的一个命令行工具。 - NFS - 网络文件系统是一种分布式文件系统协议，使得计算访问网络之间的文件如同本地挂载存储一般。 - Kafka - Kafka是一个快速、可伸缩的开源消息服务，将日志数据分发到集群内。
数据处理 除了map-reduce作为最初的Hadoop处理范式，很多其他分析和处理工具被开发出来用于其他Hadoop使用。 - MapReduce - Map reduce是多步处理，将大数据分块，然后对每块执行函数生成最终结果。 - Spark -Spark是在Hadoop等分布式计算集群上执行通用数据分析的框架。Spark提供内存内计算，并加大提高了MapReduce上数据处理速度。 - Drill -Drill是用于大数据探索的查询引擎。Drill能对文件中的自描述数据，例如JSON、Parquet、文本，以及MapR-DB和HBase表，执行动态查询，无需请求Hive元数据存储中的元数据定义。 - Pig- Pig是一个分析数据的平台。Pig由称之为“PigLatin”的数据流脚本语言及将这些脚本转化成一些列MapReduce程序的基础设施组成。 - Mahout -Mahout是一套可扩展机器学习库。它包含一套核心算法，支持数据聚类、分类或协同过滤（通常指推荐系统）。 - Oozie -OOzie是用于创建复杂Hadoop工作流的可伸缩、可扩展的调度程序系统。Oozie可用于管理工作流内多种类型的作业，包括MapReduce、Pig、Hive、Sqoop、distcp以及任意脚本和Java程序。 - Hive-Hive是构建于Hadoop之上的数据仓库基础架构，使用MapReduce提供SQL相似语法进行查询。HiveQL语言提供与SQL机构类似的一系列操作，用于转化成MapReduce程序来产生所期望的查询结果。 - HBase - HBase是Hadoop NoSQL数据库，对大量数据提供随机、实时读写访问。 - Elasticsearch -Elasticsearch是构建在Lucene之上的可伸缩搜索服务器，提供实时或近实时搜索。 - Solr- Solr是开源搜索平台，是Apache Lucene项目的一部分。
使用结果 各种工具被开发用来可视化、再挖掘或使用Hadoop分布式处理的结果。 - 数据仓库 -数据仓库适用于数据的集中式仓库。它存储的数据通常来自多个数据源，例如市场、财务和人力资源等内部操作系统数据，或web网站或外部数据源的数据。 - Kibana -Kibana是用于搜索和数据分析的基于浏览器的仪表盘。Kibana由HTML和JavaScript构建，易于仅在一个web服务器上搭建和使用。 - Banana - Banana基于Kibana，用于为存储在Solr内的可视化数据创建仪表板。 - SiLK -SiLK是基于Lucene的Solr搜索平台的接口。SiLK提供用于执行搜索并对仪表盘和报告内数据进行可视化。 - Web层 - Web网站经常向客户直接传递大数据处理结果，例如基于客户之前听过的歌曲推荐新歌。
数据仓库优化 </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 控件的Property、Aggregation和Association如何自动具有的Getter和Setter？</title>
        <url>https://mryqu.github.io/post/openui5_%E6%8E%A7%E4%BB%B6%E7%9A%84propertyaggregation%E5%92%8Cassociation%E5%A6%82%E4%BD%95%E8%87%AA%E5%8A%A8%E5%85%B7%E6%9C%89%E7%9A%84getter%E5%92%8Csetter/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>control</tag><tag>property</tag><tag>getter</tag><tag>setter</tag>
        </tags>
        <content type="html"> 定义了OpenUI5控件的Property、Aggregation、Association和Event后，该控件就会出现这些Property、Aggregation和Association的Getter和Setter，是什么机制自动生成的这些Getter和Setter的？
OpenUI5控件都继承自sap.ui.core.Control，其父类为sap.ui.core.Element，在祖父类为sap.ui.base.ManagedObject。sap.ui.base.ManagedObject类定义了Properties、Aggregations、Associations和Events这些管理特性。
Getter和Setter的生成机制都在sap.ui.core.ManagedObjectMetadata中实现的。首先我们看一下sap.ui.core.ManagedObjectMetadata这个类的源代码片段：
ManagedObjectMetadata.prototype.generateAccessors = function() { var proto = this.getClass().prototype, prefix = this.getName() &#43; &amp;quot;.&amp;quot;, methods = this._aPublicMethods, n; function add(name, fn, info) { if ( !proto[name] ) { proto[name] = (info &amp;amp;&amp;amp; info.deprecated) ? deprecation(fn, prefix &#43; info.name) : fn; } methods.push(name); } for (n in this._mProperties) { this._mProperties[n].generate(add); } for (n in this._mAggregations) { this._mAggregations[n].generate(add); } for (n in this._mAssociations) { this._mAssociations[n].generate(add); } for (n in this._mEvents) { this._mEvents[n].generate(add); } }; function Property(oClass, name, info) { info = typeof info !== &#39;object&#39; ? { type: info } : info; this.name = name; this.type = info.type || &#39;string&#39;; this.group = info.group || &#39;Misc&#39;; this.defaultValue = info.defaultValue !== null ? info.defaultValue : null; this.bindable = !!info.bindable; this.deprecated = !!info.deprecated || false; this.visibility = &#39;public&#39;; this.appData = remainder(this, info); this._oParent = oClass; this._sUID = name; this._iKind = Kind.PROPERTY; var N = capitalize(name); this._sMutator = &#39;set&#39; &#43; N; this._sGetter = &#39;get&#39; &#43; N; if ( this.bindable ) { this._sBind = &#39;bind&#39; &#43; N; this._sUnbind = &#39;unbind&#39; &#43; N; } else { this._sBind = this._sUnbind = undefined; } this._oType = null; } Property.prototype.generate = function(add) { var that = this, n = that.name; add(that._sGetter, function() { return this.getProperty(n); }); add(that._sMutator, function(v) { this.setProperty(n,v); return this; }, that); if ( that.bindable ) { add(that._sBind, function(p,fn,m) { this.bindProperty(n,p,fn,m); return this; }, that); add(that._sUnbind, function(p) { this.unbindProperty(n,p); return this; }); } };  Property类在构造时会甚至_sMutator为set&#43;&amp;lsquo;propName&amp;rsquo;，_sGetter为get&#43;&amp;lsquo;propName&amp;rsquo;；Property的generate方法会通过ManagedObjectMetadata里面的add函数将Property的_sMutator和_sGetter添加到属性的PublicMethods数组中，因此会自动出现属性的Getter和Setter方法。
基于类似的方式得到下面的汇总表：
|控件元数据|自动生成的部分方法 |&amp;mdash;&amp;ndash; |Property|_sMutator：set&#43;&amp;lsquo;propName&amp;rsquo;_sGetter：get&#43;&amp;lsquo;propName&amp;rsquo; |Aggregation（multiple=&amp;ldquo;false&amp;rdquo;）|_sMutator：set&#43;&amp;lsquo;aggrName&amp;rsquo;_sGetter：get&#43;&amp;lsquo;aggrName&amp;rsquo; |Aggregation（multiple=&amp;ldquo;true&amp;rdquo;）|_sMutator：add&#43;&amp;lsquo;aggrName&amp;rsquo;_sGetter：get&#43;&amp;lsquo;aggrName&amp;rsquo;_sInsertMutator：insert&#43;&amp;lsquo;aggrName&amp;rsquo;_sRemoveMutator：remove&#43;&amp;lsquo;aggrName&amp;rsquo;_sRemoveAllMutator：removeAll&#43;&amp;lsquo;aggrName&amp;rsquo;_sIndexGetter：indexOf&#43;&amp;lsquo;aggrName&amp;rsquo; |Association（multiple=&amp;ldquo;false&amp;rdquo;）|_sMutator：set&#43;&amp;lsquo;assoName&amp;rsquo;_sGetter：get&#43;&amp;lsquo;assoName&amp;rsquo; |Association（multiple=&amp;ldquo;true&amp;rdquo;）|_sMutator：add&#43;&amp;lsquo;assoName&amp;rsquo;_sGetter：get&#43;&amp;lsquo;assoName&amp;rsquo;_sRemoveMutator：remove&#43;&amp;lsquo;assoName&amp;rsquo;_sRemoveAllMutator：removeAll&#43;&amp;lsquo;assoName&amp;rsquo; |Event|_sMutator：attach&#43;&amp;lsquo;eventName&amp;rsquo;_sDetachMutator：detach&#43;&amp;lsquo;eventName&amp;rsquo;_sTrigger：fire&#43;&amp;lsquo;eventName&amp;rsquo;
最后展示控件从开始构造到产生属性的函数栈： sap.ui.base.ManagedObject在构造时会创建运行时的元数据，从而驱动Property、Aggregation、Association和Event的Getter和Setter的生成。
</content>
    </entry>
    
     <entry>
        <title>[Git] 操作Git仓库已删除文件</title>
        <url>https://mryqu.github.io/post/git_%E6%93%8D%E4%BD%9Cgit%E4%BB%93%E5%BA%93%E5%B7%B2%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>deleted</tag><tag>file</tag><tag>find</tag><tag>restore</tag>
        </tags>
        <content type="html">  忙着工作，忽然出了一下神，觉得自己对Git仓库已删除文件的操作还没有练习过，决定找资料学习一下。
列举所有Git仓库已删除文件 下列命令可以列举出所有提交信息及被删除的文件：
git log --diff-filter=D --summary  下列命令可以列举出所有被删除的文件，不显示提交信息：
git log --diff-filter=D --summary | grep delete  列举一个Git仓库已删除文件的提交历史信息 仅使用git log无法查看Git仓库已删除文件的提交历史信息。
git log $deletedFile fatal: ambiguous argument &#39;deletedFile&#39;: unknown revision or path not in the working tree.  下列命令则可以：
git log -- $deletedFile  恢复一个Git仓库已删除文件 找到删除该文件的提交哈希值
git rev-list -n 1 HEAD -- $deletedFile  通过删除该文件提交（$deletingCommit）的前一个提交($deletingCommit~1)恢复已删除文件:
git checkout $deletingCommit~1 -- $deletedFile  参考 Is there a way in Git to list all deleted files in the repository
Git: Getting the history of a deleted file
Find and restore a deleted file in a Git repository
Git：ls-files
</content>
    </entry>
    
     <entry>
        <title>Twitter API访问频次限制处理</title>
        <url>https://mryqu.github.io/post/twitter_api%E8%AE%BF%E9%97%AE%E9%A2%91%E6%AC%A1%E9%99%90%E5%88%B6%E5%A4%84%E7%90%86/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twitter</tag><tag>access</tag><tag>rate</tag><tag>limitation</tag><tag>twitter4j</tag>
        </tags>
        <content type="html">  在我前面的博文社交媒体API访问频次限制中，列举了Twitter API访问频次限制。
Twitter API访问频次限制 TwitterAPI访问频次按15分钟为间隔。有两类桶：15分钟内允许15次调用，及15分钟内允许180次调用。Twitter搜索属于后者，在15分钟内允许180次调用。 当向Twitter发送请求后，可以通过解析响应头来获取限制信息。该信息是基于应用/用户上下文的： - X-Rate-Limit-Limit: 对给定请求的访问速率上限 - X-Rate-Limit-Remaining: 15分钟时间窗中剩余请求数 - X-Rate-Limit-Reset: 速率限制复位前（基于UTC）的剩余时间窗秒数
一旦对Twitter的请求超过了频次限制，Twitter将返回HTTP 429 “Too ManyRequests”响应码及如下消息体：
{ &amp;quot;errors&amp;quot;: [ { &amp;quot;code&amp;quot;: 88, &amp;quot;message&amp;quot;: &amp;quot;Rate limit exceeded&amp;quot; } ] }  除了通过解析响应头，还可以通过向Twitter发送rate_limit_status请求获取API访问限制信息。
Twitter4J对Twitter API访问频次限制的处理 Twitter4J的RateLimitStatusJSONImpl类用于处理响应头中的访问限制信息： Twitter4J的TwitterResponseImpl抽象类用于存放解析过的访问限制信息以供应用程序使用： 此外，还可以注册RateLimitStatusListener监听器实例。由Twitter4J的TwitterBaseImpl类可知，当解析到响应头中的API访问频次限制信息，RateLimitStatusListener监听器实例的onRateLimitStatus方法会被调用；当收到的响应码为420&amp;rdquo;Enhance Your Claim&amp;rdquo;、503 &amp;ldquo;Service Unavailable&amp;rdquo;或429 &amp;ldquo;Too ManyRequests&amp;rdquo;，RateLimitStatusListener监听器实例的onRateLimitReached方法将会被调用。
</content>
    </entry>
    
     <entry>
        <title>[Git] 分支笔记</title>
        <url>https://mryqu.github.io/post/git_%E5%88%86%E6%94%AF%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>remote</tag><tag>branch</tag><tag>checkout</tag><tag>push</tag>
        </tags>
        <content type="html">  最近接触了一些Git远程分支的操作和管理，做个笔记。
 创建本地分支  git branch [branch]  切换本地分支  git checkout [branch]  删除本地分支  git branch -D [branch]  重命名本地分支  git branch -m [oldbranch] [newbranch]  查看分支 ```
查看本地分支 （-v选项可以显示sha1和提交消息标题） git branch git branch -v
  # 查看远程分支 git branch -r git branch -rv
# 查看本地和远程分支 git branch -a git branch -av
- 向远程分支推送（远程分支不存在则会创建远程分支）  # 期望本地分支与远程分支同名，可以先切换到本地分支进行提交 git push [remote] [branch]
# 通过-u选项同时使新创建的远程分支成为本地分支的上游分支 git push -u [remote] [branch]
# 期望本地分支与远程分支使用不同名称 git push [remote] [localbranch]:[remotebranch] # 例子： git push origin v9:v9
- 使用存在的远程分支创建本地分支，远程分支也成为新创建的本地分支的上游分支  # 期望本地分支与远程分支同名 git checkout &amp;ndash;track [remote]/[remotebranch] # 例子： git checkout &amp;ndash;track origin/v9 # 当git checkout [branch]执行时，本地分支不存在且仅与一个远程分支名匹配事， # 其效果等同上面&amp;ndash;track选项。
# 期望本地分支与远程分支使用不同名称 git checkout -b [localbranch] [remote]/[remotebranch] # 例子： git checkout -b v9test origin/v9
- 本地分支和远程分支都存在的情况下，使远程分支也成为本地分支的上游分支  # 使远程分支成为当前本地分支的上游分支 git branch -u [remote]/[remotebranch]
# 使远程分支成为某一特定本地分支的上游分支 git branch &amp;ndash;set-upstream-to=[remote]/[remotebranch] [localbranch]
- 去除本地分支的上游分支  git branch &amp;ndash;unset-upstream [branch]
- 删除远程分支  git push [remote] :[remotebranch] # 或 git push &amp;ndash;delete [remote] [remotebranch]
- 在本地库删除已废弃的远程分支  # 远程分支被别人删除后，自己本地库中该远程分支为废弃状态，可使用下列命令移除该远程分支 git remote prune [remote] ```
参考 Git Branching - Branches in a Nutshell
Git Branching - Basic Branching and Merging
Git Branching - Branch Management
Git Branching - Branching Workflows
Git Branching - Remote Branches
Git Branching - Rebasing
Git Branching - Summary
Git - Branch
Git - Checkout
</content>
    </entry>
    
     <entry>
        <title>社交媒体API访问频次限制</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93api%E8%AE%BF%E9%97%AE%E9%A2%91%E6%AC%A1%E9%99%90%E5%88%B6/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>socialmedia</tag><tag>api</tag><tag>rate</tag><tag>limit</tag><tag>quota</tag>
        </tags>
        <content type="html">  Facebook API访问频次限制  Facebook Graph API访问频次限制 Facebook Marketing API访问频次限制  FAcebook Graph API允许每用户每60分钟200次API调用。Facebook MarketingAPI随广告用户级别变化。
Twitter API访问频次限制 TwitterAPI访问频次按15分钟为间隔。有两类桶：15分钟内允许15次调用，及15分钟内允许180次调用。Twitter搜索属于后者，在15分钟内允许180次调用。
Google Analytics API Limits and Quotas  Google Analytics Core Reporting API - API Limits and Quotas Google Analytics Real Time Reporting API - API Limits and Quotas Google Analytics Multi-Channel Funnels Reporting API - API Limits and Quotas
 每个项目每天50000个请求，可增加。
 每个IP 10 QPS（query per second）。
 在Developers Console上，该配额是指per-userlimit。默认设置为1秒1个查询，可被调整为最大值10。 如果你的应用从单个IP地址发出所有API请求，你需要考虑在每个请求中使用userIP或quotaUser参数以获取对每个用户QPS的满配额。    Understand YouTube Analytics API Quota Usage 暂时没有查到Youtube Analytics API固定配额，不过看起来查询维度对配额使用的影响更大。
LinkedIn Company Pages API 配额随API变化。配额以天为间隔，按整个应用、每用户、每开发者进行配置。
微博接口访问频次限制 微博开放接口限制每段时间只能请求一定的次数。限制的单位时间有每小时、每天；限制的维度有单授权用户和单IP；部分特殊接口有单独的请求次数限制。例如： - 一个应用内单授权用户每小时只能请求微博开放接口n次； - 一个应用内单授权用户每天累计只能请求微博开放接口m次； - 一个IP地址每小时只能请求微博开放接口x次； - 发微博接口单授权用户每小时只能请求y次；
其中n、m、x、y的具体数值，微博开放平台将采用应用质量评估系统，实现智能评估应用质量，质量高的应用相应的这些数值就高，也就是请求限制小。因此限制值不固定，不同的应用有不同的限制，取决于应用自身的质量。
微信公众平台接口频率限制 新注册帐号各接口调用频率限制如下： |接口|每日限额 |&amp;mdash;&amp;ndash; |获取access_token|2000 |自定义菜单创建|1000 |自定义菜单查询|10000 |自定义菜单删除|1000 |创建分组|1000 |获取分组|1000 |修改分组名|1000 |移动用户分组|100000 |上传多媒体文件|5000 |下载多媒体文件|10000 |发送客服消息|500000 |高级群发接口|100 |上传图文消息接口|10 |删除图文消息接口|10 |获取带参数的二维码|100000 |获取关注者列表|500 |获取用户基本信息|5000000 |获取网页授权access_token|无 |刷新网页授权access_token|无 |网页授权获取用户信息|无 |设置用户备注名|10000
请注意，在测试号申请页中申请的测试号，接口调用频率限制如下： |接口|每日限额 |&amp;mdash;&amp;ndash; |获取access_token|200 |自定义菜单创建|100 |自定义菜单查询|1000 |自定义菜单删除|100 |创建分组|100 |获取分组|100 |修改分组名|100 |移动用户分组|1000 |素材管理-临时素材上传|500 |素材管理-临时素材下载|1000 |发送客服消息|50000 |获取带参数的二维码|10000 |获取关注者列表|100 |获取用户基本信息|500000 |获取网页授权access_token|无 |刷新网页授权access_token|无 |网页授权获取用户信息|无
</content>
    </entry>
    
     <entry>
        <title>Spring Accessing Facebook Data Guide调试笔记</title>
        <url>https://mryqu.github.io/post/spring_accessing_facebook_data_guide%E8%B0%83%E8%AF%95%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>social</tag><tag>facebook</tag><tag>read_stream</tag><tag>appsecret_proof</tag>
        </tags>
        <content type="html">  尝试Spring Accessing Facebook Data Guide时，除了要像Spring Accessing Twitter Data Guide调试笔记中那样设置代理，还碰到几个其他问题，这里记录一下。
Null Pointer Exception 描述
java.lang.NullPointerException: null at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_51] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_51] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_51] at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_51] at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302) ~[spring-aop-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190) ~[spring-aop-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) ~[spring-aop-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:133) ~[spring-aop-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:121) ~[spring-aop-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) ~[spring-aop-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:208) ~[spring-aop-4.2.3.RELEASE.jar:4.2.3.RELEASE] at com.sun.proxy.$Proxy49.isAuthorized(Unknown Source) ~[na:na] at hello.HelloController.helloFacebook(HelloController.java:26) ~[bin/:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_51] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_51] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_51] at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_51] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:222) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:814) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:737) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:622) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) ~[spring-webmvc-4.2.3.RELEASE.jar:4.2.3.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) ~[tomcat-embed-websocket-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:121) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.2.3.RELEASE.jar:4.2.3.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:217) ~[tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:673) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1500) [tomcat-embed-core-8.0.28.jar:8.0.28] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1456) [tomcat-embed-core-8.0.28.jar:8.0.28] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_51] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.0.28.jar:8.0.28] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]  解决方案：将classpath(&amp;ldquo;org.springframework.boot:spring-boot-gradle-plugin:1.3.0.RELEASE&amp;rdquo;)替换成classpath(&amp;ldquo;org.springframework.boot:spring-boot-gradle-plugin:1.2.3.RELEASE&amp;rdquo;)
redirect_uri is not allowed 描述：Given URL is not allowed by the Applicationconfiguration: One or more of the given URLs is not allowed by theApp&amp;rsquo;s settings. It must match the Website URL or Canvas URL, or thedomain must be a subdomain of one of the App&amp;rsquo;s domains.
解决方案： - Basci setting - Add platform - Website -http://localhost:8080/connect/facebook - Advanced setting - Embedded browser OAuth Login Invalid Scopes: read_stream 描述：read_stream在Facebook SDK2.4还是有效的，在SDK2.5时废弃了。 解决方案：将facebookConnect.html中表单scope字段的内容由&amp;rdquo;read_stream&amp;rdquo;改成&amp;rdquo;email,public_profile&amp;rdquo;。 Facebook error: API calls from the server require anappsecret_proof argument 描述：org.springframework.social.UncategorizedApiException:API calls from the server require an appsecret_proof argument.
解决方案：禁止掉Require App Secret The operation requires &amp;lsquo;read_stream&amp;rsquo; permission. facebook.feedOperations().getHomeFeed()操作就是要read_stream权限，看来是绕不开了。
org.springframework.social.InsufficientPermissionException: The operation requires &#39;read_stream&#39; permission. at org.springframework.social.facebook.api.impl.FacebookErrorHandler.handleFacebookError(FacebookErrorHandler.java:116) at org.springframework.social.facebook.api.impl.FacebookErrorHandler.handleError(FacebookErrorHandler.java:65) at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:614) at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:570) at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:545) at org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:253) at org.springframework.social.facebook.api.impl.FeedTemplate.fetchConnectionList(FeedTemplate.java:367) at org.springframework.social.facebook.api.impl.FeedTemplate.getHomeFeed(FeedTemplate.java:106) at org.springframework.social.facebook.api.impl.FeedTemplate.getHomeFeed(FeedTemplate.java:95) at hello.HelloController.helloFacebook(HelloController.java:31) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:776) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:705) at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966) at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857) at javax.servlet.http.HttpServlet.service(HttpServlet.java:618) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842) at javax.servlet.http.HttpServlet.service(HttpServlet.java:725) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:516) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1086) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:659) at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1558) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1515) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745)  解决方案：要想玩Spring Accessing Facebook Data Guide，必须为应用申请read_stream权限。 </content>
    </entry>
    
     <entry>
        <title>[Spring Boot] Hello MethodInvokingFactoryBean and MethodInvokingBean</title>
        <url>https://mryqu.github.io/post/spring_boot_hello_methodinvokingfactorybean_and_methodinvokingbean/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>methodinvokingfactor</tag><tag>methodinvokingbean</tag>
        </tags>
        <content type="html">  简介 在用spring管理我们的类的时候有时候希望有些属性值是来源于一些配置文件，系统属性，或者一些方法调用的结果，对于前两种使用方式可以使用spring的PropertyPlaceholderConfigurer类来注入，对于后一种则可以使用org.springframework.beans.factory.config.MethodInvokingFactoryBean类来生成需要注入的bean的属性。
通过MethodInvokingFactory Bean类，可注入方法返回值。MethodInvokingFactoryBean用来获得某个方法的返回值，该方法既可以是静态方法，也可以是实例方法。该方法的返回值可以注入bean实例属性，也可以直接定义成bean实例。
MethodInvokingBean是MethodInvokingFactoryBean的父类，更为简单。跟MethodInvokingFactoryBean相比，不会对容器返回任何值。
类层次关系 示例代码： package com.yqu.methodinvoker; import java.util.Arrays; import java.util.Properties; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.config.MethodInvokingBean; import org.springframework.beans.factory.config.MethodInvokingFactoryBean; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; @SpringBootApplication public class Application { private static final Logger log = LoggerFactory.getLogger(Application.class); public static void main(String[] args) { SpringApplication app = new SpringApplication(Application.class); app.setWebEnvironment(false); app.setShowBanner(false); app.run(args); log.info(&amp;quot;sysProp http.proxyHost:&amp;quot;&#43;System.getProperty(&amp;quot;http.proxyHost&amp;quot;)); log.info(&amp;quot;sysProp http.proxyPort:&amp;quot;&#43;System.getProperty(&amp;quot;http.proxyPort&amp;quot;)); } @Bean public MethodInvokingFactoryBean methodInvokingFactoryBean() { MethodInvokingFactoryBean mfBean = new MethodInvokingFactoryBean(); mfBean.setStaticMethod(&amp;quot;java.lang.System.setProperties&amp;quot;); Properties props = System.getProperties(); props.setProperty(&amp;quot;http.proxyHost&amp;quot;, &amp;quot;proxy.yqu.com&amp;quot;); props.setProperty(&amp;quot;http.proxyPort&amp;quot;, &amp;quot;80&amp;quot;); mfBean.setArguments(new Object[] { props }); return mfBean; } @Bean public MethodInvokingBean methodInvokingBean() { MethodInvokingBean mBean = new MethodInvokingBean(); mBean.setStaticMethod(&amp;quot;com.yqu.methodinvoker.Application.finish&amp;quot;); mBean.setArguments( new String[] { &amp;quot;--url&amp;quot;, &amp;quot;jdbc:hsqldb:mem:testdb&amp;quot;, &amp;quot;--user&amp;quot;, &amp;quot;sa&amp;quot;, &amp;quot;--password&amp;quot;, &amp;quot;&amp;quot; } ); return mBean; } public static void finish(String[] args) { log.info(&amp;quot;finish &amp;quot;&#43;Arrays.toString(args)); } private String getStacks() { StringBuilder sb = new StringBuilder(); StackTraceElement[] elements = Thread.currentThread().getStackTrace(); sb.append(&amp;quot;========================\n&amp;quot;); for (int i = 0; i &amp;lt; elements.length; i&#43;&#43;) { sb.append(elements[i]).append(&amp;quot;\n&amp;quot;); } return sb.toString(); } }  输出： 参考 MethodInvokingFactoryBean JavaDoc
MethodInvokingBean JavaDoc
Spring MethodInvokingFactoryBean Example
Set System Property With Spring Configuration File
Proxy Workaround for M3
</content>
    </entry>
    
     <entry>
        <title>Spring Accessing Twitter Data Guide调试笔记</title>
        <url>https://mryqu.github.io/post/spring_accessing_twitter_data_guide%E8%B0%83%E8%AF%95%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>social</tag><tag>twitter</tag><tag>timeout</tag><tag>401</tag>
        </tags>
        <content type="html">  尝试Spring Accessing Twitter Data Guide时碰到几个问题，这里记录一下。
连接超时问题 遇到I/O error on POST request for&amp;rdquo;https://api.twitter.com/oauth/request_token&amp;quot;错误:
org.springframework.web.client.ResourceAccessException: I/O error on POST request for &amp;quot;https://api.twitter.com/oauth/request_token&amp;quot;:Connection timed out: connect; nested exception is java.net.ConnectException: Connection timed out: connect at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:582) at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:547) at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:468) at org.springframework.social.oauth1.OAuth1Template.exchangeForToken(OAuth1Template.java:187) at org.springframework.social.oauth1.OAuth1Template.fetchRequestToken(OAuth1Template.java:115) at org.springframework.social.connect.web.ConnectSupport.fetchRequestToken(ConnectSupport.java:212) at org.springframework.social.connect.web.ConnectSupport.buildOAuth1Url(ConnectSupport.java:199) at org.springframework.social.connect.web.ConnectSupport.buildOAuthUrl(ConnectSupport.java:126) at org.springframework.social.connect.web.ConnectController.connect(ConnectController.java:226) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:775) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:705) at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:967) at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:869) at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:843) at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:217) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:673) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1500) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1456) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745) Caused by: java.net.ConnectException: Connection timed out: connect at java.net.DualStackPlainSocketImpl.connect0(Native Method) at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668) at sun.security.ssl.BaseSSLSocketImpl.connect(BaseSSLSocketImpl.java:173) at sun.net.NetworkClient.doConnect(NetworkClient.java:180) at sun.net.www.http.HttpClient.openServer(HttpClient.java:432) at sun.net.www.http.HttpClient.openServer(HttpClient.java:527) at sun.net.www.protocol.https.HttpsClient.(HttpsClient.java:275) at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:371) at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191) at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1104) at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:998) at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177) at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) at org.springframework.http.client.SimpleBufferingClientHttpRequest.executeInternal(SimpleBufferingClientHttpRequest.java:81) at org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48) at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:53) at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:571) ... 53 more  由于我是在国内连接Twitter，所以需要代理。Spring Social:SOCIAL-146: Enable use of Spring Social behind proxy任务对org.springframework.social.support.ClientHttpRequestFactorySelector类做了修改，现在可以通过设置系统变量http.proxyHost和http.proxyPort来解决连接超时失败这个问题。 401 (Authorization Required)问题 遇到POST request for &amp;ldquo;https://api.twitter.com/oauth/request_token&amp;quot;resulted in 401 (Authorization Required); invoking errorhandler错误，根据stackoverflow的一个帖子，将我的Twitter应用的CallbackURL随便设一个就解决了。Callback URL为空时，我用Twitter4J也没问题。帖子说Sping SocialFacebook/LinkedIn没这问题，应该是Sping Social Twitter处理有缺陷。 </content>
    </entry>
    
     <entry>
        <title>Hello Spring Social LinkedIn</title>
        <url>https://mryqu.github.io/post/hello_spring_social_linkedin/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>social</tag><tag>linkedin</tag><tag>crawler</tag><tag>datascience</tag>
        </tags>
        <content type="html">  本想玩玩Spring SocialLinkedIn，看看能从LinkedIn哪里获得什么有价值的数据。可是LinkedIn现在放开的只有r_basicprofile、r_emailaddress、rw_company_admin、w_share权限，如LinkedIn developer program transition所说的不要在认证中请求r_fullprofile、r_network、r_contactinfo、rw_nus、rw_groups和w_messages权限了。 在LinkedIn API Console中可试最多的是CompaniesAPI，可是我在LinkedIn上没有公司主页可以创建。所以浅尝则止，没什么太多可分享的。
参考 Spring Social LinkedIn Project
Spring Social Project
GitHub: spring-projects/spring-social-samples
LinkedIn Developer
LinkedIn API Console
LinkedIn developer program transition
</content>
    </entry>
    
     <entry>
        <title>Hello Spring Social Twitter</title>
        <url>https://mryqu.github.io/post/hello_spring_social_twitter/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>social</tag><tag>twitter</tag><tag>crawler</tag><tag>datascience</tag>
        </tags>
        <content type="html">  学习了Spring Accessing Twitter Data Guide，稍作修改，练习一下用Spring Social Twitter搜索推文。
HelloSpringTwitter代码 src/main/java/com/yqu/springtwitter/Application.java package com.yqu.springtwitter; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } }  src/main/java/com/yqu/springtwitter/HelloController.java package com.yqu.springtwitter; import javax.inject.Inject; import org.springframework.social.connect.ConnectionRepository; import org.springframework.social.twitter.api.SearchResults; import org.springframework.social.twitter.api.Twitter; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RequestParam; @Controller @RequestMapping(&amp;quot;/&amp;quot;) public class HelloController { private Twitter twitter; private ConnectionRepository connectionRepository; @Inject public HelloController(Twitter twitter, ConnectionRepository connectionRepository) { this.twitter = twitter; this.connectionRepository = connectionRepository; } @RequestMapping(method=RequestMethod.GET) public String helloTwitter(Model model, @RequestParam( value = &amp;quot;searchTerm&amp;quot;, defaultValue = &amp;quot;mryqu&amp;quot;, required = false) String searchTerm, @RequestParam( value = &amp;quot;limit&amp;quot;, defaultValue = &amp;quot;20&amp;quot;, required = false) int limit) { if (connectionRepository.findPrimaryConnection( Twitter.class) == null) { return &amp;quot;redirect:/connect/twitter&amp;quot;; } //OAuth1Connection conn = (OAuth1Connection) // connectionRepository.findPrimaryConnection(Twitter.class); //System.out.println(conn); model.addAttribute(twitter.userOperations().getUserProfile()); SearchResults res = twitter.searchOperations().search(searchTerm, limit); model.addAttribute(&amp;quot;tweets&amp;quot;, res.getTweets()); return &amp;quot;hello&amp;quot;; } }  src/main/resources/templates/connect/twitterConnect.html src/main/resources/templates/connect/twitterConnected.html src/main/resources/templates/hello.html src/main/resources/application.properties spring.social.twitter.appId={{put app ID here}} spring.social.twitter.appSecret={{put app secret here}}  build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.7.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;hello-spring-twitter&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-thymeleaf&amp;quot;) compile(&amp;quot;org.springframework.social:spring-social-twitter&amp;quot;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  测试 参考 Spring Social Twitter Project
Spring Social Twitter Reference
Spring Social Project
Spring Guide: Registering an Application with Twitter
Spring Guide: Accessing Twitter Data
Spring Guide: Creating a stream of live twitter data using Spring XD
</content>
    </entry>
    
     <entry>
        <title>Cisco AnyConnect Secure Mobility Client的VPN profile位置</title>
        <url>https://mryqu.github.io/post/cisco_anyconnect_secure_mobility_client%E7%9A%84vpn_profile%E4%BD%8D%E7%BD%AE/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>cisco</tag><tag>anyconnect</tag><tag>vpn</tag><tag>profile</tag>
        </tags>
        <content type="html"> 在其安装路径C:\Program Files\Cisco\Cisco AnyConnect Secure MobilityClient搜寻未果。
全盘搜索后，发现在这个位置：C:\ProgramData\Cisco\Cisco AnyConnect Secure MobilityClient\Profile
</content>
    </entry>
    
     <entry>
        <title>[Eclipse] 遭遇Unable to install breakpoint due to missing line number attribute</title>
        <url>https://mryqu.github.io/post/eclipse_%E9%81%AD%E9%81%87unable_to_install_breakpoint_due_to_missing_line_number_attribute/</url>
        <categories>
          <category>Tool</category><category>Eclipse</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>breakpoint</tag><tag>enhancerbyspringcgli</tag><tag>absent</tag><tag>line_number</tag>
        </tags>
        <content type="html"> 今天遇到了“Unable to install breakpoint due to missing line numberattribute. Modify compiler options to generate line numberattributes”问题。 检查Preferences -&amp;gt; Java -&amp;gt; Perference，&amp;rdquo;Add linenumber attributes to generated class files (used by thedebugger)&amp;ldquo;已经勾选了。 应该是SpringAOP产生的代码没有行数信息，但是我自己写的代码还是带行数信息的，因此虽然会弹出这些烦人的警告，所设断点还是其作用的。让它不再提示即可。
</content>
    </entry>
    
     <entry>
        <title>[Spring Boot] 使用多个Servlet</title>
        <url>https://mryqu.github.io/post/spring_boot_%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AAservlet/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>multiple</tag><tag>servlet</tag>
        </tags>
        <content type="html">  当使用Spring boot的嵌入式servlet容器时，可以通过Springbean或扫描Servlet组件的方式注册Servlet、Filter和Servlet规范的所有监听器(例如HttpSessionListener)。 - 当urlMapping不是很复杂时，可以通过ServletRegistrationBean、FilterRegistrationBean和ServletListenerRegistrationBean获得完整控制。如果bean实现了ServletContextInitializer接口的话则可以直接注册。 - 当使用@ServletComponentScan扫描Servlet组件时，Servlet、过滤器和监听器可以是通过@WebServlet、@WebFilter和@WebListener自动注册
示例代码 Application.java package com.yqu.multiservlet; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.context.embedded.ServletRegistrationBean; import org.springframework.context.annotation.Bean; import org.springframework.web.servlet.DispatcherServlet; @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } @Bean public ServletRegistrationBean dispatcherRegistration( DispatcherServlet dispatcherServlet) { ServletRegistrationBean registration = new ServletRegistrationBean(dispatcherServlet); registration.addUrlMappings(&amp;quot;/hirest/*&amp;quot;); printStacks(); return registration; } @Bean public ServletRegistrationBean servletRegistrationBean() { printStacks(); return new ServletRegistrationBean( new SigninServlet(), &amp;quot;/signin&amp;quot;); } private void printStacks() { StackTraceElement[] elements = Thread.currentThread().getStackTrace(); System.out.println(&amp;quot;========================&amp;quot;); for (int i = 0; i &amp;lt; elements.length; i&#43;&#43;) { System.out.println(elements[i]); } } }  SigninServlet.java package com.yqu.multiservlet; import javax.servlet.ServletConfig; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; public class SigninServlet extends HttpServlet { public void init(ServletConfig config) throws ServletException { super.init(config); } protected void doGet( HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.sendRedirect(&amp;quot;http://blog.sina.com.cn/yandongqu&amp;quot;); } }  HelloController.java package com.yqu.multiservlet; import org.springframework.hateoas.ResourceSupport; import org.springframework.http.HttpEntity; import org.springframework.http.HttpStatus; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.RestController; import static org.springframework.hateoas.mvc.ControllerLinkBuilder.linkTo; import static org.springframework.hateoas.mvc.ControllerLinkBuilder.methodOn; @RestController public class HelloController { @RequestMapping(value = &amp;quot;/&amp;quot;, method = RequestMethod.GET) @ResponseBody public HttpEntity home() { ResourceSupport home = new ResourceSupport(); home.add(linkTo(methodOn(HelloController.class).home()).withSelfRel()); return new ResponseEntity(home, HttpStatus.OK); } }  application.properties server.context-path=/HelloMultiServlet server.port=8080 applicationDefaultJvmArgs: [ &amp;quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=55558&amp;quot; ]  build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.6.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;hello-multiservlet&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) compile(&amp;quot;com.fasterxml.jackson.core:jackson-databind&amp;quot;) compile(&amp;quot;org.springframework.hateoas:spring-hateoas&amp;quot;) compile(&amp;quot;org.springframework.plugin:spring-plugin-core:1.1.0.RELEASE&amp;quot;) compile(&amp;quot;com.jayway.jsonpath:json-path:0.9.1&amp;quot;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  日志 测试  通过REST访问http://localhost:8080/HelloMultiServlet/hirest/  浏览器访问http://localhost:8080/HelloMultiServlet/signin ，则会跳转到本博客。  参考 Spring Boot Reference Guide
Using multiple dispatcher servlets / web contexts with spring boot
How to configure spring-boot servlet like in web.xml?
GitHub：spring-boot/spring-boot-samples/spring-boot-sample-servlet
</content>
    </entry>
    
     <entry>
        <title>八卦一下H2O</title>
        <url>https://mryqu.github.io/post/%E5%85%AB%E5%8D%A6%E4%B8%80%E4%B8%8Bh2o/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>机器学习</tag><tag>预测性分析平台</tag><tag>开源</tag><tag>h2o.ai</tag><tag>h2o</tag>
        </tags>
        <content type="html">  H2O的对象是需要可伸缩的快速机器学习的数据科学家和业务分析师。H2O是开源预测性分析平台。H2O提供非凡的数学能力、高性能并发处理及无比的易用性。H2O处理数据科学语言，支持R、Python、Scala、Java和一套健壮的RESTAPI。智能业务应用由H2O的NanoFastTM评分引擎驱动。
H2O介绍 H2O使采用数学和预测性分析解决当今最有挑战的业务问题成为可能。它具有其他机器学习平台当前所没有的功能：
 最佳开源技术 –享受开源技术大数据科学所带来的自由。H2O通过最流行的开源产品ApacheTMHadoop和SparkTM使客户更加灵活方便地解决最有挑战性数据问题。 易于使用的WebUI和熟悉的接口 –使用H2O的基于Web的用户界面或熟悉的编程环境（例如R、Java、Scala、Python、JSON）可以快速设置并入门。 支持所有通用数据库和文件类型 – 对微软Excel、RStudio、Tableau等软件内的大数据可以轻松浏览和建模。能够连接HDFS、S3、SQL和NoSQL数据源。可在任何地方安装和部署。 海量大数据分析 –在全部数据集而不是小样本上训练模型，通过H2O快速in-memory分布平行处理实时迭代和开发模型。 实时数据评分 –为了精确预测，使用Nanofast评分引擎可在任何环境以纳秒级对模型进行数据评分。它比当前市场上最接近的技术（说的是谁？）在评分和预测上快十倍。  H2O.ai公司 h2o.ai公司（之前叫0xdata?）于2011年(Linkedin显示2012年？)成立于加州山景城，目前员工不到50人。 H2O.ai公司的创始人有两位：SriSatish Ambati和Cliff Click。 参考 H2O.ai公司网站
GitHub：h2oai/h2o-3
Oxdata研发H2O，打造大数据新蓝图
Oxdata获890万美元融资：推H2O开源机器学习项目
</content>
    </entry>
    
     <entry>
        <title>Spring 框架: @RestController vs @Controller</title>
        <url>https://mryqu.github.io/post/spring%E6%A1%86%E6%9E%B6_restcontroller_vs_controller/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>restcontroller</tag><tag>controller</tag><tag>difference</tag><tag>区别</tag>
        </tags>
        <content type="html">  今天扫了一眼RestController注解的实现，它是@Controller和@ResponseBody的合体。
@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Controller @ResponseBody public @interface RestController { String value() default &amp;quot;&amp;quot;; }  至于@RestController与@Controller的所有区别，还不是完全明了。看了Srivatsan Sundararajan和Swapna Sagi的大作Spring Framework: @RestController vs @Controller，感觉豁然开朗。
Spring MVC框架和REST Spring基于MVC框架的注解简化了创建RESTful web服务流程。传统MVS控制器和RESTfulweb服务控制器关键区别在于HTTP响应体创建方式。传统MVC控制器依赖试图技术，而RESTfulweb服务控制器仅仅返回对象并将对象数据作为JSON/XML直接写到HTTP响应中。关于使用Spring框架创建RESTfulWEB服务的技术细节，点击这里。 图1: Spring MVC传统工作流
Spring MVC REST工作流 传统Spring MVC REST工作流步骤如下: - 客户端以URI形式向web服务发送一个请求。The client sends a request to a webservice in URI form. - 请求被DispatcherServlet拦截用于查找处理器映射（Handler Mappings）及类型。 - 在应用上下文文件中定义的处理器映射会告知DispatcherServlet用于基于请求查找控制器的策略。 - Spring MVC支持三种类型的请求URI与控制器间的映射：注解、名称转换和显式映射。 - 请求由控制器处理后，响应返回给DispatcherServlet后分发给视图。
在图1中，注意在传统工作流中ModelAndView对象由控制器转发给客户端。在方法上使用@ResponseBody注解，Spring可让应用直接从控制器返回数据，不再查找视图。从第4版起，引入@RestController注解进一步简化处理流程。两种使用方式解释如下。
使用@ResponseBody注解 当对一个方法使用@ResponseBody注解后，Spring将返回值进行转换并自动写入Http响应中。控制器类的每个方法必须使用@ResponseBody进行注解。 图2: Spring 3.x MVC RESTful web服务工作流
幕后工作 Spring在幕后注册了一系列HttpMessageConverters。HTTPMessageConverter负责根据预先定义的MIME类型将请求体转换成特定类及将特定类转换成响应体。每次一个请求匹配上@ResponseBody，Spring遍历所有已注册的HTTPMessageConverter，查找到第一个匹配上给定MIME类型和类的HTTPMessageConverter用之进行实际转换。
代码示例 下面过一个使用@ResponseBody的简单示例。
 创建名为Employee的Java POJO类。 ``` package com.example.spring.model;   import javax.xml.bind.annotation.XmlRootElement;
@XmlRootElement(name = &amp;ldquo;Employee&amp;rdquo;) public class Employee {
 String name; String email; public String getName() { return name; } public void setName(String name) { this.name = name; } public String getEmail() { return email; } public void setEmail(String email) { this.email = email; } public Employee() { }  }
2. 创建如下@Controller类：  package com.example.spring.rest;
import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.ResponseBody;
import com.example.spring.model.Employee;
@Controller @RequestMapping(&amp;ldquo;employees&amp;rdquo;) public class EmployeeController {
 Employee employee = new Employee(); @RequestMapping(value = &amp;quot;/{name}&amp;quot;, method = RequestMethod.GET, produces = &amp;quot;application/json&amp;quot;) public @ResponseBody Employee getEmployeeInJSON( @PathVariable String name) { employee.setName(name); employee.setEmail(&amp;quot;employee1@genuitec.com&amp;quot;); return employee; } @RequestMapping(value = &amp;quot;/{name}.xml&amp;quot;, method = RequestMethod.GET, produces = &amp;quot;application/xml&amp;quot;) public @ResponseBody Employee getEmployeeInXML( @PathVariable String name) { employee.setName(name); employee.setEmail(&amp;quot;employee1@genuitec.com&amp;quot;); return employee; }  }
 注意@ResponseBody被添加到每个@RequestMapping方法的返回值中。 3. 将`&amp;lt;context:component-scan&amp;gt;`和`&amp;lt;mvc:annotation-driven/&amp;gt;`添加到Spring配置文件中。 - 激活注解并扫描包以查找并在应用上下文中注册bean。 - `&amp;lt;mvc:annotation-driven/&amp;gt;`在classpath包含ackson/JAXB库文件时添加对JSON/XML读写的支持。 - 在项目classpath上添加用于JSON格式的jackson-databindjar包和用于XML格式的jaxb-api-osgi jar包。 4. 在任何服务器上部署和运行应用。 **JSON**—使用URL：`http://localhost:8080/SpringRestControllerExample/rest/employees/Bob` 输出显示如下: ![JSON输出](/images/2015/10/0026uWfMzy7evNZL66i96.png) _图3:JSON输出_ **XML**—使用URL：`http://localhost:8080/SpringRestControllerExample/rest/employees/Bob.xml` 输出显示如下:![Spring 框架: @RestController vs @Controller](/images/2015/10/0026uWfMzy7evNUZcX7b9.jpg) _图4:XML输出_ ## 使用@RestController注解 Spring4.0引入@RestController，一个特殊版本的控制器，除了添加@Controller和@ResponseBody外与一般控制器无异。通过对控制器类使用@RestController注解，无需对其请求映射方法添加@ResponseBody注解。默认@ResponseBody注解已被激活。点击[这里](http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/web/bind/annotation/RestController.html)获取更多细节。 ![Spring 4.x MVC RESTful Web服务工作流](/images/2015/10/0026uWfMzy7evOq7K6c20.png)_图5: Spring 4.x MVC RESTful Web服务工作流_ 为了在示例中使用@RestController，继续将@Controller改为@RestController且移除每个方法上的@ResponseBody注解。改动后代码如下：  package com.example.spring.rest;
import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RestController;
import com.example.spring.model.Employee;
@RestController @RequestMapping(&amp;ldquo;employees&amp;rdquo;) public class EmployeeController { Employee employee = new Employee();
@RequestMapping(value = &amp;ldquo;/{name}&amp;rdquo;, method = RequestMethod.GET, produces = &amp;ldquo;application/json&amp;rdquo;) public Employee getEmployeeInJSON(@PathVariable String name) { employee.setName(name); employee.setEmail(&amp;ldquo;employee1@genuitec.com&amp;rdquo;); return employee; }
@RequestMapping(value = &amp;ldquo;/{name}.xml&amp;rdquo;, method = RequestMethod.GET, produces = &amp;ldquo;application/xml&amp;rdquo;) public Employee getEmployeeInXML(@PathVariable String name) { employee.setName(name); employee.setEmail(&amp;ldquo;employee1@genuitec.com&amp;rdquo;); return employee; } } ``` 注意每个请求映射方法不再需要@ResponseBody注解。重新运行改动后应用，其输出与之前的结果没有变化。
结论 如大家所看到的一样，使用@RestController更加简单，是Spring v4.0之后用于创建MVC RESTfulweb服务的首选方法。
源码分析 RequestMappingHandlerAdapter类的supportsReturnType方法同时对类和方法检查是否支持@ResponseBody注解，如果支持，则handleReturnValue方法会选择合适的HttpMessageConverter处理控制器方法返回值。而@RestController注解继承了@ResponseBody，所以肯定会命中上述RequestMappingHandlerAdapter类的方法。
参考 Spring MVC之@RequestBody, @ResponseBody 详解
</content>
    </entry>
    
     <entry>
        <title>Gradle multi-project Builds on HelloSocialMedia</title>
        <url>https://mryqu.github.io/post/gradle_multi-project_builds_on_hellosocialmedia/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>multi-project</tag><tag>build</tag><tag>socialmedia</tag>
        </tags>
        <content type="html">  尝试对我的HelloSocialMedia演示代码集合使用Gradle的多项目构建，项目结构如下：
HelloSocialMedia/ build.gradle settings.gradle HelloYoutubeAnalytics/ build.gradle HelloGoogleAnalytics/ build.gradle HelloTwitter4J/ build.gradle HelloRestFB/ build.gradle  settings.gradle
rootProject.name = &#39;HelloSocialMedia&#39; include &amp;quot;HelloYoutubeAnalytics&amp;quot; include &amp;quot;HelloGoogleAnalytics&amp;quot; include &amp;quot;HelloTwitter4J&amp;quot; include &amp;quot;HelloRestFB&amp;quot; project(&amp;quot;:HelloYoutubeAnalytics&amp;quot;).name = &amp;quot;HelloSocialMedia-YoutubeAnalytics&amp;quot; project(&amp;quot;:HelloGoogleAnalytics&amp;quot;).name = &amp;quot;HelloSocialMedia-GoogleAnalytics&amp;quot; project(&amp;quot;:HelloTwitter4J&amp;quot;).name = &amp;quot;HelloSocialMedia-Twitter4J&amp;quot; project(&amp;quot;:HelloRestFB&amp;quot;).name = &amp;quot;HelloSocialMedia-RestFB&amp;quot;  build.gradle
buildscript { repositories { mavenCentral() } } subprojects { apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 }  HelloYoutubeAnalytics/build.gradle
jar { baseName = &#39;hello-youbube-analytics&#39; version = &#39;0.1.0&#39; } dependencies { compile &#39;com.google.api-client:google-api-client-gson:1.20.0&#39; exclude module: &#39;httpclient&#39; compile &#39;com.google.apis:google-api-services-youtube:v3-rev150-1.20.0&#39; compile &#39;com.google.apis:google-api-services-youtubeAnalytics:v1-rev53-1.20.0&#39; compile &#39;com.google.apis:google-api-services-youtubereporting:v1-rev1-1.20.0&#39; }  HelloGoogleAnalytics/build.gradle
jar { baseName = &#39;hello-google-analytics&#39; version = &#39;0.1.0&#39; } dependencies { compile &#39;com.google.api-client:google-api-client-gson:1.20.0&#39; exclude module: &#39;httpclient&#39; compile &#39;com.google.apis:google-api-services-analytics:v3-rev118-1.20.0&#39; }  HelloTwitter4J/build.gradle
jar { baseName = &#39;hello-twitter4j&#39; version = &#39;0.1.0&#39; } dependencies { compile &#39;org.twitter4j:twitter4j-core:4.0&#43;&#39; }  HelloRestFB/build.gradle
jar { baseName = &#39;hello-restfb&#39; version = &#39;0.1.0&#39; } dependencies { compile &#39;com.restfb:restfb:1.15.0&#39; }  参考 Gradle user guide: Multi-project Builds
</content>
    </entry>
    
     <entry>
        <title>Hello RestFB</title>
        <url>https://mryqu.github.io/post/hello_restfb/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>restfb</tag><tag>demo</tag><tag>crawler</tag><tag>socialmedia</tag>
        </tags>
        <content type="html">  RestFB是一个简单灵活的Facebook图谱API和REST客户端Java库。本演示用它来获取一个Facebook主页下的帖子、评论及回复。
获取Facebook AccessToken 示例代码 package com.yqu.restfb; import java.util.List; import com.restfb.Connection; import com.restfb.DefaultFacebookClient; import com.restfb.FacebookClient; import com.restfb.Parameter; import com.restfb.Version; import com.restfb.types.Comment; import com.restfb.types.Page; import com.restfb.types.Post; public class HelloRestFB { public static void main(String[] args) { FacebookClient facebookClient = new DefaultFacebookClient( MY_ACCESS_TOKEN, Version.VERSION_2_5); Page pageInfo = (Page) facebookClient.fetchObject(&amp;quot;YquTest&amp;quot;, Page.class, new Parameter[0]); Connection postConnection = facebookClient.fetchConnection( pageInfo.getId() &#43; &amp;quot;/feed&amp;quot;, Post.class, new Parameter[] { Parameter.with(&amp;quot;limit&amp;quot;, 10), Parameter.with(&amp;quot;include_hidden&amp;quot;, &amp;quot;true&amp;quot;) }); if (postConnection.getData().size() &amp;lt;= 0) { System.out.println(&amp;quot;No posts found.&amp;quot;); return; } for (List allPosts : postConnection) { for (Post post : allPosts) { System.out.println(&amp;quot;post:&amp;quot; &#43; post); // if (post.getComments() != null) { Connection commentConnection = facebookClient .fetchConnection( post.getId() &#43; &amp;quot;/comments&amp;quot;, Comment.class, new Parameter[] { Parameter.with(&amp;quot;limit&amp;quot;, 10), Parameter.with(&amp;quot;include_hidden&amp;quot;, &amp;quot;true&amp;quot;) }); if (commentConnection.getData().size() == 0) continue; for (List allComments : commentConnection) { for (Comment comment : allComments) { System.out.println(&amp;quot;comment:&amp;quot; &#43; comment); Connection replyConnection = facebookClient .fetchConnection(comment.getId() &#43; &amp;quot;/comments&amp;quot;, Comment.class); if (replyConnection.getData().size() == 0) continue; for (List allReplies : replyConnection) { for (Comment reply : allReplies) { System.out.println(&amp;quot;reply:&amp;quot; &#43; reply); } } } } } } } } }  build.gradle buildscript { repositories { mavenCentral() } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; jar { baseName = &#39;hello-restfb&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile &#39;com.restfb:restfb:1.15.0&#39; } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  输出 post:Post[actions=[] adminCreator=null application=null attachments=null attribution=null caption=null comments=null commentsCount=0 createdTime=Wed Oct 21 04:22:55 EDT 2015 description=null feedTargeting=null from=null fullPicture=null icon=null id=778979068896920_779019608892866 isHidden=null isPublished=null likes=null likesCount=null link=null message=test again messageTags={} metadata=null name=null objectId=null picture=null place=null privacy=null properties=[] shares=null sharesCount=0 source=null statusType=null story=null targeting=null to=[] type=null updatedTime=null withTags=[]] post:Post[actions=[] adminCreator=null application=null attachments=null attribution=null caption=null comments=null commentsCount=0 createdTime=Wed Oct 21 03:58:48 EDT 2015 description=null feedTargeting=null from=null fullPicture=null icon=null id=778979068896920_779014235560070 isHidden=null isPublished=null likes=null likesCount=null link=null message=haha messageTags={} metadata=null name=null objectId=null picture=null place=null privacy=null properties=[] shares=null sharesCount=0 source=null statusType=null story=null targeting=null to=[] type=null updatedTime=null withTags=[]] comment:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 04:23:23 EDT 2015 from=CategorizedFacebookType[category=null id=10207944381947076 metadata=null name=Andrew Qu type=null] id=779014235560070_779019725559521 isHidden=null likeCount=null likes=null message=haha metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] post:Post[actions=[] adminCreator=null application=null attachments=null attribution=null caption=null comments=null commentsCount=0 createdTime=Wed Oct 21 03:26:42 EDT 2015 description=null feedTargeting=null from=null fullPicture=null icon=null id=778979068896920_779009285560565 isHidden=null isPublished=null likes=null likesCount=null link=null message=Test2 messageTags={} metadata=null name=null objectId=null picture=null place=null privacy=null properties=[] shares=null sharesCount=0 source=null statusType=null story=null targeting=null to=[] type=null updatedTime=null withTags=[]] comment:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 03:26:55 EDT 2015 from=CategorizedFacebookType[category=null id=778979068896920 metadata=null name=YquTest type=null] id=779009285560565_779009298893897 isHidden=null likeCount=null likes=null message=Comment2 metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] reply:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 03:27:04 EDT 2015 from=CategorizedFacebookType[category=null id=778979068896920 metadata=null name=YquTest type=null] id=779009285560565_779009332227227 isHidden=null likeCount=null likes=null message=Reply2 metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] reply:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 03:59:09 EDT 2015 from=CategorizedFacebookType[category=null id=10208184284032675 metadata=null name=Bevan Li type=null] id=779009285560565_779014298893397 isHidden=null likeCount=null likes=null message=qie metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] reply:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 03:59:35 EDT 2015 from=CategorizedFacebookType[category=null id=10208184284032675 metadata=null name=Bevan Li type=null] id=779009285560565_779014342226726 isHidden=null likeCount=null likes=null message=YquTest hi metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] comment:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 03:33:53 EDT 2015 from=CategorizedFacebookType[category=null id=778979068896920 metadata=null name=YquTest type=null] id=779009285560565_779010392227121 isHidden=null likeCount=null likes=null message=Comment22 metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] post:Post[actions=[] adminCreator=null application=null attachments=null attribution=null caption=null comments=null commentsCount=0 createdTime=Wed Oct 21 01:35:54 EDT 2015 description=null feedTargeting=null from=null fullPicture=null icon=null id=778979068896920_778979332230227 isHidden=null isPublished=null likes=null likesCount=null link=null message=Test by Yqu messageTags={} metadata=null name=null objectId=null picture=null place=null privacy=null properties=[] shares=null sharesCount=0 source=null statusType=null story=null targeting=null to=[] type=null updatedTime=null withTags=[]] comment:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 01:53:24 EDT 2015 from=CategorizedFacebookType[category=null id=778979068896920 metadata=null name=YquTest type=null] id=778979332230227_778982235563270 isHidden=null likeCount=null likes=null message=what? metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] reply:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 01:59:54 EDT 2015 from=CategorizedFacebookType[category=null id=778979068896920 metadata=null name=YquTest type=null] id=778979332230227_778983255563168 isHidden=null likeCount=null likes=null message=yes metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] reply:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 03:59:00 EDT 2015 from=CategorizedFacebookType[category=null id=10208184284032675 metadata=null name=Bevan Li type=null] id=778979332230227_779014268893400 isHidden=null likeCount=null likes=null message=test metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false] comment:Comment[attachment=null canRemove=null commentCount=0 comments=null createdTime=Wed Oct 21 02:00:25 EDT 2015 from=CategorizedFacebookType[category=null id=778979068896920 metadata=null name=YquTest type=null] id=778979332230227_778983312229829 isHidden=null likeCount=null likes=null message=yup metadata=null object=null parent=null type=null userLikes=null canComment=false canHide=false]  参考 RestFB
</content>
    </entry>
    
     <entry>
        <title>Twitter的高级搜索和过滤功能</title>
        <url>https://mryqu.github.io/post/twitter%E7%9A%84%E9%AB%98%E7%BA%A7%E6%90%9C%E7%B4%A2%E5%92%8C%E8%BF%87%E6%BB%A4%E5%8A%9F%E8%83%BD/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twitter</tag><tag>search</tag><tag>query</tag><tag>filter</tag><tag>crawler</tag>
        </tags>
        <content type="html">  刚接触Twitter的搜索功能，以为仅能用关键字搜索推文。后来才知道，Twitter搜索不仅支持关键字的与或非逻辑处理，还能根据日期、地理位置、推文是否有链接、图像、视频、转推、回复等条件进行过滤。例如：
 即包含&amp;rdquo;andrew&amp;rdquo;又包含&amp;rdquo;2015&amp;rdquo;的推文``` https://twitter.com/search?q=andrew 2015&amp;amp;src=typd   - 完全匹配&amp;quot;andrew 2015&amp;quot;的推文``` https://twitter.com/search?q=&amp;quot;andew 2015&amp;quot;&amp;amp;src=typd   包含&amp;rdquo;andrew&amp;rdquo;或&amp;rdquo;2015&amp;rdquo;的中文推文``` https://twitter.com/search?q=andew OR 2015 lang:zh&amp;amp;src=typd   - 包含&amp;quot;andrew&amp;quot;的推文（包括转推）``` https://twitter.com/search?q=andrew include:retweets&amp;amp;src=typd   包含&amp;rdquo;andrew&amp;rdquo;的推文（不包括转推）``` ttps://twitter.com/search?q=andrew exclude:retweets&amp;amp;src=typd   - 包含&amp;quot;andrew&amp;quot;且有URL链接的推文``` https://twitter.com/search?q=andrew filter:links&amp;amp;src=typd   包含&amp;rdquo;andrew&amp;rdquo;且没有URL链接的推文``` https://twitter.com/search?q=andrew -filter:links&amp;amp;src=typd  ```
参考 Twitter Advanced Search
Using Twitter Advanced Search
How to Master Twitter Search: Basic Boolean Operators and Filters
</content>
    </entry>
    
     <entry>
        <title>Hello Twitter4J</title>
        <url>https://mryqu.github.io/post/hello_twitter4j/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twitter</tag><tag>twitter4j</tag><tag>api</tag><tag>crawler</tag><tag>socialmedia</tag>
        </tags>
        <content type="html">  Twitter4J是Twitter API的第三方Java库。本演示用它通过关键字搜索推文。
获取Twitter应用证书 示例代码
package com.yqu.twitter4j; import twitter4j.Query; import twitter4j.QueryResult; import twitter4j.Status; import twitter4j.Twitter; import twitter4j.TwitterFactory; public class HelloTwitter4J { // I18NOK:CLS public static void main(String[] args) { try { // The factory instance is re-useable and thread safe. Twitter twitter = TwitterFactory.getSingleton(); Query query = new Query(&amp;quot;夏洛特烦恼&amp;quot;); query.setLang(&amp;quot;zh&amp;quot;); query.setCount(10); QueryResult result = twitter.search(query); for (Status status : result.getTweets()) { System.out.println(status.getCreatedAt()&#43;&amp;quot;:&amp;quot;&#43;status.getText()); } } catch (Exception e) { e.printStackTrace(); } } }  twitter.properties
debug=true oauth.consumerKey=********************* oauth.consumerSecret=****************************************** oauth.accessToken=************************************************** oauth.accessTokenSecret=******************************************  build.gradle
buildscript { repositories { mavenCentral() } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; jar { baseName = &#39;hello-twitter4j&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile &#39;org.twitter4j:twitter4j-core:4.0&#43;&#39; } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  输出:
Sun Oct 18 06:35:44 EDT 2015:看下这个tc版的夏洛特烦恼 Sun Oct 18 05:50:21 EDT 2015:老婆和闺蜜在看夏洛特烦恼，我在这边看孩子，原因是。。。。我觉得国产电影真心没有我闺女好看。。。。 Sun Oct 18 05:30:17 EDT 2015:Photo: 夏洛特烦恼.Goodbye.Mr.Loser.2015.TC720P.X264.AAC.Mandarin.CHS-ENG.Mp4Ba... http://t.co/Hk1kGRKTKk Sun Oct 18 03:50:01 EDT 2015:很多年前微博上流行的那个段子：多希望某天醒来，发现外面阳光正好，老师站在讲台上讲课，一颗粉笔正要砸向你，周围都是年轻的面孔，桌上还有你睡觉留下的口水。夏洛特烦恼在这开始，加入：你喜欢的姑娘就坐在你的面前，那时一切还没开始，于是你狠狠地吻了下去……(那时在微博还改编过一个版本😂😂) Sun Oct 18 03:40:41 EDT 2015:&amp;lt;夏洛特烦恼&amp;gt;不就是吊丝的黄粱一梦吗，梦里爱人变坏，自己名扬天下，然后说什么还是老情人好，梦醒来一切散去，就醒悟了，爱得不要不要的，娇柔的笑点，真是日了狗了的剧情😒😒 Sun Oct 18 03:26:17 EDT 2015:曝&amp;lt;夏洛特烦恼&amp;gt;抄袭 两部电影对比揭真相(图) http://t.co/GXpMXyBU3w #news #报纸 #新闻 #新华网 Sun Oct 18 03:13:15 EDT 2015:Goodbye Mr. Loser 夏洛特烦恼 2015 https://t.co/jtK2Vxak3I 来自 @YouTube Sun Oct 18 02:41:27 EDT 2015:曝&amp;lt;夏洛特烦恼&amp;gt;抄袭 两部电影对比揭真相(图) http://t.co/EmoJbxdFRD #news #报纸 #新闻 #新华网 Sun Oct 18 02:10:18 EDT 2015:&amp;lt;夏洛特烦恼&amp;gt;票房破10亿 沈腾兑现承诺晒裸照 编者按：1905电影网讯 据悉闫非、彭大魔执导，沈腾和马丽领衔主演的穿越青春爆 http://t.co/vgPqccQxEf http://t.co/rcnRdKYpdb Sun Oct 18 02:10:02 EDT 2015:RT @basinattuite: 炸裂！&amp;lt;夏洛特烦恼&amp;gt;居然全片抄袭了&amp;lt;教父&amp;gt;导演的旧作！http://t.co/cQyPeG9mHS  参考 Twitter4J
Twitter4J code examples
Twitter4J configuration
</content>
    </entry>
    
     <entry>
        <title>[Gradle] 创建含有依赖库的jar文件</title>
        <url>https://mryqu.github.io/post/gradle_%E5%88%9B%E5%BB%BA%E5%90%AB%E6%9C%89%E4%BE%9D%E8%B5%96%E5%BA%93%E7%9A%84jar%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>dependencies</tag><tag>jar</tag><tag>build</tag><tag>include</tag>
        </tags>
        <content type="html"> 想把自己的Gradle项目打成jar文件，但是&amp;rsquo;gradle build jar&amp;rsquo;生成的jar文件不含依赖库。
按照Gradle – Create a Jar file with dependencies改写了自己的build.gradle，成功包含了依赖库。但是依赖库不再是原来的jar文件，而是以目录的形式存在。
我的build.gradle
buildscript { repositories { mavenCentral() } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; jar { baseName = &#39;HelloTwitter4J&#39; version = &#39;0.1.0&#39; } task fatJar(type: Jar) { baseName = &#39;HelloTwitter4J-all&#39; version = &#39;0.1.0&#39; manifest { attributes &amp;quot;Main-Class&amp;quot;: &amp;quot;com.yqu.cdfwebtool.twitter.TwitterRateInfo&amp;quot; } from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } } with jar } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile &#39;org.twitter4j:twitter4j-core:4.0&#43;&#39; } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  构建好的jar文件： </content>
    </entry>
    
     <entry>
        <title>Hello Youtube Analytics</title>
        <url>https://mryqu.github.io/post/hello_youtube_analytics/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>analytics</tag><tag>youtube</tag><tag>crawler</tag><tag>socialmedia</tag>
        </tags>
        <content type="html">  Google Credential设置见我之前的博文解决 &amp;ldquo;Access Not Configured. The API (YouTube Analytics API) is not enabled for your project.&amp;rdquo;。
示例代码：
package com.yqu.yt; import java.io.IOException; import java.io.PrintStream; import java.math.BigDecimal; import java.util.List; import com.google.api.client.auth.oauth2.Credential; import com.google.api.client.googleapis.auth.oauth2.GoogleCredential; import com.google.api.client.http.HttpTransport; import com.google.api.client.http.javanet.NetHttpTransport; import com.google.api.client.json.JsonFactory; import com.google.api.client.json.jackson2.JacksonFactory; import com.google.api.services.youtube.YouTube; import com.google.api.services.youtube.model.Channel; import com.google.api.services.youtube.model.ChannelListResponse; import com.google.api.services.youtubeAnalytics.YouTubeAnalytics; import com.google.api.services.youtubeAnalytics.model.ResultTable; import com.google.api.services.youtubeAnalytics.model.ResultTable.ColumnHeaders; import com.google.common.collect.Lists; public class HelloYoutubeAnalytics { //I18NOK:CLS private static final HttpTransport HTTP_TRANSPORT = new NetHttpTransport(); private static final JsonFactory JSON_FACTORY = new JacksonFactory(); private static YouTube youtube; private static YouTubeAnalytics analytics; public static void main(String[] args) { // These scopes are required to access information about the // authenticated user&#39;s YouTube channel as well as Analytics // data for that channel. List&amp;lt;string&amp;gt; scopes = Lists.newArrayList( &amp;quot;https://www.googleapis.com/auth/yt-analytics.readonly&amp;quot;, &amp;quot;https://www.googleapis.com/auth/youtube.readonly&amp;quot; ); try { // Authorize the request. String accessToken=&amp;quot;YOUR_ACCESS_TOKEN&amp;quot;; Credential credential = new GoogleCredential() .setAccessToken(accessToken); // This object is used to make YouTube Data API requests. youtube = new YouTube.Builder( HTTP_TRANSPORT, JSON_FACTORY, credential) .setApplicationName(&amp;quot;Hello Youtube Analytics&amp;quot;) .build(); // This object is used to make YouTube Analytics API requests. analytics = new YouTubeAnalytics.Builder( HTTP_TRANSPORT, JSON_FACTORY, credential) .setApplicationName(&amp;quot;Hello Youtube Analytics&amp;quot;) .build(); // Construct a request to retrieve the current user&#39;s channel ID. YouTube.Channels.List channelRequest = youtube. channels().list(&amp;quot;id,snippet&amp;quot;); channelRequest.setMine(true); channelRequest.setFields(&amp;quot;items(id,snippet/title)&amp;quot;); ChannelListResponse channels = channelRequest.execute(); // List channels associated with the user. List&amp;lt;channel&amp;gt; listOfChannels = channels.getItems(); // The user&#39;s default channel is the first item in the list. Channel defaultChannel = listOfChannels.get(0); String channelId = defaultChannel.getId(); PrintStream writer = System.out; if (channelId == null) { writer.println(&amp;quot;No channel found.&amp;quot;); } else { writer.println(&amp;quot;Default Channel: &amp;quot; &#43; defaultChannel.getSnippet().getTitle() &#43; &amp;quot; ( &amp;quot; &#43; channelId &#43; &amp;quot; )\n&amp;quot;); printData(writer, &amp;quot;Views Over Time.&amp;quot;, executeViewsOverTimeQuery(analytics, channelId)); printData(writer, &amp;quot;Top Videos&amp;quot;, executeTopVideosQuery(analytics, channelId)); printData(writer, &amp;quot;Demographics&amp;quot;, executeDemographicsQuery(analytics, channelId)); } } catch (IOException e) { System.err.println(&amp;quot;IOException: &amp;quot; &#43; e.getMessage()); e.printStackTrace(); } catch (Throwable t) { System.err.println(&amp;quot;Throwable: &amp;quot; &#43; t.getMessage()); t.printStackTrace(); } } private static ResultTable executeViewsOverTimeQuery( YouTubeAnalytics analytics, String id) throws IOException { return analytics.reports() .query(&amp;quot;channel==&amp;quot; &#43; id, // channel id &amp;quot;2015-10-01&amp;quot;, // Start date. &amp;quot;2015-10-30&amp;quot;, // End date. &amp;quot;views,uniques&amp;quot;) // Metrics. .setDimensions(&amp;quot;day&amp;quot;) .setSort(&amp;quot;day&amp;quot;) .execute(); } private static ResultTable executeTopVideosQuery( YouTubeAnalytics analytics, String id) throws IOException { return analytics.reports() .query(&amp;quot;channel==&amp;quot; &#43; id, // channel id &amp;quot;2015-10-01&amp;quot;, // Start date. &amp;quot;2015-10-30&amp;quot;, // End date. &amp;quot;views,subscribersGained,subscribersLost&amp;quot;) // Metrics. .setDimensions(&amp;quot;video&amp;quot;) .setSort(&amp;quot;-views&amp;quot;) .setMaxResults(10) .execute(); } private static ResultTable executeDemographicsQuery( YouTubeAnalytics analytics, String id) throws IOException { return analytics.reports() .query(&amp;quot;channel==&amp;quot; &#43; id, // channel id &amp;quot;2015-10-01&amp;quot;, // Start date. &amp;quot;2015-10-30&amp;quot;, // End date. &amp;quot;viewerPercentage&amp;quot;) // Metrics. .setDimensions(&amp;quot;ageGroup,gender&amp;quot;) .setSort(&amp;quot;-viewerPercentage&amp;quot;) .execute(); } private static void printData( PrintStream writer, String title, ResultTable results) { writer.println(&amp;quot;Report: &amp;quot; &#43; title); if (results.getRows() == null || results.getRows().isEmpty()) { writer.println(&amp;quot;No results Found.&amp;quot;); } else { // Print column headers. for (ColumnHeaders header : results.getColumnHeaders()) { writer.printf(&amp;quot;%二十s&amp;quot;, header.getName()); } writer.println(); // Print actual data. int colHeaderCount = results.getColumnHeaders().size(); for (List&amp;lt;object&amp;gt; row : results.getRows()) { for (int colNum = 0; colNum &amp;lt; colHeaderCount; colNum&#43;&#43;) { ColumnHeaders header = results.getColumnHeaders().get(colNum); Object column = row.get(colNum); if (&amp;quot;INTEGER&amp;quot;.equals(header.getUnknownKeys().get(&amp;quot;dataType&amp;quot;))) { long l = ((BigDecimal) column).longValue(); writer.printf(&amp;quot;%二十d&amp;quot;, l); } else if (&amp;quot;FLOAT&amp;quot;.equals(header.getUnknownKeys().get(&amp;quot;dataType&amp;quot;))) { writer.printf(&amp;quot;f&amp;quot;, column); } else if (&amp;quot;STRING&amp;quot;.equals(header.getUnknownKeys().get(&amp;quot;dataType&amp;quot;))) { writer.printf(&amp;quot;%二十s&amp;quot;, column); } else { // default output. writer.printf(&amp;quot;%二十s&amp;quot;, column); } } writer.println(); } writer.println(); } } }  输出：
Default Channel: Andrew Qu ( UC-OpYDuNCwCt-AIHC6xNYdw ) Report: Views Over Time. day views uniques 2015-10-20 1.0 1.0 Report: Top Videos video views subscribersGained subscribersLost KHqrLhJPdtE 1.0 0.0 0.0 Report: Demographics No results Found.  参考 YouTube Analytics API Client Library for Java
YouTube Analytics and Reporting APIs - Java Code Samples
GitHub：youtube/api-samples之Auth.java
YouTube Analytics API: Content Owner Reports
Google OAuth 2.0 Playground
</content>
    </entry>
    
     <entry>
        <title>解决&#34;Access Not Configured. The API (YouTube Analytics API) is not enabled for your project.&#34;</title>
        <url>https://mryqu.github.io/post/%E8%A7%A3%E5%86%B3_access_not_configured._the_youtube_analytics_api_is_not_enabled_for_your_project/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>youtube</tag><tag>analytics</tag><tag>usagelimits</tag><tag>accessnotconfigured</tag><tag>403</tag>
        </tags>
        <content type="html"> 在用Google Developers OAuth 2.0 Playground试用Google YoutubeAnalytics API时总是返回下列403 Forbidden错误:
{ &amp;quot;code&amp;quot; : 403, &amp;quot;errors&amp;quot; : [ { &amp;quot;domain&amp;quot; : &amp;quot;usageLimits&amp;quot;, &amp;quot;message&amp;quot; : &amp;quot;Access Not Configured. The API (YouTube Analytics API) is not enabled for your project. Please use the Google Developers Console to update your configuration.&amp;quot;, &amp;quot;reason&amp;quot; : &amp;quot;accessNotConfigured&amp;quot;, &amp;quot;extendedHelp&amp;quot; : &amp;quot;https://console.developers.google.com&amp;quot; } ], &amp;quot;message&amp;quot; : &amp;quot;Access Not Configured. The API (YouTube Analytics API) is not enabled for your project. Please use the Google Developers Console to update your configuration.&amp;quot; }  在https://console.developers.google.com查看我的项目，各种可能需要的API都激活了： - Analytics API - BigQuery API - Cloud Debugger API - Contacts API - Debuglet Controller API - Google Cloud Logging API - Google Cloud SQL - Google Cloud Storage - Google Cloud Storage JSON API - Google&#43; API - Google&#43; Domains API - YouTube Analytics API - YouTube Data API v3 - YouTube Reporting API
最后的解决过程如下： - 创建一个web应用证书，关键是其redirectURI为https://developers.google.com/oauthplayground。 - 关键的一点是在Google Developers OAuth 2.0 Playground中采用“Use yourown OAuthcredentials”。我觉得403错误可能是playground自动创建的证书没有和自己的项目绑定，因此认为YoutubeAnalytics API没有激活。 - 200OK出来了，搞定： 当然，类似查询也可以在Google API Explorer里面完成，而且没碰到什么障碍。 </content>
    </entry>
    
     <entry>
        <title>Use proxy on Google Analytics API</title>
        <url>https://mryqu.github.io/post/use_proxy_on_google_analytics_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>analytics</tag><tag>api</tag><tag>proxy</tag><tag>httptransport</tag>
        </tags>
        <content type="html"> 使用Google API创建HTTP传输层是这样子的，没有可以传入代理的地方。
HttpTransport httpTransport = GoogleNetHttpTransport.newTrustedTransport();  仔细研究一下com.google.api.client.googleapis.javanet.GoogleNetHttpTransport，发现其实现是使用com.google.api.client.http.javanet.NetHttpTransport.Builder生成一个com.google.api.client.http.javanet.NetHttpTransport对象。com.google.api.client.http.javanet.NetHttpTransport.Builder和com.google.api.client.http.javanet.NetHttpTransport是都支持代理的。不用GoogleNetHttpTransport这个封装，直接对com.google.api.client.http.javanet.NetHttpTransport.Builder设置代理即可生成使用代理的HttpTransport对象。
Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(&amp;quot;XXXX&amp;quot;, 80)); HttpTransport httpTransport = new NetHttpTransport.Builder().setProxy(proxy). trustCertificates(GoogleUtils.getCertificateTrustStore()).build();  </content>
    </entry>
    
     <entry>
        <title>获取Facebook App Token</title>
        <url>https://mryqu.github.io/post/%E8%8E%B7%E5%8F%96facebook_app_token/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>appid</tag><tag>appsecret</tag><tag>apptoken</tag><tag>client_credentials</tag>
        </tags>
        <content type="html">  今天看了一下如何用Facebook的App Id和App Secret获取App Token。
可以使用RestFB，一行搞定：
FacebookClient.AccessToken ac = new DefaultFacebookClient(Version.LATEST) .obtainAppAccessToken(appId, appSecret);  curl命令也超简单：
curl &amp;quot;https://graph.facebook.com/v2.5/oauth/access_token?client_id={appId}&amp;amp;client_secret={appSecret}&amp;amp;grant_type=client_credentials&amp;quot;  我的一个Facebook应用YquTest如下： 通过其App Id和App Secret进行实验，获得结果可以用上一篇博文Facebook开发调试工具提到的访问口令工具验证，完全一致！
参考 Facebook Login - Access Tokens
Facebook Login - Access Tokens - App Access Tokens
</content>
    </entry>
    
     <entry>
        <title>Facebook开发调试工具</title>
        <url>https://mryqu.github.io/post/facebook%E5%BC%80%E5%8F%91%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>graph</tag><tag>develop</tag><tag>test</tag><tag>tool</tag>
        </tags>
        <content type="html"> Facebook开发调试工具大体位于https://developers.facebook.com/tools-and-support/下。 - 访问口令工具:这里所提供的用户口令便于测试应用，会过期。应用口令不会过期，应该秘密存储。- 图谱API探索工具:测试图谱API或FQL查询。- JS JDK控制台: 可以加载FacebookJS库，执行HTML代码- URL调试器:可以让Facebook的爬虫抓取你的网站，看看你的网站在Facebook被共享时的模样。这里面我用的最多的是图谱API探索工具，其次就是看图谱API参考文档了。
</content>
    </entry>
    
     <entry>
        <title>Remote debugging on microservice at docker container inside vagrant box</title>
        <url>https://mryqu.github.io/post/remote_debugging_on_microservice_at_docker_container_inside_vagrant_box/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>remote</tag><tag>debug</tag><tag>microservice</tag><tag>docker</tag><tag>vagrant</tag>
        </tags>
        <content type="html">  Docker compose configuration foo-service: image: foo-service:latest hostname: foo-service dns: 127.0.0.1 restart: always  Vagrant configuration config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: 8787, host: 8787, auto_correct: true  Consul configuration curl -X PUT -H &#39;application/json&#39; -d &#39;java_option_server_port:java_option_xms:java_option_xmx:java_option_debug&#39; http://localhost:8500/v1/kv/config/foo-service/jvm/java_options curl -X PUT -H &#39;application/json&#39; -d &#39;-Xdebug -Xrunjdwp:transport=dt_socket,address=8787,server=y,suspend=n&#39; http://localhost:8500/v1/kv/config/foo-service/jvm/java_option_debug  IntelliJ IDEA configuration </content>
    </entry>
    
     <entry>
        <title>继续使用Win10的Windows defender</title>
        <url>https://mryqu.github.io/post/%E7%BB%A7%E7%BB%AD%E4%BD%BF%E7%94%A8win10%E7%9A%84windows_defender/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>windows</tag><tag>defender</tag><tag>win10</tag><tag>security</tag>
        </tags>
        <content type="html"> 从Win7升级到了Win10，有一天忽然发现Microsoft SecurityEssentials不见了。上网一查，才知道被Win10内置了，改叫Windows defender。可是Windowsdefender也没见运行呀？
提示说“此应用已经关闭，不会监视你的计算机”。开始追查！！！
 通过“Windows键&#43;X”进入控制面板 在安全中心，发现是腾讯的电脑管家导致Windows defender被禁用。 关闭电脑管家的实时系统防护。 收到安全性提示，选择启用Windows defender。 又见到小城墙了  </content>
    </entry>
    
     <entry>
        <title>Node.js npm代理设置</title>
        <url>https://mryqu.github.io/post/node.js_npm%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>node.js</tag><tag>npm</tag><tag>proxy</tag><tag>configuration</tag>
        </tags>
        <content type="html">  用Gradle编译当前一个项目，总是报告Node.js的包管理工具npm安装node包失败。美国那边没有问题，不知道是否跟防火墙有关。
npm的代理设置为：
npm config set proxy http://proxyServer:proxyPort npm config set https-proxy http://proxyServer:proxyPort  操作显示能解决我的部分问题！
参考 NPM小结
</content>
    </entry>
    
     <entry>
        <title>遭遇&#34;HTTPS endpoint unresponsive and insecure mode isn&#39;t enabled.&#34;</title>
        <url>https://mryqu.github.io/post/%E9%81%AD%E9%81%87https_endpoint_unresponsive_and_insecure_mode_isnt_enabled./</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>docker-compose</tag><tag>pull</tag><tag>dockerexception</tag><tag>https</tag><tag>unresponsive</tag>
        </tags>
        <content type="html"> 今天使用docker-compose去获取最新的镜像，遭遇&amp;rdquo;HTTPS endpoint unresponsive andinsecure mode isn&amp;rsquo;t enabled.&amp;ldquo;错误。
mryqu$ docker-compose pull Pulling cadvisor (docker.mryqu.com/google/cadvisor:latest)... Traceback (most recent call last): File &amp;quot;&amp;lt;string&amp;gt;&amp;quot;, line 3, in &amp;lt;module&amp;gt; File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.cli.main&amp;quot;, line 32, in main File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.cli.docopt_command&amp;quot;, line 21, in sys_dispatch File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.cli.command&amp;quot;, line 34, in dispatch File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.cli.docopt_command&amp;quot;, line 24, in dispatch File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.cli.command&amp;quot;, line 66, in perform_command File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.cli.main&amp;quot;, line 235, in pull File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.project&amp;quot;, line 285, in pull File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/compose.service&amp;quot;, line 713, in pull File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/docker.client&amp;quot;, line 590, in pull File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/docker.auth.auth&amp;quot;, line 60, in resolve_repository_name File &amp;quot;/code/build/docker-compose/out00-PYZ.pyz/docker.auth.auth&amp;quot;, line 39, in expand_registry_url docker.errors.DockerException: HTTPS endpoint unresponsive and insecure mode isn&#39;t enabled.  解决方法：
docker-compose pull --allow-insecure-ssl  </content>
    </entry>
    
     <entry>
        <title>Hello Google Analytics</title>
        <url>https://mryqu.github.io/post/hello_google_analytics/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>analytics</tag><tag>api</tag><tag>crawler</tag><tag>socialmedia</tag>
        </tags>
        <content type="html">  Google Credential设置见我之前的博文Google Analytics API Error 403: &amp;ldquo;User does not have any Google Analytics Account&amp;rdquo;。
示例代码：
package com.yqu.ga; import java.io.File; import java.io.IOException; import java.io.InputStream; import java.util.List; import com.google.api.client.googleapis.auth.oauth2.GoogleCredential; import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport; import com.google.api.client.http.HttpRequest; import com.google.api.client.http.HttpRequestInitializer; import com.google.api.client.http.HttpTransport; import com.google.api.client.json.JsonFactory; import com.google.api.client.json.gson.GsonFactory; import com.google.api.services.analytics.Analytics; import com.google.api.services.analytics.AnalyticsScopes; import com.google.api.services.analytics.model.Accounts; import com.google.api.services.analytics.model.GaData; import com.google.api.services.analytics.model.GaData.ColumnHeaders; import com.google.api.services.analytics.model.GaData.Query; import com.google.api.services.analytics.model.Profiles; import com.google.api.services.analytics.model.Webproperties; public class HelloAnalytics { // I18NOK:CLS private static enum AuthType { SERVICE_ACCOUNT, SERVICE_ACCOUNT_P12, OAUTH }; protected static final String APPLICATION_NAME = &amp;quot;Hello Analytics&amp;quot;; protected static final JsonFactory JSON_FACTORY = GsonFactory .getDefaultInstance(); protected static final String CREDENTIAL_FILE = &amp;quot;/YquGATest-xxxx.json&amp;quot;; protected static final String CREDENTIAL_FILE_P12 = &amp;quot;/YquGATest-xxxx.p12&amp;quot;; private static final String ACCOUNTID_P12 = &amp;quot;xxxx@developer.gserviceaccount.com&amp;quot;; public static void main(String[] args) { try { Analytics analytics = initializeAnalytics(AuthType.OAUTH, &amp;quot;xxxx&amp;quot;); String profile = getFirstProfileId(analytics); System.out.println(&amp;quot;First Profile Id: &amp;quot; &#43; profile); printResults(getResults(analytics, profile)); } catch (Exception e) { e.printStackTrace(); } } protected static HttpRequestInitializer setHttpTimeout( final HttpRequestInitializer requestInitializer) { return new HttpRequestInitializer() { @Override public void initialize(HttpRequest httpRequest) throws IOException { requestInitializer.initialize(httpRequest); // 3 minutes connect timeout httpRequest.setConnectTimeout(3 * 60000); // 3 minutes read timeout httpRequest.setReadTimeout(3 * 60000); } }; } protected static Analytics initializeAnalytics(AuthType authType, String accessToken) throws Exception { // Initializes an authorized analytics service object. HttpTransport httpTransport = GoogleNetHttpTransport .newTrustedTransport(); GoogleCredential credential = null; if (authType.equals(AuthType.OAUTH)) { credential = new GoogleCredential() .setAccessToken(accessToken); } else if (authType.equals(AuthType.SERVICE_ACCOUNT)) { // Construct a GoogleCredential object with the // service account email // and json file downloaded from the developer console. try (final InputStream in = ClassLoader.class .getResourceAsStream(CREDENTIAL_FILE);) { credential = GoogleCredential.fromStream( in, httpTransport, JSON_FACTORY) .createScoped(AnalyticsScopes.all()); } } else { // Construct a GoogleCredential object with the // service account email // and p12 file downloaded from the developer console. File p12File = new File(ClassLoader.class.getResource( CREDENTIAL_FILE_P12).getFile()); credential = new GoogleCredential.Builder() .setTransport(httpTransport) .setJsonFactory(JSON_FACTORY) .setServiceAccountId(ACCOUNTID_P12) .setServiceAccountPrivateKeyFromP12File(p12File) .setServiceAccountScopes(AnalyticsScopes.all()) .build(); } // Construct the Analytics service object. return new Analytics.Builder(httpTransport, JSON_FACTORY, setHttpTimeout(credential)) .setApplicationName(APPLICATION_NAME).build(); } protected static String getFirstProfileId(Analytics analytics) throws IOException { // Get the first view (profile) ID for the authorized user. String profileId = null; // Query for the list of all accounts associated with // the service account. Accounts accounts = analytics.management().accounts() .list().execute(); if (accounts.getItems().isEmpty()) { System.err.println(&amp;quot;No accounts found&amp;quot;); } else { String firstAccountId = accounts.getItems() .get(0).getId(); // Query for the list of properties associated with // the first account. Webproperties properties = analytics.management() .webproperties() .list(firstAccountId).execute(); if (properties.getItems().isEmpty()) { System.err.println(&amp;quot;No Webproperties found&amp;quot;); } else { String firstWebpropertyId = properties.getItems() .get(0).getId(); // Query for the list views (profiles) associated // with the property. Profiles profiles = analytics.management() .profiles() .list(firstAccountId, firstWebpropertyId) .execute(); if (profiles.getItems().isEmpty()) { System.err.println(&amp;quot;No views (profiles) found&amp;quot;); } else { // Return the first (view) profile associated // with the property. profileId = profiles.getItems().get(0).getId(); } } } return profileId; } protected static GaData getResults( Analytics analytics, String profileId) throws IOException { // Query the Core Reporting API for the number of sessions // in the past seven days. return analytics .data() .ga() .get(&amp;quot;ga:&amp;quot; &#43; profileId, &amp;quot;30daysAgo&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;ga:sessions,ga:pageviews&amp;quot;) .setDimensions(&amp;quot;ga:source,ga:medium&amp;quot;) .setSegment(&amp;quot;gaid::-14&amp;quot;) .setSort(&amp;quot;-ga:sessions&amp;quot;) .setFilters(&amp;quot;ga:medium!=referral&amp;quot;) .setMaxResults(5).setStartIndex(10).execute(); } protected static void printResults(GaData results) { System.out.println(&amp;quot;Printing results for profile: &amp;quot; &#43; results.getProfileInfo().getProfileName()); if (results.getRows() == null || results.getRows().isEmpty()) { System.out.println(&amp;quot;No results Found.&amp;quot;); } else { // Print the query that was run System.out.println(); System.out.println(&amp;quot;Query:&amp;quot;); Query query = results.getQuery(); System.out.println(&amp;quot; Dimensions: &amp;quot; &#43; query.getDimensions()); System.out.println(&amp;quot; Segment: &amp;quot; &#43; query.getSegment()); System.out.println(&amp;quot; Metrics: &amp;quot; &#43; query.getMetrics()); List sort = query.getSort(); System.out.println(&amp;quot; Sort: &amp;quot; &#43; (sort == null ? &amp;quot;&amp;quot; : sort.toString())); System.out.println(&amp;quot; Interval: &amp;quot; &#43; query.getStartDate() &#43; &amp;quot; ~ &amp;quot; &#43; query.getEndDate()); System.out.println(&amp;quot; Filters: &amp;quot; &#43; query.getFilters()); System.out.println(); System.out.println(&amp;quot;Results contain sampled data: &amp;quot; &#43; results.getContainsSampledData().toString()); System.out.println(); // Print column headers. for (ColumnHeaders header : results.getColumnHeaders()) { System.out.printf(&amp;quot;0s&amp;quot;, header.getName()); } System.out.println(); // Print actual data. for (List row : results.getRows()) { for (String column : row) { System.out.printf(&amp;quot;0s&amp;quot;, column); } System.out.println(); } System.out.println(); } } }  输出：
First Profile Id: xxxx Printing results for profile: Corporate Site (Master Profile) Query: Dimensions: ga:source,ga:medium Segment: gaid::-14 Metrics: [ga:sessions, ga:pageviews] Sort: [-ga:sessions] Interval: 30daysAgo ~ today Filters: ga:medium!=referral Results contain sampled data: true ga:source ga:medium ga:sessions ga:pageviews display cpm 1832 1906 linkedin cpc-su 1713 2501 facebook-ads cpc 1420 1576 twitter cpc 1365 1704 feedburner feed 1356 3967  参考 Google Analytics
Google Analytics API
Google Analytics API Client Library for Java
Google API Explorer: Analytics
Google Analytics API JavaDoc
GitHub: google/google-api-java-client
Hello Analytics API: Java quickstart for service accounts
Instructions for the Google Analytics API Command-Line Samples
Google Api Java Client: Timeouts and Errors
Google Analytics Query Explorer
[](http://mvnrepository.com/artifact/com.google.http-client/google-http-client)
Google Developer Console
Google Developer Console Help
Using OAuth 2.0 to Access Google APIs
Google OAuth 2.0 Playground
</content>
    </entry>
    
     <entry>
        <title>Twitter开发调试工具</title>
        <url>https://mryqu.github.io/post/twitter%E5%BC%80%E5%8F%91%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twitter</tag><tag>api</tag><tag>develop</tag><tag>test</tag><tag>tool</tag>
        </tags>
        <content type="html"> Twitter开发调试工具主要位于https://dev.twitter.com/下。
 API status: 显示Twitter API可用性和性能[](http://photo.blog.sina.com.cn/showpic.html#blogid=72ef7bea0102w3q5&amp;amp;url=http://album.sina.com.cn/pic/0026uWfMgy71J0epHONd1)![Twitter开发调试工具](/images/2015/9/0026uWfMgy71J0jgKtO60.jpg) API参考文档 API控制台工具：测试TwitterAPI 管理自己的应用：对自己的应用进行配置  我用的最多的是API参考文档，其次是API控制台工具。
</content>
    </entry>
    
     <entry>
        <title>SocialMedia API Policy and Terms</title>
        <url>https://mryqu.github.io/post/socialmedia_api_policy_and_terms/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>socialmedia</tag><tag>policy</tag><tag>terms</tag>
        </tags>
        <content type="html">  Facebook Facebook Terms and Policies: https://www.facebook.com/policies Statement of Rights and Responsibilities: https://www.facebook.com/legal/terms Data Policy: https://www.facebook.com/about/privacy
Twitter Twitter Terms of Service: https://twitter.com/tos?lang=en Twitter Privacy Policy: https://twitter.com/privacy?lang=en Twitter Developer Agreement: https://dev.twitter.com/overview/terms/agreement Twitter Developer Policy: https://dev.twitter.com/overview/terms/policy
Google (Google Analytics &amp;amp; YouTube Analytics) Google APIs Terms of Service: https://developers.google.com/terms/ Google Privacy Policy: https://www.google.com/intl/en/policies/privacy/
LinkedIn User Agreement: https://www.linkedin.com/legal/user-agreement Privacy Policy: https://www.linkedin.com/legal/privacy-policy API Terms of Use: https://developer.linkedin.com/legal/api-terms-of-use Cookies on the LinkedIn site: https://www.linkedin.com/legal/cookie_policy
微博 开发者协议: http://open.weibo.com/wiki/开发者协议 应用运营管理规范: http://open.weibo.com/wiki/应用运营管理规范 微连接合作伙伴分级管理办法: http://open.weibo.com/wiki/微连接合作伙伴分级管理办法 应用审核产品指南: http://open.weibo.com/wiki/应用审核产品指南 应用安全开发注意事项: http://open.weibo.com/wiki/应用安全开发注意事项 应用设计规范: http://open.weibo.com/wiki/应用设计规范
微信 微信公众平台服务协议: https://mp.weixin.qq.com/cgi-bin/readtemplate?t=home/agreement_tmpl
</content>
    </entry>
    
     <entry>
        <title>社交媒体API/SDK</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93api_sdk/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>社交媒体</tag><tag>api</tag><tag>接口</tag><tag>编程</tag><tag>语言</tag>
        </tags>
        <content type="html"> 社交媒体API编程平台/语言Facebook API:应用广告：提升应用安装量和使用率。Analytics for Apps：制定基于数据的客户和广告决策。匿名登录：用户无需分享自己的信息即可注册使用您的应用。应用邀请：方便用户直接与好友分享他们喜欢的应用。应用链接：应用间链接的开放式跨平台标准。应用盈利：Facebook 和 LiveRail 广告助您实现应用盈利。移动应用受众网络：Facebook 广告助您实现应用盈利。游戏：实现游戏跨平台，覆盖数百万Facebook玩家。Facebook 登录：方便用户跨设备注册您的应用。Messenger：扩大展示，帮助用户发现您的应用。Parse：更快速地构建强大的移动应用。分享：借助 Facebook，提升网站和应用内容的知名度。社交插件：让您的网站和应用更具社交性和吸引力的最简单方式。ThreatExchange：分享威胁信息，保障用户安全。官方:iOSAndroidUnityJavascriptPHP
第三方:Cocos2d-xCFlashHTML5JavaJavaScriptLuaNode.jsObjective-CQtRubyUnityV-PlayWinJSTwitter API：TweetsUsersEntitiesPlaces官方:iOSAndroidOSXWebJava第三方:iOSAndroidJavaASPC&#43;&#43;ClojureColdFusion.NETGoJavascript/node.jsLua/Corona SDKObjective-CPerlPHPPythonRubyGoogle Analytics Configuration APIs：自动执行帐户和用户配置。Management API：访问和管理帐户、媒体资源、数据视图等Google Analytics（分析）实体。Provision API：创建新的Google Analytics（分析）帐户。Google Analytics Reporting APIs：借助报告API，您可以自动化复杂的报告任务，进而节省时间。您还可以使用相应API将Google Analytics（分析）数据与您自己的业务数据整合在一起，从而获得更深入的分析数据。Core Reporting API：通过查询维度和指标来创建自定义报告。Multi-channel Funnels Reporting API：查看您用户的归因和转化路径数据。Real-time Reporting API：查看您的媒体资源上当前发生的活动。Embed API：几分钟内即可将信息中心嵌入第三方网站。Metadata API：查看 API 维度和指标列表以及属性。官方:JavaPythonPHPJavascriptLinkedIn SDK:登录LinkedIn在LinkedIn进行内容分享在用户档案上添加（学位、证书等）信息管理公司页官方:iOSAndroidJavascriptREST APIYouTube API：Youtube Play API：使用内嵌播放器在你的应用直接播放视频，定制回放体验。实际上YouTube提供的是IFRAME、Android API、iOS API、PLAYER PARAMETERS。Youtube Data API：搜索YouTube内容、上传视频、创建和管理播放列表等功能。Youtube Analytics and Reporting API：获取YouTube视频和频道的统计、流行指标等信息。Youtube Analytics API：支持生成定制YouTube分析报告的实时有针对性的查询。API提供过滤和排序参数，因此调用程序无需支持这些功能。每个API请求指定数据范围，也能获取每周和每月的数据集，调用程序无需存储获得的数据集，也无需跨数据范围汇总统计信息。Youtube Reporting API：为频道或内容所有人获取包含YouTube分析数据的一批报告。用于能够导入大数据集并进行过滤、排序、数据挖据的应用。每个报告包含预定义字段集合。开发者使用该API预定报表任务，每个任务标识YouTube所要生成的报告。YouTube生成可以异步下载的日报告。每个报告包含唯一的一天数据。YouTube Live Streaming API：预定YouTube现场直播，管理直播视频流。官方:JavaJavaScript.NETObjective-CPHPPythonDartGoNode.jsRuby微博API：粉丝服务微博评论用户关系账号收藏搜索提醒短链公共服务位置服务地理信息地图引擎支付OAuth 2.0授权官方:JavaPHPFlashPythonJavascriptC&#43;&#43;AndroidiOSWP7Windows 8第三方:nodejsC#RubyDelphi微信移动开发微信登录微信智能接口微信支付接口接口微信网站开发微信登录微信智能接口微信公众平台：运营者通过公众号为微信用户提供资讯和服务的平台基础接口分享接口图像接口音频接口智能接口设备信息地理位置摇一摇周边界面操作微信扫一扫微信小店微信卡券微信支付数据统计接口官方:iOSAndroidWP8
REST
JS 在线文档/存储 API/SDK内容见在线文档/存储 API/SDK。
</content>
    </entry>
    
     <entry>
        <title>Twitter的访问令牌（AccessToken）不过期</title>
        <url>https://mryqu.github.io/post/twitter%E7%9A%84%E8%AE%BF%E9%97%AE%E4%BB%A4%E7%89%8Caccesstoken%E4%B8%8D%E8%BF%87%E6%9C%9F/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>twitter</tag><tag>accesstoken</tag><tag>expire</tag><tag>refresh</tag>
        </tags>
        <content type="html"> Twitter OAuth FAQ中提到了目前Twitter访问令牌不会过期，省事了。
</content>
    </entry>
    
     <entry>
        <title>Google Analytics之segment（分块、分割、细分）</title>
        <url>https://mryqu.github.io/post/google_analytics%E4%B9%8Bsegment%E5%88%86%E5%9D%97%E5%88%86%E5%89%B2%E7%BB%86%E5%88%86/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>analytics</tag><tag>segment</tag><tag>socialmedia</tag>
        </tags>
        <content type="html">  Segment是指你的GoogleAnalytics（分析）数据子集。例如，在你的整个用户群中，你可使用一个segment指定来自特定国家或城市的用户，使用另一个segment指定购买特定产品系列或访问网站上特定部分的用户。 Segment可让你隔离出这些数据子集并进行分析，从而检查并响应业务中的各个子趋势。例如，如果你发现特定地理区域的用户所购买的特定产品系列的数量低于正常水平，就可以查看是不是因为竞争对手在以更低的价格销售同类型的产品。如果是这样，你可以通过向那些用户提供忠诚度折扣来弱化竞争对手在价格方面的优势。 你还可以使用segment作为再营销受众群体的基础。例如，您可针对男装页面的访问者创建一个用户细分，然后利用重点宣传您添加到这些页面上的新产品的再营销广告系列来专门定位这些用户（再营销受众群体）。
Segment类型 Segment代表会话子集或用户子集： - 会话子集：例如，源自广告系列 A 的所有会话；发生购买行为的所有会话 - 用户子集：例如，之前有过购买行为的用户；向其购物车添加了商品，但未完成购买的用户
先了解一下 Google Analytics（分析）用户模型有助于了解segment的工作原理。 GoogleAnalytics（分析）用户模型由三大要素构成： - 用户 - 会话 - 用户到达您的网站资源并与之互动。所有这些用户互动都会被划组到所谓的会话中。 - 点击（Hit）-在会话中，用户会与您的网站资源互动。每次互动都被称为一次点击。这些点击包括网页浏览、事件、交易等等。
一个用户可以有多个会话，每个会话可以有多次点击。下图直观显示了这一关系： Google Analytics（分析）用户模型
使用Segment Segment是非破坏性的过滤器，不会更改您的基础数据。应用segment之后，它会在您浏览报告的过程中始终保持有效状态，直到您将其移除。您一次最多可以应用四个细分，并可在报告中将各个细分的结果放在一起比较。 除分析数据之外，Segment还可以用于构建再营销受众群体。 GoogleAnalytics（分析）包含预定义segment（系统segment），您可以按原样使用这些segment，也可以通过复制并修改这些segment来创建新的自定义segment。您也可以从头开始构建自己的segment。另外，您可以从 Google Analytics（分析）解决方案库导入segment，这是一个免费的市场，GoogleAnalytics（用户）可在其中分享各种segment以及开发的其他解决方案。
Segment的定义和范围 在 Google Analytics（分析）报告中，您可以通过创建基于维度和指标的过滤器来定义segment： - 用户类型完全匹配“回访用户” - 国家/地区完全匹配“美国” - 电子商务转化率&amp;gt;“0.2%”
除了您在过滤器中使用的维度和指标之外，您还可以为过滤器设置数据范围。您可以使用三种范围： - 点击：单次操作中的行为，例如查看网页或播放视频。 - 会话：单次会话中的行为；例如在会话过程中用户完成的目标或产生的收入。 - 用户：在所用日期范围（最多 90天）内所有会话中的行为；例如，在日期范围内的所有会话中用户完成的所有目标或产生的所有收入。
你可以使用segment生成工具定义组成segment的过滤器。
Segment限制 Segment需要遵守以下限制：
Segment总数上限  每个帐户 1000个segment 每个数据视图中，每位用户 100个segment 每个数据视图，所有用户共享100个segment 这些限制适用于系统segment以及您创建或导入的所有segment。达到这些数量上限后，你就无法创建或导入更多segment。  应用于报告的segment 你一次最多可以在报告中应用4个segment。
日期范围 使用基于用户的segment时，您在报告中应用的日期范围不能超过90天。如果您已将日期范围设置为90天以上，那么当您创建基于用户的segment时，GoogleAnalytics（分析）会从开始日期算起，将该日期范围重置为90天。 基于“第一次会话的日期”的segment的最大范围是31天。
多渠道路径 请不要在多渠道路径报告中使用segment，你可以使用转化segment。
Segment与filter的区别 在Googleanalytics中，Segment与filter都侧重于对数据的切片。在很多情况下，应用这两者返回的结果是相同的，但其本质是不同的。那么，何时应该使用segment，何时应该使用filter呢？ 如果你想选择整个访问则使用segment，如果想查找所有访问中的特定事件、pageviews等则使用filter。 Segment：对于segment，每个访问都检查是否符合segment条件。对于满足条件的会话，所有行都会被获得。对于不满足条件的会话，不会获得任何行。 Filter：对于filter，所有访问的所有行都会检查是否满足条件，仅满足过滤器条件的行会被获得。 个人感受：Google analytics中的filter就像SQL中的where语句，而segment就像SQL中groupby相应的having语句。
参考 About Segments
The key difference between segments and filters
</content>
    </entry>
    
     <entry>
        <title>Google Anaytics和Youtube Analytics的维度和指标</title>
        <url>https://mryqu.github.io/post/google_anaytics%E5%92%8Cyoutube_analytics%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%92%8C%E6%8C%87%E6%A0%87/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>analytics</tag><tag>youtube</tag><tag>dimension</tag><tag>metric</tag>
        </tags>
        <content type="html">  Google Anaytics的维度和指标 Google Anaytics 维度和指标浏览器
Youtube Analytics的维度和指标 Youtube Anaytics 维度 Youtube Anaytics指标
参考 Google Anaytics Core Reporting API - 常用查询
</content>
    </entry>
    
     <entry>
        <title>Google Analytics API Error 403: &#34;User does not have any Google Analytics Account&#34;</title>
        <url>https://mryqu.github.io/post/google_analytics_api_error_403_user_does_not_have_any_google_analytics_account/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>analytics</tag><tag>api</tag><tag>account</tag>
        </tags>
        <content type="html">  试用Google Analytics API，使用service account的认证方式，结果它报错:“User doesnot have any Google Analytics Account”。
解决方法：
 在Google开发者控制台中确认Analytics API已经使能  Service account的邮箱域为@developer.gserviceaccount.com  拥有适当的AccountID和ProfileID，并将serviceaccount（至少以读取和分析权限）添加到Google Analytics profile   参考 Google&amp;rsquo;s instructions for adding an email address to an Analytics profile
</content>
    </entry>
    
     <entry>
        <title>遭遇“GsonFactory cannot be resolved”</title>
        <url>https://mryqu.github.io/post/%E9%81%AD%E9%81%87gsonfactory_cannot_be_resolved/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>gsonfactory</tag><tag>google</tag><tag>gson</tag><tag>httpclient</tag><tag>gradle</tag>
        </tags>
        <content type="html">  遭遇GsonFactory无法解析的错误：
...\HelloAnalytics.java:7: error: package com.google.api.client.json.gson does not exist import com.google.api.client.json.gson.GsonFactory; ^ ...\HelloAnalytics.java:28: error: cannot find symbol private static final JsonFactory JSON_FACTORY = GsonFactory.getDefaultInstance(); ^ symbol: variable GsonFactory location: class HelloAnalytics 2 errors :compileJava FAILED  解决方案为在_gradle.build_中添加：
compile &#39;com.google.api-client:google-api-client-gson:1.20.0&#39; exclude module: &#39;httpclient&#39;  参考 GsonFactory cannot be found
</content>
    </entry>
    
     <entry>
        <title>Google Anaytics资料</title>
        <url>https://mryqu.github.io/post/google_anaytics%E8%B5%84%E6%96%99/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>google</tag><tag>analytics</tag><tag>资料</tag>
        </tags>
        <content type="html">  Google官方资料  Google Analytics（分析）帮助中心 (英文版) Google Analytics团队的博客 Avinash的Google&#43; Google Analytics Fundamental：有组织性的一个教程  外部资料  An introduction to Google Analytic (slideshare)：相当于一个全面的、提纲式的Google Analytics学习笔记 Google Analytics CheatSheet：制作的一个更加简练的备忘录 蓝鲸网站分析笔记：关注GoogleAnalytics应用 流量的秘密——Google Analytics网站分析与优化技巧》 Advanced Web Metrics with Google Analytics》 Google analytics-understanding Vistor Behavior》 《网站分析实战》  </content>
    </entry>
    
     <entry>
        <title>kitematic代理设置</title>
        <url>https://mryqu.github.io/post/kitematic%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>ketematic</tag><tag>proxy</tag><tag>vpn</tag><tag>docker</tag><tag>devops</tag>
        </tags>
        <content type="html">  在DockerToolbox目录下创建一个批处理脚本文件kitematic_proxy.cmd，插入下面代码，将&amp;rdquo;YOUR_PROXY&amp;rdquo;替换为所用的代理（http://host:port）。
set proxy=YOUR_PROXY SET HTTP_PROXY=%proxy% SET HTTPS_PROXY=%proxy% for /f %%i in (&#39;docker-machine.exe ip default&#39;) do set DOCKER_HOST=%%i SET NO_PROXY=%DOCKER_HOST% set DOCKER_HOST=tcp://%DOCKER_HOST%:2376 cd Kitematic Kitematic.exe  参考 kitematic Proxy/VPN error reports #1031
</content>
    </entry>
    
     <entry>
        <title>Postman使用笔记</title>
        <url>https://mryqu.github.io/post/postman%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>NetWork</category>
        </categories>
        <tags>
          <tag>postman</tag><tag>rest</tag><tag>api</tag><tag>工具</tag><tag>测试</tag>
        </tags>
        <content type="html">  以前用过cURL和rest-shell进行RESTAPI测试，最近偶然看到了Postman。Postman是一款Chrome插件，强大、便利，可以在浏览器里直接测试RESTAPI。
安装 在Chrome浏览器里进入Postman插件安装地址即可安装Postman。 使用 Postman可以模拟各种Http请求，并且可以额外设置特殊的URL参数、Http头以及Basic Auth、DigestAuth、OAuth 1.0等认证信息。在展现Http响应上，Postman支持完美打印，JSON、XML或是HTML都会整理成人类阅读的格式，有助于我们更清楚地查看响应内容。特色功能  更多的Http方法：Postman除了支持GET、POST、PUT、PATCH、DELETE、HEAD、OPTIONS这些常用Http方法，还支持COPY、LINK、UNLINK、PURGE。 集合（Collection）功能：Postman可以管理Http请求的集合，在做完单个测试时，可以将该请求存入特定集合内。这样在后继的重复测试，无需重新输入Http请求，就可以快速测试并获得结果。集合支持输入或导出，便于团队共享。 设置环境变量（Environment）：Postman可以管理环境变量。一般我们有可能有多种环境：development、staging或local，每种环境下的请求URL有可能各不相同。通过环境变量，在切换环境测试时无需重写Http请求。  参考 Postman插件安装地址
Github：postmanlabs
Postman官方博客
HTTP Link and Unlink Methods
</content>
    </entry>
    
     <entry>
        <title>利用curl完成Google API、Facebook、DropBox、OneDrive等社交媒体的OAuth认证</title>
        <url>https://mryqu.github.io/post/%E5%88%A9%E7%94%A8curl%E5%AE%8C%E6%88%90google_apifacebookdropboxonedrive%E7%AD%89%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E7%9A%84oauth%E8%AE%A4%E8%AF%81/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>curl</tag><tag>oauth</tag><tag>google</tag><tag>facebook</tag><tag>onedrive</tag>
        </tags>
        <content type="html">  Twitter 没法用curl完成Twitter认证，可以尝试witter/twurl。
Facebook 通过curl命令获取访问密钥：
curl &amp;quot;https://graph.facebook.com/oauth/access_token?client_id={YOUR_APP_ID}&amp;amp;client_secret={YOUR_APP_SECRET}&amp;amp;grant_type=client_credentials&amp;quot;  Google API 这里Google应用的客户端ID格式大概为XXX-YYY.apps.googleusercontent.com。
Google Analytics 首先通过浏览器访问下列链接获取code：
https://accounts.google.com/o/oauth2/v2/auth?scope=https://www.googleapis.com/auth/analytics.readonly%20profile&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;response_type=code&amp;amp;client_id={YOUR_APP_ID}  通过curl命令获取访问密钥：
curl -X POST -H &amp;quot;Content-Type: application/x-www-form-urlencoded&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;code={GOTTEN_CODE}&amp;amp;client_id={YOUR_APP_ID}&amp;amp;client_secret={YOUR_APP_SECRET}&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;grant_type=authorization_code&#39; &amp;quot;https://www.googleapis.com/oauth2/v4/token&amp;quot;  Youtube Analytics 首先通过浏览器访问下列链接获取code：
https://accounts.google.com/o/oauth2/v2/auth?scope=https://www.googleapis.com/auth/yt-analytics.readonly%20profile&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;response_type=code&amp;amp;client_id={YOUR_APP_ID}  通过curl命令获取访问密钥：
curl -X POST -H &amp;quot;Content-Type: application/x-www-form-urlencoded&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;code={GOTTEN_CODE}&amp;amp;client_id={YOUR_APP_ID}&amp;amp;client_secret={YOUR_APP_SECRET}&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;grant_type=authorization_code&#39; &amp;quot;https://www.googleapis.com/oauth2/v4/token&amp;quot;  Google drive &amp;amp; sheets 首先通过浏览器访问下列链接获取code：
https://accounts.google.com/o/oauth2/v2/auth?scope=https://www.googleapis.com/auth/spreadsheets%20https://www.googleapis.com/auth/drive%20profile&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;response_type=code&amp;amp;client_id={YOUR_APP_ID}  通过curl命令获取访问密钥：
curl -X POST -H &amp;quot;Content-Type: application/x-www-form-urlencoded&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;code={GOTTEN_CODE}&amp;amp;client_id={YOUR_APP_ID}&amp;amp;client_secret={YOUR_APP_SECRET}&amp;amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;grant_type=authorization_code&#39; &amp;quot;https://www.googleapis.com/oauth2/v4/token&amp;quot;  DropBox 首先通过浏览器访问下列链接获取code：
https://api.dropbox.com/1/oauth2/authorize?client_id={YOUR_APP_ID}&amp;amp;response_type=code&amp;amp;state=kx123  通过curl命令获取访问密钥：
curl -X POST -H &amp;quot;Content-Type: application/x-www-form-urlencoded&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;code={GOTTEN_CODE}&amp;amp;client_id={YOUR_APP_ID}&amp;amp;client_secret={YOUR_APP_SECRET}&amp;amp;grant_type=authorization_code&#39; &amp;quot;https://api.dropboxapi.com/1/oauth2/token&amp;quot;  OneDrive 首先通过浏览器访问下列链接获取code：
https://login.live.com/oauth20_authorize.srf?client_id={YOUR_APP_ID}&amp;amp;scope=onedrive.readonly&#43;wl.signin&amp;amp;response_type=code&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf  通过curl命令获取访问密钥：
curl -X POST -H &amp;quot;Content-Type: application/x-www-form-urlencoded&amp;quot; -H &amp;quot;Cache-Control: no-cache&amp;quot; -d &#39;client_id={YOUR_APP_ID}&amp;amp;redirect_uri=https://login.live.com/oauth20_desktop.srf&amp;amp;code={GOTTEN_CODE}&amp;amp;grant_type=authorization_code&#39; &amp;quot;https://login.live.com/oauth20_token.srf&amp;quot;  </content>
    </entry>
    
     <entry>
        <title>第三方开源Facebook Java API</title>
        <url>https://mryqu.github.io/post/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BC%80%E6%BA%90facebook_java_api/</url>
        <categories>
          <category>DataBuilder</category>
        </categories>
        <tags>
          <tag>facebook</tag><tag>java</tag><tag>api</tag><tag>library</tag><tag>third-party</tag>
        </tags>
        <content type="html"> |API|许可类型|活跃度|最后更新|文档|注释 |&amp;mdash;&amp;ndash; |SpringSocial|Apache 2.0|活跃||有|良好 |RestFB|MIT|活跃||有|良好 |BatchFB|MIT|一般||有|良好 |Facebook BlackBerry SDK|MIT|停止更新|2011-8-22|无|老项目，停止更新较早 |FB4J|GPLv2|停止更新|2010-02-14|无|老项目，停止更新较早 |FB Java API|MIT|项目宣布停止|2013-2-5|有|主页建议转到RestFB |JFALibrary|GPLv3|停止更新|2011-5-22|无|很少的提交，且停止更新较早 |Javabook|Apache 2.0|停止更新|2007-9-5|有一点|项目在停止更新前未完成
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] addr2line使用</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_addr2line%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>addr2line</tag><tag>objdump</tag><tag>readelf</tag><tag>linenumber</tag><tag>debug</tag>
        </tags>
        <content type="html">  GNU Binutils的Addr2line工具是一个可以将指令的地址和可执行程序转换成文件名、函数名和源代码行数的工具。这种功能对于将跟踪地址转换成更有意义的内容来说简直是太棒了。
下面是一个小示例testAddr2line.c：
#include &amp;quot;stdio.h&amp;quot; void test() { printf(&amp;quot;Hello Addr2line\n&amp;quot;); } int main() { test(); return 0; }  编译时使用-g选项包含调试符号条，使用-Wl,-Map=testAddr2line.map选项输出MapFile。
gcc -Wl,-Map=testAddr2line.map -g -o testAddr2line testAddr2line.c  testAddr2line.map部分内容如下： testAddr2line中也包含符号表信息，因而可以使用objdump查找：
hadoop@node51054:~/ctest$ objdump -t testAddr2line | grep &#39;main\|test&#39; testAddr2line: file format elf64-x86-64 0000000000000000 l df *ABS* 0000000000000000 testAddr2line.c 0000000000000000 F *UND* 0000000000000000 __libc_start_main@@GLIBC_2.2.5 0000000000400547 g F .text 0000000000000015 main 0000000000400536 g F .text 0000000000000011 test  使用addr2line：
hadoop@node51054:~/ctest$ addr2line -e testAddr2line 400536 /home/hadoop/ctest/testAddr2line.c:3 hadoop@node51054:~/ctest$ addr2line -e testAddr2line -f 400536 test /home/hadoop/ctest/testAddr2line.c:3 hadoop@node51054:~/ctest$ addr2line -e testAddr2line 400547 /home/hadoop/ctest/testAddr2line.c:6 hadoop@node51054:~/ctest$ addr2line -e testAddr2line -f 400547 main /home/hadoop/ctest/testAddr2line.c:6 hadoop@node51054:~/ctest$ hadoop@node51054:~/ctest$ addr2line -e testAddr2line -f 0x0000000000400547 main /home/hadoop/ctest/testAddr2line.c:6  addr2line如何找到的源代码行数的呢？在可执行程序中都包含有调试信息，其中很重要的一份数据就是程序源文件与编译后的机器代码之间的对应关系目录表、文件名表和行数语句。上述信息存储在可执行程序的.debug_line域，使用命令readelf-w testAddr2line可以输出DWARF的调试信息。 这里说明机器二进制编码的0x400536位置开始对应于源码中的第3行，0x400547开始就对应与源码的第6行了。
addr2line也可以用于对系统segfault日志信息定位错误位置。下面为一个日志信息示例：
/home/mryqu/ctest/mydrv.so(mytracex&#43;0x2e) [0x7f713e0c696e] /home/mryqu/ctest/mydrv.so(exceptionHandler&#43;0x13a) [0x7f713e08428a] /home/mryqu/ctest/mymk.so(myExcept&#43;0x5b) [0x7f71433b55bb] /home/mryqu/ctest/mymk.so(my_signal_handler&#43;0x160) [0x7f71433b5c00] /lib64/libpthread.so.0(&#43;0xf790) [0x7f71448ad790] /home/mryqu/ctest/mytable.so(__XXXX_avx_rep_memcpy&#43;0x130) [0x7f713a340030] /home/mryqu/ctest/mytable.so(&#43;0x4652e) [0x7f713a1bb52e] /home/mryqu/ctest/mytable.so(&#43;0x42829) [0x7f713a1b7829] /home/mryqu/ctest/mydrv.so(&#43;0x1f4d3) [0x7f713e0944d3] /home/mryqu/ctest/mydrv.so(&#43;0x2c850) [0x7f713e0a1850] /home/mryqu/ctest/mydrv.so(&#43;0x29fb1) [0x7f713e09efb1] /home/mryqu/ctest/mymk.so(myMain&#43;0x8d) [0x7f71433b367d] /home/mryqu/ctest/mymk.so(myMain&#43;0x6f) [0x7f71433b53ff] /lib64/libpthread.so.0(&#43;0x7a51) [0x7f71448a5a51] /lib64/libc.so.6(clone&#43;0x6d) [0x7f7143f339ad]  当指令指针位置为动态库偏移量时，如mydrv.so(&#43;0x2c850)，则可以直接使用addr2line查看源码位置。 当指令指针位置为函数偏移量时，如mydrv.so(exceptionHandler&#43;0x13a)，则需要先查找函数（如exceptionHandler）相对动态库的偏移量，之后与相对偏移量相加以用于addr2line命令。 可以通过objdump-S命令查找函数的指令指针地址： 参考 addr2line - linux man page
用 Graphviz 可视化函数调用
addr2line objdump命令使用方法
</content>
    </entry>
    
     <entry>
        <title>一张图学习一门语言</title>
        <url>https://mryqu.github.io/post/%E4%B8%80%E5%BC%A0%E5%9B%BE%E5%AD%A6%E4%B9%A0%E4%B8%80%E9%97%A8%E8%AF%AD%E8%A8%80/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>python3</tag>
        </tags>
        <content type="html"> 今天看到两个不错的图： - Javascript in one pic - Python3 in one pic
</content>
    </entry>
    
     <entry>
        <title>从Gradle bootRun任务向Spring Boot应用传递环境变量</title>
        <url>https://mryqu.github.io/post/%E4%BB%8Egradle_bootrun%E4%BB%BB%E5%8A%A1%E5%90%91spring_boot%E5%BA%94%E7%94%A8%E4%BC%A0%E9%80%92%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>bootrun</tag><tag>spring</tag><tag>boot</tag><tag>system_property</tag>
        </tags>
        <content type="html">  尝试了从Gradle bootRun任务中传递环境变量给Spring Boot应用，下面是示例代码和演示。
示例代码 Application.java package com.yqu.gradlesysprop; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; @SpringBootApplication public class Application { private static final Logger log = LoggerFactory.getLogger(Application.class); public static void main(String[] args) { SpringApplication app = new SpringApplication(Application.class); app.setWebEnvironment(false); app.setShowBanner(false); app.run(args); } @Bean public CommandLineRunner demo1() { return (args) -&amp;gt; { log.info(&amp;quot;mryqu.prop.test=&amp;quot;&#43; System.getProperty(&amp;quot;mryqu.prop.test&amp;quot;)); }; } }  build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.6.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;hello-gradlesysprop&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) }  演示 参考 How to pass system property to gradle task
</content>
    </entry>
    
     <entry>
        <title>Gradle代理配置</title>
        <url>https://mryqu.github.io/post/gradle%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>proxy</tag><tag>configuration</tag>
        </tags>
        <content type="html">  HTTP代理配置 gradlew -Dhttp.proxyHost=[myServer] -Dhttp.proxyPort=[myPort] -Dhttp.proxyUser=[myUser] -Dhttp.proxyPassword=[myPassword]  HTTPS代理配置 gradlew -Dhttps.proxyHost=[myServer] -Dhttps.proxyPort=[myPort] -Dhttps.proxyUser=[myUser] -Dhttps.proxyPassword=[myPassword]  </content>
    </entry>
    
     <entry>
        <title>[Hibernate Tools] 通过数据库表生成JPA Entity类</title>
        <url>https://mryqu.github.io/post/hibernate_tools_%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E7%94%9F%E6%88%90jpa_entity%E7%B1%BB/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Hibernate</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>table</tag><tag>generate</tag><tag>jpa</tag><tag>entity</tag>
        </tags>
        <content type="html">  本文与前一博文[Hibernate Tools] 通过JPA Entity类生成数据库表 正好相反，实践一下如何通过数据库表生成JPA Entity类。
在Eclipse中安装JBoss Tools中的Hibernate Tools插件 创建JPA项目PetStoreDemo 使用向导创建JPA项目 项目基本设置 设置JPA Facet 此处选用了Generatic 2.1平台，用户库HIBERNATE_JPA包含如下jar文件： - hibernate-commons-annotations.jar - hibernate-core.jar - hibernate-jpa-2.1-api.jar
通过数据库表生成JPA Entity类 执行“Generate Entities from Tables” 选择库表 设置库表关联关系 定制生成Entity的默认行为 设置单个Entity 生成结果 下面以Item为例，展示生成结果。
package com.yqu.jpetstore; import java.io.Serializable; import javax.persistence.*; import java.math.BigDecimal; @Entity @Table(name=&amp;quot;item&amp;quot;) @NamedQuery(name=&amp;quot;Item.findAll&amp;quot;, query=&amp;quot;SELECT i FROM Item i&amp;quot;) public class Item implements Serializable { private static final long serialVersionUID = 1L; private String itemid; private String attr1; private String attr2; private String attr3; private String attr4; private String attr5; private BigDecimal listprice; private String status; private BigDecimal unitcost; private Product product; private Supplier supplierBean; public Item() { } @Id @GeneratedValue(strategy=GenerationType.AUTO) @Column(unique=true, nullable=false, length=10) public String getItemid() { return this.itemid; } public void setItemid(String itemid) { this.itemid = itemid; } @Column(length=80) public String getAttr1() { return this.attr1; } public void setAttr1(String attr1) { this.attr1 = attr1; } @Column(length=80) public String getAttr2() { return this.attr2; } public void setAttr2(String attr2) { this.attr2 = attr2; } @Column(length=80) public String getAttr3() { return this.attr3; } public void setAttr3(String attr3) { this.attr3 = attr3; } @Column(length=80) public String getAttr4() { return this.attr4; } public void setAttr4(String attr4) { this.attr4 = attr4; } @Column(length=80) public String getAttr5() { return this.attr5; } public void setAttr5(String attr5) { this.attr5 = attr5; } @Column(precision=10, scale=2) public BigDecimal getListprice() { return this.listprice; } public void setListprice(BigDecimal listprice) { this.listprice = listprice; } @Column(length=2) public String getStatus() { return this.status; } public void setStatus(String status) { this.status = status; } @Column(precision=10, scale=2) public BigDecimal getUnitcost() { return this.unitcost; } public void setUnitcost(BigDecimal unitcost) { this.unitcost = unitcost; } //bi-directional many-to-one association to Product @ManyToOne @JoinColumn(name=&amp;quot;productid&amp;quot;, nullable=false) public Product getProduct() { return this.product; } public void setProduct(Product product) { this.product = product; } //bi-directional many-to-one association to Supplier @ManyToOne @JoinColumn(name=&amp;quot;supplier&amp;quot;) public Supplier getSupplierBean() { return this.supplierBean; } public void setSupplierBean(Supplier supplierBean) { this.supplierBean = supplierBean; } }  </content>
    </entry>
    
     <entry>
        <title>[Hibernate Tools] 通过JPA Entity类生成数据库表</title>
        <url>https://mryqu.github.io/post/hibernate_tools_%E9%80%9A%E8%BF%87jpa_entity%E7%B1%BB%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Hibernate</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>jpa</tag><tag>entity</tag><tag>generate</tag><tag>table</tag>
        </tags>
        <content type="html">  以前用过hbm2ddlAnt任务通过Hibernate映射文件生成数据库DDL。现在使用JPA后，不知道还有没有标准工具了。找了一圈，还是HibernateTools。
在Eclipse中安装JBoss Tools中的Hibernate Tools插件 创建JPA项目CustomerDemo 使用向导创建JPA项目 项目基本设置 设置JPA Facet 此处选用了EclipseLink 2.5.x平台。如选择Generatic2.1平台，在生成数据库Schema时会报“Generate Tables from Entities is notsupported by the Generic Platform”。 用户库ECLIPSELINK_JPA包含如下jar文件： - eclipselink.jar - javax.persistence.jar - org.eclipse.persistence.jpa.modelgen.jar - org.eclipse.persistence.jpars.jar
Entity类代码及设置 Customer类 package hello; import javax.persistence.*; @Entity @Table(name=&amp;quot;CUSTOMER&amp;quot;) public class Customer { @Id @Column(name=&amp;quot;CUSTOMER_ID&amp;quot;, nullable=false, updatable=false, unique=true) @GeneratedValue(strategy=GenerationType.AUTO) private Long id; @Column(name = &amp;quot;CUSTOMER_FNAME&amp;quot;) private String firstName; @Column(name = &amp;quot;CUSTOMER_LNAME&amp;quot;) private String lastName; protected Customer() {} public Customer(String firstName, String lastName) { this.firstName = firstName; this.lastName = lastName; } @Override public String toString() { return String.format( &amp;quot;Customer[id=%d, firstName=&#39;%s&#39;, lastName=&#39;%s&#39;]&amp;quot;, id, firstName, lastName); } public Long getId() { return id; } public String getFirstName() { return firstName; } public String getLastName() { return lastName; } }  加入Persistent Unit 让Customer类由应用中的EntityManager实例管理。 生成数据库表 生成的createDDL.sql文件：
CREATE TABLE CUSTOMER (CUSTOMER_ID BIGINT NOT NULL UNIQUE, CUSTOMER_FNAME VARCHAR(255), CUSTOMER_LNAME VARCHAR(255), PRIMARY KEY (CUSTOMER_ID)) CREATE TABLE SEQUENCE (SEQ_NAME VARCHAR(50) NOT NULL, SEQ_COUNT DECIMAL(38), PRIMARY KEY (SEQ_NAME)) INSERT INTO SEQUENCE(SEQ_NAME, SEQ_COUNT) values (&#39;SEQ_GEN&#39;, 0)  生成的dropDLL.sql文件：
DROP TABLE CUSTOMER DELETE FROM SEQUENCE WHERE SEQ_NAME = &#39;SEQ_GEN&#39;  </content>
    </entry>
    
     <entry>
        <title>试用了一下Kitematic</title>
        <url>https://mryqu.github.io/post/%E8%AF%95%E7%94%A8%E4%BA%86%E4%B8%80%E4%B8%8Bkitematic/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>kitematic</tag><tag>docker</tag><tag>toolbox</tag><tag>devops</tag>
        </tags>
        <content type="html">  Kitematic是一个一个简单的 Docker容器管理GUI程序，它可以在Windows/Mac上更快速、更简单的运行Docker。Kitematic 完全自动化了 Docker安装和设置过程，并提供了一个直观的图形用户接口（GUI）来在Windows/Mac上运行 Docker。Kitematic集成了Docker Machine来在Windows/Mac上分发一个虚拟机并安装 Docker引擎。 在Kitematic上可以在DockerHub上查询Docker镜像、并用之创建容器。也可以对特定Docker容器进行环境变量、Volume和端口等配置。对记不住Docker命令的懒人是一个福利。
参考 Kitematic官网
Github：Kitematic
DOCKER ONLINE MEETUP: KITEMATIC IN ACTION
</content>
    </entry>
    
     <entry>
        <title>SAS过程步支持的第三方编程语言</title>
        <url>https://mryqu.github.io/post/sas%E8%BF%87%E7%A8%8B%E6%AD%A5%E6%94%AF%E6%8C%81%E7%9A%84%E7%AC%AC%E4%B8%89%E6%96%B9%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>sas</tag><tag>3rd</tag><tag>programming</tag><tag>language</tag>
        </tags>
        <content type="html"> 编程语言接口描述C, C&#43;&#43;PROC PROTOPROC PROTO可以以批处理模式注册以外部的C或C&#43;&#43;程序。当C函数在PROC PROTO注册后，他们能被FCMP过程里声明的任何SAS函数或子程序调用, 也能被COMPILE过程里声明的任何SAS函数、子程序或方法块调用。PROC FCMPSAS函数编译器(FCMP)过程可以创建、测试和存储SAS函数和CALL子程序，这些SAS函数和CALL子程序之后可用于其它SAS过程步或数据步。PROC FCMP能够使用数据步语法创建存储在数据集内的SAS函数和CALL子程序。该过程步接受数据步语句的轻微变化，你可以使用PROC FCMP所创建的SAS函数和CALL子程序中SAS编程语言的大部分功能。GroovyPROC GROOVYPROC GROOVY是在SAS9.3引入，为特定Groovy内联代码提供提交快的SAS程序，也能运行存储在外部文件中的Groovy程序。 JavaJAVAOBJSAS9提供的数据步组件对象。示例： ``` data _null_; length s_out $200; declare JavaObj j1 (&#39;java/lang/String&#39;,&#39;KE&#39;); declare JavaObj j2 (&#39;java/lang/String&#39;,&#39;XIAO&#39;); j1.callStringMethod (&#39;concat&#39;, j2, s_out); put s_out=; j1.delete(); j2.delete(); run; ```  PROC JLAUNCHJLaunch过程步允许在SAS显示管理器系统（DMS）内启动Java GUI程序。示例： ``` proc jlaunch direct librefs debug app=&#39;com/sas/analytics/cmpfunceditor/app/FCmpFunctionEditorApp&#39;; picklist name=&#39;base/cmpedit.txt&#39;; run; ``` LuaPROC LUAPROC LUA是在SAS9.4引入，为特定Lua内联代码提供提交快的SAS程序，也能运行存储在外部文件中的Lua程序。RPROC IMLPROC IML提供了灵活的矩阵编程语言，可以与R集成。示例： ``` libname mmsamp &#34;!sasroot\mmcommon\sample&#34;; proc iml; run ExportDatasetToR(&#34;mmsamp.hmeq_train&#34; , &#34;mm_inds&#34;); submit /R; attach(mm_inds) # ----------------------------------------------- # FITTING THE LOGISTIC MODEL # ----------------------------------------------- logiten </content>
    </entry>
    
     <entry>
        <title>漫谈Lua</title>
        <url>https://mryqu.github.io/post/%E6%BC%AB%E8%B0%88lua/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>lua</tag><tag>sas</tag><tag>scripting</tag>
        </tags>
        <content type="html">  由于工作需要，最近用了有一段时间Lua了。对于脚本语言，研究生时用过Tcl/Tk做过网络仿真，后来在工作中学过Ruby，不同程度地用过UnixShell、Python、Javascript、Groovy、SAS和R。这里面UnixShell脚本是针对特定操作系统的脚本，SAS和R是用于特定业务领域（统计）的脚本，Javascript现在做Web富客户端是绕不过去的。用于通用领域的脚本中，Python容易上手、成熟、工具库多、资源丰富、文档等支持强大、几乎能做任何事：网络、图形界面、桌面程序、服务器、图形处理、算法等。可是我司将Lua而不是Python集成到SAS9.4中了。我使用它之前对其一无所知。很好奇Lua的历史及特长，是什么让我司对其青睐有加的？
何为Lua？ Lua是一门强大、快速、轻量级、可嵌入的脚本语言。Lua结合了简单的程序语法以及基于关联数组和扩展语义的强大数据描述结构。Lua是动态类型语言，通过在基于寄存器的虚拟机上解析字节码进行运行，具有自动内存管理和增量垃圾回收功能，极适于配置、脚本和快速原型开发。
Lua的历史 Lua在葡萄牙语中指的是“月亮”。Lua由PUC-Rio（巴西的里约热内卢天主教大学）的一个团队设计、实现和维护。Lua由Tecgraf（原先的计算机图形技术组）发明，并作为自由软件发行。现在由LabLua维护。Tecgraf和LabLua是PUC-Rio计算机科学系的两个实验室。 Lua发明者：Waldemar, Roberto, Luiz由Ernani d&amp;rsquo;Almeida拍摄(2011)
|Lua版本|发行时间|Lua版本|发行时间 |&amp;mdash;&amp;ndash; |1.0|未公开|3.1|1998/07/11 |1.1|1994/07/08|3.2|1999/07/08 |2.1|1995/02/07|4.0|2000/11/06 |2.2|1995/11/28|5.0|2003/04/11 |2.3|未公开|5.1|2006/02/21 |2.4|1996/03/14|5.2|2011/12/16 |2.5|1996/11/19|5.3|2015/01/12 |3.0|1997/07/01
Lua优点  经过验证的、健壮的语言：Lua已经被用于很多工业程序中(例如Adobe的PhotoshopLightroom)、嵌入式系统中(例如，用于巴西数字电视的Ginga中间件)和游戏中(例如魔兽和愤怒的小鸟)。Lua是游戏开发中主要的脚本语言。 速度快：许多Benchmark显示Lua是解释性脚本语言中最快的。例如Lua vs Python3 on X64 Ubuntu显示Lua比Python3要快很多。此外，LuaJIT项目提供在目标平台上的即时编译，可以让Lua有更优越的性能。 可移植：Lua以很小的包进行分发，在任何具有标准C编译器的系统上即可开箱即用地进行编译。Lua可在所有Unix发行版和Windows、移动设备、嵌入微处理器和IBM大型机等系统上运行。 可嵌入：Lua以很小的体积提供了一个快速语言引擎，所以易于嵌入到应用中。Lua具有简明、很好文档化的API，允许同其他语言紧密集成。非常易于通过其他语言库扩展Lua，同样其他语言程序也易于用Lua扩展。Lua不仅仅被用于扩展C和C&#43;&#43;程序，还有Java、C#、Smalltalk、Fortran、Ada、Erlang、甚至注入Perl和Ruby之类的其他脚本语言。 功能强大（但简明）：Lua设计的一个基本概念是对所实现的功能提供元机制，而不是在语言中直接提供一堆功能。例如，Lua不是一个纯的面向对象语言，但它提供了用于实现类和集成的元机制。Lua的元机制带来概念的简明性并保持语言很小，同时允许语义以非传统的方式进行扩展。 体积小：Lua5.3.1的压缩包包含源代码和文档，体积仅276K，解压缩后为1.1M。源代码包含大约23000行C代码。在64位Linux下，使用所有标准Lua库的Lua解释器占242K，Lua库占414K。 免费：开源，使用非常自由的MIT许可证发布软件。  为什么SAS集成Lua？ 通过本文所附文章，基本可以判断SAS集成Lua是内部驱动的； 首先Lua有助SAS研发部门完成特定产品的开发工作（特别是新版本的SAS预报服务器）。Lua的优雅语法、现代设计和对数据结构的支持，可以让你以新的方式书写SAS程序，克服SAS宏语言的种种不足。SAS中的Lua生态系统有助于提升质量、一致性和重用性，从而以更低的代价获得更高的生产力。
参考 Lua官网
The History of Lua
Lua 为什么在游戏编程领域被广泛运用？
Lua简明教程
Driving SAS® with Lua
Using Lua within your SAS programs
The implementation of Lua 5.0
The LuaJIT Project
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] sap.ui.define源码分析</title>
        <url>https://mryqu.github.io/post/openui5_sap.ui.define%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>sap.ui.define</tag><tag>javascript</tag><tag>modularization</tag>
        </tags>
        <content type="html">  jQuery.sap.define通过名字、依赖、模块值或工厂定义一个Javascript模块。
jQuery.sap.define函数源码在jquery.sap.global.js，执行时可在sap-ui-core.js中找到。
通过判断jQuery.sap.define的sModuleName参数类型是否为string类型，获得参数实际对应使用用途，通过移换参数获得真实的sResourceName（js文件路径）、vFactory（模块工厂）、aDependencies（依赖模块）及bExport。
通过[OpenUI5] jQuery.sap.declare源码分析里介绍过的declareModule函数宣称当前模块已存在，通过[OpenUI5] jQuery.sap.require源码分析里介绍过的requireModule函数解析当前模块的每一个依赖。
sap.ui.define = function(sModuleName, aDependencies, vFactory, bExport) { var sResourceName, i; // optional id if ( typeof sModuleName === &#39;string&#39; ) { sResourceName = sModuleName &#43; &#39;.js&#39;; } else { // shift parameters bExport = vFactory; vFactory = aDependencies; aDependencies = sModuleName; sResourceName = _execStack[_execStack.length - 1]; } // convert module name to UI5 module name syntax (might fail!) sModuleName = urnToUI5(sResourceName); // optional array of dependencies if ( !jQuery.isArray(aDependencies) ) { // shift parameters bExport = vFactory; vFactory = aDependencies; aDependencies = []; } else { // resolve relative module names var sPackage = sResourceName.slice(0,1 &#43; sResourceName.lastIndexOf(&#39;/&#39;)); for (i = 0; i &amp;lt; aDependencies.length; i&#43;&#43;) { if ( /^\.\//.test(aDependencies[i]) ) { // 2 == length of &#39;./&#39; prefix aDependencies[i] = sPackage &#43; aDependencies[i].slice(2); } } } if ( log.isLoggable() ) { log.debug(&amp;quot;define(&amp;quot; &#43; sResourceName &#43; &amp;quot;, &amp;quot; &#43; &amp;quot;[&#39;&amp;quot; &#43; aDependencies.join(&amp;quot;&#39;,&#39;&amp;quot;) &#43; &amp;quot;&#39;]&amp;quot; &#43; &amp;quot;)&amp;quot;); } var oModule = declareModule(sResourceName); // note: dependencies will be converted from RJS to URN inside requireAll requireAll(aDependencies, function(aModules) { // factory if ( log.isLoggable() ) { log.debug(&amp;quot;define(&amp;quot; &#43; sResourceName &#43; &amp;quot;): calling factory &amp;quot; &#43; typeof vFactory); } if ( bExport ) { // ensure parent namespace jQuery.sap.getObject(sModuleName, 1); } if ( typeof vFactory === &#39;function&#39; ) { oModule.content = vFactory.apply(window, aModules); } else { oModule.content = vFactory; } // HACK: global export if ( bExport ) { if ( oModule.content == null ) { log.error(&amp;quot;module &#39;&amp;quot; &#43; sResourceName &#43; &amp;quot;&#39; returned no content, but should be exported&amp;quot;); } else { if ( log.isLoggable() ) { log.debug(&amp;quot;exporting content of &#39;&amp;quot; &#43; sResourceName &#43; &amp;quot;&#39;: as global object&amp;quot;); } jQuery.sap.setObject(sModuleName, oModule.content); } } }); }; function requireAll(aDependencies, fnCallback) { var aModules = [], i, sDepModName; for (i = 0; i &amp;lt; aDependencies.length; i&#43;&#43;) { sDepModName = aDependencies[i]; log.debug(sLogPrefix &#43; &amp;quot;require &#39;&amp;quot; &#43; sDepModName &#43; &amp;quot;&#39;&amp;quot;); requireModule(sDepModName &#43; &amp;quot;.js&amp;quot;); // best guess for legacy modules that don&#39;t use sap.ui.define // TODO implement fallback for raw modules aModules[i] = mModules[sDepModName &#43; &amp;quot;.js&amp;quot;].content || jQuery.sap.getObject(urnToUI5(sDepModName &#43; &amp;quot;.js&amp;quot;)); log.debug(sLogPrefix &#43; &amp;quot;require &#39;&amp;quot; &#43; sDepModName &#43; &amp;quot;&#39;: done.&amp;quot;); } fnCallback(aModules); }  参考 sap.ui.define jsDoc
An introduction to sap.ui.define
[OpenUI5] jQuery.sap.require源码分析
[OpenUI5] jQuery.sap.declare源码分析
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] jQuery.sap.declare源码分析</title>
        <url>https://mryqu.github.io/post/openui5_jquery.sap.declare%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>jquery.sap.declare</tag><tag>javascript</tag><tag>modularization</tag>
        </tags>
        <content type="html"> jQuery.sap.declare用于宣称一个模块已存在。
在OpenUI5开发指南&amp;ndash;精粹&amp;ndash;优化应用&amp;ndash;模块化和依赖管理中对declare介绍是:
 Modules can declare themselves by calling the static jQuery.sap.declare functionwith their name. This helpsSAPUI5tocheck at runtime whether a loaded module contains the expectedcontent by comparing the required name against the declared name.As a side effect,jQuery.sap.declare ensures that the parent namespace of the module name exists in the currentglobal namespace (window).Formore information, see jQuery.sap.declare.
For modules without declaration, the framework assumes that themodule has the expected content and declares it with the name thatwas used for loading. In some cases a module declaration ismandatory, see Avoiding Duplicates.
 jQuery.sap.declare函数源码在jquery.sap.global.js，执行时可在sap-ui-core.js中找到。
通过下面的源代码可知，jQuery.sap.delcare首先通过ui5ToRJS将javascript类名转换为js文件名，例如sap.m.Dialog转换为sap/m/Dialog.js，然后执行declareModule函数。declareModule函数查找Core的mModules是否包含该模块且非初始状态，如果不包含该模块则添加模块并直接设为READY状态（说是避免循环？！！），且在_execStack为空时当作bootstrap模块加入_execStack。最后在bCreateNamespace不为false的情况下，执行jQuery.sap.getObject检查类的命名空间是否存在，不存在则创建。例如sap.m.Dialog，如果命名空间不存在的话，则创建window.sap={}，window.sap.m={}。
 var sNamespaceObj = sModuleName; // check for an object as parameter for sModuleName // in case of this the object contains the module name and the type // which could be {modName: &amp;quot;sap.ui.core.Dev&amp;quot;, type: &amp;quot;view&amp;quot;} if (typeof (sModuleName) === &amp;quot;object&amp;quot;) { sNamespaceObj = sModuleName.modName; sModuleName = ui5ToRJS(sModuleName.modName) &#43; (sModuleName.type ? &amp;quot;.&amp;quot; &#43; sModuleName.type : &amp;quot;&amp;quot;) &#43; &amp;quot;.js&amp;quot;; } else { sModuleName = ui5ToRJS(sModuleName) &#43; &amp;quot;.js&amp;quot;; } declareModule(sModuleName); // ensure parent namespace even if module was declared already // (as declare might have been called by require) if (bCreateNamespace !== false) { // ensure parent namespace jQuery.sap.getObject(sNamespaceObj, 1); } return this; }; function declareModule(sModuleName) { var oModule; // sModuleName must be a unified resource name of type .js jQuery.sap.assert(/\.js$/.test(sModuleName), &amp;quot;must be a Javascript module&amp;quot;); oModule = mModules[sModuleName] || (mModules[sModuleName] = { state : INITIAL }); if ( oModule.state &amp;gt; INITIAL ) { return oModule; } if ( log.isLoggable() ) { log.debug(sLogPrefix &#43; &amp;quot;declare module &#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39;&amp;quot;); } // avoid cycles oModule.state = READY; // the first call to declareModule is assumed to identify the bootstrap module // Note: this is only a guess and fails e.g. when multiple modules are loaded // via a script tag to make it safe, we could convert &#39;declare&#39; calls to // e.g. &#39;subdeclare&#39; calls at build time. if ( _execStack.length === 0 ) { _execStack.push(sModuleName); oModule.url = oModule.url || _sBootstrapUrl; } return oModule; }  一般在定制控件的js文件中，第一行就是通过jQuery.sap.declare宣称该控件已经存在了。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] jQuery.sap.require源码分析</title>
        <url>https://mryqu.github.io/post/openui5_jquery.sap.require%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>jquery.sap.require</tag><tag>javascript</tag><tag>modularization</tag>
        </tags>
        <content type="html">  jQuery.sap.require用于解析一个或多个模块依赖。
jQuery.sap.require函数源码在jquery.sap.global.js，执行时可在sap-ui-core.js中找到。
通过下面的源代码可知，jQuery.sap.require首先通过ui5ToRJS将javascript类名转换为js文件名，例如sap.m.Dialog转换为sap/m/Dialog.js，然后执行requireModule函数。
requireModule函数查找该模块在sap.ui.core.Core对象的mModules中是否存在，不存在则添加并设为INITIAL状态，判断模块是否已经被加载、执行过，如果没有则设为LOADING状态并通过ajax以同步方式加载代码（如果当前是debug模式则选择-dbg版本的js文件URL），加载失败设为FAILED状态，加载成功则设为LOADED状态并执行代码，执行失败设为FAILED状态，执行成功设为READY状态。
jQuery.sap.require = function(vModuleName, fnCallback) { if ( arguments.length &amp;gt; 1 ) { // legacy mode with multiple arguments, each representing a dependency for (var i = 0; i &amp;lt; arguments.length; i&#43;&#43;) { jQuery.sap.require(arguments[i]); } return this; } // check for an object as parameter for sModuleName // in case of this the object contains the module name and the type // which could be {modName: &amp;quot;sap.ui.core.Dev&amp;quot;, type: &amp;quot;view&amp;quot;} if (typeof (vModuleName) === &amp;quot;object&amp;quot;) { jQuery.sap.assert(!vModuleName.type || jQuery.inArray(vModuleName.type, mKnownSubtypes.js) &amp;gt;= 0, &amp;quot;type must be empty or one of &amp;quot; &#43; mKnownSubtypes.js.join(&amp;quot;, &amp;quot;)); vModuleName = ui5ToRJS(vModuleName.modName) &#43; (vModuleName.type ? &amp;quot;.&amp;quot; &#43; vModuleName.type : &amp;quot;&amp;quot;) &#43; &amp;quot;.js&amp;quot;; } else { vModuleName = ui5ToRJS(vModuleName) &#43; &amp;quot;.js&amp;quot;; } requireModule(vModuleName); return this; // TODO }; function ui5ToRJS(sName) { if ( /^sap\.ui\.thirdparty\.jquery\.jquery-/.test(sName) ) { return &amp;quot;sap/ui/thirdparty/jquery/jquery-&amp;quot; &#43; sName.slice(&amp;quot;sap.ui.thirdparty.jquery.jquery-&amp;quot;.length); } else if ( /^jquery\.sap\./.test(sName) ) { return sName; } return sName.replace(/\./g, &amp;quot;/&amp;quot;); } function requireModule(sModuleName) { var m = rJSSubtypes.exec(sModuleName), sBaseName, sType, oModule, aExtensions, i; // only for robustness, should not be possible // by design (all callers append &#39;.js&#39;) if ( !m ) { log.error(&amp;quot;can only require Javascript module, not &amp;quot; &#43; sModuleName); return; } // in case of having a type specified ignore the type for the module path // creation and add it as file extension sBaseName = sModuleName.slice(0, m.index); sType = m[0]; // must be a normalized resource // name of type .js sType can be empty // or one of view|controller|fragment oModule = mModules[sModuleName] || (mModules[sModuleName] = { state : INITIAL }); if ( log.isLoggable() ) { log.debug(sLogPrefix &#43; &amp;quot;require &#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39; of type &#39;&amp;quot; &#43; sType &#43; &amp;quot;&#39;&amp;quot;); } // check if module has been loaded already if ( oModule.state !== INITIAL ) { if ( oModule.state === PRELOADED ) { oModule.state = LOADED; execModule(sModuleName); } if ( oModule.state === READY ) { if ( log.isLoggable() ) { log.debug(sLogPrefix &#43; &amp;quot;module &#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39; has already been loaded (skipped).&amp;quot;); } return this; } else if ( oModule.state === FAILED ) { throw new Error(&amp;quot;found in negative cache: &#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39; from &amp;quot; &#43; oModule.url &#43; &amp;quot;: &amp;quot; &#43; oModule.error); } else { // currently loading return this; } } // set marker for loading modules (to break cycles) oModule.state = LOADING; // if debug is enabled, try to load debug module first aExtensions = window[&amp;quot;sap-ui-loaddbg&amp;quot;] ? [&amp;quot;-dbg&amp;quot;, &amp;quot;&amp;quot;] : [&amp;quot;&amp;quot;]; for (i = 0; i &amp;lt; aExtensions.length &amp;amp;&amp;amp; oModule.state !== LOADED; i&#43;&#43;) { // create module URL for the current extension oModule.url = getResourcePath(sBaseName, aExtensions[i] &#43; sType); if ( log.isLoggable() ) { log.debug(sLogPrefix &#43; &amp;quot;loading &amp;quot; &#43; (aExtensions[i] ? aExtensions[i] &#43; &amp;quot; version of &amp;quot; : &amp;quot;&amp;quot;) &#43; &amp;quot;&#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39; from &#39;&amp;quot; &#43; oModule.url &#43; &amp;quot;&#39;&amp;quot;); } jQuery.ajax({ url : oModule.url, dataType : &#39;text&#39;, async : false, success : function(response, textStatus, xhr) { oModule.state = LOADED; oModule.data = response; }, error : function(xhr, textStatus, error) { oModule.state = FAILED; oModule.error = xhr ? xhr.status &#43; &amp;quot; - &amp;quot; &#43; xhr.statusText : textStatus; } }); } // execute module __after__ loading it, this reduces the required stack space! if ( oModule.state === LOADED ) { execModule(sModuleName); } if ( oModule.state !== READY ) { throw new Error(&amp;quot;failed to load &#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39; from &amp;quot; &#43; oModule.url &#43; &amp;quot;: &amp;quot; &#43; oModule.error); } } // sModuleName must be a normalized resource name of type .js function execModule(sModuleName) { var oModule = mModules[sModuleName], sOldPrefix, sScript, vAMD; if ( oModule &amp;amp;&amp;amp; oModule.state === LOADED &amp;amp;&amp;amp; typeof oModule.data !== &amp;quot;undefined&amp;quot; ) { // check whether the module is known to use an existing AMD loader, // remember the AMD flag vAMD = mAMDShim[sModuleName] &amp;amp;&amp;amp; typeof window.define === &amp;quot;function&amp;quot; &amp;amp;&amp;amp; window.define.amd; try { if ( vAMD ) { // temp. remove the AMD Flag from the loader delete window.define.amd; } if ( log.isLoggable() ) { log.debug(sLogPrefix &#43; &amp;quot;executing &#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39;&amp;quot;); sOldPrefix = sLogPrefix; sLogPrefix = sLogPrefix &#43; &amp;quot;: &amp;quot;; } // execute the script in the window context oModule.state = EXECUTING; _execStack.push(sModuleName); if ( typeof oModule.data === &amp;quot;function&amp;quot; ) { oModule.data.call(window); } else { sScript = oModule.data; // sourceURL: Firebug, Chrome, Safari and IE11 debugging help, // appending the string seems to cost ZERO performance // Note: IE11 supports sourceURL even when running in IE9 or IE10 mode // Note: make URL absolute so Chrome displays the file tree correctly // Note: do not append if there is already a sourceURL / sourceMappingURL if (sScript &amp;amp;&amp;amp; !sScript.match(/\/\/[#@] source(Mapping)?URL=.*$/)) { sScript &#43;= &amp;quot;\n//# sourceURL=&amp;quot; &#43; URI(oModule.url).absoluteTo(sDocumentLocation); } // framework internal hook to intercept the loaded script and modify // it before executing the script - e.g. useful for client side coverage if (typeof jQuery.sap.require._hook === &amp;quot;function&amp;quot;) { sScript = jQuery.sap.require._hook(sScript, sModuleName); } if (_window.execScript &amp;amp;&amp;amp; (!oModule.data || oModule.data.length &amp;lt; MAX_EXEC_SCRIPT_LENGTH) ) { try { oModule.data &amp;amp;&amp;amp; _window.execScript(sScript); // execScript fails if data is empty } catch (e) { _execStack.pop(); // eval again with different approach - // should fail with a more informative exception jQuery.sap.globaleval_r(oModule.data); throw e; // rethrow err in case globalEval succeeded unexpectedly } } else { _window.eval_r(sScript); } } _execStack.pop(); oModule.state = READY; oModule.data = undefined; // best guess for legacy modules that don&#39;t use sap.ui.define // TODO implement fallback for raw modules oModule.content = oModule.content || jQuery.sap.getObject(urnToUI5(sModuleName)); if ( log.isLoggable() ) { sLogPrefix = sOldPrefix; log.debug(sLogPrefix &#43; &amp;quot;finished executing &#39;&amp;quot; &#43; sModuleName &#43; &amp;quot;&#39;&amp;quot;); } } catch (err) { oModule.state = FAILED; oModule.error = ((err.toString &amp;amp;&amp;amp; err.toString()) || err.message) &#43; (err.line ? &amp;quot;(line &amp;quot; &#43; err.line &#43; &amp;quot;)&amp;quot; : &amp;quot;&amp;quot; ); oModule.data = undefined; if ( window[&amp;quot;sap-ui-debug&amp;quot;] &amp;amp;&amp;amp; (/sap-ui-xx-show(L|-l)oad(E|-e)rrors=(true|x|X)/.test(location.search) || oCfgData[&amp;quot;xx-showloaderrors&amp;quot;]) ) { log.error(&amp;quot;error while evaluating &amp;quot; &#43; sModuleName &#43; &amp;quot;, embedding again via script tag to enforce a stack trace (see below)&amp;quot;); jQuery.sap.includeScript(oModule.url); return; } } finally { // restore AMD flag if ( vAMD ) { window.define.amd = vAMD; } } } }  参考 jQuery.sap.require jsDoc
</content>
    </entry>
    
     <entry>
        <title>[Hive] Hive 表UDTF和汇聚UDAF学习</title>
        <url>https://mryqu.github.io/post/hive_hive_%E8%A1%A8udtf%E5%92%8C%E6%B1%87%E8%81%9Audaf%E5%AD%A6%E4%B9%A0/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>table</tag><tag>aggregate</tag><tag>udtf</tag><tag>udaf</tag>
        </tags>
        <content type="html">  在之前的博文[Hive] Hive Macro和UDF实践中，我对Hive的宏和普通UDF进行学习并实践，这里将针对Hive表UDF（UDTF）和汇聚UDF（UDAF）进行学习。 普通UDF可以对一行表数据进行处理输出一个单元格数据；UDTF可以对一行表数据进行处理输出多列甚至多行数据；UDAF可以对整表数据进行处理输出某种汇聚结果。
UDTF Hive支持的内建UDTF有explode()、json_tuple()和inline()等函数。
查看UDTF介绍 选个好理解的explode函数吧。
hive&amp;gt; describe function explode; OK explode(a) - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns Time taken: 0.009 seconds, Fetched: 1 row(s)  测试内建UDTF 像inline函数需要根元素为ARRAY，第二层元素为STRUCT，搭建环境有点麻烦。所以还是接着擼explode函数吧。 上述命令将三行的列a中数组元素拆成7行字符串。那执行&amp;rdquo;select explode(a), b fromcomplex_datatypes_example;&amp;ldquo;会返回什么呢？结果是7行还是3行？ 谜底就是错误提示&amp;rdquo;Only a single exp.ression in the SELECT clause issupported with UDTF&amp;rsquo;s.&amp;ldquo;！
UDTF实现 一个定制UDTF要继承GenericUDTF抽象类并实现initialize、process及close方法。Hive调用initialize方法以将参数类型通知给UDTF。UDTF必须返回UDTF之后产生的行对象相应的对象观察器。一旦initialize()被调用后，Hive将使用process()方法将行传递给UDTF。在process()方法中，UDTF生成行并调用forward()方法将行转给其他运算符。当所有的行都传递给UDTF后，Hive将最终调用close()方法。 通过FunctionRegistry类可知explode函数的实现类为GenericUDTFExplode。下面通过GenericUDTFExplode对照参考四Hive Developer Guide - UDTF学习一下UDTF实现。 - GenericUDTFExplode继承了抽象父类GenericUDTF - 在initialize方法中，GenericUDTFExplode检查输入列是否为ARRAY或MAP类型，不是的话抛出异常。如果输入列为ARRAY类型，则输出列名为col，类型为输入列数组中元素类型；如果输入列为MAP类型，则输出列名为key和value，类型分别为输入列MAP的键与值相应的类型； - 在process方法中，针对输入列ARRAY或MAP的每一个元素调用forward()方法将所生成的行转给其他运算符； - 在close()方法中，实现为空。
UDAF Hive支持的内建UDAF有sum()、count()、min()和histogram_numeric()等函数。
查看UDAF介绍 sum函数就是一个UDAF&amp;mdash;简单实惠，好用不贵！
hive&amp;gt; describe function sum; OK sum(x) - Returns the sum of a set of numbers Time taken: 0.007 seconds, Fetched: 1 row(s)  测试内建UDAF hive&amp;gt; select a from primitive_dataytpes_example; OK 123 NULL -1 hive&amp;gt; select sum(a) from primitive_dataytpes_example; Query ID = hadoop_20150819044201_XXXX Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks determined at compile time: 1 In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt; In order to limit the maximum number of reducers: set hive.exec.reducers.max=&amp;lt;number&amp;gt; In order to set a constant number of reducers: set mapreduce.job.reduces=&amp;lt;number&amp;gt; Starting Job = job_1438392006960_0012, Tracking URL = http://node50064.mryqu.com:8088/proxy/application_1438392006960_0012/ Kill Command = /usr/local/hadoop/bin/hadoop job -kill job_1438392006960_0012 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1 2015-08-19 04:42:47,272 Stage-1 map = 0%, reduce = 0% 2015-08-19 04:42:54,771 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.44 sec 2015-08-19 04:43:54,907 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.44 sec 2015-08-19 04:44:55,668 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.44 sec 2015-08-19 04:45:56,611 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.44 sec 2015-08-19 04:46:20,327 Stage-1 map = 100%, reduce = 67%, Cumulative CPU 7.87 sec 2015-08-19 04:46:28,516 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 16.81 sec MapReduce Total cumulative CPU time: 16 seconds 810 msec Ended Job = job_1438392006960_0012 MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 16.81 sec HDFS Read: 9201 HDFS Write: 4 SUCCESS Total MapReduce CPU Time Spent: 16 seconds 810 msec OK 122 Time taken: 268.746 seconds, Fetched: 1 row(s)  UDAF实现 Hive允许两种UDAF：简单UDAF和通用UDAF。 - 简单UDAF，显而易见更好好实现，但是由于使用了Java反射技术造成性能下降，并且不接受可变长参数等特性。简单UDAF需要继承org.apache.hadoop.hive.ql.exec.UDAF类，并实现一个或多个UDAFEvaluator接口的静态内部类。在运行时，Hive使用反射查找匹配接受参数的UDAFEvaluator，这会降低作业性能，但是易于实现。percentile函数的实现类UDAFPercentile就是一个简单UDAF。 - 通用UDAF可以接受所有特性，但是不如简单UDAF实现的那么直观。 要实现一个通用UDAF，需要实现resolver类和evaluator类。resolver负责类型检查，操作符重载，通过输入参数类型发现正确的evaluator。evaluator真正实现UDAF的逻辑。通常来说，顶层UDAF类实现org.apache.hadoop.hive.ql.udf.GenericUDAFResolver接口(GenericUDAFResolver2接口继承自GenericUDAFResolver接口，AbstractGenericUDAFResolver抽象类实现了GenericUDAFResolver2接口)，重写getEvaluator(TypeInfo[]parameters)或getEvaluator(GenericUDAFParameterInfoparamInfo)方法实现以返回给Hive正确的evalutor，实现较复杂，但相对反射更加高效。顶层UDAF类中实现一个或多个GenericUDAFEvaluator接口的静态内部类evaluator实现UDAF的逻辑。 所有的evaluator都必须继承自抽线类GenericUDAFEvaluator，该类提供了一些扩展类必须实现的抽象方法。
|方法|介绍 |&amp;mdash;&amp;mdash; |init|由Hive调用以初始化你的UDAF类实例 |getNewAggregationBuffer|返回用于存储临时汇聚结果的对象 |iterate|将一行新数据传给汇聚缓冲区 |terminatePartial|以可持久化的方式返回当前汇聚内容。持久化在这里意指返回值只能为Java原始类型、数组、原始包装类型（例如Double）及HadoopWritable、List、Map。不可以使用自己定义的类（即使实现Serializable）。 |merge|将terminatePartial返回的局部汇聚合并到当前汇聚 |terminate|将最终汇聚结果返回Hive
通用UDAF的例子很多，例如sum函数的实现类GenericUDAFSum、count函数的实现类GenericUDAFCount&amp;hellip;&amp;hellip;
下面通过GenericUDAFSum对照参考五Hive Generic UDAF Case Study学习一下UDAF实现。 - GenericUDAFSum继承了抽象类AbstractGenericUDAFResolver。 - GenericUDAFSum类中首先构建一个Log对象用于往Hive日志写告警和错误消息。 - GenericUDAFSum类重写的两个getEvaluator方法，会收到UDAF被调用时的SQL参数信息，进行类型检查和选出evaluator工作： 1. 首先我们检查是否为一个参数，不是的话抛出异常； 2. 接着检查参数所对应的SQL类型为原始类型，不是的话抛出异常； 3. 如果SQL类型为BYTE、SHORT、INT、LONG之一的话，返回GenericUDAFSumLongevalutor实例； 4. 如果SQL类型为TIMESTAMP、FLOAT、DOUBLE、STRING、VARCHAR、CHAR之一的话，返回GenericUDAFSumDoubleevalutor实例； 5. 如果SQL类型为DECIMAL的话，返回GenericUDAFSumHiveDecimal evalutor实例； 6. 如果SQL类型不是上述类型的话，抛出异常。 - 这里以GenericUDAFSumDouble为例介绍一下evaluator 。 - init方法中初始化result为0，通知输出列类型为writableDouble； - iterate方法中将输入行相应单元格数值加入汇聚缓冲区的sum； - merge方法中将局部汇聚值加入当前汇聚缓冲区的sum； - terminate返回当前汇聚缓冲区的sum - terminatePartial（GenericUDAFSumEvaluator类）通过调用terminate返回当前汇聚缓冲区的sum
参考 Hive Operators and User-Defined Functions (UDFs)
Hive Plugins
Hive Data Definition Language - Create/Drop/Reload Function
Hive Developer Guide - UDTF
Hive Generic UDAF Case Study
Apache Hive Customization Tutorial Series
</content>
    </entry>
    
     <entry>
        <title>安装Gerrit的commit-msg钩子</title>
        <url>https://mryqu.github.io/post/%E5%AE%89%E8%A3%85gerrit%E7%9A%84commit-msg%E9%92%A9%E5%AD%90/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>gerrit</tag><tag>git</tag><tag>commit-msg</tag><tag>hook</tag><tag>change-id</tag>
        </tags>
        <content type="html"> 对Gerrit进行首次提交前需要安装commit-msg钩子，每次总忘，每次都总是搜邮件，还是记博客里方便些。
gitdir=$(git rev-parse --git-dir); scp -p -P 29418 [your username]@[your Gerrit review server]:hooks/commit-msg {gitdir}/hooks/  参考
Gerrit：commit-msg Hook
Gerrit工作流
</content>
    </entry>
    
     <entry>
        <title>[Hive] Hive Macro和UDF实践</title>
        <url>https://mryqu.github.io/post/hive_hive_macro%E5%92%8Cudf%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>macro</tag><tag>udf</tag><tag>user-defined</tag><tag>function</tag>
        </tags>
        <content type="html">  测试数据库 hive&amp;gt; describe empinfo; OK firstname string lastname string id int age int city string state string Time taken: 0.047 seconds, Fetched: 6 row(s) hive&amp;gt; select * from empinfo; OK John Jones 99980 45 Payson Arizona Mary Jones 99982 25 Payson Arizona Eric Edwards 88232 32 San Diego California Mary Ann Edwards 88233 32 Phoenix Arizona Ginger Howell 98002 42 Cottonwood Arizona Sebastian Smith 92001 23 Gila Bend Arizona Gus Gray 22322 35 Bagdad Arizona Mary Ann May 32326 52 Tucson Arizona Erica Williams 32327 60 Show Low Arizona Leroy Brown 32380 22 Pinetop Arizona Elroy Cleaver 32382 22 Globe Arizona Time taken: 0.033 seconds, Fetched: 12 row(s)  Hive Macro hive&amp;gt; CREATE TEMPORARY MACRO nominal_age(age int) age&#43;1; OK Time taken: 0.006 seconds hive&amp;gt; select firstname,lastname,age,nominal_age(age) from empinfo; OK John Jones 45 46 Mary Jones 25 26 Eric Edwards 32 33 Mary Ann Edwards 32 33 Ginger Howell 42 43 Sebastian Smith 23 24 Gus Gray 35 36 Mary Ann May 52 53 Erica Williams 60 61 Leroy Brown 22 23 Elroy Cleaver 22 23 Time taken: 0.039 seconds, Fetched: 11 row(s) hive&amp;gt; DROP TEMPORARY MACRO IF EXISTS nominal_age; OK Time taken: 0.005 seconds  Hive UDF 查看当前所安装的Hive支持的UDF hive&amp;gt; show functions;  查看UDF介绍 选个好理解的concat函数吧。
hive&amp;gt; describe function concat; OK concat(str1, str2, ... strN) - returns the concatenation of str1, str2, ... strN or concat(bin1, bin2, ... binN) - returns the concatenation of bytes in binary data bin1, bin2, ... binN Time taken: 0.004 seconds, Fetched: 1 row(s) hive&amp;gt; describe function extended concat; OK concat(str1, str2, ... strN) - returns the concatenation of str1, str2, ... strN or concat(bin1, bin2, ... binN) - returns the concatenation of bytes in binary data bin1, bin2, ... binN Returns NULL if any argument is NULL. Example: &amp;gt; SELECT concat(&#39;abc&#39;, &#39;def&#39;) FROM src LIMIT 1; &#39;abcdef&#39; Time taken: 0.006 seconds, Fetched: 5 row(s)  测试内建UDF concat函数是软柿子么？为啥还拿它掐呢！！
hive&amp;gt; select concat(firstname,&#39; &#39;,lastname),age from empinfo; OK John Jones 45 Mary Jones 25 Eric Edwards 32 Mary Ann Edwards 32 Ginger Howell 42 Sebastian Smith 23 Gus Gray 35 Mary Ann May 52 Erica Williams 60 Leroy Brown 22 Elroy Cleaver 22 Time taken: 0.039 seconds, Fetched: 11 row(s)  定制UDF 参考二中的代码挺好，改一改再添个自己的Hello类吧。我的代码、编译和jar命令都做在一个Shell脚本中，方便理解这个流程： mryqu_udf.jar文件内容如下：
hadoop@node50064:~/hivetest$ jar -tf mryqu_udf.jar META-INF/ META-INF/MANIFEST.MF com/ com/mryqu/ com/mryqu/hive/ com/mryqu/hive/udf/ com/mryqu/hive/udf/Lower.class com/mryqu/hive/udf/Hello.class com/mryqu/hive/udf/Hello.java com/mryqu/hive/udf/Lower.java  首先测试一下临时函数，临时函数仅能工作在Hive会话范围内，不会将其元数据写入Hive的元数据数据库，因此使用mryqu_udf.jar中类，必须在每个会话都创建临时函数。测试过程如下：
hive&amp;gt; add jar /home/hadoop/hivetest/mryqu_udf.jar; Added [/home/hadoop/hivetest/mryqu_udf.jar] to class path Added resources: [/home/hadoop/hivetest/mryqu_udf.jar] hive&amp;gt; list jars; /home/hadoop/hivetest/mryqu_udf.jar hive&amp;gt; create temporary function my_hello as &#39;com.mryqu.hive.udf.Hello&#39;; OK Time taken: 0.01 seconds hive&amp;gt; create temporary function my_lower as &#39;com.mryqu.hive.udf.Lower&#39;; OK Time taken: 0.004 seconds hadoop@node50064:~/hivetest$ hive -e &#39;show functions&#39; | grep my OK Time taken: 1.549 seconds, Fetched: 216 row(s) hadoop@node50064:~/hivetest$ hive&amp;gt; select firstname, my_hello(firstname), my_lower(firstname) from empinfo; OK John Hello John john Mary Hello Mary mary Eric Hello Eric eric Mary Ann Hello Mary Ann mary ann Ginger Hello Ginger ginger Sebastian Hello Sebastian sebastian Gus Hello Gus gus Mary Ann Hello Mary Ann mary ann Erica Hello Erica erica Leroy Hello Leroy leroy Elroy Hello Elroy elroy Time taken: 0.037 seconds, Fetched: 11 row(s) hive&amp;gt; drop temporary function if exists my_hello; OK Time taken: 0.005 seconds hive&amp;gt; drop temporary function if exists my_lower; OK Time taken: 0.003 seconds hive&amp;gt; delete jar /home/hadoop/hivetest/mryqu_udf.jar; Deleted [/home/hadoop/hivetest/mryqu_udf.jar] from class path hive&amp;gt; list jars; hive&amp;gt;  接下来试试持久函数（permanentfunction）。参考三中特意提及Hive如果不是在本地模式下，JAR资源文件也不能是本地URI，可以使用HDFSURI等。 看到区别没，showfunctions返回结果包含我所创建的my_hello和my_lower函数。Hive元数据FUNCS表确实包含my_hello和my_lower的信息： 下面的命令同样可以创建上述创建持久函数：
hive&amp;gt; create function my_hello as &#39;com.mryqu.hive.udf.Hello&#39; using jar &#39;hdfs://node50064.mryqu.com:9000/user/hive/mryqu_udf.jar&#39;; hive&amp;gt; create function my_lower as &#39;com.mryqu.hive.udf.Lower&#39; using jar &#39;hdfs://node50064.mryqu.com:9000/user/hive/mryqu_udf.jar&#39;;  最后删除这些持久函数：
hive&amp;gt; drop function if exists my_hello; OK Time taken: 0.013 seconds hive&amp;gt; drop function if exists my_lower; OK Time taken: 0.011 seconds  这里仅对普通Hive UDF进行实践，后继学习将在博文Hive 表UDTF和汇聚UDAF学习 中记录。
参考 Hive Operators and User-Defined Functions (UDFs)
Hive Plugins
Hive Data Definition Language - Create/Drop/Reload Function
Hive Data Definition Language - Create/DropMacro
Apache Hive Customization Tutorial Series
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 加载时替换JavaScript源文件</title>
        <url>https://mryqu.github.io/post/openui5_%E5%8A%A0%E8%BD%BD%E6%97%B6%E6%9B%BF%E6%8D%A2javascript%E6%BA%90%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>replace</tag><tag>javascript</tag><tag>source</tag><tag>loading</tag>
        </tags>
        <content type="html"> 我有一些自己定制的OpenUI5控件，有时会修改某个方法内的逻辑，这个好处理，在ChromedevTool直接修改加载后JS代码并保存就可以直接调试。如果修改了property、aggregation或者init方法内的逻辑的话，由于错过了初始化就不灵了，而重新加载的话又丢失了自己新加的调试代码。
我的解决方法如下： - 清除Chrome缓存 - 在sap-ui-core-dbg.js里requireModule方法内设置断点，设置断点条件为response.indexOf(&amp;ldquo;Dialog.extend(\&amp;ldquo;mryqu.test.control.KexiaoDialog&amp;rdquo;)&amp;gt;0这样当OpenUI5加载KexiaoDialog.js文件时就会触发断点。 - 重新加载我的OpenUI5项目：http://www.mryqu.com/test123/?sap-ui-debug=true&amp;amp;sap-ui-preload=false - 当断点被触发时，在Console执行：
 response=&#39;(function ()\n\ {\n\ &amp;quot;use strict&amp;quot;;\n\ \n\ jQuery.sap.require(&amp;quot;sap.m.Button&amp;quot;);\n\ jQuery.sap.require(&amp;quot;sap.m.Dialog&amp;quot;);\n\ jQuery.sap.require(&amp;quot;sap.m.HBox&amp;quot;);\n\ jQuery.sap.require(&amp;quot;sap.m.Input&amp;quot;);\n\ jQuery.sap.require(&amp;quot;sap.m.RadioButton&amp;quot;);\n\ jQuery.sap.require(&amp;quot;sap.m.VBox&amp;quot;);\n\ jQuery.sap.require(&amp;quot;sap.m.Text&amp;quot;);\n\ \n\ var Button = sap.m.Button;\n\ var Dialog = sap.m.Dialog;\n\ var HBox = sap.m.HBox;\n\ var Icon = sap.ui.core.Icon;\n\ var Input = sap.m.Input;\n\ var RadioButton = sap.m.RadioButton;\n\ var Text = sap.m.Text;\n\ var VBox = sap.m.VBox;\n\ \n\ Dialog.extend(&amp;quot;mryqu.test.control.KexiaoDialog&amp;quot;, {\n\ metadata: {\n\ properties: {\n\ &amp;quot;tableName&amp;quot; : {type : &amp;quot;string&amp;quot;, defaultValue : &amp;quot;&amp;quot;},\n\ },\n\ associations: {\n\ invoker: {type: &amp;quot;sap.ui.core.Control&amp;quot;, multiple: false}\n\ }\n\ },\n\ \n\ renderer: &amp;quot;sap.m.DialogRenderer&amp;quot;,\n\ ............. ............. exit: function() {\n\ if(Dialog.prototype.exit)\n\ Dialog.prototype.exit.apply(this, arguments);\n\ },\n\ \n\ _onAfterClose: function(evt) {\n\ this.destroy();\n\ }\n\ });\n\ }());&#39;;  response中字符串是控件的新代码。有两点注意事项： 1. 如果源文件包含&amp;rsquo;，则需要使用\&amp;lsquo;转义； 2. 对源文件中的换行使用\n\。
这样就可以轻轻松松地从起始点替换掉一个Javascript文件，可以开心玩耍了。
</content>
    </entry>
    
     <entry>
        <title>[Hive] Hive表文件存储格式</title>
        <url>https://mryqu.github.io/post/hive_hive%E8%A1%A8%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>ddl</tag><tag>table</tag><tag>storage</tag><tag>format</tag>
        </tags>
        <content type="html">  Hive支持的内建表文件存储格式如下：
|存储格式|介绍 |&amp;mdash;&amp;ndash; |TEXTFILE|按照纯文本文件格式存储。如果配置hive.default.fileformat没有设置的话，TEXTFILE是默认文件格式。
此存储格式下，数据不做压缩的话，磁盘开销大，数据解析开销大。使用Gzip、Bzip2、Snappy等进行压缩使用（系统自动检查，执行查询时自动解压）的话，Hive不能对数据进行切分，从而无法对数据进行并行操作。 |SEQUENCEFILE|按照压缩的Sequence File格式存储。
SequenceFile一般是在HDFS FileSystem中生成，供map调用的原始文件。Hive中的SequenceFile继承自Hadoop API 的SequenceFile，不过它的key为空，使用value存放实际的值，这样是为了避免MR在运行map阶段的排序过程。 |RCFILE|按照RCFile (Record Columnar File)格式存储。
在Hive0.6.0引入。RCFile是在计算机集群中判断如何存储关系型表的数据存放结构，是Facebook、俄亥俄州立大学、中科院计算所联合研究成果。FCFile结构是由数据存储格式、数据压缩方式、数据读取优化技术等多种模块的系统组合，可以实现数据存放的四个要求：(1)快速加载，(2) 快速处理查询，(3) 高效利用存储空间 (4) 非常适用于动态数据访问模式。
它遵循“先按行划分，再垂直划分”的设计理念。当查询过程中，针对它并不关心的列时，它会在IO上跳过这些列。需要说明的是，RCFile在map阶段从远端拷贝仍然是拷贝整个数据块，并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列，并跳到需要读取的列，而是通过扫描每一个row group的头部定义来实现的，但是在整个HDFS Block 级别的头部并没有定义每个列从哪个rowgroup起始到哪个row group结束。所以在读取所有列的情况下，RCFile的性能反而没有SequenceFile高。 |ORC|在Hive 0.11.0引入。ORC(Optimized RowColumnar)存储源自于RCFile。FCFile把每列都当作二进制blob处理，而ORC存储列元数据，针对列类型使用特定的读写器。ORC支持ACID、内建索引和复杂类型。官网上介绍“其性能显著快于RCFile或Parquet”。Facebook和Yahoo等大公司都在使用。 |PARQUET|在Hive 0.13.0引入。Parquet源自于google Dremel系统。Parquet最初的设计动机是存储嵌套式数据，将这类数据存储成列式格式，以方便对其高效压缩和编码，且使用更少的IO操作取出需要的数据，这也是Parquet相比于ORC的优势，它能够透明地将Protobuf和thrift类型的数据进行列式存储，在Protobuf和thrift被广泛使用的今天，与parquet进行集成，是一件非容易和自然的事情。除了上述优势外，相比于ORC,Parquet没有太多其他可圈可点的地方，比如它不支持update操作（数据写成后不可修改），不支持ACID等。 |AVRO|在Hive 0.13.0引入。Avro是数据序列化系统，由Hadoop项目开发的。
测试 $ echo -e &#39;1\x01foo&#39; &amp;gt; tabft.txt $ echo -e &#39;2\x01bar&#39; &amp;gt;&amp;gt; tabft.txt $ hive hive&amp;gt; create table tabft (id int, name string); hive&amp;gt; quit; $ hadoop fs -put tabft.txt /user/hive/warehouse/tabft $ hive hive&amp;gt; create table tabft_txt (id int, name string) STORED AS TEXTFILE; hive&amp;gt; insert into table tabft_txt select * from tabft; hive&amp;gt; create table tabft_seq (id int, name string) STORED AS SEQUENCEFILE; hive&amp;gt; insert into table tabft_seq select * from tabft; hive&amp;gt; create table tabft_rc (id int, name string) STORED AS RCFILE; hive&amp;gt; insert into table tabft_rc select * from tabft; hive&amp;gt; create table tabft_orc (id int, name string) STORED AS ORC; hive&amp;gt; insert into table tabft_orc select * from tabft; hive&amp;gt; create table tabft_parq (id int, name string) STORED AS PARQUET; hive&amp;gt; insert into table tabft_parq select * from tabft; hive&amp;gt; create table tabft_avro (id int, name string) STORED AS AVRO; hive&amp;gt; insert into table tabft_avro select * from tabft;  获取Sequence文件信息 在我的环境下，按照压缩的Sequence File格式存储后的文件是非压缩的。 获取ORC文件信息 参考 Hive 语言手册 - DDL
Hadoop WIKI：SequenceFile
SequenceFile文件
WIKI：RCFile
Apache ORC官网
Orcfile文件格式解析（1）
Apache Parquet官网
Apache Avro官网
Hive文件存储格式的测试比较
大数据开源列式存储引擎Parquet和ORC
</content>
    </entry>
    
     <entry>
        <title>[Hive] Hive数据类型</title>
        <url>https://mryqu.github.io/post/hive_hive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>binary</tag><tag>timestamp</tag><tag>union</tag><tag>datatype</tag>
        </tags>
        <content type="html">  Hive支持的数据类型 原始数据类型  TINYINT SMALLINT INT BIGINT BOOLEAN FLOAT DOUBLE STRING BINARY TIMESTAMP DECIMAL DECIMAL(precision, scale) DATE VARCHAR CHAR   复杂数据类型  array_type map_type struct_type union_type  原始数据类型 在理解原始数据类型时，耗时最多的是TIMESTAMP和BINARY，下文会着重介绍我对这两种类型的理解。 首先创建表primitive_dataytpes_example，字段之间的分隔符没有采用默认的ctrlA，而是使用逗号分隔：
create table primitive_dataytpes_example ( a TINYINT, b SMALLINT, c INT, d BIGINT, e BOOLEAN, f FLOAT, g DOUBLE, h STRING, i BINARY, j TIMESTAMP, k DECIMAL, l DECIMAL (10,2), m DATE, n VARCHAR(20), o CHAR(20) ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LINES TERMINATED BY &#39;\n&#39;;  接下来插入一条记录（dummy表的使用见参考三）：
insert into primitive_dataytpes_example select 123,130,32769,2147483649,true,7.0E-4,7.0E-4,&#39;kkxx&#39;,&#39;ABCD&#39;,current_timestamp(),7.0E-4,7.0E&#43;4,&#39;2011-10-08&#39;,&#39;kkk&#39;,&#39;xxx&#39; from dummy limit 1;  hadoop@node50064:~/hivetest$ hadoop fs -ls /user/hive/warehouse/primitive_dataytpes_example Found 1 items -rwxrwxr-x 2 hadoop supergroup 126 2015-08-12 17:12 /user/hive/warehouse/primitive_dataytpes_example/000000_0 hadoop@node50064:~/hivetest$ hadoop fs -cat /user/hive/warehouse/primitive_dataytpes_example/000000_0 123,130,32769,2147483649,true,7.0E-4,7.0E-4,kkxx,QUJDRA==,2015-08-12 17:12:30.123,0,70000,2011-10-08,kkk,xxx hadoop@node50064:~/hivetest$ echo `echo QUJDRA== | base64 --decode` ABCD hadoop@node50064:~/hivetest$  上述Hive存储表文件说明以下几件事情： - 对于7.0E-4，FLOAT类型字段f和DOUBLE类型字段g按输入保存，而DECIMAL字段k则按0保存；对于7.0E&#43;4，DECIMAL(10,2)类型字段l按70000保存。这说明Hive会按字段设置的精度存储数据； - 对于current_timestamp()，TIMESTAMP类型字段j保存的字符格式为 &amp;ldquo;YYYY-MM-DDHH:MM:SS.fff&amp;rdquo;，精度为3而不是9； - 对于&amp;rdquo;ABCD&amp;rdquo;，BINARY类型字段i存放了其BASE64编码结果。
下面我们直接在Hive表放置文件数据。 上述实验说明以下几件事情： - 即使文件中数据为77777E-4，DECIMAL字段k显示为8；对于存储数据7.0E&#43;4，DECIMAL(10,2)类型字段l显示70000；对于字符串&amp;rdquo;abcdefghijklmnopqrstuvwxyz&amp;rdquo;，VARCHAR(20)类型字段n和CHAR(20)类型字段o都显示前20个字符。这说明Hive会按字段设置的精度显示数据； - 对于存储数据2012-10-0810:51:00.123456789，TIMESTAMP类型字段j确实可以按精度9显示； - 对于存储数据BASE64编码RGF0YQo=和a3gxMjMK，BINARY类型字段i能够显示原始字符串。
复杂数据类型 在理解复杂数据类型时，耗时最多的是UNION，下文会着重介绍我对这种类型的理解。 首先创建表complex_dataytpes_example： 然后插入两条记录（dummy表的使用见参考三）：
hive&amp;gt; insert into complex_datatypes_example select array(&#39;hello&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;), map(1,&#39;k&#39;,2,&#39;x&#39;,3,&#39;g&#39;), named_struct(&#39;name&#39;,&#39;kx&#39;,&#39;age&#39;,123),create_union(0, 123,false, &amp;quot;0.2012&amp;quot;) from dummy limit 1; hive&amp;gt; insert into complex_datatypes_example select array(&#39;kx&#39;,&#39;xk&#39;), map(123,&#39;hello&#39;,321,&#39;world&#39;), named_struct(&#39;name&#39;,&#39;xk&#39;,&#39;age&#39;,321),create_union(1, 123,false, &amp;quot;0.2012&amp;quot;) from dummy limit 1; hive&amp;gt; insert into complex_datatypes_example select array(&#39;bye&#39;), map(666,&#39;mryqu&#39;), named_struct(&#39;name&#39;,&#39;kxxk&#39;,&#39;age&#39;,444),create_union(2, 123,false, &amp;quot;0.2012&amp;quot;) from dummy limit 1;  结果如下： 以下是我的体会： - 在对struct插值时，仅在列名为col1、col2&amp;hellip;时可以使用struct UDF，否则必须使用named_structUDF。- 在对union插值时，必须用tag选择用那个类型，在我的示例里：0-INT，1-BOOLEAN，2-STRING，这个tag下标不能越界。三个类型的值也必须存在，否则插入失败。- 对于我的示例，使用了FIELDS、COLLECTION ITEMS、MAPKEYS、LINES四种分隔标识符。对于union类型字段，存储的数据中tag和值也用COLLECTIONITEMS进行分隔，而且只存储所选类型的值。为什么create_unionUDF必须要提供其他类型的值的？有点脱裤子放屁——多此一举了！
对于array的查询，可以选择下标；对于map的查询，可以选择键；对于struct的查询，可以选择其子字段。
hive&amp;gt; select a[0],a[1],b[666],c.age from complex_datatypes_example; OK hello 1 NULL 123 kx xk NULL 321 bye NULL mryqu 444 Time taken: 0.042 seconds, Fetched: 3 row(s) hive&amp;gt; select size(a), map_keys(b) from complex_datatypes_example; OK 4 [1,2,3] 2 [123,321] 1 [666] Time taken: 0.035 seconds, Fetched: 3 row(s)  参考 Hive Data Types
Hive Data Definition Language
Hive UDF not supported in insert/values command
Hive Operators and User-Defined Functions (UDFs)
Hive Data Types with Examples
Hive 中的复合数据结构简介以及一些函数的用法说明
[](http://www.hadooptpoint.com/hadoop-hive-data-types-with-examples/)
[](http://querydb.blogspot.fr/2015/11/hive-complex-data-types.html)
</content>
    </entry>
    
     <entry>
        <title>了解Objenesis</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3objenesis/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>objenesis</tag><tag>java</tag><tag>类</tag><tag>实例化</tag>
        </tags>
        <content type="html">  Objenesis是一个很小的Java库，作用是绕过构造器创建一个实例。Java已经支持通过Class.newInstance()动态实例化Java类，但是这需要Java类有个适当的构造器。很多时候一个Java类无法通过这种途径创建，例如： - 构造器需要参数 - 构造器有副作用 - 构造器会抛出异常
Objenesis可以绕过上述限制。它一般用于： - 序列化、远程处理和持久化：无需调用代码即可将Java类实例化并存储特定状态。 - 代理、AOP库和Mock对象：可以创建特定Java类的子类而无需考虑super()构造器。 - 容器框架：可以用非标准方式动态实例化Java类。例如Spring引入Objenesis后，Bean不再必须提供无参构造器了。
Objenesis内部提供了多个不同JVM上的解决方案：参考 Objenesis项目官网
GitHub：Objenesis
</content>
    </entry>
    
     <entry>
        <title>[Hive] Hive UDF not supported in insert/values command</title>
        <url>https://mryqu.github.io/post/hive_hive_udf_not_supported_in_insertvalues_command/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>insert</tag><tag>udf</tag><tag>tok_function</tag><tag>timestamp</tag>
        </tags>
        <content type="html">  创建一个带有timestamp字段的表，想要在insert/values语句中使用UDF，结果报错。
hive&amp;gt; create table t2 (id int, time timestamp); OK Time taken: 0.045 seconds hive&amp;gt; insert into t2 values(1,current_timestamp()); FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expr*ession of type TOK_FUNCTION not supported in insert/values  参考一中提到：”Hive does not support literals for complex types (array,map, struct, union), so it is not possible to use them in INSERTINTO&amp;hellip;VALUES clauses. This means that the user cannot insert datainto a complex datatype column using the INSERT INTO&amp;hellip;VALUESclause.“，但是并没有提及不支持UDF？ 网上找了找，可以转用INSERT&amp;hellip;SELECT这种方式。
insert into t2 select 1, from_utc_timestamp(&#39;2015-08-09 20:12:02&#39;,&#39;GMT&#39;);  实践证明这种方式不可行，不过据说是我的Hive版本不够导致的。 最后先创建一个有数据的dummy表，然后再使用INSERT&amp;hellip;SELECT这种方式。实践证明该方法可行。
create table dummy (story string); insert into table dummy values (&#39;kx&#39;); insert into t2 select 1, from_utc_timestamp(&#39;2015-08-09 20:12:02&#39;,&#39;GMT&#39;) from dummy limit 1;  参考 Hive Data Manipulation Language - Inserting values into tables from SQL
Hive Operators and User-Defined Functions (UDFs)
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 自定义控件属性支持的数据类型</title>
        <url>https://mryqu.github.io/post/openui5_%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%A7%E4%BB%B6%E5%B1%9E%E6%80%A7%E6%94%AF%E6%8C%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>control</tag><tag>perperty</tag><tag>data</tag><tag>type</tag>
        </tags>
        <content type="html"> 创建一个OpenUI5控件时免不了声明几个属性，例如：
metadata: { properties: { &amp;quot;msg&amp;quot; : {type : &amp;quot;string&amp;quot;, defaultValue : &amp;quot;kx123&amp;quot;}, &amp;quot;byProxy&amp;quot; : {type : &amp;quot;boolean&amp;quot;, defaultValue : true} }, publicMethods: [ ], events: { complete : {enablePreventDefault : true} } }  可是属性都支持那些数据类型呢？搜了一下OpenUI5 开发指南，并没有找到什么有用的信息。还是得从代码里面寻觅，结果发现答案就在sap.ui.base.DataType里。
var mTypes = { &amp;quot;any&amp;quot; : createType(&amp;quot;any&amp;quot;, { defaultValue : null, isValid : function(vValue) { return true; } }), &amp;quot;boolean&amp;quot; : createType(&amp;quot;boolean&amp;quot;, { defaultValue : false, isValid : function(vValue) { return typeof vValue === &amp;quot;boolean&amp;quot;; } }), &amp;quot;int&amp;quot; : createType(&amp;quot;int&amp;quot;, { defaultValue : 0, isValid : function(vValue) { return typeof vValue === &amp;quot;number&amp;quot; &amp;amp;&amp;amp; Math.floor(vValue) == vValue; } }), &amp;quot;float&amp;quot; : createType(&amp;quot;float&amp;quot;, { defaultValue : 0.0, isValid : function(vValue) { return typeof vValue === &amp;quot;number&amp;quot;; } }), &amp;quot;string&amp;quot; : createType(&amp;quot;string&amp;quot;, { defaultValue : &amp;quot;&amp;quot;, isValid : function(vValue) { return typeof vValue === &amp;quot;string&amp;quot; || vValue instanceof String; } }), &amp;quot;object&amp;quot; : createType(&amp;quot;object&amp;quot;, { defaultValue : null, isValid : function(vValue) { return typeof vValue === &amp;quot;object&amp;quot; || typeof vValue === &amp;quot;function&amp;quot;; } }) }; DataType.getType = function(sTypeName) { var oType = mTypes[sTypeName]; if ( !oType ) { // check for array types if (sTypeName.indexOf(&amp;quot;[]&amp;quot;) &amp;gt; 0) { var sComponentTypeName = sTypeName.slice(0, -2), oComponentType = this.getType(sComponentTypeName); oType = oComponentType &amp;amp;&amp;amp; createArrayType(oComponentType); if ( oType ) { mTypes[sTypeName] = oType; } return oType; } else { oType = jQuery.sap.getObject(sTypeName); if ( oType instanceof DataType ) { mTypes[sTypeName] = oType; } else if ( jQuery.isPlainObject(oType) ) { oType = mTypes[sTypeName] = createEnumType(sTypeName, oType); } } } return oType; };  有上面代码可知，首先支持的基本数据类型有any、boolean、int、float、string和object。
支持数组，但是数组项的类型必须是支持的基本数据类型。例如&amp;rdquo;messages&amp;rdquo; : {type : &amp;ldquo;string[]&amp;ldquo;,defaultValue : &amp;ldquo;mryqu,kx123&amp;rdquo;}。其中类型以[]结尾，值用逗号分隔。
此外支持可枚举的属性，即扁平对象(使用&amp;rdquo;{}&amp;ldquo;或&amp;rdquo;new Object&amp;rdquo;创建而得) 对sap.ui.base.DataType的值解析、属性有效性验证等细节除了研究代码外，还可参考单元测试DataType.qunit.html。
</content>
    </entry>
    
     <entry>
        <title>在Ubuntu中安装rpm包</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8ubuntu%E4%B8%AD%E5%AE%89%E8%A3%85rpm%E5%8C%85/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>ubuntu</tag><tag>rpm</tag><tag>alien</tag><tag>apt</tag>
        </tags>
        <content type="html"> RPM(RPM Package Manager，原Red Hat PackageManager)是基于RedHat的Linux分发版的包管理系统，用于rpm包的管理（诸如安装、卸载、升级等），其原始设计理念是开放式的，现在包括OpenLinux、Mandrake、SuSE以及TurboLinux等Linux分发版都有采用。 APT软件管理系统是Debian 系统(包含Debian和 Ubuntu)的包管理系统，用于deb包的的管理（诸如安装、卸载、升级等）。deb格式是Debian系统专属安装包格式，进入2.x时代之后由Cydia作者JayFreeman（saurik）将其与APT软件管理系统一起移植到iPhone平台上。 Alien是一个将不同Linux包分发文件转换成deb的程序，支持Linux标准规范, RPM、deb、Stampede(.slp)和Slackware (tgz)包之间的转换。 Alien工具安装： 在Ubuntu下，alien已经添加在源中,可以使用sudo apt-get install alien命令进行安装。
Alien使用： - rpm转deb：sudo alien --script (filename).rpm - Debian系统直接安装rpm：sudo alien -i --script(filename).rpm - deb转rpm：sudo alien --to-rpm (filename).deb - tar.gz转deb：sudo alien -k (filename).tar.gz - tar.bz2转deb：sudo alien -d (filename).tar.bz2 - tgz转deb：sudo alien --to-deb ~/(filename).tgz
由于Teradata数据库的安装文件仅支持Redhat、SUSE和IBM s390xLinux，而我的docker容器是基于Debian系统的，所以今天尝试了一下Alien。但是效果不佳：ttu14中的rpm可以转换成deb，但是直接安装失败；ttu15中的rpm直接安装失败，但是转换deb失败。
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 判断jQuery版本</title>
        <url>https://mryqu.github.io/post/javascript_%E5%88%A4%E6%96%ADjquery%E7%89%88%E6%9C%AC/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>jquery</tag><tag>version</tag><tag>check</tag>
        </tags>
        <content type="html"> 学习了一下如何判断jQuery版本：
if (window.jQuery) { jQuery().jquery; }  测试： </content>
    </entry>
    
     <entry>
        <title>在jQuery AJAX中使用statusCode</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8jquery_ajax%E4%B8%AD%E4%BD%BF%E7%94%A8statuscode/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>jquery</tag><tag>ajax</tag><tag>statuscode</tag><tag>javascript</tag><tag>html5</tag>
        </tags>
        <content type="html">  jQuery.ajax中提供了statusCode设置，以便根据响应状态值进行相应处理。
var data = JSON.stringify({ name: &amp;quot;mryqu&amp;quot;, count: 123 }); $.ajax({ //cache: false, url: &amp;quot;/test&amp;quot;, type: &amp;quot;post&amp;quot;, contentType: &amp;quot;application/json&amp;quot;, dataType: &amp;quot;json&amp;quot;, data: data, beforeSend: function (xhr) { console.log(&amp;quot;beforeSend called&amp;quot;); }, statusCode: { 401: function() { console.log(&amp;quot;statusCode 401 called&amp;quot;); }, 449: function() { console.log(&amp;quot;statusCode 449 called&amp;quot;); } }, error: function (oResult, textStatus, errorThrown) { if (oResult.status !==401 &amp;amp;&amp;amp; oResult.status !==449) { console.log(&amp;quot;error called&amp;quot;); } }, success: function (oResult) { console.log(&amp;quot;success called&amp;quot;); } });  有时候发现statusCode不被调用，所以我更喜欢用下面这种更保险的方式。
var data = JSON.stringify({ name: &amp;quot;mryqu&amp;quot;, count: 123 }); $.ajax({ //cache: false, url: &amp;quot;/test&amp;quot;, type: &amp;quot;post&amp;quot;, contentType: &amp;quot;application/json&amp;quot;, dataType: &amp;quot;json&amp;quot;, data: data, beforeSend: function (xhr) { console.log(&amp;quot;beforeSend called&amp;quot;); }, error: function (oResult, textStatus, errorThrown) { if (oResult.status ===401) { console.log(&amp;quot;statusCode 401 called&amp;quot;); } else if (oResult.status ===449) { console.log(&amp;quot;statusCode 449 called&amp;quot;); } else { console.log(&amp;quot;error called&amp;quot;); } }, success: function (oResult) { console.log(&amp;quot;success called&amp;quot;); } });  参考 jQuery.ajax()
jQuery ajax - avoiding error() callback when statusCode() callback invoked
</content>
    </entry>
    
     <entry>
        <title>Consul服务设置实践</title>
        <url>https://mryqu.github.io/post/consul%E6%9C%8D%E5%8A%A1%E8%AE%BE%E7%BD%AE%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Tool</category><category>Consul</category>
        </categories>
        <tags>
          <tag>consul</tag><tag>service</tag><tag>check</tag><tag>register</tag><tag>deregister</tag>
        </tags>
        <content type="html">  在向Consul注册/注销外部服务节点中，我实践对Consul节点注册和注销，本帖我实践一些对Consul服务的查看和注销。
查看当前数据中心所有注册的服务：
curl http://localhost:8500/v1/catalog/services  查看当前数据中心注册的服务foo的信息：
curl http://localhost:8500/v1/catalog/service/foo  注销服务节点foo上关联的检查service:foo-192-168-0-123：
curl -X PUT -H &#39;application/json&#39; -d &#39;{&amp;quot;Node&amp;quot;: &amp;quot;kexiao&amp;quot;, &amp;quot;CheckID&amp;quot;: &amp;quot;service:foo-192-168-0-123&amp;quot;}&#39; http://localhost:8500/v1/catalog/deregister  注销服务节点foo上关联的服务foo-192-168-0-123：
curl -X PUT -H &#39;application/json&#39; -d &#39;{&amp;quot;Node&amp;quot;: &amp;quot;kexiao&amp;quot;, &amp;quot;ServiceID&amp;quot;: &amp;quot;foo-192-168-0-123&amp;quot;}&#39; http://localhost:8500/v1/catalog/deregister  参考 Consul - Catalog HTTP Endpoint
</content>
    </entry>
    
     <entry>
        <title>[Spring Boot] 创建超媒体驱动的Mail服务</title>
        <url>https://mryqu.github.io/post/spring_boot_%E5%88%9B%E5%BB%BA%E8%B6%85%E5%AA%92%E4%BD%93%E9%A9%B1%E5%8A%A8%E7%9A%84mail%E6%9C%8D%E5%8A%A1/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>mail</tag><tag>hateoas</tag><tag>rest</tag>
        </tags>
        <content type="html">  Spring与Mail的集成 Spring框架为邮件发送提供了一个有用的工具库，可为用户屏蔽底层邮件系统细节，并负责代表客户端负责低层资源处理。 org.springframework.mail包是Spring框架邮件支持的根级包。发送邮件的核心接口是MailSender 接口；封装了简单邮件_from_和_to_等属性的简单对象类是 SimpleMailMessage 。该包也包含对底层邮件系统进行更高级抽象的分层检查异常，其根异常为MailException。 org.springframework.mail.javamail.JavaMailSender 接口MailSender为添加了专业的_JavaMail_ 功能，例如MIME消息支持。JavaMailSender 也为JavaMailMIME消息提供了回调接口org.springframework.mail.javamail.MimeMessagePreparator。
Spring HATEOAS HATEOAS (Hypermedia as the Engine of ApplicationState，超媒体即应用状态引擎)是REST应用架构的一个约束。Spring HATEOAS是一个用于支持实现超媒体驱动的RESTWeb服务的开发库。它提供一些API用于同Spring特别是SpringMVC一起使用时轻松创建遵循HATEOAS原则的REST表述，其试图解决的核心问题是链接创建和表述装配。功能： - 用于链接、资源表述模型的模型类 - 用于指向Spring MVC控制器方法的链接建造者API - 对HAL之类的多媒体格式的支持
示例 Application.java package com.yqu.mail; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } }  MailServerVO.java package com.yqu.mail; import org.springframework.hateoas.ResourceSupport; import java.io.Serializable; import java.util.Properties; public class MailServerVO extends ResourceSupport implements Serializable { private String host; private Integer port; private String userName; private String password; private String defaultEncoding; private Properties properties; public MailServerVO() {} public MailServerVO( String host, Integer port, String userName, String password, String defaultEncoding, Properties properties) { this.host = host; this.port = port; this.userName = userName; this.password = password; this.defaultEncoding = defaultEncoding; this.properties = properties; } public String getHost() { return host; } public void setHost(String host) { this.host = host; } public Integer getPort() { return port; } public void setPort(Integer port) { this.port = port; } public String getUserName() { return userName; } public void setUserName(String userName) { this.userName = userName; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public String getDefaultEncoding() { return defaultEncoding; } public void setDefaultEncoding(String defaultEncoding) { this.defaultEncoding = defaultEncoding; } public Properties getProperties() { return properties; } public void setProperties(Properties properties) { this.properties = properties; } @Override public String toString() { return &amp;quot;MailServerVO{&amp;quot; &#43; &amp;quot;host=&#39;&amp;quot; &#43; host &#43; &#39;\&#39;&#39; &#43; &amp;quot;, port=&amp;quot; &#43; port &#43; &amp;quot;, userName=&#39;&amp;quot; &#43; userName &#43; &#39;\&#39;&#39; &#43; &amp;quot;, password=&#39;&amp;quot; &#43; password &#43; &#39;\&#39;&#39; &#43; &amp;quot;, defaultEncoding=&#39;&amp;quot; &#43; defaultEncoding &#43; &#39;\&#39;&#39; &#43; &amp;quot;, properties=&amp;quot; &#43; properties &#43; &amp;quot;} &amp;quot;; } }  AddressVO.java package com.yqu.mail; public class AddressVO { private String address; private String personal; public AddressVO() {} public AddressVO( String address, String personal) { this.address = address; this.personal = personal; } public String getAddress() { return address; } public void setAddress(String address) { this.address = address; } public String getPersonal() { return personal; } public void setPersonal(String personal) { this.personal = personal; } @Override public String toString() { return &amp;quot;AddressVO{&amp;quot; &#43; &amp;quot;address=&#39;&amp;quot; &#43; address &#43; &#39;\&#39;&#39; &#43; &amp;quot;, personal=&#39;&amp;quot; &#43; personal &#43; &#39;\&#39;&#39; &#43; &#39;}&#39;; } }  MailVO.java package com.yqu.mail; import org.springframework.hateoas.ResourceSupport; import java.io.Serializable; import java.util.Arrays; import java.util.Map; public class MailVO extends ResourceSupport implements Serializable { private String subject; private String plainText; private String htmlText; private String charset; private boolean multipart = true; private AddressVO[] to; private AddressVO[] cc; private AddressVO[] bcc; private AddressVO from; private AddressVO replyTo; public MailVO() {} public MailVO( String subject, String plainText, String htmlText, String charset, boolean multipart, AddressVO[] to, AddressVO[] cc, AddressVO[] bcc, AddressVO from, AddressVO replyTo) { this.subject = subject; this.plainText = plainText; this.htmlText = htmlText; this.charset = charset; this.multipart = multipart; this.to = to; this.cc = cc; this.bcc = bcc; this.from = from; this.replyTo = replyTo; } public String getSubject() { return subject; } public void setSubject(String subject) { this.subject = subject; } public String getPlainText() { return plainText; } public void setPlainText(String plainText) { this.plainText = plainText; } public String getHtmlText() { return htmlText; } public void setHtmlText(String htmlText) { this.htmlText = htmlText; } public String getCharset() { return charset; } public void setCharset(String charset) { this.charset = charset; } public boolean isMultipart() { return multipart; } public void setMultipart(boolean multipart) { this.multipart = multipart; } public AddressVO[] getTo() { return to; } public void setTo(AddressVO[] to) { this.to = to; } public AddressVO[] getCc() { return cc; } public void setCc(AddressVO[] cc) { this.cc = cc; } public AddressVO[] getBcc() { return bcc; } public void setBcc(AddressVO[] bcc) { this.bcc = bcc; } public AddressVO getFrom() { return from; } public void setFrom(AddressVO from) { this.from = from; } public AddressVO getReplyTo() { return replyTo; } public void setReplyTo(AddressVO replyTo) { this.replyTo = replyTo; } @Override public String toString() { return &amp;quot;MailVO{&amp;quot; &#43; &amp;quot;subject=&#39;&amp;quot; &#43; subject &#43; &#39;\&#39;&#39; &#43; &amp;quot;, plainText=&#39;&amp;quot; &#43; plainText &#43; &#39;\&#39;&#39; &#43; &amp;quot;, htmlText=&#39;&amp;quot; &#43; htmlText &#43; &#39;\&#39;&#39; &#43; &amp;quot;, charset=&#39;&amp;quot; &#43; charset &#43; &#39;\&#39;&#39; &#43; &amp;quot;, multipart=&amp;quot; &#43; multipart &#43; &amp;quot;, to=&amp;quot; &#43; Arrays.toString(to) &#43; &amp;quot;, cc=&amp;quot; &#43; Arrays.toString(cc) &#43; &amp;quot;, bcc=&amp;quot; &#43; Arrays.toString(bcc) &#43; &amp;quot;, from=&amp;quot; &#43; from &#43; &amp;quot;, replyTo=&amp;quot; &#43; replyTo &#43; &amp;quot;} &amp;quot;; } }  Util.java package com.yqu.mail; import org.springframework.core.io.ByteArrayResource; import org.springframework.mail.javamail.JavaMailSender; import org.springframework.mail.javamail.JavaMailSenderImpl; import org.springframework.mail.javamail.MimeMessageHelper; import javax.mail.MessagingException; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import javax.mail.internet.MimeUtility; import javax.mail.util.ByteArrayDataSource; import java.io.UnsupportedEncodingException; import java.util.ArrayList; import java.util.List; import java.util.Map; import java.util.Properties; public class Util { public static MailServerVO convert (JavaMailSenderImpl mailServer) { if(mailServer!=null) { return new MailServerVO( mailServer.getHost(), mailServer.getPort(), mailServer.getUsername(), mailServer.getPassword(), mailServer.getDefaultEncoding(), mailServer.getJavaMailProperties()); } return null; } public static JavaMailSenderImpl update( JavaMailSenderImpl sender, MailServerVO vo) { if(vo!=null) { Properties javaMailProperties = sender.getJavaMailProperties(); String host = vo.getHost(); Integer port = vo.getPort(); String userName = vo.getUserName(); String password = vo.getPassword(); String defaultEncoding = vo.getDefaultEncoding(); Properties properties = vo.getProperties(); if (host != null) { sender.setHost(host); } if (port != null) { sender.setPort(port); } if (userName != null) { sender.setUsername(userName); } if (password != null) { sender.setPassword(password); } if (defaultEncoding != null) { sender.setDefaultEncoding(defaultEncoding); } if (properties != null &amp;amp;&amp;amp; !properties.isEmpty()) { properties.stringPropertyNames().forEach( propKey -&amp;gt; javaMailProperties.setProperty( propKey, properties.getProperty(propKey))); sender.setJavaMailProperties(javaMailProperties); } } return sender; } public static AddressVO convert(InternetAddress addr) { if(addr!=null) { return new AddressVO(addr.getAddress(),addr.getPersonal()); } return null; } public static InternetAddress convert(AddressVO vo) { if(vo!=null) { InternetAddress addr = new InternetAddress(); addr.setAddress(vo.getAddress()); if(vo.getPersonal()!=null) { try { addr.setPersonal(vo.getPersonal(), &amp;quot;UTF-8&amp;quot;); } catch (UnsupportedEncodingException e) { } } return addr; } return null; } public static InternetAddress[] convert(AddressVO[] vos) { if(vos!=null) { InternetAddress[] addrs = new InternetAddress[vos.length]; for(int i=0;i attachments = mail.getAttachments(); Map inlineData = mail.getInline(); if (plainText==null &amp;amp;&amp;amp; htmlText==null) { throw new IllegalArgumentException( &amp;quot;Please provide plainText or htmlText.&amp;quot;); } if(!validateAddress(to) &amp;amp;&amp;amp; !validateAddress(cc) &amp;amp;&amp;amp; !validateAddress(bcc)) { throw new IllegalArgumentException( &amp;quot;Please provide &#39;to&#39;, &#39;cc&#39;, or &#39;bcc&#39;.&amp;quot;); } MimeMessage msg = sender.createMimeMessage(); try { MimeMessageHelper helper = new MimeMessageHelper( msg, multipart, charset == null ? &amp;quot;UTF-8&amp;quot; : charset); helper.setFrom(from); if (replyTo != null) { helper.setReplyTo(replyTo); } if (validateAddress(to)) { helper.setTo(to); } if (validateAddress(cc)) { helper.setCc(cc); } if (validateAddress(bcc)) { helper.setBcc(bcc); } helper.setSubject(subject); if(multipart &amp;amp;&amp;amp; plainText!=null &amp;amp;&amp;amp; htmlText!=null) { helper.setText(plainText, htmlText); } else if (plainText==null) { helper.setText(htmlText, true); } else { helper.setText(plainText); } msg = helper.getMimeMessage(); } catch (MessagingException e) { throw new IllegalArgumentException( &amp;quot;Please reconfigure mail content&amp;quot;, e); } return msg; } return null; } }  MailService.java package com.yqu.mail; import org.springframework.mail.javamail.JavaMailSenderImpl; import java.util.List; public interface MailService { public MailServerVO getMailServer(); public void setMailServer(MailServerVO mailServer); void sendMail(MailVO mail); void sendMails(List mail); }  MailServiceImpl.java package com.yqu.mail; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.mail.javamail.JavaMailSenderImpl; import org.springframework.mail.javamail.MimeMessageHelper; import org.springframework.stereotype.Service; import javax.mail.internet.MimeMessage; import java.util.ArrayList; import java.util.List; import java.util.concurrent.atomic.AtomicLong; @Service public class MailServiceImpl implements MailService { private JavaMailSenderImpl sender; public MailServiceImpl() {} public MailServiceImpl(MailServerVO mailServer) { setMailServer(mailServer); } @Override public MailServerVO getMailServer() { return Util.convert(sender); } @Override public void setMailServer(MailServerVO mailServer) { if(sender==null) sender = new JavaMailSenderImpl(); sender = Util.update(sender, mailServer); } @Override public void sendMail(MailVO mail) { if(mail!=null) { MimeMessage msg = Util.convert(sender, mail); sender.send(msg); } } @Override public void sendMails(List mails) { if(mails!=null &amp;amp;&amp;amp; !mails.isEmpty()) { List list = new ArrayList&amp;lt;&amp;gt;(); mails.forEach(mail -&amp;gt; list.add(Util.convert(sender, mail))); sender.send(list.toArray(new MimeMessage[mails.size()])); } } }  MailController.java package com.yqu.mail; import static org.springframework.hateoas.mvc.ControllerLinkBuilder.*; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.hateoas.ResourceSupport; import org.springframework.http.HttpEntity; import org.springframework.http.HttpStatus; import org.springframework.http.ResponseEntity; import org.springframework.mail.javamail.JavaMailSenderImpl; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.*; import org.springframework.web.servlet.ModelAndView; import java.util.List; @RestController public class MailController { @Autowired private MailService service; @RequestMapping(value = &amp;quot;/&amp;quot;, method = RequestMethod.GET) @ResponseBody public HttpEntity home() { ResourceSupport home = new ResourceSupport(); home.add(linkTo(methodOn(MailController.class).home()).withSelfRel()); home.add(linkTo(methodOn(MailController.class).sendMail(null)).withRel(&amp;quot;mail&amp;quot;)); home.add(linkTo(methodOn(MailController.class).sendMails(null)).withRel(&amp;quot;mails&amp;quot;)); home.add(linkTo(methodOn(MailController.class).getMailServer()).withRel(&amp;quot;server&amp;quot;)); return new ResponseEntity(home, HttpStatus.OK); } @RequestMapping(value = &amp;quot;/mail/server&amp;quot;, method = RequestMethod.GET) @ResponseBody public HttpEntity getMailServer() { MailServerVO mailServer = service.getMailServer(); mailServer.add(linkTo(methodOn(MailController.class).getMailServer()).withSelfRel()); mailServer.add(linkTo(methodOn(MailController.class).setMailServer(null)).withRel(&amp;quot;server&amp;quot;)); return new ResponseEntity(mailServer, HttpStatus.OK); } @RequestMapping(value = &amp;quot;/mail/server&amp;quot;, method = RequestMethod.POST) @ResponseBody public HttpEntity setMailServer(@RequestBody MailServerVO newMailServer) { if(newMailServer!=null) { service.setMailServer(newMailServer); } MailServerVO mailServer = service.getMailServer(); mailServer.add(linkTo(methodOn(MailController.class).getMailServer()).withSelfRel()); mailServer.add(linkTo(methodOn(MailController.class).setMailServer(null)).withRel(&amp;quot;update&amp;quot;)); return new ResponseEntity(mailServer, HttpStatus.OK); } @RequestMapping(value = &amp;quot;/mail&amp;quot;, method = RequestMethod.POST) @ResponseBody public ResponseEntity sendMail(@RequestBody MailVO mail) { if(mail!=null) { service.sendMail(mail); } return new ResponseEntity(mail, HttpStatus.OK); } @RequestMapping(value = &amp;quot;/mails&amp;quot;, method = RequestMethod.POST) @ResponseBody public ResponseEntity&amp;gt; sendMails(@RequestBody List mails) { if(mails!=null) { service.sendMails(mails); } return new ResponseEntity&amp;gt;(mails, HttpStatus.OK); } }  application.properties server.context-path=/HelloSpringMail server.port=8080 applicationDefaultJvmArgs: [ &amp;quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=55558&amp;quot; ]  build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.6.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;hello-spring-mail&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-mail&amp;quot;) compile(&amp;quot;com.fasterxml.jackson.core:jackson-databind&amp;quot;) compile(&amp;quot;org.springframework.hateoas:spring-hateoas&amp;quot;) compile(&amp;quot;org.springframework.plugin:spring-plugin-core:1.1.0.RELEASE&amp;quot;) compile(&amp;quot;com.jayway.jsonpath:json-path:0.9.1&amp;quot;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  参考 Spring Reference：Mail Integration
Spring HATEOAS
Understanding HATEOAS
Spring Guide：Building a Hypermedia-Driven RESTful Web Service
Spring – Sending e-mail with attachment
</content>
    </entry>
    
     <entry>
        <title>Java Mail</title>
        <url>https://mryqu.github.io/post/java_mail/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>mail</tag><tag>sun</tag><tag>apache</tag><tag>spring</tag>
        </tags>
        <content type="html">  JavaMail API JavaMail最新版本为1.5.4。 支持的邮件协议有： - SMTP：简单邮件传输协议（Simple Mail Transfer Protocol），由RFC 821 定义，定义了发送电子邮件的机制。在JavaMailAPI环境中，基于JavaMail的程序将和公司或因特网服务供应商的SMTP服务器通信。SMTP 服务器会中转消息给接收方 SMTP服务器以便最终让用户经由 POP 或 IMAP 获得。这不是要求SMTP服务器成为开放的中继，尽管SMTP服务器支持身份验证，不过还是得确保它的配置正确。JavaMailAPI不支持像配置服务器来中继消息或添加/删除邮件账号这类任务的实现。 - POP：邮局协议（Post Office Protocol）。目前用的是版本 3，也称POP3，由RFC 1939定义。本协议主要用于支持使用客户端远程管理在服务器上的电子邮件。POP协议支持“离线”邮件处理。其具体过程是：邮件发送到服务器上，电子邮件客户端调用邮件客户机程序以连接服务器，并下载所有未阅读的电子邮件。使用POP时，用户熟悉的许多性能并不是由POP协议支持的，如查看有几封新邮件消息这一性能。这些性能内建于如Eudora或Microsoft Outlook之类的程序中，它们能记住一些事，诸如最近一次收到的邮件，还能计算出有多少是新的。所以当使用JavaMailAPI时，如果您想要这类信息，您就必须自己算。 - IMAP： 因特网消息访问协议（Internet Message Access Protocol）。目前用的是版本 4，也称IMAP4。由RFC 2060定义，是更高级的用于接收消息的协议。它与POP3协议的主要区别是用户可以不用把所有的邮件全部下载，可以通过客户端直接对服务器上的邮件进行操作。IMAP4改进了POP3的不足，用户可以通过浏览信件头来决定是否收取、删除和检索邮件的特定部分，还可以在服务器上创建或更改文件夹或邮箱。它除了支持POP3协议的脱机操作模式外，还支持联机操作和断连接操作。它为用户提供了有选择的从邮件服务器接收邮件的功能、基于服务器的信息处理功能和共享信箱功能。IMAP4的脱机模式不同于POP3，它不会自动删除在邮件服务器上已取出的邮件，其联机模式和断连接模式也是将邮件服务器作为“远程文件服务器”进行访问，更加灵活方便。IMAP4支持多个邮箱。
MIME：多用途因特网邮件扩展标准（Multipurpose Internet MailExtensions）。它不是邮件传输协议。但对传输内容的消息、附件及其它的内容定义了格式。这里有很多不同的有效文档：RFC 822、RFC 2045、RFC 2046 和 RFC 2047。作为一个 JavaMailAPI的用户，您通常不必对这些格式操心。无论如何，一定存在这些格式而且程序会用到它。
JavaMail API不在Java JDK中，javax.mail.jar包含了JavaMailAPI及Sun的参考设计，其中包括SMTP、IMAP和POP3协议提供者。 JavaMail API 类包: - javax.mail： The JavaMailTM API提供为邮件系统建模的类。 - javax.mail.event： 用于JavaMail API的监听器和事件。 - javax.mail.internet：特定互联网邮件系统的类。 - javax.mail.search：用于JavaMail API的消息搜索术语。 - javax.mail.util： JavaMail API工具类。Sun参考设计的类包: - com.sun.mail.dsn：支持创建和解析传递状态通知。 - com.sun.mail.gimap： 支持Gmail特定IMAP协议扩展的实验性IMAP协议提供者。 - com.sun.mail.imap：用于访问IMAP消息存储的IMAP协议提供者。 - com.sun.mail.pop3：用于访问POP3消息存储的POP3协议提供者。 - com.sun.mail.smtp：用于访问SMTP服务器的SMTP协议提供者。 - com.sun.mail.util： 用于JavaMail API的工具类。 - com.sun.mail.util.logging： 包含用于JavaTM平台核心日志功能的JavaMailTM扩展。
Apache Commons Email Apache Commons Email是构建在JavaMail API之上的工具库，旨在简化设计，当前版本1.4。主要的一些类: - SimpleEmail - 用于发送简单的文本邮件。 - MultiPartEmail - 用于发送多部分消息（multipartmessages）。允许带有（内联或附加的）附件的文本邮件。 - HtmlEmail -用于发送HTML格式邮件。具有MultiPartEmail所有的功能，可以更容易添加附件，并支持内嵌的图像。 - ImageHtmlEmail - 用于发送带内联图像的HTML格式邮件。具有HtmlEmail所有的功能，但将所有图像引用转成内联图像。 - EmailAttachment -可用于轻松处理附件的简单容器类，用于MultiPartEmail和HtmlEmail实例。
在我学习Activiti的过程了解到：Activiti的BPMN引擎对MAIL任务的实现就采用的是Apache CommonsEmail，详见MailActivityBehavior.java。
Spring与Mail的集成 Spring框架为邮件发送提供了一个有用的工具库，可为用户屏蔽底层邮件系统细节，并负责代表客户端负责低层资源处理。 org.springframework.mail包是Spring框架邮件支持的根级包。发送邮件的核心接口是MailSender接口；封装了简单邮件_from_和_to_等属性的简单对象类是SimpleMailMessage。该包也包含对底层邮件系统进行更高级抽象的分层检查异常，其根异常为MailException。 org.springframework.mail.javamail.JavaMailSender 接口MailSender为添加了专业的_JavaMail_ 功能，例如MIME消息支持。JavaMailSender 也为JavaMailMIME消息提供了回调接口org.springframework.mail.javamail.MimeMessagePreparator。
参考 JavaMail API
JavaMail JavaDoc
Apache Commons Email
Spring Reference：Mail Integration
[](http://spring.io/guides/gs/rest-hateoas/)
</content>
    </entry>
    
     <entry>
        <title>向Consul注册/注销外部服务节点</title>
        <url>https://mryqu.github.io/post/%E5%90%91consul%E6%B3%A8%E5%86%8C%E6%88%96%E6%B3%A8%E9%94%80%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1%E8%8A%82%E7%82%B9/</url>
        <categories>
          <category>Tool</category><category>Consul</category>
        </categories>
        <tags>
          <tag>consul</tag><tag>external</tag><tag>service</tag><tag>register</tag><tag>deregister</tag>
        </tags>
        <content type="html">  已有一个docker上的微服务节点foo，但是有可能需要使用系统外部的foo服务集群。 切换到系统外部的foo服务集群的操作过程如下：
docker-compose stop foo docker-compose rm foo curl -X PUT -H &#39;application/json&#39; -d &#39;{&amp;quot;Node&amp;quot;: &amp;quot;foo&amp;quot;, &amp;quot;Address&amp;quot;: &amp;quot;foo.cluster.yqu.com&amp;quot;, &amp;quot;Service&amp;quot;: {&amp;quot;Service&amp;quot;:&amp;quot;foo&amp;quot;, &amp;quot;tags&amp;quot;: [&amp;quot;controller&amp;quot;], &amp;quot;port&amp;quot;: 12221}}&#39; http://localhost:8500/v1/catalog/register docker-compose restart consul  切换回系统内部过程的foo服务节点操作过程如下：
curl -X PUT -H &#39;application/json&#39; -d &#39;{&amp;quot;Node&amp;quot;: &amp;quot;foo&amp;quot;}&#39; http://localhost:8500/v1/catalog/deregister docker-compose up -d foo docker-compose restart consul  注销foo服务节点操作过程如下：
curl -X PUT -H &#39;application/json&#39; -d &#39;{&amp;quot;Node&amp;quot;: &amp;quot;foo&amp;quot;}&#39; http://localhost:8500/v1/catalog/deregister docker-compose stop foo docker-compose rm foo docker-compose restart consul  参考 Consul Guide：Registering An External Service
</content>
    </entry>
    
     <entry>
        <title>[Hive] Hive JDBC实践</title>
        <url>https://mryqu.github.io/post/hive_hive_jdbc%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>jdbc</tag><tag>compile</tag><tag>practice</tag>
        </tags>
        <content type="html">  HiveJdbcClient.java 使用参考一中的示例代码:
import java.sql.SQLException; import java.sql.Connection; import java.sql.ResultSet; import java.sql.Statement; import java.sql.DriverManager; public class HiveJdbcClient { private static String driverName = &amp;quot;org.apache.hive.jdbc.HiveDriver&amp;quot;; public static void main(String[] args) throws SQLException { try { Class.forName(driverName); } catch (ClassNotFoundException e) { // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); } //&amp;quot;hadoop&amp;quot; is the name of the user the queries should run as in my cluster. Connection con = DriverManager.getConnection( &amp;quot;jdbc:hive2://localhost:10000/default&amp;quot;, &amp;quot;hadoop&amp;quot;, &amp;quot;{PASSWORD_OF_USER_HADOOP}&amp;quot;); Statement stmt = con.createStatement(); String tableName = &amp;quot;testHiveDriverTable&amp;quot;; stmt.execute(&amp;quot;drop table if exists &amp;quot; &#43; tableName); stmt.execute(&amp;quot;create table &amp;quot; &#43; tableName &#43; &amp;quot; (key int, value string)&amp;quot;); // show tables String sql = &amp;quot;show tables &#39;&amp;quot; &#43; tableName &#43; &amp;quot;&#39;&amp;quot;; System.out.println(&amp;quot;Running: &amp;quot; &#43; sql); ResultSet res = stmt.executeQuery(sql); if (res.next()) { System.out.println(res.getString(1)); } // describe table sql = &amp;quot;describe &amp;quot; &#43; tableName; System.out.println(&amp;quot;Running: &amp;quot; &#43; sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1) &#43; &amp;quot;\t&amp;quot; &#43; res.getString(2)); } // load data into table // NOTE: filepath has to be local to the hive server // NOTE: /tmp/a.txt is a ctrl-A separated file with two fields per line String filepath = &amp;quot;/tmp/a.txt&amp;quot;; sql = &amp;quot;load data local inpath &#39;&amp;quot; &#43; filepath &#43; &amp;quot;&#39; into table &amp;quot; &#43; tableName; System.out.println(&amp;quot;Running: &amp;quot; &#43; sql); stmt.execute(sql); // select * query sql = &amp;quot;select * from &amp;quot; &#43; tableName; System.out.println(&amp;quot;Running: &amp;quot; &#43; sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(String.valueOf(res.getInt(1)) &#43; &amp;quot;\t&amp;quot; &#43; res.getString(2)); } // regular hive query sql = &amp;quot;select count(1) from &amp;quot; &#43; tableName; System.out.println(&amp;quot;Running: &amp;quot; &#43; sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1)); } } }  hive_jdbc.sh #!/bin/bash javac HiveJdbcClient.java HADOOP_HOME=/usr/local/hadoop HIVE_HOME=/usr/local/hive # default hive record format is one record per line, # delimited by ctrlA character # The commands below create 2 records # (1,foo) and (2,bar) echo -e &#39;1\x01foo&#39; &amp;gt; /tmp/a.txt echo -e &#39;2\x01bar&#39; &amp;gt;&amp;gt; /tmp/a.txt CLASSPATH=.:$HIVE_HOME/conf:$(hadoop classpath) for i in ${HIVE_HOME}/lib/*.jar ; do CLASSPATH=$CLASSPATH:$i done java -cp $CLASSPATH HiveJdbcClient  注1：hadoopclasspath用于输出hadoop所用的CLASSPATH，但是它对同一目录下的jar文件使用*.jar，根据帮助还可以使用hadoopclasspath &amp;ndash;glob将其展开，就可以看到每一个jar文件了。 注2：cat/tmp/a.txt显示结果是过滤掉非显示字符ctrlA的，可以使用less/tmp/a.txt显示带有控制符ctrlA的内容。 运行 参考 HiveServer2 Clients - JDBC Client Sample Code
HiveClient
</content>
    </entry>
    
     <entry>
        <title>[Hive] HCatalog和WebHCat学习</title>
        <url>https://mryqu.github.io/post/hive_hcatalog%E5%92%8Cwebhcat%E5%AD%A6%E4%B9%A0/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>hcatalog</tag><tag>cli</tag><tag>webhcat</tag><tag>元数据服务</tag>
        </tags>
        <content type="html">  HCatalog 访问数据的常见方法之一是通过表抽象，该方法通常用于访问关系型数据库，并且为许多开发者所熟知(和广泛采用)。一些流行的Hadoop系统，例如Hive和Pig，也采用了这种方法。这种抽象解除了数据如何存储(HDFS文件、HBase表)与应用程序如何处理数据(表格式)之间的耦合。此外，它允许从较大的数据语料库中&amp;rdquo;过滤&amp;rdquo;感兴趣的数据。 Hadoop的元数据服务HCatalog扩展了Hive的元存储，同时保留了HiveDDL中用于表定义的组件。其结果是，Hive的表抽象(当使用了HCatalog时)可以用于Pig和MapReduce应用程序，这带来了以下一些主要优势：
 它使得数据消费者不必知道其数据存储的位置和方式。 它允许数据生产者修改物理数据存储和数据模型，同时仍然支持以旧格式存储的现有数据，从而数据消费者不需要修改他们的处理流程。 它为Pig、Hive和MapReduce提供了共享的结构和数据模型。  HCatalog支持读写任何SerDe支持的文件格式。默认情况下，HCatalog支持RCFile、CSV、JSON、SequenceFile和ORC文件格式。如果使用订制格式，必须提供InputFormat、OutputFormat和SerDe。 WebHCat WebHCat是WebHCat的REST API。这样应用无需使用Hadoop API就可以通过HTTP请求访问HadoopMapReduce (或YARN)、Pig、Hive及HCatalog DDL。WebHCat所使用的代码和数据必须存放在HDFS中。HCatalogDDL命令在请求后即直接执行，MapReduce、Pig和Hive作业则放置在WebHCat(Templeton)服务器的队列中，并监控进展过程或按需停止。程序员指定Pig、Hive和MapReduce结果存放的HDFS位置。 使用 HCatalog和WebHCat都已随Hive安装，所以可以直接使用
使用HCatalog HCatalog CLI与Hive CLI功能大致一样：
cd $HIVE_HOME ./hcatalog/bin/hcat  使用WebHCat 在.bashrc中添加PYTHON_CMD：
export PYTHON_CMD=/usr/bin/python  启动WebHCat服务器：
cd $HIVE_HOME ./hcatalog/sbin/webhcat_server.sh start  参考 HCatalog
HCatalog CLI
WebHCat
GitHub: apache/hcatalog
GitHub: apache/hive/hcatalog
apache/hive/hcatalog/webhcat
</content>
    </entry>
    
     <entry>
        <title>[Hive] Hive CLI和Beeline学习</title>
        <url>https://mryqu.github.io/post/hive_hive_cli%E5%92%8Cbeeline%E5%AD%A6%E4%B9%A0/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>shell</tag><tag>hadoop</tag><tag>cli</tag><tag>beeline</tag>
        </tags>
        <content type="html">  Hive CLI学习 Hive CLI使用手册很简单，但是看完了对有些参数还是不太理解，所以就翻翻Hive CLI源码对照学习吧。
-f和-i选项的区别 Hive CLI使用手册说-i指定的是初始化SQL文件，-f指定的是SQL脚本文件。 通过阅读源码可知，所谓的初始化SQL文件就是你期望每次执行HiveCLI都要在其他操作之前运行的一些SQL命令。执行完初始化SQL，可以接着执行-e选项中的SQL命令、-f选项中的SQL脚本文件或交互输入的命令；而-f选项和-e选项二者只能存在其一，执行完-f选项后退出CLI。
hiverc文件 当没有指定-i参数时，CLI会尝试加载$HIVE_HOME/bin/.hiverc、$HIVE_CONF_DIR/.hiverc和$HOME/.hiverc作为初始化文件。只要存在，这些.hiverc都会被加载执行。 通过CliDriver类的processInitFiles方法可知，执行初始化SQL时始终采用静默模式，即不显示执行进度信息，只显示最后结果；执行-f选项中SQL脚本时是否采用静默模式由-S选项控制。
Hive CLI如何处理shell命令、Hive命令和SQL的？ HiveCLI既可以处理一个SQL脚本文件、也可以处理多个SQL命令。它通过处理多行命令，以&amp;rdquo;;&amp;ldquo;为分隔符，获取单个命令列表。一个单个命令，即可能是&amp;ndash;开头的注释行，也可能是!开头的shell命令，此外SQL命令和Hive自身支持的命令。 - 对于shell命令，Hive CLI是通过ShellCmdExecutor执行的； - 对于SQL命令，Hive CLI是通过org.apache.hadoop.hive.ql.Driver执行的； - 对于Hive命令，HiveCLI通过SetProcessor、ResetProcessor、DfsProcessor、AddResourceProcessor、ListResourceProcessor、DeleteResourceProcessor、CompileProcessor、ReloadProcessor、CryptoProcessor这些处理进行执行。
&amp;ndash;hiveconf、&amp;ndash;define (-d)、&amp;ndash;hivevar之间的关系 首先我们看一下OptionsProcessor类，它通过Apache Commons CLI解析Hive CLI命令参数: - 其process_stage1方法将&amp;ndash;hiveconf参数置入系统属性中，将&amp;ndash;define和&amp;ndash;hivevar参数置入CliSessionState对象的hiveVariables字段 - 其process_stage2方法将&amp;ndash;hiveconf参数置入CliSessionState对象的cmdProperties字段
接下来看一下CliSessionState对象的hiveVariables字段和cmdProperties字段使用情况: - CliDriver.run方法将CliSessionState对象的cmdProperties字段中的键值对覆盖HiveConf对象，然后置入CliSessionState对象的overriddenConfigurations字段 - CliSessionState对象的hiveVariables字段主要用于变量替换，包括替换提示符（CliDriver.run）、替换source命令所跟文件路径及shell命令（CliDriver.processCmd）、替换SQL（Driver.compile）、替换Hive命令（DfsProcessor.run、&amp;hellip;&amp;hellip;）
总之： - &amp;ndash;hiveconf参数在命令行中设置Hive的运行时配置参数，优先级高于hive-site.xml,但低于Hive交互Shell中使用Set命令设置。 - &amp;ndash;define (-d)和&amp;ndash;hivevar没有区别，都是用于变量替换的。
hivehistory文件 Hive CLI会创建$HOME/.hivehistory文件，并在其中记录命令历史记录。
-v参数打印出的SQL语句是变量替换后的吗？ 不是，打印的是原始SQL语句。 看了Hive CLI源码后的疑惑  CliDriver类主函数实例化一个CliDriver对象，而在executeDriver方法中不用自身实例，偏偏又实例化一个CliDriver对象cli来，为啥？ &amp;ndash;hiveconf参数会被放入CliSessionState对象的cmdProperties字段和overriddenConfigurations字段，难道不能合并成一份么？  Hive Beeline学习 BeeLine类的dispatch负责将特定命令行分发给适合的CommandHandler。 - 其中以!起始的SQLLine命令由execCommandWithPrefix方法处理，具体实现见Commands类的同名方法。- 其他命令则由Commands类的sql方法处理
参考 Hive LanguageManual CLI
Hive LanguageManual VariableSubstitution
Hive CLI source code
Beeline – Command Line Shell
Hive Beeline CLI source code
</content>
    </entry>
    
     <entry>
        <title>[Gradle] 列举插件</title>
        <url>https://mryqu.github.io/post/gradle_%E5%88%97%E4%B8%BE%E6%8F%92%E4%BB%B6/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>devops</tag><tag>list</tag><tag>plugin</tag>
        </tags>
        <content type="html"> 下列方法可以列举出当前build.gradle牵涉的插件:
project.plugins.each { println it }  </content>
    </entry>
    
     <entry>
        <title>[Ambari] 了解Ambari</title>
        <url>https://mryqu.github.io/post/ambari_%E4%BA%86%E8%A7%A3ambari/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Ambari</category>
        </categories>
        <tags>
          <tag>ambari</tag><tag>hadoop</tag><tag>provision</tag><tag>manage</tag><tag>monitor</tag>
        </tags>
        <content type="html"> 今天看到一篇帖子《Ambari——大数据平台的搭建利器》介绍了Apache Ambari的使用。感觉Ambari确实不错，很便捷地支持Apache Hadoop集群的配置、管理和监控，堪称利器！
Ambari对系统管理员提供如下功能： - 配置Hadoop集群 - Ambari提供逐步的的安装向导在任意多台机器上安装Hadoop集群。 - Ambari处理集群中Hadoop服务的配置。 - 管理Hadoop集群 - Ambari提供对整个集群范围内启动、停止和重新配置Hadoop服务的集中管理。 - 监控Hadoop集群 - Ambari提供仪表盘用于监控Hadoop集群（HDFS、MapReduce、HBase、Hive和HCatalog）的健康和状态。 - Ambari通过Ambari 运维指标系统收集指标。 - Ambari提供用于系统告警的Ambari告警框架，可在需要时（例如节点宕机、剩余磁盘空间不足等）通知你。
Ambari对应用开发者和系统集成者提供如下功能： - 通过Ambari REST APIs轻松将Hadoop配置、管理和监控功能与自己的应用集成。
Ambari当前可在一些64位Linux系统上安装。
另，Ambari中文为洋麻。
</content>
    </entry>
    
     <entry>
        <title>[Hive] 遇到Relative path in absolute URI:${system:java.io.tmpdir}/${system:user.name}</title>
        <url>https://mryqu.github.io/post/hive_%E9%81%87%E5%88%B0relative_path_in_absolute_uri%E9%97%AE%E9%A2%98/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>java.io.tmpdir</tag><tag>systemuser.name</tag><tag>relativepath</tag><tag>absoluteuri</tag>
        </tags>
        <content type="html"> 安装问Hive，启动一下CLI试一下效果。结果直接崩了，错误提示：Relative path in absolute URI:${system:java.io.tmpdir}/${system:user.name}。 Hive AdminManual Configuration里面的例子是将hive.exec.scratchdir设定为/tmp/mydir。即使按照示例来配置，还是会报hive.downloaded.resources.dir属性错误。后来看到网上有人说主要是Hadoop路径不支持带&amp;rdquo;:&amp;ldquo;，所以会报错。
解决方法： - hive.exec.local.scratchdir: /tmp/${user.name} - hive.downloaded.resources.dir: /tmp/${user.name}_resources
可以登入Hive Shell了！ </content>
    </entry>
    
     <entry>
        <title>[Hive] 安装Hive 1.2.x</title>
        <url>https://mryqu.github.io/post/hive_%E5%AE%89%E8%A3%85hive_1.2.x/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hive</category>
        </categories>
        <tags>
          <tag>hive</tag><tag>metastore</tag><tag>embedded</tag><tag>local</tag><tag>remote</tag>
        </tags>
        <content type="html">  我的Hadoop集群为node50064、node50069和node51054。本文的Hive和MySQL软件仅在node50064上安装。
安装Hive-内嵌元数据存储模式 Hive驱动、元数据存储接口和数据库(derby)使用相同的JVM。元数据保持在内嵌的derby数据库，只允许一个会话连接到数据库。 下载并解压缩Hive wget http://apache.cs.utah.edu/hive/hive-1.2.x/apache-hive-1.2.x-bin.tar.gz tar -xzf apache-hive-1.2.x-bin.tar.gz sudo mv apache-hive-1.2.x-bin /usr/local/hive sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/hive  环境变量设置 export HADOOP_HOME=/usr/local/hadoop export HADOOP_PREFIX=$HADOOP_HOME export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_PREFIX/lib/native export HADOOP_OPTS=&amp;quot;$HADOOP_OPTS -Djava.library.path=$HADOOP_PREFIX/lib/native&amp;quot; export HIVE_HOME=/usr/local/hive export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$HIVE_HOME/bin  最后通过source~/.bashrc刷新配置文件。
conf/hive-env.sh 首先通过cd $HIVE_HOME/conf;cp hive-env.sh.template hive-env.sh;chmod 774hive-env.sh创建并设置hive-env.sh执行权限。 修改后的主要部分内容如下：
# Set HADOOP_HOME to point to a specific hadoop install directory export HADOOP_HOME=${HADOOP_HOME:-/usr/local/hadoop} # Hive Configuration Directory can be controlled by: export HIVE_CONF_DIR=/usr/local/hive/conf # Folder containing extra ibraries required for hive compilation/execution can be controlled by: export HIVE_AUX_JARS_PATH=/usr/local/hive/lib  conf/hive-site.xml 首先通过cd $HIVE_HOME/conf;cp hive-default.xml.templatehive-site.xml创建hive-site.xml，并作如下改动： 参数解释如下： - Hive表对应的真正HDFS文件存放位置由hive.metastore.warehouse.dir指定。 - Hive在客户端和HDFS实例机器上使用临时目录。这些目录用于存储每个查询的临时/中间数据集，当查询结束后由Hive客户端清除。然而如果hive客户端非正常终止，一些数据将被遗留。HDFS集群上临时目录由hive.exec.scratchdir指定，Hive客户端机器上临时目录由hive.exec.local.scratchdir指定。 注意，当向表/分区写数据时，Hive首先向目标表文件系统（由hive.exec.scratchdir指定）的临时位置写数据，然后再移往目标表。这种机制对HDFS、S3甚至NFS等文件系统都适用。
创建HDFS目录 在Hive中创建表之前使用HDFS命令创建/tmp和/user/hive/warehouse(即hive.metastore.warehouse.dir所指)并设置模式为g&#43;w。
hadoop@node50064:~$hadoop fs -mkdir /tmp hadoop@node50064:~$hadoop fs -mkdir /user/hive/warehouse hadoop@node50064:~$hadoop fs -chmod g&#43;w /tmp hadoop@node50064:~$hadoop fs -chmod g&#43;w /user/hive/warehouse  初始化Derby数据库 schematool -dbType derby -initSchema  运行Hive控制台 安装Hive-本地元数据存储模式 在Hive所在机器上安装MySQL，把元数据放到MySQL数据库内。 安装MySQL sudo apt-get update sudo apt-get install mysql-server sudo apt-get install libmysql-java  配置MySQL create user &#39;hive&#39; identified by &#39;hive&#39;; grant all privileges on *.* to &#39;hive&#39;@&#39;%&#39; with grant option; flush privileges;  conf/hive-site.xml 在内嵌模式所用的conf/hive-site.xml做若下修改：
|属性|改动 |&amp;mdash;&amp;ndash; |javax.jdo.option.ConnectionDriverName|org.apache.derby.jdbc.EmbeddedDriver=&amp;gt;com.mysql.jdbc.Driver |javax.jdo.option.ConnectionURL|jdbc:derby:;databaseName=metastore_db;create=true=&amp;gt;jdbc:mysql://node50064.mryqu.com:3306/hive?createDatabaseIfNotExist=true |javax.jdo.option.ConnectionUserName|APP=&amp;gt;hive |javax.jdo.option.ConnectionPassword|mine=&amp;gt;hive
向Hive库添加MySQL JDBC驱动 cd $HIVE_HOME/lib ln -s /usr/share/java/mysql-connector-java.jar  运行Hive控制台 安装Hive-远程元数据存储模式 Hive驱动和元数据存储接口运行在不同JVM上（甚至不同机器上）。这样数据库与Hive用户完全隔离，且数据库密码也不会为Hive用户所知。 conf/hive-site.xml 设置hive.metastore.uris值为thrift://hive-metastore-machine:9083 。
启动Hive元数据存储接口 hive --service metastore &amp;amp;  确认启动：
netstat -an | grep 9083  参考 Hive AdminManual &amp;amp; MetastoreAdmin
Hive Metastore Configuration
Different ways of configuring Hive metastore
HIVE完全分布式集群安装过程（元数据库: MySQL）
</content>
    </entry>
    
     <entry>
        <title>选择候选样式表</title>
        <url>https://mryqu.github.io/post/%E9%80%89%E6%8B%A9%E5%80%99%E9%80%89%E6%A0%B7%E5%BC%8F%E8%A1%A8/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>browser</tag><tag>candidate</tag><tag>css</tag>
        </tags>
        <content type="html">  有的浏览器提供选择如下的候选层叠样式表。IE Firefox Firefox可以通过F10调出菜单。Chrome 不支持。
</content>
    </entry>
    
     <entry>
        <title>[Pig] 安装Pig 0.15.0</title>
        <url>https://mryqu.github.io/post/pig_%E5%AE%89%E8%A3%85pig_0.15.0/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Pig</category>
        </categories>
        <tags>
          <tag>pig</tag><tag>mapreduce</tag><tag>install</tag><tag>apache</tag><tag>hadoop</tag>
        </tags>
        <content type="html">  安装Pig 我的Hadoop集群为node50064、node50069和node51054。本文的Pig软件仅在node50064上安装。
下载并解压缩Pig wget http://apache.cs.utah.edu/pig/pig-0.15.0/pig-0.15.0.tar.gz tar -xzf pig-0.15.0.tar.gz sudo mv pig-0.15.0 /usr/local/pig sudo chown -R &amp;quot;hadoop:hadoop&amp;quot; /usr/local/pig  环境变量设置 export HADOOP_HOME=/usr/local/hadoop export HADOOP_PREFIX=$HADOOP_HOME export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_PREFIX/lib/native export HADOOP_OPTS=&amp;quot;$HADOOP_OPTS -Djava.library.path=$HADOOP_PREFIX/lib/native&amp;quot; export PIG_HOME=/usr/local/pig export PIG_CLASSPATH=$HADOOP_HOME/conf export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$PIG_HOME/bin  最后通过source~/.bashrc刷新配置文件。
conf/pig.properties pig.properties用于配置Pig各种参数。参数说明如下： 运行Pig控制台 参考 你用pig分析access_log日志中ip访问次数
</content>
    </entry>
    
     <entry>
        <title>为Artifactory设置proxy和remote repository</title>
        <url>https://mryqu.github.io/post/%E4%B8%BAartifactory%E8%AE%BE%E7%BD%AEproxy%E5%92%8Cremote_repository/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>artifacotry</tag><tag>proxy</tag><tag>remote</tag><tag>reposiotry</tag>
        </tags>
        <content type="html">  设置proxy 我一上来先设置代理，否则连不上远程仓库呀。 设置remote repository 远程仓库已经默认设置了jcenter，估计很少需要其他仓库了。 但不管三七二十一，还是把mavenCentral和gradlePlugins加上吧。全部勾选了Suppress POMConsistency Checks，取消勾选Handle Snapshots。 - mavenCentral: http://repo1.maven.org/maven2/ - gradlePlugins: https://plugins.gradle.org/m2/
测试结果显示，所需构件实际上都是从jcenter下载的，其他两个暂时还没用到。
</content>
    </entry>
    
     <entry>
        <title>整理贴：八卦一下CoreOS</title>
        <url>https://mryqu.github.io/post/%E6%95%B4%E7%90%86%E8%B4%B4%E5%85%AB%E5%8D%A6%E4%B8%80%E4%B8%8Bcoreos/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>coreos</tag><tag>容器</tag><tag>docker</tag><tag>rkt</tag><tag>devops</tag>
        </tags>
        <content type="html">  CoreOS是一个轻量级容器化Linux发行版，专为大型数据中心而设计，旨在通过轻量的系统架构和灵活的应用程序部署能力简化数据中心的维护成本和复杂度。
CoreOS的历史 2013年2月，美国的dotCloud公司发布了一款新型的Linux容器软件Docker，并建立了一个网站发布它的首个演示版本（见Docker第一篇官方博客）。而几乎同时，2013年3月，美国加州，年轻的帅小伙Alex Polvi正在自己的车库开始他的第二次创业。此前，他的首个创业公司Cloudkick卖给了云计算巨头Rackspcace（就是OpenStack的东家）。 有了第一桶金的Alex这次准备干一票大的，他计划开发一个足以颠覆传统的服务器系统的Linux发行版。为了提供能够从任意操作系统版本稳定无缝地升级到最新版系统的能力，Alex急需解决应用程序与操作系统之间的耦合问题。因此，当时还名不见经传的Docker容器引起了他的注意，凭着敏锐直觉，Alex预见了这个项目的价值，当仁不让地将Docker做为了这个系统支持的第一套应用程序隔离方案。不久以后，他们成立了以自己的系统发行版命名的组织：CoreOS。事实证明，采用Docker这个决定，后来很大程度上成就了CoreOS的生态系统。 CoreOS特点 首先，CoreOS没有提供包管理工具，而是通过容器化的运算环境向应用程序提供运算资源。应用程序之间共享系统内核和资源，但是彼此之间又互不可见。这样就意味着应用程序将不会再被直接安装到操作系统中，而是通过Docker 运行在容器中。这种方式使得操作系统、应用程序及运行环境之间的耦合度大大降低。相对于传统的部署方式而言，在 CoreOS集群中部署应用程序更加灵活便捷，应用程序运行环境之间的干扰更少，而且操作系统自身的维护也更加容易。 其次， CoreOS 采用双系统分区 (dual root partition)设计。两个分区分别被设置成主动模式和被动模式并在系统运行期间各司其职。主动分区负责系统运行，被动分区负责系统升级。一旦新版本的操作系统被发布，一个完整的系统文件将被下载至被动分区，并在系统下一次重启时从新版本分区启动，原来的被动分区将切换为主动分区，而之前的主动分区则被切换为被动分区，两个分区扮演的角色将相互对调。同时在系统运行期间系统分区被设置成只读状态，这样也确保了CoreOS 的安全性。CoreOS 的升级过程在默认条件下将自动完成，并且通过 cgroup对升级过程中使用到的网络和磁盘资源进行限制，将系统升级所带来的影响降至最低。 另外，CoreOS 使用 Systemd 取代 SysV 作为系统和服务的管理工具。与 SysV 相比，Systemd不但可以更好的追踪系统进程，而且也具备优秀的并行化处理能力，加之按需启动等特点，并结合 Docker 的快速启动能力，在 CoreOS集群中大规模部署 Docker容器与使用其他操作系统相比在性能上的优势将更加明显。Systemd 的另一个特点是引入了“target” 的概念，每个 target 应用于一个特定的服务，并且可以通过继承一个已有的 target扩展额外的功能，这样使得操作系统对系统上运行的服务拥有更好的控制力。 通过对系统结构的重新设计，CoreOS剔除了任何不必要的软件和服务。在一定程度上减轻了维护一个服务器集群的复杂度，帮助用户从繁琐的系统及软件维护工作中解脱出来。虽然CoreOS最初源自于Google ChromeOS，但是从一开始就决定了 CoreOS更加适合应用于一个集群环境而不是一个传统的服务器操作系统。 CoreOS相关工具 除了操作系统之外，CoreOS 团队和其他团队还提供了若干工具帮助用户管理 CoreOS 集群以及部署Docker容器。
etcd 在CoreOS 集群中处于骨架地位的是 etcd。 etcd 是一个分布式 key/value存储服务，CoreOS 集群中的程序和服务可以通过 etcd 共享信息或做服务发现 。etcd 基于非常著名的 raft一致性算法：通过选举形式在服务器之中选举 Lead 来同步数据，并以此确保集群之内信息始终一致和可用。etcd 以默认的形式安装于每个CoreOS 系统之中。在默认的配置下，etcd使用系统中的两个端口：4001和7001，其中4001提供给外部应用程序以HTTP&#43;Json的形式读写数据，而7001则用作在每个etcd 之间进行数据同步。用户更可以通过配置 CA Cert让 etcd 以 HTTPS的方式读写及同步数据，进一步确保数据信息的安全性。
fleet fleet 是一个通过Systemd对CoreOS 集群中进行控制和管理的工具。fleet 与 Systemd 之间通过 D-Bus API 进行交互，每个fleet agent 之间通过 etcd 服务来注册和同步数据。fleet提供的功能非常丰富，包括查看集群中服务器的状态、启动或终止 Docker容器、读取日志内容等。更为重要的是 fleet可以确保集群中的服务一直处于可用状态。当出现某个通过 fleet创建的服务在集群中不可用时，如由于某台主机因为硬件或网络故障从集群中脱离时，原本运行在这台服务器中的一系列服务将通过fleet被重新分配到其他可用服务器中。虽然当前 fleet 还处于非常早期的状态，但是其管理 CoreOS集群的能力是非常有效的，并且仍然有很大的扩展空间，目前已提供简单的 API 接口供用户集成。
Kubernetes Kuberenetes 是由Google 开源的一个适用于集群的Docker容器管理工具。用户可以将一组容器以 “POD” 形式通过 Kubernetes部署到集群之中。与 fleet 更加侧重 CoreOS 集群的管理不同，Kubernetes生来就是一个 容器的管理工具。Kubernetes 以“POD” 为单位管理一系列彼此联系的容器，这些容器被部署在同一台物理主机中、拥有同样地网络地址并共享存储配额。
flannel (rudder) flannel (rudder)是 CoreOS 团队针对 Kubernetes 设计的一个覆盖网络 (overlay network)工具，其目的在于帮助每一个使用 Kuberentes 的 CoreOS 主机拥有一个完整的子网。Kubernetes 会为每一个POD 分配一个独立的 IP 地址，这样便于同一个 POD 中的容器彼此连接，而之前的 CoreOS并不具备这种能力。为了解决这一问题，flannel 通过在集群中创建一个覆盖网格网络 (overlay mesh network)为主机设定一个子网。
CoreOS公司和Docker公司之争 Docker可能成为容器技术的代名词，但它不是唯一的容器技术，而且不是所有人都认为它应该成为容器技术的标准。2014年12月，CoreOS宣布推出自己的容器技术（rkt，刚开始叫rocket）和格式（appc），这个项目得到了一些主要参与者如谷歌、RedHat和VMware的支持。CoreOS公司与Docker公司决裂了。 两家公司竞争的焦点集中在容器编排平台上。编排平台提供了一套工具，用来部署和管理大量的容器，把这些容器联网，共同组成一个以容器为基础的软件基础设施。Linux 容器本身非常适合用作开发平台，如果以容器为基础构建整个软件栈，还需要提供几类编排工具，包括： - 容器调度器：把容器部署到物理服务器上； - 集群信息存储：容器数据的共享和协调； - 软件定义网络：容器联网； - 资源管理和监控等。
容器领域的所有公司好像都把编排作为自己产品与众不同的关键点，希望凭借这一点建立影响和寻求回报。除了 CoreOS 公司和 Docker公司，Red Hat 公司的 Project Atomic、Ubuntu 的 Snappy Core、Joyent 公司的 Triton和Apache Mesos 项目都是容器编排领域强有力的竞争者。尤其值得注意的是， Microsoft 公司宣布： 2016 年， WindowsServer 和 .net 用户将可以使用Windows 容器。 上个月，在Linux基金会的安排下，这两家公司将与谷歌、微软、 亚马逊等其他利益相关者携手共进，联手打造开放容器技术项目（OCP）。OCP是一个非营利性组织，其受特许建立通用的容器软件技术标准。
CoreOS试用 如果想要尝试CoreOS的话, 可以使用虚拟机进行安装实验. - 安装VirtualBox - 安装Vagrant - 运行:
 git clone https//github.com/coreos/coreos-vagrant/ cd coreos-vagrant vagrant up vagrant ssh  参考 CoreOS官网
Linux黑客车库创业：服务器操作系统CoreOS颠覆互联网
漫步云端：CoreOS实践指南（一）
CoreOS实践指南（二）：架设CoreOS集群
CoreOS实践指南（三）：系统服务管家Systemd
CoreOS实践指南（四）：集群的指挥所Fleet
CoreOS实践指南（五）：分布式数据存储Etcd（上）
CoreOS实践指南（六）：分布式数据存储Etcd（下）
CoreOS实践指南（七）：Docker容器管理服务
CoreOS实践指南（八）：Unit文件详解
CoreOS实践指南（九）：在CoreOS上的应用服务实践（上）
CoreOS实践指南（十）：在CoreOS上的应用服务实践（下）
CoreOS 实战
</content>
    </entry>
    
     <entry>
        <title>阅读《微服务资源指南》</title>
        <url>https://mryqu.github.io/post/%E9%98%85%E8%AF%BB%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%B5%84%E6%BA%90%E6%8C%87%E5%8D%97/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>微服务</tag><tag>优缺点</tag><tag>经验教训</tag><tag>指南</tag><tag>读后感</tag>
        </tags>
        <content type="html">  Martin Fowler近日发布了《Microservices Resource Guide》，介绍的是Martin自己和别人的书/文章汇集，一开头回顾了微服务的特征，接着谈到了对微服务和单块架构的权衡，最后大篇幅列举了很多公司开发微服务的经验教训。
微服务特征  通过服务进行组件化 围绕业务能力进行组织服务 团队为产品而不是项目工作 智能端点和哑管道：取其Unix思想，REST协议或消息总线仅仅是管道，服务才是智能的 去中心化治理 去中心化数据管理 基础设施自动化 故障设计：服务故障检测、自恢复 演进式设计  微服务与单块架构之间的权衡 微服务带来收益…  强模块边界: 微服务增强模块结构，这对大的团队尤为重要。  独立的部署: 简单服务更易于部署，由于它们都是自治的，服务出现问题不容易导致系统故障。  技术多样性: 使用微服务可以混合多种编程语言、开发框架和数据存储技术。  …也伴随着代价  分布: 由于远程调用慢并一直具有故障风险。很难开发分布式系统代码。  最终一致性: 对于分布式系统维护强一致性非常困难，这意味着必须管理最终一致性。  运维复杂: 需要一个成熟的运维团队来管理很多经常需要重新部署的服务。  微服务与单块架构的生产率 经验教训（略） 先看完《Build Microservices》，然后再读一遍所有列举资料，再单开新的博客帖子介绍吧。
</content>
    </entry>
    
     <entry>
        <title>Gradle Docker Plugin介绍</title>
        <url>https://mryqu.github.io/post/gradle_docker_plugin%E4%BB%8B%E7%BB%8D/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>docker</tag><tag>plugin</tag><tag>bmuschko</tag><tag>devops</tag>
        </tags>
        <content type="html">  gradle-docker-plugin gradle-docker-plugin是由《Gradle实战》作者BenjaminMuschko实现的Gradle插件，用来管理Docker镜像和容器。gradle-docker-plugin实际上包括两个插件： - com.bmuschko.docker-remote-api:提供通过远程API与Docker进行交互的定制任务- com.bmuschko.docker-java-application:为Java应用创建和上传Docker镜像build.gradle buildscript { repositories { jcenter() } dependencies { classpath &#39;com.bmuschko:gradle-docker-plugin:2.4.1&#39; } } apply plugin: &#39;java&#39; apply plugin: &#39;application&#39; apply plugin: &#39;com.bmuschko.docker-java-application&#39; apply plugin: &#39;com.bmuschko.docker-remote-api&#39;  参考 GitHub：bmuschko/gradle-docker-plugin
</content>
    </entry>
    
     <entry>
        <title>执行Gradle artifactoryPublish任务时碰到HTTP 409 Conflict错误</title>
        <url>https://mryqu.github.io/post/%E6%89%A7%E8%A1%8Cgradle_artifactorypublish%E4%BB%BB%E5%8A%A1%E6%97%B6%E7%A2%B0%E5%88%B0http_409_conflict%E9%94%99%E8%AF%AF/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>artifactorypublish</tag><tag>409</tag><tag>conflict</tag><tag>snapshot/release</tag>
        </tags>
        <content type="html"> 这篇博文算是《尝试Artifactory》的姐妹篇。我打算将《尝试Artifactory》中的&amp;rsquo;libs-snapshot-local&amp;rsquo;和&amp;rsquo;libs-snapshot&amp;rsquo;换成&amp;rsquo;libs-release-local&amp;rsquo;和&amp;rsquo;libs-release&amp;rsquo;，以便将我的构件发布到发布版仓库里。结果遭遇如下错误：
C:\test123\HelloArtifactory&amp;gt;gradlew artifactoryPublish [buildinfo] Not using buildInfo properties file for this build. :generatePomFileForMavenJavaPublication :compileJava  查看Artifactory日志，才知道根本原因在于创建的是SNAPSHOT而libs-release-local只处理发布版构建。The repository &amp;lsquo;libs-release-local&amp;rsquo; rejected the artifact&amp;rsquo;libs-release-local:com/yqu/HelloArtifactory/0.1.0-SNAPSHOT/HelloArtifactory-0.1.0-SNAPSHOT.jar&amp;rsquo;due to its snapshot/release handling policy。 解决方案有如下两种： - 修改libs-release-local属性，勾选Handle Snapshots选择框（工作流不正规啦） - 将gradle.properties中的version由0.1.0-SNAPSHOT改成0.1.0即可
</content>
    </entry>
    
     <entry>
        <title>Gradle Git Plugin介绍</title>
        <url>https://mryqu.github.io/post/gradle_git_plugin%E4%BB%8B%E7%BB%8D/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>git</tag><tag>plugin</tag><tag>ajoberstar</tag><tag>devops</tag>
        </tags>
        <content type="html">  Grgit和gradle-git Git是一个很流行的分布式版本管理工具。能在构建过程中与Git进行交互，可以提供更强大和更一致的结果。
JGit提供了与Git仓库交互的强大JavaAPI。然而，在Groovy上下本使用它会笨重，需要在所要执行的表达式包一堆换七八糟的东东。Grgit是Andre wOberstar实现的JGit封装器，为基于Groovy的工具与Git仓库交互提供了更简洁流畅的API。 gradle-git同样是由Andrew Oberstar实现的一系列Gradle插件： - org.ajoberstar.grgit - 提供一个Grgit实例，允许与Gradle项目所在的Git仓库交互 - org.ajoberstar.github-pages - 向Github仓库的gh-pages分支发布文件 - org.ajoberstar.release-base -提供用于从项目状态和所在Git仓库推断当前项目版本和创建新版本的通用结构 - org.ajoberstar.release-opinion -用于org.ajoberstar.release-base的默认选项，遵从语义版本控制（Semantic Versioning）下面是一个Gradle任务示例，用于从Git仓库克隆项目。
build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath &#39;org.ajoberstar:gradle-git:1.2.0&#39; } } import org.ajoberstar.gradle.git.tasks.* task cloneGitRepo(type: GitClone) { def destination = file(&amp;quot;destination_folder&amp;quot;) uri = &amp;quot;your_git_repo_uri&amp;quot; destinationPath = destination bare = false enabled = !destination.exists() //to clone only once }  参考 GitHub：ajoberstar/gradle-git
GitHub：ajoberstar/grgit
</content>
    </entry>
    
     <entry>
        <title>[Spring Boot] 监控和管理Spring Boot应用</title>
        <url>https://mryqu.github.io/post/spring_boot_%E7%9B%91%E6%8E%A7%E5%92%8C%E7%AE%A1%E7%90%86spring_boot%E5%BA%94%E7%94%A8/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>management</tag><tag>monitoring</tag>
        </tags>
        <content type="html">  本博文在[Spring Boot] Hello Spring LDAP 基础上稍作修改，尝试一下监控和管理Spring Boot应用。
application.properties改动 server.context-path=/HelloSpringLdapOdm server.port=8080 spring.profiles.active=test,dev # spring.dao.exceptiontranslation.enabled=false yqu.ldap.url=ldap://127.0.0.1:18880 yqu.ldap.userDN=uid=admin,ou=system yqu.ldap.password=secret yqu.ldap.base=dc=jayway,dc=se yqu.ldap.clean=true management.port=8081 management.address=127.0.0.1 endpoints.shutdown.enabled=true applicationDefaultJvmArgs: [ &amp;quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=55558&amp;quot; ]  测试  autoconfig: Displays an auto-configuration report showing allauto-configuration candidates and the reason why they ‘were’ or‘were not’ applied. beans: Displays a complete list of all the Spring beans in yourapplication. configprops: Displays a collated list of all@ConfigurationProperties. dump: Performs a thread dump. env: Exposes properties from Spring’sConfigurableEnvironment. health: Shows application health information (when theapplication is secure, a simple ‘status’ when accessed over anunauthenticated connection or full message details whenauthenticated). info: Displays arbitrary application info. mappings: Displays a collated list of all @RequestMappingpaths. metrics: Shows ‘metrics’ information for the currentapplication. trace: Displays trace information (by default the last few HTTPrequests). shutdown: Allows the application to be gracefully shutdown (notenabled by default).  参考 Spring Boot: Monitoring and management over HTTP
Spring Boot Actuator：Production-ready features
</content>
    </entry>
    
     <entry>
        <title>如何链接并执行GitHub上的JavaScript文件</title>
        <url>https://mryqu.github.io/post/%E5%A6%82%E4%BD%95%E9%93%BE%E6%8E%A5%E5%B9%B6%E6%89%A7%E8%A1%8Cgithub%E4%B8%8A%E7%9A%84javascript%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>github</tag><tag>javascript</tag><tag>execute</tag><tag>mime</tag><tag>text/plain</tag>
        </tags>
        <content type="html"> 想要玩一下jquery-mockjax，其原始文件为https://raw.githubusercontent.com/jakerella/jquery-mockjax/master/dist/jquery.mockjax.js ，加入我的html文件进行测试。结果却遇到下列问题：
Refused to execute script from ... because its MIME type (text/plain) is not executable, and strict MIME type checking is enabled.  查到了StackOverflow上的一个帖子Link and execute external JavaScript file hosted on GitHub ，原来GitHub开始使用X-Content-Type-Options:nosniff以令更多的现代浏览器执行严格MIME类型检查，之后返回原始文件的MIME类型故意让浏览器不能使用。帖子中提到的临时解决方法是将raw.githubusercontent.com替换为rawgit.com。我将上一链接替换成https://rawgit.com/jakerella/jquery-mockjax/master/dist/jquery.mockjax.js ，解决问题！
</content>
    </entry>
    
     <entry>
        <title>[Spring Boot] Hello CommandLineRunner</title>
        <url>https://mryqu.github.io/post/spring_boot_hello_commandlinerunner/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>commandlinerunner</tag><tag>order</tag><tag>dependson</tag>
        </tags>
        <content type="html">  通过CommandLineRunner，可在所有Spring Bean和ApplicationContext被创建后执行一些可以访问命令行参数的任务。如想指定多个CommandLineRunnerBean的执行顺序，可以实现org.springframework.core.Ordered接口或添加org.springframework.core.annotation.Order注解。
示例代码 Application.java package com.yqu.cmdlinerunner; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.boot.Banner; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.DependsOn; import org.springframework.core.annotation.Order; import org.springframework.core.annotation.OrderUtils; import java.util.Arrays; @SpringBootApplication public class Application { private static final Logger log = LoggerFactory.getLogger(Application.class); public static void main(String[] args) { SpringApplication app = new SpringApplication(Application.class); app.setWebEnvironment(false); app.setShowBanner(false); app.run(args); } @Bean(name=&amp;quot;demo1&amp;quot;) @DependsOn(&amp;quot;demo2&amp;quot;) @Order(8) public CommandLineRunner demo1() { return (args) -&amp;gt; { log.info(&amp;quot;demo1:order=&amp;quot;&#43; OrderUtils.getOrder(this.getClass())&#43; &amp;quot;:args=&amp;quot;&#43;Arrays.toString(args)); //log.info(getStacks()); }; } @Bean(name=&amp;quot;demo2&amp;quot;) @Order(1) public CommandLineRunner demo2() { return (args) -&amp;gt; { log.info(&amp;quot;demo2:order=&amp;quot;&#43; OrderUtils.getOrder(this.getClass())&#43; &amp;quot;:args=&amp;quot;&#43; Arrays.toString(args)); }; } private String getStacks() { StringBuilder sb = new StringBuilder(); StackTraceElement[] elements = Thread.currentThread().getStackTrace(); sb.append(&amp;quot;========================\n&amp;quot;); for (int i = 0; i &amp;lt; elements.length; i&#43;&#43;) { sb.append(elements[i]).append(&amp;quot;\n&amp;quot;); } return sb.toString(); } }  MyCmdLineRunner1.java package com.yqu.cmdlinerunner; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.boot.CommandLineRunner; import org.springframework.core.annotation.Order; import org.springframework.core.annotation.OrderUtils; import org.springframework.stereotype.Component; import java.util.Arrays; @Component @Order(6) public class MyCmdLineRunner1 implements CommandLineRunner { private static final Logger log = LoggerFactory.getLogger(MyCmdLineRunner1.class); private String getStacks() { StringBuilder sb = new StringBuilder(); StackTraceElement[] elements = Thread.currentThread().getStackTrace(); sb.append(&amp;quot;========================\n&amp;quot;); for (int i = 0; i &amp;lt; elements.length; i&#43;&#43;) { sb.append(elements[i]).append(&amp;quot;\n&amp;quot;); } return sb.toString(); } public void run(String... args) { log.info(&amp;quot;MyCmdLineRunner1:order=&amp;quot;&#43; OrderUtils.getOrder(this.getClass())&#43; &amp;quot;:args=&amp;quot;&#43; Arrays.toString(args)); //log.info(getStacks()); } }  MyCmdLineRunner2.java package com.yqu.cmdlinerunner; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.boot.CommandLineRunner; import org.springframework.core.annotation.Order; import org.springframework.core.annotation.OrderUtils; import org.springframework.stereotype.Component; import java.util.Arrays; @Component @Order(5) public class MyCmdLineRunner2 implements CommandLineRunner { private static final Logger log = LoggerFactory.getLogger(MyCmdLineRunner2.class); public void run(String... args) { log.info(&amp;quot;MyCmdLineRunner2::order=&amp;quot;&#43; OrderUtils.getOrder(this.getClass())&#43; &amp;quot;:args=&amp;quot;&#43; Arrays.toString(args)); } }  build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.6.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;hello-springboot-commandlinerunner&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  测试 java -jar build/libs/hello-commandlinerunner-0.1.0.jar.jar --spring.yquopt1=k --spring.yquopt2=x  结果如下： CommandLineRunner分析 通过如下调用堆栈可知，CommandLineRunnerBean都是由org.springframework.boot.SpringApplication.runCommandLineRunners调用执行的。
org.springframework.boot.SpringApplication.runCommandLineRunners(SpringApplication.java:673) org.springframework.boot.SpringApplication.afterRefresh(SpringApplication.java:691) org.springframework.boot.SpringApplication.run(SpringApplication.java:322) com.yqu.cmdlinerunner.Application.main(Application.java:25)  org.springframework.core.annotation.AnnotationAwareOrderComparator负责对CommandLineRunnerBean进行排序。排序规则为：
 如果有一方是org.springframework.core.PriorityOrdered接口实现，而另一方不是，则PriorityOrdered接口实现一方获胜； 检查org.springframework.core.Ordered接口或org.springframework.core.annotation.Order注解获得order，值小者胜； 其他没有order的则置为Ordered.LOWEST_PRECEDENCE，顺序不定。  在上述测试中，MyCmdLineRunner2的order为5，MyCmdLineRunner1的order为6，因此MyCmdLineRunner2在MyCmdLineRunner1之前执行。 Application的demo1和demo2方法设置了@order注解，但是调试可知lamda表达式生成类并没有@order注解信息，因此执行顺序排在后面。这是需要注意的地方。 此外，Bean初始化顺序跟CommandLineRunner执行顺序也没有关系。
参考 Spring Boot Reference Guide
</content>
    </entry>
    
     <entry>
        <title>[Spring Boot] Hello Spring LDAP</title>
        <url>https://mryqu.github.io/post/spring_boot_hello_spring_ldap/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>ldap</tag><tag>odm</tag><tag>boot</tag><tag>apacheds</tag>
        </tags>
        <content type="html">  这个帖子设定了标题后，一直忙于其他事情，拖延了两个月终于能够结贴了。
部分示例代码 LdapConfiugration.java package com.yqu.ldap.odm; import com.yqu.ldap.odm.dao.*; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.*; import org.springframework.core.env.Environment; import org.springframework.ldap.core.LdapTemplate; import org.springframework.ldap.core.support.LdapContextSource; import javax.annotation.PostConstruct; @Configuration @ComponentScan(basePackages={&amp;quot;com.yqu.ldap.odm&amp;quot;}) public class LdapConfiugration { @Autowired private Environment _environment; private static Log log = LogFactory.getLog(LdapConfiugration.class); @PostConstruct private void init() { log.debug(&amp;quot;environment: yqu.ldap.url:&amp;quot; &#43; _environment.getProperty(&amp;quot;yqu.ldap.url&amp;quot;)); log.debug(&amp;quot;environment: yqu.ldap.userDN:&amp;quot; &#43; _environment.getProperty(&amp;quot;yqu.ldap.userDN&amp;quot;)); log.debug(&amp;quot;environment: yqu.ldap.password:&amp;quot; &#43; _environment.getProperty(&amp;quot;yqu.ldap.password&amp;quot;)); log.debug(&amp;quot;environment: yqu.ldap.base:&amp;quot; &#43; _environment.getProperty(&amp;quot;yqu.ldap.base&amp;quot;)); } @Bean(name=&amp;quot;ldapContextSource&amp;quot;) public LdapContextSource ldapContextSource() { String url = _environment.getProperty(&amp;quot;yqu.ldap.url&amp;quot;); String user = _environment.getProperty(&amp;quot;yqu.ldap.userDN&amp;quot;); String password = _environment.getProperty(&amp;quot;yqu.ldap.password&amp;quot;); String base = _environment.getProperty(&amp;quot;yqu.ldap.base&amp;quot;); if (url == null || user == null || password == null) { log.error(&amp;quot;Missing properties for contextSource.&amp;quot;); } LdapContextSource context = new LdapContextSource(); context.setUrl(url); context.setUserDn(user); context.setPassword(password); context.setBase(base); return context; } @Bean(name=&amp;quot;ldapTemplate&amp;quot;) @DependsOn(&amp;quot;ldapContextSource&amp;quot;) public LdapTemplate ldapTemplate() { LdapTemplate ldapTemplate = new LdapTemplate(); ldapTemplate.setContextSource(ldapContextSource()); return ldapTemplate; } @Bean(name=&amp;quot;personDao&amp;quot;) @DependsOn(&amp;quot;ldapTemplate&amp;quot;) public PersonDao personDao() { OdmPersonDaoImpl personDao = new OdmPersonDaoImpl(); personDao.setLdapTemplate(ldapTemplate()); return personDao; } }  LdapTestConfig.java package com.yqu.ldap.odm; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.DependsOn; import org.springframework.context.annotation.Profile; import org.springframework.core.io.ClassPathResource; import org.springframework.ldap.core.ContextSource; import org.springframework.ldap.test.EmbeddedLdapServerFactoryBean; import org.springframework.ldap.test.LdifPopulator; @Profile(&amp;quot;test&amp;quot;) @Configuration @ComponentScan(basePackages={&amp;quot;com.yqu.ldap.odm&amp;quot;}) public class LdapTestConfig { @Bean @DependsOn(&amp;quot;embeddedLdapServer&amp;quot;) public LdifPopulator ldifPopulator(ContextSource contextSource) { LdifPopulator ldifPopulator = new LdifPopulator(); ldifPopulator.setContextSource(contextSource); ldifPopulator.setResource(new ClassPathResource(&amp;quot;setup_data.ldif&amp;quot;)); ldifPopulator.setBase(&amp;quot;dc=jayway,dc=se&amp;quot;); ldifPopulator.setClean(true); ldifPopulator.setDefaultBase(&amp;quot;dc=jayway,dc=se&amp;quot;); return ldifPopulator; } @Bean(name = &amp;quot;embeddedLdapServer&amp;quot;) public EmbeddedLdapServerFactoryBean embeddedLdapServerFactoryBean() { EmbeddedLdapServerFactoryBean embeddedLdapServerFactoryBean = new EmbeddedLdapServerFactoryBean(); embeddedLdapServerFactoryBean.setPartitionName(&amp;quot;example&amp;quot;); embeddedLdapServerFactoryBean.setPartitionSuffix(&amp;quot;dc=jayway,dc=se&amp;quot;); embeddedLdapServerFactoryBean.setPort(18880); return embeddedLdapServerFactoryBean; } }  application.properties server.context-path=/HelloSpringLdapOdm server.port=8080 spring.profiles.active=test,dev # spring.dao.exceptiontranslation.enabled=false yqu.ldap.url=ldap://127.0.0.1:18880 yqu.ldap.userDN=uid=admin,ou=system yqu.ldap.password=secret yqu.ldap.base=dc=jayway,dc=se yqu.ldap.clean=true applicationDefaultJvmArgs: [ &amp;quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=55558&amp;quot; ]  setup_data.ldif dn: c=Sweden,dc=jayway,dc=se objectclass: top objectclass: country c: Sweden description: The country of Sweden dn: ou=company1,c=Sweden,dc=jayway,dc=se objectclass: top objectclass: organizationalUnit ou: company1 description: First company in Sweden dn: cn=Some Person,ou=company1,c=Sweden,dc=jayway,dc=se objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson uid: some.person userPassword: password cn: Some Person sn: Person description: Sweden, Company1, Some Person telephoneNumber: &#43;46 555-123456 dn: cn=Some Person2,ou=company1,c=Sweden,dc=jayway,dc=se objectclass: top objectclass: person objectclass: organizationalPerson objectclass: inetOrgPerson uid: some.person2 userPassword: password cn: Some Person2 sn: Person2 description: Sweden, Company1, Some Person2 telephoneNumber: &#43;46 555-654321  build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.6.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;hello-springldapodm&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) compile(&amp;quot;org.springframework:spring-tx&amp;quot;) compile(&amp;quot;org.springframework.ldap:spring-ldap-core:2.0.3.RELEASE&amp;quot;) compile(&amp;quot;org.springframework.ldap:spring-ldap-test:2.0.3.RELEASE&amp;quot;) compile(&amp;quot;org.springframework.data:spring-data-commons&amp;quot;) compile(&amp;quot;com.fasterxml.jackson.core:jackson-databind:2.6.0&amp;quot;) compile(&amp;quot;org.springframework.hateoas:spring-hateoas&amp;quot;) compile(&amp;quot;org.springframework.plugin:spring-plugin-core:1.1.0.RELEASE&amp;quot;) compile(&amp;quot;com.jayway.jsonpath:json-path:0.9.1&amp;quot;) compile(&amp;quot;org.apache.commons:commons-lang3:3.4&amp;quot;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  测试 列举所有用户 查询某一用户 添加新用户 更新某一用户 再次列举所有用户 参考 Spring LDAP Reference
GitHub：spring-projects/spring-ldap
spring-ldap samples
Spring Guide：Authenticating a User with LDAP
LDAP Parameters
ApacheDS
</content>
    </entry>
    
     <entry>
        <title>apt-get在基于Ubuntu基础镜像Dockerfile中的常见用法</title>
        <url>https://mryqu.github.io/post/apt-get%E5%9C%A8%E5%9F%BA%E4%BA%8Eubuntu%E5%9F%BA%E7%A1%80%E9%95%9C%E5%83%8Fdockerfile%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>dockerfile</tag><tag>ubuntu</tag><tag>apt-get</tag><tag>debian_frontend</tag><tag>devops</tag>
        </tags>
        <content type="html">  首先，在Ubuntu的Docker官方镜像中是没有缓存Apt的软件包列表的。因此在做其他任何基础软件的安装前，都需要至少先做一次apt-get update。 有时为了加快apt-get安装软件的速度，还需要修改Apt源的列表文件/etc/apt/sources.list。相应的操作用命令表示如下：
# 使用Ubuntu官方的Apt源，也可以根据实际需要修改为国内源的地址 echo &amp;quot;deb http://archive.ubuntu.com/ubuntu trusty main universe\n&amp;quot; &amp;gt; /etc/apt/sources.list echo &amp;quot;deb http://archive.ubuntu.com/ubuntu trusty-updates main universe\n&amp;quot; &amp;gt;&amp;gt; /etc/apt/sources.list  在容器构建时，为了避免使用apt-get install安装基础软件的过程中需要进行的交互操作，使用-y参数来避免安装非必须的文件，从而减小镜像的体积。
apt-get -y --no-install-recommends install  使用apt-get autoremove命令移除为了满足包依赖而安装的、但不再需要的包；使用apt-get clean命令清除所获得包文件的本地仓库。 DEBIAN_FRONTEND这个环境变量，告知操作系统应该从哪儿获得用户输入。如果设置为&amp;rdquo;noninteractive&amp;rdquo;，你就可以直接运行命令，而无需向用户请求输入（所有操作都是非交互式的）。这在运行apt-get命令的时候格外有用，因为它会不停的提示用户进行到了哪步并且需要不断确认。非交互模式会选择默认的选项并以最快的速度完成构建。请确保只在Dockerfile中调用的RUN命令中设置了该选项，而不是使用ENV命令进行全局的设置。因为ENV命令在整个容器运行过程中都会生效，所以当你通过BASH和容器进行交互时，如果进行了全局设置那就会出问题。
# 正确的做法 - 只为这个命令设置ENV变量 RUN DEBIAN_FRONTEND=noninteractive apt-get install -y python3 # 错误地做法 - 为接下来的任何命令都设置ENV变量，包括正在运行地容器 ENV DEBIAN_FRONTEND noninteractive RUN apt-get install -y python3  我的示例如下：
FROM ubuntu:trusty MAINTAINER mryqu RUN \ DEBIAN_FRONTEND=noninteractive apt-get update &amp;amp;&amp;amp; \ DEBIAN_FRONTEND=noninteractive apt-get -y install wget curl &amp;amp;&amp;amp; \ DEBIAN_FRONTEND=noninteractive apt-get -y autoremove &amp;amp;&amp;amp; \ DEBIAN_FRONTEND=noninteractive apt-get clean  参考 Ubuntu manuals: apt-get man page
使用Docker部署Python应用的一些最佳实践
docker部署OpenStack API
</content>
    </entry>
    
     <entry>
        <title>Docker的镜像存储在哪里和长什么样子</title>
        <url>https://mryqu.github.io/post/docker%E7%9A%84%E9%95%9C%E5%83%8F%E5%AD%98%E5%82%A8%E5%9C%A8%E5%93%AA%E9%87%8C%E5%92%8C%E9%95%BF%E4%BB%80%E4%B9%88%E6%A0%B7%E5%AD%90/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>docker</tag><tag>image</tag><tag>存储位置</tag><tag>存储方式</tag><tag>devops</tag>
        </tags>
        <content type="html">  接触docker后，我就有个疑问：我们用docker pull镜像后，该镜像是存储在哪里的？是以一个特俗的二进制类型存储的么？后来阅读了Docker的镜像存储在哪里这篇博文，得以解惑，并进行了验证。 Docker的镜像存储在/var/lib/docker目录下，存储方式有点像Git那样有reference和实际的objects，并且是实际内容是diff那样的增量存放。 Docker的镜像存储在哪里 有个疑问就是我们用docker pull镜像后，该镜像是存储在哪里的？ 当你仅仅是使用docker启动一个实例的时候，是超级简单的，但是当你制作自己的Dockerfile时，可能会有一些迷惑，那就是我的docker镜像存储在哪里了。这个听起来让我感觉有点一筹莫展，对于dockerimage的存储我还是一无所知。最后你只能把镜像发布到公共DockerIndex上面，但是，在过去一段时间内你是无法删除它的，但是现在你可以通过官方的WEB界面来删除它了。
Image VS Dockerfile 这个看起来有点混淆，但是它们是有差别的，docke使用images运行你的代码，而不是Dockerfile。Dockerfile是你用dockerbuild命令来构建image的。如果你在浏览器中浏览DockerIndex，你会发现有很多images显示在上面，但是你不能看见构建它们的Dockerfile。当你使用dockerpush命令发布image时，它不会发布你的源代码，它只会发布从你源代码构建出来的镜像。
Registry VS Index 下一个混淆的是Registry和Index，它们是怎么样区分的？index是管理公共web接口上的accounts、permission、search、tagging和所有精细的方面的。而registry是存储和提供实际image的，它委托index进行身份验证。当你运行dockersearch命令的时候，它搜索的是index，而不是registry。实际上，它可能搜索的是index知道的多个registry。当你运行dockerpush或者dockerpull命令时，index决定你是否有权限访问和修改images，当index同意你操作后，registry是提供images存储的地方，index会计算出哪个registry有你需要的镜像，并把请求转发过去。当你在本地运行dockerimages命令时，你可能是同时和index和registry进行交互。
Repository docker&amp;rsquo;s使用镜像就像使用GitHub一样容易，但是有三个混淆的地方： - repository和registry之间的区别 - repository和image之间的区别 - repository和index username之间的区别
其实repository并不是其中任何一个组件，而是指所有的组件。当你运行dockerimages命令时，你会看到如下： images列表看起来像repositories列表？实际上images是GUIDs，但这并不是如何和他们交互。当你执行dockerbuild或者dockercommit命令时，你可以指定image的名称，这个名称的格式是username/image_name，但这并不一定，它可以是任何形式的，他甚至可以是你已经知道的发布的镜像名称。当你执行dockerpush的时候，index会查看镜像名称，检查该镜像是否在repository中，如果在，接着检查你是否有权限访问该repository，如果有权限，则允许你push新版本的image到该repository上。因此，一个registry保留了它收集到的repository的名称，它本身跟踪收集到的images的GUIDs。thisis also where tags comein，你可以tag一个image，并且存储多个版本使用不同的GUIDs在同一个repository中，访问不同的标记的版本image，可以使用username/image_name:tag。 从上图中你可以看到我们有三个不同版本的image叫ubuntu12，每个的tag都是不同的，repository使用ubuntu12的名称来保存这些，因此，当我们看到ubuntu12的时候，它像一个image名称，但是实际上它使repository名称，repository名称有特殊的设计架构，index可以从第一部分解析出username，并且找出他在哪里。因此，当出现一个guol/ubuntu时会产生混淆，官方的repository名称是类似username/image_name这样的，我们想当然的认为repository名称是image_name，但是根据docker的文档发现repository的名称有时指的是全部的名称，有时指的是image_name。比如就像ubuntu，它就没有username，是不是有点乱了&amp;hellip;&amp;hellip;&amp;hellip;
Local Storage on the Docker Host 我们已经了解完如何和远程存储进行交互了，但是当你运行dockerimages的时候，仅仅给你看到的是你的机器上有哪些image。这些镜像在哪里呢？第一个要查看的地方是/var/lib/docker/。 查看repositories-aufs文件的内容，它的内容是在你本机上的repositories。 看看，它完全匹配了docker images的输出内容。 现在我们来看看/var/lib/docker/graph/的内容。 我倒，显示的非常不友好啊，看看docker是怎样跟踪这些镜像的，是基于repositories-aufs文件，构建了一个映射到repository名称和tag的关系表。我们看看ubuntu12的仓库，它有三个镜像，标记分别是12.04、precise、latest。采用的是IDd431f556799d35dfae1278a1ee41a393db70058dedb9a7fc554b0506b5b241cb，我们看看这个目录里面有什么。 只有两个文件： - json：保存image的metadata - layersize：只是一个数字，表明layer的大小
主要镜像的差异在/var/lib/docker/aufs/diff/目录下，每次都会把镜像改变的部分存储在该目录下的相关ID目录里面。 参考：Where are Docker images stored?
</content>
    </entry>
    
     <entry>
        <title>断路器（CircuitBreaker）设计模式</title>
        <url>https://mryqu.github.io/post/%E6%96%AD%E8%B7%AF%E5%99%A8circuitbreaker%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>设计模式</tag><tag>断路器</tag><tag>circuitbreaker</tag><tag>云</tag><tag>微服务</tag>
        </tags>
        <content type="html">  断路器是电器时代的一个重要组成部分，后面总是有保险丝熔断或跳闸的断路器是安全的重要保障。 微服务最近几年成为软件架构的热门话题，其益处多多。但需要知道的是，一旦开始将单块系统进行分解，就上了分布式系统的山头。 在云或分布式系统环境中，任何对一致性或可靠性的表述就是谎言。我们必须假设微服务的行为或其服务器位置会经常变动，其结果就是组件有时会提供低质量服务甚至可能彻底无法提供服务。这些微服务的故障如果没有处理好，将导致整个系统的故障。 微服务的故障可能是瞬时故障：如慢的网络连接、超时，资源过度使用而暂时不可用；也可能是不容易预见的突发事件的情况下需要更长时间来纠正的故障。 分布式服务的容错是一个不得不考虑的问题，通常的做法有两种： - 重试机制：对于预期的短暂故障问题，通过重试模式是可以解决的。 - 断路器（CircuitBreaker）模式：将受保护的服务封装在一个可以监控故障的断路器对象中，当故障达到一定门限，断路器将跳闸（trip），所有后继调用将不会发往受保护的服务而由断路器对象之间返回错误。对于需要更长时间解决的故障问题，不断重试就没有太大意义了，可以使用断路器模式。 断路器模式设计状态机 注意事项 在决定如何实现这个模式时，您应考虑以下几点： - 异常处理。通过断路器调用操作的应用程序必须能够处理在操作不可用时可能被抛出的异常，该类异常的处理方式都是应用程序特有的。例如，应用程序会暂时降级其功能，调用备选操作尝试相同的任务或获取相同的数据，或者将异常通知给用户让其稍后重试。 - 异常类型。一个请求可能由于各种原因失败，其中有一些可能表明故障严重类型高于其他故障。例如，一个请求可能由于需要几分钟才能恢复的远程服务崩溃而失败，也可能由于服务暂时超载造成的超时而失败。断路器有可能可以检查发生的异常类型，并根据这些异常本质调整策略。例如，促使切换到开状态（跳闸）的服务超时异常个数要远多于服务完全不可用导致的故障个数。 - 日志记录。一个断路器应记录所有失败的请求（如果可能的话记录所有请求），以使管理员能够监视它封装下受保护操作的运行状态。 - 可恢复性。应该配置断路器成与受保护操作最匹配的恢复模式。例如，如果断路器设定出入开状态的时间很长，即使底层操作故障已经解决它还会返回错误。如果开状态到半开状态切换过快，底层操作故障还没解决它就会再次调用受保护操作。 - 测试失败的操作。在开状态下，断路器可能不用计时器来确定何时切换到半开状态，而是通过周期性地查验远程服务或资源以确定它是否已经再次可用。这个检查可能采用上次失败的操作的形式，也可以使用由远程服务提供的专门用于测试服务健康状况的特殊操作。 - 手动复位。在一个系统中，如果一个失败的操作的恢复时间差异很大，提供一个手动复位选项以使管理员能够强行关闭断路器（和复位故障计数器）可能是有益的。同样，如果受保护操作暂时不可用，管理员可以强制断路器进入放状态（并重新启动超时定时器）。 - 并发。同一断路器可以被应用程序的大量并发实例访问。断路器实现不应阻塞并发请求或对每一请求增加额外开销。 - 资源分化。当断路器使用某类可能有多个底层独立数据提供者的资源时需要特别小心。例如，一个数据存储包含多个分区(shard)，部分分区出现暂时的问题，其他分区可能完全工作正常。如果该场景中的错误响应是合并响应，应用程序在部分故障分区很可能会阻塞整个请求时仍会试图访问某些工作正常的分区。 - 加速断路。有时失败响应对于断路器实现来说包含足够的信息用于判定应当立即跳闸并保持最小时间量的跳闸状态。例如，从过载共享资源的错误响应可能指示不推荐立即重试，且应用程序应当隔几分钟时间之后重试。如果一个请求的服务对于特定Web服务器不可用，可以返回HTTP协议定义的“HTTP 503 ServiceUnavailable”响应。该响应可以包含额外的信息，例如预期延迟持续时间。 - 重试失败请求。在开状态下，断路器可以不是快速地简单返回失败，而是将每个请求的详细信息记录日志并在远程资源或服务重新可用时安排重试。 - 对外部服务的不恰当超时。当对外部服务配置的超时很大时，断路器可能无法保护其故障操作，断路器内的线程在指示操作失败之前仍将阻塞到外部服务上，同时很多其他应用实例仍会视图通过断路器调用服务。
断路器模式业界Java实现 GitHub：jrugged：CircuitBreaker类源代码
GitHub：Netflix/hystrix
参考 Martin Fowler：CircuitBreaker
MSDN：Circuit Breaker Pattern
Protect your software with the Circuit Breaker design pattern
[](http://wiki.jikexueyuan.com/project/cloud-design-patterns/circuit-breaker-model.html)
</content>
    </entry>
    
     <entry>
        <title>聊聊mavenCenter和JCenter</title>
        <url>https://mryqu.github.io/post/%E8%81%8A%E8%81%8Amavencenter%E5%92%8Cjcenter/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>mavencenter</tag><tag>jcenter</tag><tag>中央仓库</tag>
        </tags>
        <content type="html"> Gradle支持从maven中央仓库和JCenter上获取构件，那这两者有什么区别呢？
maven中央仓库（http://repo1.maven.org/maven2/）是由Sonatype公司提供的服务，它是ApacheMaven、SBT和其他构建系统的默认仓库，并能很容易被ApacheAnt/Ivy、Gradle和其他工具所使用。开源组织例如Apache软件基金会、Eclipse基金会、JBoss和很多个人开源项目都将构件发布到中央仓库。maven中央仓库已经将内容浏览功能禁掉了，可在http://search.maven.org/查询构件。
https://jcenter.bintray.com）是由JFrog公司提供的Bintray中的Java仓库。它是当前世界上最大的Java和Android开源软件构件仓库。所有内容都通过内容分发网络（CDN）使用加密https连接获取。JCenter是Goovy Grape内的默认仓库，Gradle内建支持（jcenter()仓库），非常易于在（可能除了Maven之外的）其他构建工具内进行配置。
JCenter相比mavenCenter构件更多，性能也更好。但还是有些构件仅存在mavenCenter中。
</content>
    </entry>
    
     <entry>
        <title>ICU4J介绍</title>
        <url>https://mryqu.github.io/post/icu4j%E4%BB%8B%E7%BB%8D/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>icu</tag><tag>icu4j</tag><tag>unicode</tag>
        </tags>
        <content type="html">  ICU (International Components for Unicode)是为软件应用提供Unicode和全球化支持的一套成熟、广泛使用的C/C&#43;&#43;和Java类库集，可在所有平台的C/C&#43;&#43;和Java软件上获得一致的结果。 ICU首先是由Taligent公司开发的，Taligent公司被合并为IBM公司全球化认证中心的Unicode研究组后，ICU由IBM和开源组织合作继续开发。开始ICU只有Java平台的版本，后来这个平台下的ICU类被吸纳入SUN公司开发的JDK1.1，并在JDK以后的版本中不断改进。 C&#43;&#43;和C平台下的ICU是由JAVA平台下的ICU移植过来的，移植过的版本被称为ICU4C，来支持这C/C&#43;&#43;两个平台下的国际化应用。ICU4J和ICU4C区别不大，但由于ICU4C是开源的，并且紧密跟进Unicode标准，ICU4C支持的Unicode标准总是最新的；同时，因为JAVA平台的ICU4J的发布需要和JDK绑定，ICU4C支持Unicode标准改变的速度要比ICU4J快的多。 ICU的功能主要有: - 代码页转换:对文本数据进行Unicode、几乎任何其他字符集或编码的相互转换。ICU的转化表基于IBM过去几十年收集的字符集数据，在世界各地都是最完整的。 - 排序规则（Collation）:根据特定语言、区域或国家的管理和标准比较字数串。ICU的排序规则基于Unicode排序规则算法加上来自公共区域性数据仓库（Commonlocale data repository）的区域特定比较规则。 - 格式化:根据所选区域设置的惯例，实现对数字、货币、时间、日期、和利率的格式化。包括将月和日名称转换成所选语言、选择适当缩写、正确对字段进行排序等。这些数据也取自公共区域性数据仓库。 - 时间计算: 在传统格里历基础上提供多种历法。提供一整套时区计算API。 - Unicode支持:ICU紧密跟进Unicode标准，通过它可以很容易地访问Unicode标准制定的很多Unicode字符属性、Unicode规范化、大小写转换和其他基础操作。 - 正则表达式: ICU的正则表达式全面支持Unicode并且性能极具竞争力。 - Bidi: 支持不同文字书写顺序混合文字（例如从左到右书写的英语，或者从右到左书写的阿拉伯文和希伯来文）的处理。 - 文本边界: 在一段文本内定位词、句或段落位置、或标识最适合显示文本的自动换行位置。
下面的示例是使用ICU4J检测文本编码：
package com.yqu.icu4j; import java.io.IOException; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; import com.ibm.icu.text.CharsetDetector; import com.ibm.icu.text.CharsetMatch; public class EncodingDetector { public static void tryEncoding(String fileName) throws IOException { System.out.println(&amp;quot;===Getting encoding of &amp;quot; &#43; fileName); Path path = Paths.get(fileName); byte[] data = Files.readAllBytes(path); CharsetDetector detector = new CharsetDetector(); detector.setText(data); CharsetMatch match = detector.detect(); String encoding = match.getName(); System.out.println(&amp;quot;The Content in &amp;quot; &#43; encoding); CharsetMatch[] matches = detector.detectAll(); System.out.println(&amp;quot;All possibilities:&amp;quot;); for (CharsetMatch m : matches) { System.out.println(&amp;quot; CharsetName:&amp;quot; &#43; m.getName() &#43; &amp;quot; Confidence:&amp;quot; &#43; m.getConfidence()); } } public static void main(String[] args) { try { tryEncoding(&amp;quot;c:/utf8.txt&amp;quot;); tryEncoding(&amp;quot;c:/gbk.txt&amp;quot;); } catch (Exception e) { e.printStackTrace(); } } }  测试的是内容如下、但编码分别为uft8和GBK的两个文件：
GB18030编码向下兼容GBK和GB2312，兼容的含义是不仅字符兼容，而且相同字符的编码也相同。GB18030收录了所有Unicode3.1中的字符，包括中国少数民族字符，GBK不支持的韩文字符等等，也可以说是世界大多民族的文字符号都被收录在内。  结果为：
===Getting encoding of c:/utf8.txt The Content in UTF-8 All possibilities: CharsetName:UTF-8 Confidence:100 CharsetName:windows-1252 Confidence:40 CharsetName:windows-1253 Confidence:21 CharsetName:Big5 Confidence:10 CharsetName:GB18030 Confidence:10 CharsetName:Shift_JIS Confidence:10 CharsetName:UTF-16LE Confidence:10 CharsetName:UTF-16BE Confidence:10 CharsetName:KOI8-R Confidence:2 CharsetName:windows-1255 Confidence:2 CharsetName:windows-1250 Confidence:2 CharsetName:windows-1254 Confidence:1 CharsetName:windows-1255 Confidence:1 ===Getting encoding of c:/gbk.txt The Content in GB18030 All possibilities: CharsetName:GB18030 Confidence:100 CharsetName:EUC-KR Confidence:41 CharsetName:EUC-JP Confidence:41 CharsetName:Big5 Confidence:10 CharsetName:Shift_JIS Confidence:10 CharsetName:UTF-16LE Confidence:10 CharsetName:UTF-16BE Confidence:10 CharsetName:ISO-8859-7 Confidence:3 CharsetName:ISO-8859-6 Confidence:3 CharsetName:ISO-8859-1 Confidence:3 CharsetName:ISO-8859-9 Confidence:1 CharsetName:ISO-8859-2 Confidence:1  此外，由于GB18030和Big5编码有交集，在尝试的过程中发现测试内容很少时可能会发生误判。
参考 ICU项目主页
</content>
    </entry>
    
     <entry>
        <title>Java解析YAML</title>
        <url>https://mryqu.github.io/post/java%E8%A7%A3%E6%9E%90yaml/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>yaml</tag><tag>java</tag><tag>snakeyaml</tag><tag>yamlbeans</tag><tag>jackson</tag>
        </tags>
        <content type="html">  继前博文YAML介绍了YAML语法，本文将着重研究Java解析YAML。当前还在维护的YAML解析器/生成器有： - SnakeYAML - 完整的YAML 1.1解析器，尤其是SnakeYAML能够分析来自于规范的所有示例 - 支持Unicode，包括UTF-8/UTF-16的输入/输出 - 为序列化和反序列化本地的Java对象提供了高级API - 支持YAML类型库中的所有类型 - 比较理性的错误信息 - YamlBeans：支持YAML 1.0和1.1 - FasterXML/jackson-dataformat-yaml：处于原型阶段
SnakeYAML和YamlBeans都在GoogleCode仓库时，SnakeYAML的使用人数和提交者均优于YamlBeans。目前大多数帖子还是推荐选用SnakeYAML，而SpringBoot读取YAML配置采用的就是SnakeYAML。为了测试SnakeYAML，我首先创建了一个HelloSnakeYAML项目。
conf.yaml spring: application: name: cruncher datasource: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://localhost/test server: port: 9000  Contact.java package com.yqu.yaml; import java.util.List; public class Contact { private String name; private int age; private List phoneNumbers; public Contact(String name, int age, List phoneNumbers) { this.name = name; this.age = age; this.phoneNumbers = phoneNumbers; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public List getPhoneNumbers() { return phoneNumbers; } public void setPhoneNumbers(List phoneNumbers) { this.phoneNumbers = phoneNumbers; } }  Phone.java package com.yqu.yaml; public class Phone { private String name; private String number; public Phone(String name, String number) { this.name = name; this.number = number; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getNumber() { return number; } public void setNumber(String number) { this.number = number; } }  HelloSnakeYAML.java package com.yqu.yaml; import org.yaml.snakeyaml.Yaml; import java.io.FileInputStream; import java.io.FileWriter; import java.net.URL; import java.util.Arrays; import java.util.List; public class HelloSnakeYAML { public static void test1() { try { Yaml yaml = new Yaml(); URL url = HelloSnakeYAML.class.getClassLoader().getResource(&amp;quot;conf.yaml&amp;quot;); if (url != null) { Object obj = yaml.load(new FileInputStream(url.getFile())); System.out.println(obj); } } catch (Exception e) { e.printStackTrace(); } } public static void test2() { try { Contact c1 = new Contact(&amp;quot;test1&amp;quot;, 1, Arrays.asList( new Phone(&amp;quot;home&amp;quot;, &amp;quot;1111&amp;quot;), new Phone(&amp;quot;work&amp;quot;, &amp;quot;2222&amp;quot;))); Contact c2 = new Contact(&amp;quot;test2&amp;quot;, 23, Arrays.asList( new Phone(&amp;quot;home&amp;quot;, &amp;quot;1234&amp;quot;), new Phone(&amp;quot;work&amp;quot;, &amp;quot;4321&amp;quot;))); List contacts = Arrays.asList(c1, c2); Yaml yaml = new Yaml(); yaml.dump(contacts, new FileWriter(&amp;quot;contact.yaml&amp;quot;)); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { test1(); test2(); } }  build.gradle buildscript { repositories { mavenCentral() maven { url &amp;quot;http://repo.spring.io/libs-milestone&amp;quot; } } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; jar { baseName = &#39;HelloSnakeYAML&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() maven { url &amp;quot;http://repo.spring.io/libs-milestone&amp;quot; } } sourceCompatibility = 1.7 targetCompatibility = 1.7 dependencies { compile(&amp;quot;org.yaml:snakeyaml:1.15&amp;quot;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  test1结果 {spring={application={name=cruncher}, datasource={driverClassName=com.mysql.jdbc.Driver, url=jdbc:mysql://localhost/test}}, server={port=9000}}  test2结果 - !!com.yqu.yaml.Contact age: 1 name: test1 phoneNumbers: - {name: home, number: &#39;1111&#39;} - {name: work, number: &#39;2222&#39;} - !!com.yqu.yaml.Contact age: 23 name: test2 phoneNumbers: - {name: home, number: &#39;1234&#39;} - {name: work, number: &#39;4321&#39;}  </content>
    </entry>
    
     <entry>
        <title>在线工具</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>在线工具</tag>
        </tags>
        <content type="html">  smallpdf 坛子里提到一个PDF工具网站http://smallpdf.com/cn，有人试用了说不错。支持如下功能： - PDF压缩:大幅压缩PDF文件大小 - JPG转PDF:将图片转换成如您所需的PDF文件 - PDF转JPG:将页面转换成图片，或从PDF文件提取图片 - PDF转Word:将PDF转换成具有最佳质量的Word文件 - PDF转Excel:将PDF格式的电子表格转成可编辑的Excel文件 - PDF转PPT:将PDF格式的幻灯片转成Powerpoint演示文件 - Word转PDF:WORD文件转PDF格式 - Excel转PDF:Excel表格转PDF - PPT转PDF:PPT演示文件转PDF文件 - 合并PDF:将数个PDF文件合并为一个文件 - PDF分割:从所选页面创建新文件 - PDF解密:针对受密码保护的文件进行解密
新浪微博.短网址 http://sina.lt/提供如下功能： - 网址压缩 - 短网址还原 - 外链图库
LaTeX 编译器 JaxEdit
</content>
    </entry>
    
     <entry>
        <title>尝试Artifactory</title>
        <url>https://mryqu.github.io/post/%E5%B0%9D%E8%AF%95artifactory/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>artifactory</tag><tag>packagemanagement</tag><tag>jfrog</tag><tag>repository</tag><tag>gradle</tag>
        </tags>
        <content type="html">  Artifactory简介 首先，JFrogArtifactory是统一构件仓库管理器，全面支持任何语言或技术创建的软件包。Artifactory是一个适合企业的仓库管理器，支持安全、集群和高可用的Docker注册。与所有主流CI/CD和DevOps工具进行集成，Artifactory提供了端到端的自动化的解决方案用以追踪从开发阶段到生产环境阶段中的构件。 安装Artifactory 在https://www.jfrog.com/open-source/下载开源版的jFrogArtifactory，按照JFrog Artifactory用户指南即可轻松安装和使用。 发布构件 使用Gradle构建脚本生成器 gradle.properties artifactory_contextUrl=http://localhost:8081/artifactory artifactory_user=admin artifactory_password=password group = com.yqu version = 0.1.0-SNAPSHOT description = Hello artifactory  build.gradle buildscript { repositories { maven { url &amp;quot;https://plugins.gradle.org/m2/&amp;quot; } } dependencies { //Check for the latest version here: // http://plugins.gradle.org/plugin/com.jfrog.artifactory classpath &amp;quot;org.jfrog.buildinfo:build-info-extractor-gradle:&#43;&amp;quot; } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;maven-publish&#39; apply plugin: &amp;quot;com.jfrog.artifactory&amp;quot; jar { baseName = &#39;HelloArtifactory&#39; } artifacts { archives jar } publishing { publications { maven { from components.java } } } artifactory { //The base Artifactory URL if not overridden by the publisher/resolver contextUrl = &amp;quot;${artifactory_contextUrl}&amp;quot; publish { repository { repoKey = &#39;libs-snapshot-local&#39; username = &amp;quot;${artifactory_user}&amp;quot; password = &amp;quot;${artifactory_password}&amp;quot; maven = true } defaults { publications (&#39;maven } } resolve { repository { repoKey = &#39;libs-snapshot&#39; username = &amp;quot;${artifactory_user}&amp;quot; password = &amp;quot;${artifactory_password}&amp;quot; maven = true } } } repositories { maven { url &#39;http://localhost:8081/artifactory/plugins-release&#39; credentials { username = &amp;quot;${artifactory_user}&amp;quot; password = &amp;quot;${artifactory_password}&amp;quot; } } mavenCentral() } sourceCompatibility = 1.7 targetCompatibility = 1.7  发布结果 C:\test123\HelloArtifactory&amp;gt;gradlew clean build artifactoryPublish [buildinfo] Not using buildInfo properties file for this build. :clean :compileJava warning: [options] bootstrap class path not set in conjunction with -source 1.7 1 warning :processResources UP-TO-DATE :classes :jar :assemble :compileTestJava UP-TO-DATE :processTestResources UP-TO-DATE :testClasses UP-TO-DATE :test UP-TO-DATE :check UP-TO-DATE :build :generatePomFileForMavenJavaPublication :artifactoryPublish Deploying artifact: http://localhost:8081/artifactory/libs-snapshot-local/com/yqu/HelloArtifactory/0.1.0-SNAPSHOT/HelloArtifactory-0.1.0-SNAPSHOT.jar Deploying artifact: http://localhost:8081/artifactory/libs-snapshot-local/com/yqu/HelloArtifactory/0.1.0-SNAPSHOT/HelloArtifactory-0.1.0-SNAPSHOT.pom Deploying build descriptor to: http://localhost:8081/artifactory/api/build Build successfully deployed. Browse it in Artifactory under http://localhost:8081/artifactory/webapp/builds/HelloArtifactory/XXXXXXXX  参考 Artifactory: Working with Gradle
Gradle: Publishing artifacts
Publish JAR artifact using Gradle to Artifactory
</content>
    </entry>
    
     <entry>
        <title>[Gradle] buildScript块与allprojects块及根级别的repositories区别</title>
        <url>https://mryqu.github.io/post/gradle_buildscript%E5%9D%97%E4%B8%8Eallprojects%E5%9D%97%E5%8F%8A%E6%A0%B9%E7%BA%A7%E5%88%AB%E7%9A%84repositories%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>buildscript</tag><tag>repositories</tag><tag>difference</tag><tag>allprojects</tag>
        </tags>
        <content type="html">  一直对为什么buildScript块里定义了repositories而allprojects段或根还定义repositories没有思考过，偶然有了念头想要探究一下。
build.gradle： buildscript {
repositories { &amp;hellip; } dependencies { &amp;hellip; } } allprojects { repositories { &amp;hellip; } dependencies { &amp;hellip; } }repositories { &amp;hellip; } dependencies { &amp;hellip; }
buildScript块主要是为了Gradle脚本自身的执行，获取脚本依赖插件。我在写的一篇博客《尝试Artifactory》中Gradle脚本需要com.jfrog.artifactory插件才能执行成功，而这个插件是从URL为https://plugins.gradle.org/m2/的Maven仓库获得。 根级别的repositories主要是为了当前项目提供所需依赖包，比如log4j、spring-core等依赖包可从mavenCentral仓库获得。 allprojects块的repositories用于多项目构建，为所有项目提供共同所需依赖包。而子项目可以配置自己的repositories以获取自己独需的依赖包。
参考 What&amp;rsquo;s the difference between buildscript and allprojects in build.gradle?
Gradle buildscript dependencies
Gradle: Project
</content>
    </entry>
    
     <entry>
        <title>[Spring Data] 调试H2数据库</title>
        <url>https://mryqu.github.io/post/spring_data_%E8%B0%83%E8%AF%95h2%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>h2</tag><tag>database</tag><tag>console</tag>
        </tags>
        <content type="html">  我将Spring的两个入门指南Building a RESTful Web Service和Accessing Data with JPA融到一起，测试成功。那接下来的一个问题就是怎么查看H2数据库内容并进行调试？
配置H2 Web控制台 为了解决这个问题，我首先增加了src/resources/application.properties配置文件，内容如下：
spring.profiles.active=dev spring.h2.console.enabled=true  在H2 Web控制台上操作 启动Spring Boot应用，在浏览器中进入http://localhost:8080/h2_console/即可进入H2数据库的Web控制台了。 配置IDEA IntelliJ数据源 如果不使用H2 Web控制台的话，在IDEA IntelliJ集成开发环境中也可以通过配置H2数据源进行数据库操作。 解决数据库表不存在问题 上面的玩法有个问题，那就是没看到Accessing Data with JPA里面创建的CUSTOMER表，对不对？为了解决这个问题，在src/resources/application.properties配置文件增加如下内容：
spring.profiles.active=dev spring.h2.console.enabled=true spring.datasource.url=jdbc:h2:~/test;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE spring.datasource.driverClassName=org.h2.Driver spring.datasource.username=sa spring.datasource.password=  搞定，收工！
参考 Using H2’s web console in Spring Boot
Common application properties for Spring Boot
H2 Console
</content>
    </entry>
    
     <entry>
        <title>制作JavaSE8的chm版本JavaDoc</title>
        <url>https://mryqu.github.io/post/%E5%88%B6%E4%BD%9Cjavase8%E7%9A%84chm%E7%89%88%E6%9C%ACjavadoc/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>javadoc</tag><tag>chm</tag><tag>javadoc.chm</tag><tag>java8</tag>
        </tags>
        <content type="html">  Java8文档 在线版Java8文档: http://docs.oracle.com/javase/8/docs/ 下载版Java文档链接：http://www.oracle.com/technetwork/java/javase/downloads/index.html#docs - JavaSE6文档下载链接: http://www.oracle.com/technetwork/java/javase/downloads/jdk-6u25-doc-download-355137.html - JavaSE7文档下载链接: http://www.oracle.com/technetwork/java/javase/documentation/java-se-7-doc-download-435117.html - JavaSE8文档下载链接: http://www.oracle.com/technetwork/java/javase/documentation/jdk8-doc-downloads-2133158.html
工具 Github：subchen/javadoc.chm
制作过程  将javadoc.chm-master.zip的javadoc.chm-2.1.0.jar和lib目录解压缩到当前目录 将jdk-8u45-docs-all.zip的docs目录解压缩到当前目录  java -Xms256m -Xmx512m -cp javadoc.chm-2.1.0.jar;lib/commons-lang-2.6.jar;lib/commons-io-2.4.jar;lib/commons-collections-3.2.1.jar;lib/commons-logging-1.1.1.jar;lib/log4j-1.2.17.jar;lib/velocity-1.7.jar jerbrick.tools.chm.Application docs/api  执行docs/api/build.bat生成chm文件   </content>
    </entry>
    
     <entry>
        <title>了解Registrator</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3registrator/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>docker</tag><tag>consul</tag><tag>registrator</tag><tag>service</tag><tag>devops</tag>
        </tags>
        <content type="html">  支持 DNS和基于HTTP发现机制的服务发现工具Consul让我们印象深刻。它提供了定制化的注册服务健康检查并标记不健康实例的功能远胜于其他类似的工具。更多时兴的工具与Consul的集成使其功能更加强大。在使用Docker的场景里，有了Registrator的帮助，只需要很小的工作量就可以自动化地向Consul注册Docker容器，使得管理基于容器技术的配置更加容易。 Registrator通过检查Docker容器是否上线，自动为Docker容器注册/注销服务。Registrator支持可插拔服务注册中心，当前包括Consul、etcd和SkyDNS 2。
用法  运行Consul容器  $ docker run -d --name=consul --net=host consul-server -bootstrap  运行Registrator容器 Registrator被设计为在每个主机运行一次。也可以在每个集群仅运行一个Registrator，但是通过确保Registrator运行在每个主机上可以获得更好的伸缩性和更简化的配置。假定使用某种程度的自动化，在所有地方都运行反而讽刺性地比某个地方运行更简单。  $ docker run -d \ --name=registrator \ --net=host \ --volume=/var/run/docker.sock:/tmp/docker.sock \ gliderlabs/registrator:latest \ consul://localhost:8500  &amp;ndash;volume=/var/run/docker.sock:/tmp/docker.sock可以让Registrator访问DockerAPI； &amp;ndash;net=host有助于Registrator获得主机级IP和主机名； consul://localhost:8500是服务注册中心URI。 运行其他服务的容器 $ docker run -d -P --name=redis redis Registrator通过Docker API可以监听Docker容器的启动/关闭，并自动注册/注销服务: ``` $ curl $(boot2docker ip):8500/v1/catalog/services {&amp;ldquo;consul&amp;rdquo;:[],&amp;ldquo;redis&amp;rdquo;:[]}   $ curl $(boot2docker ip):8500/v1/catalog/service/redis [{&amp;ldquo;Node&amp;rdquo;:&amp;ldquo;boot2docker&amp;rdquo;,&amp;ldquo;Address&amp;rdquo;:&amp;ldquo;10.0.2.15&amp;rdquo;,&amp;ldquo;ServiceID&amp;rdquo;:&amp;ldquo;boot2docker:redis:6379&amp;rdquo;,&amp;ldquo;ServiceName&amp;rdquo;:&amp;ldquo;redis&amp;rdquo;,&amp;ldquo;ServiceTags&amp;rdquo;:null,&amp;ldquo;ServiceAddress&amp;rdquo;:&amp;ldquo;&amp;rdquo;,&amp;ldquo;ServicePort&amp;rdquo;:32768}] ```
参考 Github：gliderlabs/registrator
Registrator Quickstart
Docker Hub：gliderlabs/registrator
Scalable Architecture DR CoN: Docker, Registrator, Consul, Consul Template and Nginx
</content>
    </entry>
    
     <entry>
        <title>了解Consul template</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3consul_template/</url>
        <categories>
          <category>Tool</category><category>Consul</category>
        </categories>
        <tags>
          <tag>consul</tag><tag>consul-template</tag><tag>devops</tag>
        </tags>
        <content type="html">  支持 DNS 和基于HTTP发现机制的服务发现工具Consul让我们印象深刻。它提供了定制化的注册服务健康检查并标记不健康实例的功能远胜于其他类似的工具。更多时兴的工具与Consul的集成使其功能更加强大。ConsulTemplate守护进程提供了一个便捷方式直接使用Consul的信息来填充配置文件。 consul-template 查询一个Consul实例并对文件系统任意数量模板进行更新。此外，consul-template 在更新过程结束后可选地执行任意多个命令。 consul-template 项目提供了一些例子，通过Consul信息生成负载均衡器HAProxy、缓存引擎Varnish和web服务器Apachehttpd的配置文件。
参考 Github：hashicorp/consul-template
Scalable Architecture DR CoN: Docker, Registrator, Consul, Consul Template and Nginx
</content>
    </entry>
    
     <entry>
        <title>使用Consul的十二要素应用（Twelve-Factor App）</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8consul%E7%9A%84%E5%8D%81%E4%BA%8C%E8%A6%81%E7%B4%A0%E5%BA%94%E7%94%A8twelve-factor_app/</url>
        <categories>
          <category>Tool</category><category>Consul</category>
        </categories>
        <tags>
          <tag>consul</tag><tag>twelve-factor</tag><tag>envconsul</tag><tag>consul-template</tag><tag>devops</tag>
        </tags>
        <content type="html">  十二要素应用（The Twelve-Factor App）主张web应用应该从环境变量里获取其配置。这一实践很快被现代PaaS服务采用以用于允许简单的配置变更。 使用Consul，很容易将这一实践用于你自己的数据中心。如果你基础架构的某些方面部分使用PaaS，Consul是配置数据中心化的一个很好的方式。 在这篇文章中，我们将展示Consul和envconsul如何在不修改应用程序的情况下被用于设置配置值和在配置变更时触发自动重启。
为什么使用环境变量? 根据十二要素应用，web应用配置应该使用环境变量。跟配置文件或Java系统属性这样的机制比，环境变量有很多优点： - 环境变量是一个与开发语言和操作系统无关的标准。 - 环境变量更难被意外提交到代码库。 - 环境变量跟易于在development、staging、QA这样不同的环境之间改变。 - 无论如何部署，环境变量易于设置和更新。
例如Heroku这样的完整PaaS解决方案公开一些有用的API以用于为应用自动设置/读取环境变量。 当手动部署应用时，以往这样的事会更复杂一些。而使用Consul，程序员就可以很容易地设置和读取配置，运营工程师就可以很容易地提供支持和维护。
Consul键值对和Envconsul Consul能够存储键值对数据。对于设置和获取键值对数据，Consul拥有简单的API和美丽且直观的web界面。对于存储配置数据来说，它是完美的。 很容易看到如何设置和读取配置数据，但是对于配置数据如何变成应用的环境变量还不是很清楚。envconsul是一个解决该类问题的轻量级解决方案。 使用envconsul，环境变量存储在ConsulKV中并具有某些（以&amp;rdquo;/&amp;ldquo;分割的）前缀。例如，为了配置服务&amp;rdquo;foo&amp;rdquo;，我们可能存储如下配置：
$ curl -X PUT -d &#39;false&#39; http://localhost:8500/v1/kv/foo/enabled true  这会在键foo/enabled中存储值false。 之后，使用envconsul, 我们可以将这些键转换为环境变量：
$ envconsul foo env ENABLED=false  envconsul是一个对UNIX非常友好的应用。他有两个必需的参数：一个用于查找数据的KV前缀和一个应用及其可选参数。在上例中，我们告诉envconsul配置位于前缀foo下，且我们想运行应用env，该应用仅仅是输出环境变量。 在示例结果中，我们可以清楚地看到ENABLED如我们在ConsulKV中所设置的false。
如果将env改成你自己的应用，那么环境变量将暴露给你的应用。例如，为了运行一个Rails服务器你可能做如下操作。注意在真实生产场景中，你可能不直接运行Rails内建服务器，但是它不失为一个好案例：
$ envconsul foo bin/rails server ...  自动重载 使用PaaS，当你修改任何配置时你的应用将自动重启。我们可以以最小的代价通过Consul和Envconsul实现相同效果。 通过对envconsul添加-reload标志，一旦配置键发生增删改，envconsul将中断(SIGTERM)并重启你的应用：
$ envconsul -reload foo bin/rails server ...  注：该功能已经在0.4.0版本移除。 Consul HTTP API支持对给定前缀KV中的变更进行长轮询。一旦KV中发生变更，Envconsul通过这种方式可以高效地进行检测。
改良流程 对应用配置使用Consul和envconsul可以将PaaS化应用配置易用性带入你自己的原生环境。 对于开发者而言，他们可以无需跟运营工程师沟通或重新部署应用就可以设置配置。 对于运营来说，Consul对整个基础架构的服务发现和配置提供了统一的解决方案。Consul自动复制数据并存储在磁盘上以方便备份，运营工程师也可以高枕无忧了。
我的实践 Envconsul获取的环境变量既可以直接给启动服务器的命令使用（例如上面启动Rails内建服务器的bin/rails命令）；也可以通过python之类的脚本存成Java系统属性文件，通过chpst这样可以加载环境变量/系统属性文件的命令间接给Java命令使用。
envconsul \ -once \ -log-level info \ -consul localhost:8500 \ -upcase=false \ -prefix config/foo/jvm \ foo env /usr/local/tomcat/bin/catalina.sh run  参考 Twelve-Factor Applications with Consul
Github：hashicorp/envconsul
Github：hashicorp/consul-template
Github：mhamrah/docker-envconsul
Docker Hub：panteras/paas-in-a-box
Scalable Architecture DR CoN: Docker, Registrator, Consul, Consul Template and Nginx
</content>
    </entry>
    
     <entry>
        <title>Vagrant运行Docker的几种方法</title>
        <url>https://mryqu.github.io/post/vagrant%E8%BF%90%E8%A1%8Cdocker%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>docker</tag><tag>shell</tag><tag>compose</tag><tag>devops</tag>
        </tags>
        <content type="html">  Vagrant的docker provisioner能够自动安装Docker、下载Docker容器、随着vagrant up命令自动运行容器。 Vagrantfile Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.provision &amp;quot;docker&amp;quot; do |d| d.pull_images &amp;quot;consul&amp;quot; d.run &amp;quot;consul&amp;quot; d.pull_images &amp;quot;rabbitmq&amp;quot; d.run &amp;quot;rabbitmq&amp;quot; end end  仅使用Vagrant的docker provisioner安装Docker，使用脚本下载并运行Docker容器 Vagrantfile # Install Docker config.vm.provision &amp;quot;docker&amp;quot; # Download Docker images, create and start containers config.vm.provision :shell, :path =&amp;gt; &amp;quot;runMyDockers.sh&amp;quot;  runMyDockers.sh #!/bin/bash docker rm -f consul 2&amp;gt;/dev/null docker create --hostname consul --name consul -v /data/consul1:/data --dns 127.0.0.1 --restart always -p 8500:8500 --env CONSUL_OPTIONS=-bootstrap consul:dev docker start consul docker rm -f rabbitmq 2&amp;gt;/dev/null docker create --name rabbitmq --hostname rabbitmq -p 5672:5672 -v /data/rabbitmq:/data --dns 127.0.0.1 --restart always --link consul:consul rabbitmq:dev docker start rabbitmq  仅使用Vagrant的docker provisioner安装Docker，使用Docker Compose下载并运行Docker容器。 Docker Compose是一个定义和运行Docker复杂程序的工具。使用Compose，可以在一个文件内定义多容器应用程序，并有控制所有/单个容器启动停止的相应命令。 Compose有一整套命令来对应用的整个生命周期进行管理： - 启动、停止和重构建服务 - 查看运行服务的状态 - 将运行服务的日志输出整理成数据流。 - 对一个服务运行一次性指令
Vagrantfile # Install Docker config.vm.provision &amp;quot;docker&amp;quot; # Install Docker compose config.vm.provision &amp;quot;shell&amp;quot;, inline: &amp;lt;&amp;lt;-END set -x sudo curl -L https://github.com/docker/compose/releases/download/1.3.0rc1/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose sudo curl -L https://raw.githubusercontent.com/docker/compose/1.3.0rc1/contrib/completion/bash/docker-compose &amp;gt; /etc/bash_completion.d/docker-compose sudo chmod &#43;x /usr/local/bin/docker-compose END # Launch docker-compose to pull Docker images, create and start containers config.vm.provision &amp;quot;shell&amp;quot;, inline: &amp;lt;&amp;lt;-END set -x cd /vagrant docker-compose up -d --allow-insecure-ssl END  docker-compose.yml consul: image: consul:dev hostname: consul dns: 127.0.0.1 restart: always ports: - &amp;quot;8500:8500&amp;quot; volumes: - /data/consul:/data environment: - CONSUL_OPTIONS=-bootstrap rabbitmq: image: rabbitmq:dev hostname: rabbitmq dns: 127.0.0.1 restart: always ports: - &amp;quot;5672:5672&amp;quot; volumes: - /data/rabbitmq:/data links: - consul:consul  参考 Vagrant: Docker provisioner
Docker Compose
</content>
    </entry>
    
     <entry>
        <title>Consul实践</title>
        <url>https://mryqu.github.io/post/consul%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Tool</category><category>Consul</category>
        </categories>
        <tags>
          <tag>consul</tag><tag>服务注册</tag><tag>服务发现</tag><tag>服务配置</tag><tag>服务编排</tag>
        </tags>
        <content type="html">  Consul简介 最近除了在用Hashicorp公司的Vagrant，也使用了Consul。Consul是一款以跨数据中心、高可用的方式提供服务注册、发现、配置和编排的工具。Consul可以用来回答一个企业的基础设施中，诸如下列这些问题：
 “服务X在哪里” “服务Y的实例是否健康” “当前正在运行的服务是什么” “服务Z的配置是怎样的” “在我的平台上是否还有其他人在执行操作A？”  Consul通过DNS或HTTP API提供服务发现功能，同时支持跨数据中心的内部服务或外部服务的发现。使用shell脚本实现了健康检查，并允许创建自定义的服务验证协议。Consul还提供了高可用的键值对存储，由此可以暴露一致的存储值，用于配置参数的调优，而不必非要执行配置管理工具。可调优的操动实例包括指定服务的位置、指明系统处于维护模式，或者设置服务的QoS参数。Consul还提供了一套编排原语、通过UDP协议跨数据中心广播异步“事件”、通过TCP协议让指定的计算机同步执行“exec”指令，以及通过实现长轮询、react、事件机制或者其他操作实现定制化的监控。
安装Consul echo Installing dependencies... sudo apt-get install -y unzip curl echo Fetching Consul... cd /tmp/ wget https://dl.bintray.com/mitchellh/consul/0.5.2_linux_amd64.zip -O consul.zip echo Installing Consul... unzip consul.zip sudo chmod &#43;x consul sudo mv consul /usr/mryqu/consul echo Fetching Consul UI... cd /tmp/ wget https://dl.bintray.com/mitchellh/consul/0.5.2_web_ui.zip -O dist.zip echo Installing Consul UI... unzip dist.zip sudo chmod &#43;x dist sudo mv dist /usr/mryqu/consul/dist  引导一个数据中心 首先以服务器模式运行第一个Consul代理。Consul需要使用-bootstrap-expect指定集群节点个数，使用-data-dirparameter指定一个数据目录名，使用-ui-dir参数指定Consul UI目录:
$&amp;gt;consul agent -server -bootstrap-expect 1 -data-dir /usr/mryqu/consul/consuldata -ui-dir /usr/mryqu/consul/dist  UI默认地址是http://localhost:8500/ui 如果UI没有启动，需要添加额外的-client 0.0.0.0参数重启Consult代理。
注册服务 服务定义是最常用的注册服务的方式。下面是注册一个名为vmstat的服务定义示例：
$&amp;gt;echo ‘{“service”: {“name”: “vmstat”,”tags”:[“master”],”address”: “127.0.0.1”,”port”: 8012,”checks”:[{“script”: “/usr/mryqu/consul/scripts/chkvm.sh”,”interval”: “10s”} ]}} ‘ \ &amp;gt; /usr/mryqu/consul/services/vm.json  重启consul并指定服务目录.
$&amp;gt;consul agent -server -client 0.0.0.0 -bootstrap-expect 1 -data-dir /usr/mryqu/consul/consuldata -ui-dir /usr/mryqu/consul/dist -config-dir /usr/mryqu/consul/services  通过日志可以发现该服务已被同步。
[INFO] agent: Synced service ‘vmstat’ [INFO] agent: Synced check ‘service:vmstat’  Consul UI http://localhost:8500/ui/ 显示如下： 参考 Consul官网
Consul介绍
Consul指南
Vagrant Consul Demo
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] sap.ui.core.ResizeHandler</title>
        <url>https://mryqu.github.io/post/openui5_sap.ui.core.resizehandler/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>resize</tag><tag>handler</tag><tag>javascript</tag><tag>html5</tag>
        </tags>
        <content type="html">  OpenUI5里窗口大小放生变化，各个控件如何收到通知跟着相应变化的呢？
sap.ui.core.Core 首先我们看一下sap.ui.core.Core的源代码：
Core._I_INTERVAL = 200; ResizeHandler.prototype.I_INTERVAL = Core._I_INTERVAL; Core.prototype.attachIntervalTimer = function(fnFunction, oListener) { if (!this.oTimedTrigger) { var IntervalTrigger = sap.ui.requireSync(&amp;quot;sap/ui/core/IntervalTrigger&amp;quot;); this.oTimedTrigger = new IntervalTrigger(Core._I_INTERVAL); } this.oTimedTrigger.addListener(fnFunction, oListener); };  sap.ui.core.Core里面会起一个定时器，以200毫秒间隔周期触发。
sap.ui.core.ResizeHandler 接下来我们看一下sap.ui.core.ResizeHandler的源代码：
function initListener(){ if (!this.bRegistered &amp;amp;&amp;amp; this.aResizeListeners.length &amp;gt; 0) { this.bRegistered = true; sap.ui.getCore().attachIntervalTimer(this.checkSizes, this); } } ResizeHandler.prototype.checkSizes = function() { var bDebug = log.isLoggable(); if ( bDebug ) { log.debug(&amp;quot;checkSizes:&amp;quot;); } jQuery.each(this.aResizeListeners, function(index, oResizeListener){ if (oResizeListener) { var bCtrl = !!oResizeListener.oControl, oDomRef = bCtrl ? oResizeListener.oControl.getDomRef() : oResizeListener.oDomRef; if ( oDomRef &amp;amp;&amp;amp; jQuery.contains(document.documentElement, oDomRef)) { //check that domref is still active var iOldWidth = oResizeListener.iWidth, iOldHeight = oResizeListener.iHeight, iNewWidth = oDomRef.offsetWidth, iNewHeight = oDomRef.offsetHeight; if (iOldWidth != iNewWidth || iOldHeight != iNewHeight) { oResizeListener.iWidth = iNewWidth; oResizeListener.iHeight = iNewHeight; var oEvent = jQuery.Event(&amp;quot;resize&amp;quot;); oEvent.target = oDomRef; oEvent.currentTarget = oDomRef; oEvent.size = {width: iNewWidth, height: iNewHeight}; oEvent.oldSize = {width: iOldWidth, height: iOldHeight}; oEvent.control = bCtrl ? oResizeListener.oControl : null; if ( bDebug ) { log.debug(&amp;quot;resize detected for &#39;&amp;quot; &#43; oResizeListener.dbg &#43; &amp;quot;&#39;: &amp;quot; &#43; oEvent.oldSize.width &#43; &amp;quot;x&amp;quot; &#43; oEvent.oldSize.height &#43; &amp;quot; -&amp;gt; &amp;quot; &#43; oEvent.size.width &#43; &amp;quot;x&amp;quot; &#43; oEvent.size.height); } oResizeListener.fHandler(oEvent); } } } }); if (ResizeHandler._keepActive != true &amp;amp;&amp;amp; ResizeHandler._keepActive != false) { //initialize default ResizeHandler._keepActive = false; } if (!jQuery.sap.act.isActive() &amp;amp;&amp;amp; !ResizeHandler._keepActive) { clearListener.apply(this); } };  sap.ui.core.ResizeHandler监听上一定时器触发事件，执行自己的checkSizes函数。checkSizes函数遍历向自己注册的所有Resize监听器，查看其元素Dom引用是否在窗口内且大小改变，则执行该Resize监听器的fHandler回调处理resize事件。
元素对resize事件的处理 最后我以sap.m.Dialog为例，看一下元素对resize事件的处理。
在Dialog.prototype.open里执行了_registerResizeHandler函数，其注册了_onResize回调用于处理resize事件；在Dialog.prototype.close里执行了_deregisterContentResizeHandler函数。
Dialog.prototype._onResize = function () { var $dialog = this.$(), $dialogContent = this.$(&#39;cont&#39;); //if height is set by manually resizing return; if (this._oManuallySetSize) { return; } if (!this.getContentHeight()) { //reset the height so the dialog can grow $dialogContent.css({ height: &#39;auto&#39; }); //set the newly calculated size by getting it //from the browser rendered layout - by the max-height $dialogContent.height(parseInt($dialog.height(), 10)); } if (this.getStretch() || this._bDisableRepositioning) { return; } this._applyCustomTranslate(); };  </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 通过sap.ui.core.Core的registerElement和deregisterElement函数监控View和控件的构造和析构</title>
        <url>https://mryqu.github.io/post/openui5_%E9%80%9A%E8%BF%87sap.ui.core.core%E7%9A%84registerelement%E5%92%8Cderegisterelement%E5%87%BD%E6%95%B0%E7%9B%91%E6%8E%A7view%E5%92%8C%E6%8E%A7%E4%BB%B6%E7%9A%84%E6%9E%84%E9%80%A0%E5%92%8C%E6%9E%90%E6%9E%84/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>core</tag><tag>registerelement</tag><tag>deregisterelement</tag><tag>control</tag>
        </tags>
        <content type="html"> 在sap.ui.core.Core中有registerElement和deregisterElement函数，它们可用于在调试中监控Element（包括View和控件）的构造和析构。 - registerElement：在控件构造时被调用- deregisterElement：在控件析构时被调用通过下面的代码可知，Core类的mElements存储着元素Id和元素的散列表：
Core.prototype.registerElement = function(oElement) { var sId = oElement.getId(), oldElement = this.mElements[sId]; if ( oldElement &amp;amp;&amp;amp; oldElement !== oElement ) { if ( oldElement._sapui_candidateForDestroy ) { jQuery.sap.log.debug(&amp;quot;destroying dangling template &amp;quot; &#43; oldElement &#43; &amp;quot; when creating new object with same ID&amp;quot;); oldElement.destroy(); } else { // duplicate ID detected =&amp;gt; fail or at least log a warning if (this.oConfiguration.getNoDuplicateIds()) { jQuery.sap.log.error(&amp;quot;adding element with duplicate id &#39;&amp;quot; &#43; sId &#43; &amp;quot;&#39;&amp;quot;); throw new Error(&amp;quot;Error: adding element with duplicate id &#39;&amp;quot; &#43; sId &#43; &amp;quot;&#39;&amp;quot;); } else { jQuery.sap.log.warning(&amp;quot;adding element with duplicate id &#39;&amp;quot; &#43; sId &#43; &amp;quot;&#39;&amp;quot;); } } } this.mElements[sId] = oElement; }; Core.prototype.deregisterElement = function(oElement) { delete this.mElements[oElement.getId()]; };  可以通过过如下方法获得所有已注册元素的Id。
var keys = $.map(this.mElements, function(v, i){ return i; });  </content>
    </entry>
    
     <entry>
        <title>使用Vagrant创建开发环境</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8vagrant%E5%88%9B%E5%BB%BA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>puppet</tag><tag>chef</tag><tag>读书笔记</tag>
        </tags>
        <content type="html"> 用思维导图写了一篇[Packt Publishing] Creating Development Environments with Vagrant读书笔记。 </content>
    </entry>
    
     <entry>
        <title>通过环境变量修改VAGRANT BOX参数</title>
        <url>https://mryqu.github.io/post/%E9%80%9A%E8%BF%87%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E4%BF%AE%E6%94%B9vagrant_box%E5%8F%82%E6%95%B0/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>env</tag><tag>configuraiton</tag>
        </tags>
        <content type="html">  在如下示例Vagrantfile文件片段，VAGRANTBOX的内存和CPU核数首先查询环境变量，如果没有设相关环境变量的话则使用默认值。
config.vm.provider &amp;quot;virtualbox&amp;quot; do |v| v.memory = ENV.has_key?(&#39;VAGRANTBOX_MEM&#39;) ? ENV[&#39;VAGRANTBOX_MEM&#39;].to_i : 1024 v.cpus = ENV.has_key?(&#39;VAGRANTBOX_CPUS&#39;) ? ENV[&#39;VAGRANTBOX_CPUS&#39;].to_i : 2 v.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--natdnshostresolver1&amp;quot;, &amp;quot;on&amp;quot;] v.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--cpuexecutioncap&amp;quot;, &amp;quot;75&amp;quot;] end  参考 Vagrant VirtualBox Configuration
</content>
    </entry>
    
     <entry>
        <title>cAdvisor实践</title>
        <url>https://mryqu.github.io/post/cadvisor%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>cadvisor</tag><tag>docker</tag><tag>devops</tag>
        </tags>
        <content type="html">  cAdvisor (Container Advisor)为运行容器的用户提供出色的资源使用和性能特征。这是一个运行守护进程，能够搜集、集料、处理和导出运行中的容器的信息。特别需要指出，每个容器都有资源隔离参数、历史资源使用、以及完整历史数据的柱状图。 cAdvisor目前支持Docker容器和lmctfy容器。
运行cAdvisor容器 配置boot2docker与宿主机之间的端口转移 查看cAdvisor 参考 GitHub：cAdvisor
</content>
    </entry>
    
     <entry>
        <title>DockerUI实践</title>
        <url>https://mryqu.github.io/post/dockerui%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>dockerui</tag><tag>docker</tag><tag>boot2docker</tag><tag>远程api</tag><tag>devops</tag>
        </tags>
        <content type="html">  DockerUI是Docker远程API的Web接口,它是由下列技术栈构成的纯客户端，因此很容易连接和管理Docker。 - Angular.js - Bootstrap - Gritter - Spin.js - Golang - Vis.js
运行DockerUI容器 配置boot2docker与宿主机之间的端口转移 另一种方式是在启动容器之前执行：
boot2docker ssh -L 9000:localhost:9000  查看DockerUI 直接使用Docker远程API 参考 GitHub：crosbymichael/dockerui
Docker Remote API
</content>
    </entry>
    
     <entry>
        <title>Spring FileUpload限制调整笔记</title>
        <url>https://mryqu.github.io/post/spring_fileupload%E9%99%90%E5%88%B6%E8%B0%83%E6%95%B4%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>fileupload</tag><tag>limit</tag><tag>tuning</tag><tag>tomcat</tag><tag>spring</tag>
        </tags>
        <content type="html">  Tomcat配置 HTTP Connector - maxPostSize配置 maxPostSize: 在POST请求中容器FORMURL参数解析所能处理的最大字节数。该参数可以通过设置为小于零的负值禁掉该限制。如果没有设置，该属性为2097152(2M字节)。 该配置可在$CATALINA_BASE/conf/server.xml内修改: Tomcat 7.0.63之前maxPostSzie=&amp;ldquo;0&amp;rdquo;视为禁掉该限制。
multipart-config配置 max-file-size: 单个上传文件允许的最大字节数。默认-1，无限制。 max-request-size: 真个请求允许的最大字节数。默认-1，无限制。
这两个配置可在web.xml内修改： 如果上传文件超过限制，则会抛出Exception。示例：
org.apache.tomcat.util.http.fileupload.FileUploadBase$SizeLimitExceededException: the request was rejected because its size (61198097) exceeds the configured maximum (20971520) at org.apache.tomcat.util.http.fileupload.FileUploadBase$FileItemIteratorImpl.(FileUploadBase.java:811) at org.apache.tomcat.util.http.fileupload.FileUploadBase.getItemIterator(FileUploadBase.java:256) at org.apache.tomcat.util.http.fileupload.FileUploadBase.parseRequest(FileUploadBase.java:280) at org.apache.catalina.connector.Request.parseParts(Request.java:2730) at org.apache.catalina.connector.Request.parseParameters(Request.java:3064) at org.apache.catalina.connector.Request.getParameter(Request.java:1093) at org.apache.catalina.connector.RequestFacade.getParameter(RequestFacade.java:380) at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:70) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:68) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:668) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1521) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1478) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Unknown Source)  通过org.apache.tomcat.util.http.fileupload.FileUploadBase源代码可知，Tomcat中对应属性为fileSizeMax和sizeMax。 这两个参数也可以在Spring代码通过annotation设置：
@WebServlet(name = &amp;quot;fileUploadServlet&amp;quot;, urlPatterns = {&amp;quot;/upload&amp;quot;}) @MultipartConfig(location=&amp;quot;/tmp&amp;quot;, fileSizeThreshold=0, maxFileSize=5242880, // 5 MB maxRequestSize=20971520) // 20 MB public class FileUploadServlet extends HttpServlet { protected void doPost( HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { //handle file upload }  Spring Boot与Embeded Tomcat 通过org.springframework.boot.autoconfigure.web.MultipartProperties源代码可知，其maxFileSize默认为&amp;rdquo;1Mb&amp;rdquo;， maxRequestSize默认为&amp;rdquo;10Mb&amp;rdquo;。 通过org.springframework.boot.context.embedded.MultipartConfigFactory源代码可知，其maxFileSize默认为-1，maxRequestSize默认为-1。 值得注意的是，这两个默认值与The Java EE 6 Tutorial描述一致，与org.springframework.boot.autoconfigure.web.MultipartProperties源代码是不一致的。 可以通过如下代码，完成HTTP Connector - maxPostSize配置和MultiPartConfig配置。
@SpringBootApplication public class Application { @Autowired private MultipartConfigElement _multipartConfigElement; public static void main(String[] args) { SpringApplication.run(Application.class, args); } @Bean public ServletRegistrationBean dispatcherRegistration( DispatcherServlet dispatcherServlet) { ServletRegistrationBean registration = new ServletRegistrationBean( dispatcherServlet); registration.addUrlMappings(&amp;quot;/tmp&amp;quot;); registration.setMultipartConfig(_multipartConfigElement); return registration; } @Bean public EmbeddedServletContainerCustomizer containerCustomizer() throws FileNotFoundException { return new EmbeddedServletContainerCustomizer() { @Override public void customize(ConfigurableEmbeddedServletContainer container) { if(container instanceof TomcatEmbeddedServletContainerFactory) { TomcatEmbeddedServletContainerFactory containerFactory = (TomcatEmbeddedServletContainerFactory) container; containerFactory.addConnectorCustomizers( new TomcatConnectorCustomizer(){ @Override public void customize(Connector connector) { connector.setMaxPostSize(-1); } }); } }; }; } @Bean public MultipartConfigElement multipartConfigElement() { MultipartConfigElement retval = null; MultipartConfigFactory factory = new MultipartConfigFactory(); factory.setMaxFileSize(&amp;quot;5M&amp;quot;); factory.setMaxRequestSize(&amp;quot;20M&amp;quot;); retval = factory.createMultipartConfig(); return retval; } }  参考 Apache Tomcat Configuration Reference - The HTTP Connector
Tomcat settings: maxPostSize
Spring guides：uploading files
Spring MVC 4 File Upload Example using Servlet 3 MultiPartConfigElement
Spring 4 Java Config for MultipartResolver for Servlet 3.0
</content>
    </entry>
    
     <entry>
        <title>Docker Compose笔记</title>
        <url>https://mryqu.github.io/post/docker_compose%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>docker</tag><tag>compose</tag><tag>multi-container</tag><tag>devops</tag>
        </tags>
        <content type="html">  Docker Compose概述 DockerCompose
前身 Fig
 Compose是用于在Docker内定义和运行多容器应用程序的工具。使用Compose，可以在一个文件内定义多容器应用程序，然后使用一个命令运行应用。 Compose对开发环境、交付准备服务器（stagingservers）和持续集成（CI）很有帮助，不建议用于生产环境。 使用Compose基本上是三步流程： - 通过一个Dockerfile定义应用环境，以便在其他地方复制； - 在docker-compose.yml中定义组成应用的服务，因此他们可以在一个隔离的环境一起运行； - 最后，运行docker-compose up，Compose将启动并运行整个应用。
docker-compose.yml大概是这个样子的:
web: build: . ports: - &amp;quot;5000:5000&amp;quot; volumes: - .:/code links: - redis redis: image: redis  Compose包含管理应用整个生命周期的命令: - 启动、停止和重建服务 - 查看运行的服务状态 - 对运行的服务的日志输出生成数据流 - 对一个服务运行一次性命令
Docker Compose安装 curl -L https://github.com/docker/compose/releases/download/VERSION_NUM/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose $ chmod &#43;x /usr/local/bin/docker-compose  Docker Compose命令 更新整个应用 mryqu$ docker-compose stop # stop the containers mryqu$ docker-compose pull # download updated images mryqu# docker-compose up -d # creates new containers and starts them  更新单个服务 mryqu$ docker-compose stop foo # stop the foo service mryqu$ docker-compose pull foo # download foo service mryqu$ docker-compose up -d foo # start the new foo service  参考 Overview of Docker Compose
GitHub：docker/compose
Fig
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 自定义控件示例</title>
        <url>https://mryqu.github.io/post/openui5_%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%A7%E4%BB%B6%E7%A4%BA%E4%BE%8B/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>custom</tag><tag>control</tag><tag>web</tag><tag>javascript</tag>
        </tags>
        <content type="html">  最近在写一个OpenUI5自定义控件，参考了如下文章，搞定。
需要注意的是，控件内的property在init函数内不会获得构造函数的属性值。通过源码可知，EventProvider.extend.constructor内先回调用init函数，然后再调用applySettings将构造函数内的属性设置进去。
constructor : function(sId, mSettings, oScope) { EventProvider.call(this); // no use to pass our arguments if (typeof (sId) != &amp;quot;string&amp;quot; &amp;amp;&amp;amp; arguments.length &amp;gt; 0) { // shift arguments in case sId was missing, but mSettings was given oScope = mSettings; mSettings = sId; if (mSettings &amp;amp;&amp;amp; mSettings.id) { sId = mSettings[&amp;quot;id&amp;quot;]; } else { sId = null; } } if (!sId) { sId = this.getMetadata().uid() || jQuery.sap.uid(); } else { var preprocessor = ManagedObject._fnIdPreprocessor; sId = (preprocessor ? preprocessor.call(this, sId) : sId); var oType = DataType.getType(&amp;quot;sap.ui.core.ID&amp;quot;); if (!oType.isValid(sId)) { throw new Error(&amp;quot;\&amp;quot;&amp;quot; &#43; sId &#43; &amp;quot;\&amp;quot; is not a valid ID.&amp;quot;); } } this.sId = sId; // managed object interface // create an empty property bag that uses a map of // defaultValues as its prototype this.mProperties = this.getMetadata().createPropertyBag(); this.mAggregations = {}; this.mAssociations = {}; this.mMethods = {}; // private properties this.oParent = null; this.aDelegates = []; this.aBeforeDelegates = []; this.iSuppressInvalidate = 0; this.oPropagatedProperties = {oModels:{}, oBindingContexts:{}}; this.mSkipPropagation = {}; // data binding this.oModels = {}; this.oBindingContexts = {}; this.mElementBindingContexts = {}; this.mBindingInfos = {}; this.sBindingPath = null; this.mBindingParameters = null; this.mBoundObjects = {}; // apply the owner id if defined this._sOwnerId = ManagedObject._sOwnerId; // make sure that the object is registered before initializing // and to deregister the object in case of errors try { // registers the object in the Core if (this.register) { this.register(); } // TODO: generic concept for init hooks? if ( this._initCompositeSupport ) { this._initCompositeSupport(mSettings); } // Call init method here instead of specific Controls constructor. if (this.init) { this.init(); }  参考 How to Create a Custom Control Using SAPUI5 Framework
How to create custom control from scratch
Creating Custom Controls in SAPUI5
Custom controls for SAPUI5/OpenUI5
</content>
    </entry>
    
     <entry>
        <title>使用Docker的现代十二要素应用</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8docker%E7%9A%84%E7%8E%B0%E4%BB%A3%E5%8D%81%E4%BA%8C%E8%A6%81%E7%B4%A0%E5%BA%94%E7%94%A8/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>twelve-factor</tag><tag>docker</tag><tag>devops</tag>
        </tags>
        <content type="html">  【编者的话】“十二要素应用”为开发SaaS应用提供了方法上的指导，而Docker能够提供打包依赖，解耦后端服务等特性，使得两者非常吻合。这篇文章介绍了Docker特性怎样满足了开发“十二要素应用”的对应要点。Docker非常适合开发“十二要素应用”。
“十二要素应用”为构建SaaS应用提供了方法论，是由知名PaaS云计算平台Heroku的创始人AdamWiggins提出的。请参考这篇[文章](http://www.infoq.com/cn/news/2012/09/12-factor-app)。  Dockerfile与docker-compose.yml正在成为用代码定义服务的标准，通过它们可以定义服务的所有内容：依赖、环境、端口、各种进程以及后端服务。Docker镜像和容器为操作系统提供了保证，使得开发环境和生产环境可以有效地保持一致。这篇文章简单地介绍了Docker是怎样满足“十二要素应用”的核心要点的。它解释了用Docker开发一个典型的“Rails/Postgres/Redis/web/worker”所应用的技术。后续文章将通过代码深入介绍如何应用这些技术。
II. 依赖—显示地声明和隔离依赖关系 Docker镜像基于显示的Dockerfile构建，而Docker容器作为独立的运行环境。Dockerfile提供了显示声明基础操作系统的方法（FROM）,而且通过运行命令来安装附加的系统包以及应用的依赖包（RUN）。通过这些方法，你可以声明你需要ubuntu 14.04、Ruby 2.2.2、Node 0.11，然后一次性安装。
III. 配置—在环境中储存配置 Docker容器非常依赖Linux的环境变量进行配置。docker-compose.yml有一个环境变量的哈希表，你可以通过它显示的定义容器的环境变量。这些默认的或者未定义的值将在运行时从主机中继承。另外，还有Dokckerfile的ENV命令以及『docker run –env=[]』和『docker run–env-file=[]』运行选项可以设置环境变量。通过这些方法，你可以声明你的应用需要环境变量GITHUB_AUTH_TOKEN。 VII. 端口绑定—通过端口绑定来提供服务 Docker非常依赖端口绑定。docker-compose.yml有一个端口阵列，可以通过它显示的定义“主机:容器”的端口绑定。『docker run –pHOST:CONTAINER』让你可以在运行时定义端口绑定。通过这些方法，你可以声明你的应用的网络服务器将监听端口5000，而且你可以通过主机的端口5000获取服务。
IV. 后端服务—把后端服务当作附加资源 Docker容器与其它容器几乎完全隔离，所以需要通过网络与后端服务进行通信。docker-compse.yml有一个链接哈希表，你可以通过它指定你的应用所需要依赖的其他容器服务。‘docker-composeup’命令将首先开启这些后端服务，然后配置应用容器中网络连接信息的环境变量。通过这些方法，你可以声明你的应用需要Postgres 9.4和Redis3.0服务，让你的应用通过主机名和端口号与他们建立连接。
VI. 进程—以一个或者多个无状态进程运行应用 默认情况下，Docker容器是不带储存的进程。docker-compose.yml定义了一系列服务，每一个服务都有自己的镜像或者构建文件(Dockerfile)以及命令。通过这些方法，你可以声明你的应用同时有一个网络进程和工作进程。
XII. 管理进程—后台管理任务当做一次性进程运行 Docker镜像可以很容易地运行一次性进程。‘docker run myapp CMD’可以在与你的网络进程一致的环境中运行任意命令。通过这些方法，你可以基于你的Postgres数据库运行交互式的bash或者运行一次性的’rakedb:migrate’进程。
现有技术 若没有Docker，OS X的开发工具链是这样的：Homebrew作为系统依赖包， Postgres和Redis作为开发服务,Ruby的Bundler作为跨平台开发依赖，一系列的Shell脚本和foreman让所有工具在本地同时运行起来，以及一个独立的基于Linux的构建服务负责将应用打包到生产环境。这样的工作流并没有错误，但是Docker提供一个更简洁的方式。有了Dockerfile和docker-compose.yml文件，我们将不再需要任何OSX系统依赖，服务包或者跨平台的语言依赖。一个简单的“dicker-composeup”命令可以提供一个完整的Linux开发环境，并且能够轻易地将“十二要素应用”移植到生产机器。
原英文链接：Modern Twelve-Factor Apps With Docker
原译文链接：现代“十二要素应用”与Docker
</content>
    </entry>
    
     <entry>
        <title>读八种Docker开发模式</title>
        <url>https://mryqu.github.io/post/%E8%AF%BB%E5%85%AB%E7%A7%8Ddocker%E5%BC%80%E5%8F%91%E6%A8%A1%E5%BC%8F/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>docker</tag><tag>development</tag><tag>pattern</tag><tag>devops</tag>
        </tags>
        <content type="html"> Eight Docker Development Patterns（原文）
八种Docker开发模式（译文）
八种Docker开发模式（介绍）
目前，我对可重用的基础容器和支持共享文件夹的开发容器这两种模式接触的多一些。
</content>
    </entry>
    
     <entry>
        <title>Docker速查笔记</title>
        <url>https://mryqu.github.io/post/docker%E9%80%9F%E6%9F%A5%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>docker</tag><tag>速查表</tag><tag>命令</tag><tag>dockerfile</tag><tag>笔记</tag>
        </tags>
        <content type="html">  常用命令 登入运行的docker容器内 docker exec -it $dockerContainerName /bin/bash  查看docker日志 docker logs --tail=&amp;quot;5&amp;quot; -f $dockerContainerName  清除容器及镜像 docker stop $(docker ps -a -q) #停止所有容器 docker rm $(docker ps -a -q) #删除所有容器 docker rmi $(docker images -q) #删除所有镜像  参考 Docker Cheat Sheet
Docker Command Line
Dockerfile reference
The Docker Book
</content>
    </entry>
    
     <entry>
        <title>Spring Boot Example：Rest Exception Handling</title>
        <url>https://mryqu.github.io/post/spring_boot_examplerest_exception_handling/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>rest</tag><tag>exceptionhandler</tag><tag>gradle</tag>
        </tags>
        <content type="html">  要给同事做个Rest异常处理的演示，顺便用用Spring Boot和Gradle构建。 首先新建一个项目：rest-exception-handling。
rest-exception-handling/src/main/java/com/yqu/rest目录 Application.java package com.yqu.rest; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } }  GreetingController.java package com.yqu.rest; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.http.HttpStatus; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.*; import org.springframework.web.servlet.ModelAndView; @RestController public class GreetingController { @Autowired private GreetingService service; @RequestMapping(value = &amp;quot;/&amp;quot;, method = RequestMethod.GET) public ModelAndView home(Model m){ System.out.println(&amp;quot;home&amp;quot;); return new ModelAndView(&amp;quot;index&amp;quot;); } @RequestMapping(value = &amp;quot;/greeting&amp;quot;, method = RequestMethod.GET) public @ResponseBody GreetingVO greeting(@RequestParam(value=&amp;quot;name&amp;quot;) String name) throws GreetingException { System.out.println(&amp;quot;greeting &amp;quot; &#43; name); return service.getGreeing(name); } @ExceptionHandler(GreetingException.class) @ResponseStatus(HttpStatus.BAD_REQUEST) public @ResponseBody GreetingErrorMessage greetingExceptionHandler(GreetingException ex) { System.out.println(&amp;quot;greetingExceptionHandler&amp;quot;); return new GreetingErrorMessage(0,ex.getMessage()); } }  GreetingErrorMessage.java package com.yqu.rest; public class GreetingErrorMessage { private int errCode; private String errMsg; public GreetingErrorMessage(int errCode, String errMsg) { this.errCode = errCode; this.errMsg = errMsg; } public int getErrCode() { return errCode; } public void setErrCode(int errCode) { this.errCode = errCode; } public String getErrMsg() { return errMsg; } public void setErrMsg(String errMsg) { this.errMsg = errMsg; } }  GreetingException.java package com.yqu.rest; public class GreetingException extends Exception { public GreetingException() { } public GreetingException(String msg) { super(msg); } }  GreetingService.java package com.yqu.rest; public interface GreetingService { public GreetingVO getGreeing(String name) throws GreetingException; }  GreetingServiceImpl.java package com.yqu.rest; import org.springframework.stereotype.Service; import java.util.concurrent.atomic.AtomicLong; @Service public class GreetingServiceImpl implements GreetingService { private static final String template = &amp;quot;Hello, %s!&amp;quot;; private final AtomicLong counter = new AtomicLong(); @Override public GreetingVO getGreeing(String name) throws GreetingException { if(name.equalsIgnoreCase(&amp;quot;war&amp;quot;)) throw new GreetingException(&amp;quot;no greeting for &amp;quot;&#43;name); else if(name.equalsIgnoreCase(&amp;quot;hell&amp;quot;)) throw new RuntimeException(&amp;quot;no idea about &amp;quot;&#43;name); return new GreetingVO(counter.incrementAndGet(), String.format(template, name)); } }  GreetingVO.java package com.yqu.rest; public class GreetingVO { private final long id; private final String content; public GreetingVO(long id, String content) { this.id = id; this.content = content; } public long getId() { return id; } public String getContent() { return content; } }  rest-exception-handling/src/main/resources目录 application.properties spring.view.prefix: /WEB-INF/jsp/ spring.view.suffix: .jsp  rest-exception-handling/src/main/webapp/WEB-INF/jsp目录 index.jsp &amp;lt;%@ page language=&amp;quot;java&amp;quot; pageEncoding=&amp;quot;UTF-8&amp;quot; contentType=&amp;quot;text/html; charset=UTF-8&amp;quot;%&amp;gt; &amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Hello Rest Exception Handling&amp;lt;/title&amp;gt; &amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=UTF-8&amp;quot; /&amp;gt; &amp;lt;script src=&amp;quot;webjars/jquery/2.1.4/jquery.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt; getGreeting = function() { $.ajax({ url: &amp;quot;greeting?name=&amp;quot;&#43;$(&#39;#name&#39;).val(), type: &amp;quot;GET&amp;quot;, success: function(res) { $(&#39;#greetingContent&#39;).text(JSON.stringify(res)); $(&#39;#greetingError&#39;).text(&amp;quot;&amp;quot;); }, error: function (res) { $(&#39;#greetingContent&#39;).text(&amp;quot;&amp;quot;); $(&#39;#greetingError&#39;).text(res.responseText); } }); }; $(document).ready(function() { getGreeting(); }); &amp;lt;/script&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;div class=&amp;quot;content&amp;quot;&amp;gt; &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;name&amp;quot; required onchange=&amp;quot;getGreeting()&amp;quot; value=&amp;quot;yqu&amp;quot;/&amp;gt; &amp;lt;div id=&amp;quot;greetingContent&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div id=&amp;quot;greetingError&amp;quot; style=&amp;quot;color: #D8000C;background-color: #FFBABA;&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt;  rest-exception-handling目录 build.gradle buildscript { repositories { mavenCentral() maven { url &amp;quot;http://repo.spring.io/libs-milestone&amp;quot; } } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.3.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;rest-exception-handling&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() maven { url &amp;quot;http://repo.spring.io/libs-milestone&amp;quot; } } sourceCompatibility = 1.7 targetCompatibility = 1.7 dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) compile(&amp;quot;org.webjars:jquery:2.1.4&amp;quot;) compile(&amp;quot;org.apache.tomcat.embed:tomcat-embed-jasper&amp;quot;) compile(&amp;quot;javax.servlet:jstl&amp;quot;) testCompile(&amp;quot;junit:junit&amp;quot;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  参考 Spring Guide：Rest Service
Spring Boot Reference Guide
Utilizing WebJars in Spring Boot
Exception Handling in Spring MVC
Spring MVC @ExceptionHandler Example
</content>
    </entry>
    
     <entry>
        <title>Vagrant base box列表</title>
        <url>https://mryqu.github.io/post/vagrant_base_box%E5%88%97%E8%A1%A8/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>basebox</tag><tag>ubuntu</tag>
        </tags>
        <content type="html">  Vagrant的basebox都是打包了最小安装版操作系统(仅有一些跟Vagrant通讯必需的工具)的虚拟机镜像。vagrantbox.es列出了很多Vagrant的basebox，其中既有官方的box也有很多非官方的box。 有的box名是lucid32.box、lucid64.box、precise32.box、precise64.box，一开始不明白怎么回事，后来知道用的是Ubuntu的英文代码。
Ubuntu各种版本的英文代码 |版本|英文代号|中译 |&amp;mdash;&amp;ndash; |Ubuntu 4.10|Warty Warthog|多疣的疣猪 |Ubuntu 5.04|Hoary Hedgehog|白发的刺猬 |Ubuntu 5.10|Breezy Badger|活泼的獾 |Ubuntu 6.06|Dapper Drake|整洁的公鸭 |Ubuntu 6.10|Edgy Eft|尖利的小蜥蜴 |Ubuntu 7.04|Feisty Fawn|烦躁不安的鹿 |Ubuntu 7.10|Gutsy Gibbon|胆大的长臂猿 |Ubuntu 8.04|Hardy Heron|坚强的鹭 |Ubuntu 8.10|Intrepid Ibex|无畏的羱羊 |Ubuntu 9.04|Jaunty Jackalope|活泼的鹿角兔 |Ubuntu 9.10|Karmic Koala|幸运的树袋熊 |Ubuntu 10.04|Lucid Lynx|清醒的猞猁 |Ubuntu 10.10|Maverick Meerkat|标新立异的的狐獴 |Ubuntu 11.04|Natty Narwhal|敏捷的独角鲸 |Ubuntu 11.10|Oneiric Ocelot|有梦的虎猫 |Ubuntu 12.04|Precise Pangolin|精准的穿山甲 |Ubuntu 12.10|Quantal Quetzal|量子的格查尔鸟 |Ubuntu 13.04|Raring Ringtail|铆足了劲的环尾猫熊 |Ubuntu 13.10|Saucy Salamander|活泼的蝾螈 |Ubuntu 14.04|Trusty Tahr|可靠的塔尔羊 |Ubuntu 14.10|Utopic Unicorn|乌托邦的独角兽 |Ubuntu 15.04|Vivid Vervet|活泼的长尾黑颚猴 |Ubuntu 15.10|Wily Werewolf|老谋深算的狼人
我家里现在装的是衍生版本Ubuntu Kylin 15.04，不知道英文代码是不是vivid Vervet？
Debian各种版本的英文代码 |版本|代号|发布期|玩具总动员的对应角色|脚注 |&amp;mdash;&amp;ndash; |1.1|Buzz|1996.6.17|巴斯光.，电影主角之一的太空人|使用Linux内核2.0 |1.2|Rex|1996.12.12|抱抱龙|&amp;nbsp; |1.3|Bo|1997.6.2|放羊的女孩“宝贝”|&amp;nbsp; |2.0|Hamm|1998.7.24|小猪储蓄罐“火腿”|&amp;nbsp; |2.1|Slink|1999.3.9|弹簧狗|APT面世 |2.2|Potato|2000.8.15|蛋头先生|&amp;nbsp; |3.0|Woody|2002.7.19|胡迪，电影主角之一的牛仔|&amp;nbsp; |3.1|Sarge|2005.6.6|绿色塑胶玩具士兵的首领“队长”|&amp;nbsp; |4.0|Etch|2007.4.8|画板|&amp;nbsp; |5.0|Lenny|2009.2.14|望远镜|&amp;nbsp; |6.0|Squeeze|2011.2.6|三只眼的外星人|其i386及amd64架构为长期支持版本，
是第一个包含长期支持的Debian版本，
支持到2016.2。 |7.0|Wheezy|2013.5.5|吱吱（第二部玩具总动员的一个角色，
是一只带着领结的玩具企鹅）|上一个稳定版本 |8.0|Jessie|2015.4.25|翠丝（第二部玩具总动员的一个角色，
是一个为虚拟的电视剧 Woody&amp;rsquo;s Roundup
而塑造的女牛仔人物）|目前的稳定版本，默认init系统切换为systemd
</content>
    </entry>
    
     <entry>
        <title>玩一下gradle-jvmsrc-plugin</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E4%B8%80%E4%B8%8Bgradle-jvmsrc-plugin/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>plugin</tag><tag>jvmsrc</tag>
        </tags>
        <content type="html"> 玩了一下gradle-jvmsrc-plugin插件，使用这个插件后执行gradlecreateJvmSrcDirs可以根据Gradle项目的JVM语言插件（(java、groovy、scala、android等），自动创建默认的源代码、测试和资源包目录。例如： - src/main/resources - src/main/java/ - src/main/groovy/ - src/test/java/ - src/test/groovy/ - src/test/resources
刚上手总是报错，看了一下CreateJvmSourceDirs.groovy，定位到packageToDirectoryPath方法：
* What went wrong: Execution failed for task &#39;:HelloJvmsrc:createJvmSrcDirs&#39;. &amp;gt; character to be escaped is missing  按照如下gradle-jvmsrc-plugin的说明，要配置基础包名。可是真按它介绍的带有.分割的包名就会出错，简单改成&amp;rdquo;com&amp;rdquo;这种没有.分割的包名就可以避免错误。
jvmsrc { packageName &amp;quot;com.mycompany.myproject.mymodule&amp;quot; }  此外，gradle-jvmsrc-plugin对空目录默认生成.gitkeep文件。 总体来说，用处不是很大，可以偷点懒！
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] sap.m.Input的change回调</title>
        <url>https://mryqu.github.io/post/openui5_sap.m.input%E7%9A%84change%E5%9B%9E%E8%B0%83/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>input</tag><tag>change</tag><tag>manual</tag><tag>javascript</tag>
        </tags>
        <content type="html"> 用sap.m.Input的change回调，当值在输入界面被修改后就会调用。今天试了一下，如果通过Model设置改变值的话，其change回调不会被调用。
这种特性正好用于判断是否为界面手工修改。在我的用例中，有一个表名和一个表表述。如果改动表名，表描述跟着相应更新；但是一旦用户手工输入表描述后，上述规则不再生效。
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 使用TeraSort测试集群性能</title>
        <url>https://mryqu.github.io/post/hadoop_%E4%BD%BF%E7%94%A8terasort%E6%B5%8B%E8%AF%95%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>terasort</tag><tag>teragen</tag><tag>performance</tag><tag>benchmark</tag>
        </tags>
        <content type="html">  Terasort是Hadoop自带的用于集群性能基准测试的工具，其源码位于https://github.com/apache/hadoop/tree/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort下。
TeraSort用法 该性能基准测试工具针对Hadoop集群的HDFS和MapReduce层进行综合测试。完整的测试步骤为：
 使用TeraGen程序生成官方GraySort输入数据集。(注：SortBenchmark是JimGray自98年建立的一项排序竞技活动，其对排序的输入数据制定了详细规则，要求使用其提供的gensort工具生成输入数据。而Hadoop的TeraGen数据生成工具的算法与gensort一致。） 在输入数据上运行真正的TeraSort性能基准测试工具 通过TeraValidate程序验证排序后的输出数据  TeraGen程序生成数据的格式为（详见TeraSort.generateRecord方法实现）： - 10字节键：一个16字节随机数的高10字节 - 2字节常量：0x0011 - 32字节rowid - 4字节常量：0x8899AABB - 48字节填充：由一个16字节随机数的低48比特生成 - 4字节常量:0xCCDDEEFF
也就是说TeraGen程序生成的一行数据有100字节。TeraGen程序参数需要指定行数，可指定单位： - t：1000,000,000,000 - b：1000,000,000 - m：1000,000 - k：1000
TeraSort测试 依次运行teragen、terasort和teravalidate：
hadoop@node50064:~$ yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.x.jar teragen 5m /user/hadoop/teragen-data hadoop@node50064:~$ yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.X.jar terasort /user/hadoop/teragen-data /user/hadoop/terasort-data 15/05/24 08:29:03 INFO terasort.TeraSort: starting 15/05/24 08:29:04 INFO input.FileInputFormat: Total input paths to process : 2 Spent 123ms computing base-splits. Spent 2ms computing TeraScheduler splits. Computing input splits took 127ms Sampling 4 splits of 4 Making 1 from 100000 sampled records Computing parititions took 558ms Spent 686ms computing partitions. ...... 15/05/24 08:34:23 INFO terasort.TeraSort: done hadoop@node50064:~$ yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.X.jar teravalidate /user/hadoop/terasort-data /user/hadoop/teravalidate-data  terasort默认使用一个reduce任务，如需通过增加reduce任务提升性能的话，可以通过指定mapreduce.job.reduces增加reduce任务个数，例如-Dmapreduce.job.reduces=2。
完成性能分析后，删除HDFS上的数据：
hadoop@node50064:~$ hadoop fs -rm -r -skipTrash tera* Deleted teragen-data Deleted terasort-data Deleted teravalidate-data  </content>
    </entry>
    
     <entry>
        <title>winrar中文版免费了</title>
        <url>https://mryqu.github.io/post/winrar%E4%B8%AD%E6%96%87%E7%89%88%E5%85%8D%E8%B4%B9%E4%BA%86/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>winrar</tag><tag>免费</tag>
        </tags>
        <content type="html"> 5月21日，软众信息宣布中国WinRAR完全免费。以下是新闻稿全文：我们很荣幸地宣布，经过15年多的时间，现在终于向中国的个人用户提供一款完全免费的WinRAR简体中文版了。这是因为WinRAR 的全球独家发行商 win.rar GmbH 和 www.winrar.com.cn 希望藉此来感谢数亿中国用户长久以来对WinRAR的信任。许多公司曾试图复制类似WinRAR的产品，但是我们的用户始终相信我们并坚持使用原版软件，即使这意味着不得不在同一台计算机上运行两种压缩工具。 随着全新的 WinRAR免费非商用个人版本的发布，我们为中国的每一个用户提供当今市面上安全、可靠、出色的压缩软件。您现在可以正式从www.winrar.com.cn 完全免费地下载和使用WinRAR，无需搜索或下载破解产品，也不必寻找非法版本，或冒着安全风险从不安全的网站进行下载。WinRAR 现在还包含最佳的RAR5 压缩算法，该软件可以从www.winrar.com.cn 获取官方版本。 现在，您完全可以享受始终使用 WinRAR 的最新版本。 我们已经与中国实力雄厚、值得信赖的公司展开合作，来为您实现这一目标。www.winrar.com.cn 能够以对用户极其友好的方式，提供我们本地中国合作伙伴的产品。如果您不喜欢我们合作伙伴的产品，您只需点击取消即可。不存在任何秘密跟踪、监视或隐藏的安装！我们在未来将继续与中国最大的互联网和软件公司建立良好关系，由此来为您提供可以信赖的其他优秀产品和服务。我们已与 360和百度等互联网公司建立合作伙伴关系来推进在中国和国外的合作，而且我们期望持续完善这些业务，并与其他值得信任的知名公司建立新的合作伙伴关系。在2012 年，win.rar GmbH（德国总部）的创始人兼总裁 Burak Canboy已决定迁居中国，其后便一直住在我们这个美好的国家，亲自协调我们已在中国建立的合作伙伴关系，尤其要表达他对众多中国用户长久以来信赖WinRAR 软件的敬意。 “我们知道中国已经有4亿多用户在使用 WinRAR，我们十分高兴并感谢您如今仍然深爱WinRAR。非常感谢您信任我们的产品！”
http://www.winrar.com.cn/download.htm 中国个人免费版（64位）官网下载：http://www.winrar.com.cn/download/winrarx64-521scp.exe （数字签名：UTC&#43;8，2015.05.20，16:09:31） 中国个人免费版（32位）官网下载：http://www.winrar.com.cn/download/winrar521scp.exe
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 使用ChainMapper和ChainReducer运行MapReduce作业链</title>
        <url>https://mryqu.github.io/post/hadoop_%E4%BD%BF%E7%94%A8chainmapper%E5%92%8Cchainreducer%E8%BF%90%E8%A1%8Cmapreduce%E4%BD%9C%E4%B8%9A%E9%93%BE/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>chainmapper</tag><tag>chainreducer</tag><tag>jobchaining</tag><tag>mapreduce</tag>
        </tags>
        <content type="html">  启动多个MapReduce作业并实现作业控制，大概有以下几种方式：
 在Driver中通过waitForCompletion方法同步启动并运行作业，根据执行结果同样同步启动并运行后继作业。作业控制逻辑完全是自己实现，仅适用于作业不多的应用。 使用ChainMapper和ChainReducer运行MapReduce作业链 使用Oozie管理复杂MapReduce工作流 本文将针对第二种方式进行学习总结。  使用MapReduce作业链模式的数据和执行流如下：
 一或多个mapper shuffle阶段 一个reducer 零或多个mapper 即，mapper可以输出给mapper，也可以输出给reducer；reducer只能输出给mapper；reducer之前必有shuffle阶段。  JobChaining示例 JobChainingDemo.java源码 londonbridge.txt London Bridge is falling down, Falling down, falling down. London Bridge is falling down, My fair lady. Build it up with wood and clay, Wood and clay, wood and clay, Build it up with wood and clay, My fair lady. Wood and clay will wash away, Wash away, wash away, Wood and clay will wash away, My fair lady. Build it up with bricks and mortar, Bricks and mortar, bricks and mortar, Build it up with bricks and mortar, My fair lady. Bricks and mortar will not stay, Will not stay, will not stay, Bricks and mortar will not stay, My fair lady. Build it up with iron and steel, Iron and steel, iron and steel, Build it up with iron and steel, My fair lady. Iron and steel will bend and bow, Bend and bow, bend and bow, Iron and steel will bend and bow, My fair lady. Build it up with silver and gold, Silver and gold, silver and gold, Build it up with silver and gold, My fair lady. Silver and gold will be stolen away, Stolen away, stolen away, Silver and gold will be stolen away, My fair lady. Set a man to watch all night, Watch all night, watch all night, Set a man to watch all night, My fair lady. Suppose the man should fall asleep, Fall asleep, fall asleep, Suppose the man should fall asleep, My fair lady. Give him a pipe to smoke all night, Smoke all night, smoke all night, Give him a pipe to smoke all night, My fair lady.  执行  hadoop jar YquMapreduceDemo.jar JobChainingDemo /user/hadoop/chain_input/londonbridge.txt /user/hadoop/chain_output2  测试结果 </content>
    </entry>
    
     <entry>
        <title>[Hadoop] 使用DFSIO测试集群I/O性能</title>
        <url>https://mryqu.github.io/post/hadoop_%E4%BD%BF%E7%94%A8dfsio%E6%B5%8B%E8%AF%95%E9%9B%86%E7%BE%A4io%E6%80%A7%E8%83%BD/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>dfsio</tag><tag>performance</tag><tag>benchmark</tag><tag>test</tag>
        </tags>
        <content type="html">  DFSIO是Hadoop自带的用于集群分布式I/O性能基准测试的工具，其源码为https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java。
DFSIO 用法 hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.X-tests.jar TestDFSIO 15/05/22 19:50:22 INFO fs.TestDFSIO: TestDFSIO.1.8 Missing arguments. Usage: TestDFSIO [genericOptions] -read [-random | -backward | -skip [-skipSize Size]] | -write | -append | -truncate | -clean [-compression codecClassName] [-n rFiles N] [-size Size[B|KB|MB|GB|TB]] [-resFile resultFileName] [-bufferSize Bytes]  DFSIO可以测试写操作和读操作，以MapReduce作业的方式运行，返回整个集群的I/O性能报告。DFSIO读写测试的位置在hdfs://namendoe:8020/benchmarks/TestDFSIO/io_data，其中读测试不会自己产生数据，必须先执行DFSIO写测试。
 -read：读测试，对每个文件读-size指定的字节数 -write：写测试，对每个文件写-size指定的字节数 -append：追加测试，对每个文件追加-size指定的字节数 -truncate：截断测试，对每个文件截断至-size指定的字节数 -clean：清除TestDFSIO在HDFS上生成数据 -n：文件个数 -size：每个文件的大小 -resFile：生成测试报告的本地文件路径 -bufferSize：每个mapper任务读写文件所用到的缓存区大小，默认为1000000字节。
DFSIO测试  写10个100MB的文件 hadoop@node50064:~$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.X-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 100MB -resFile /tmp/DFSIO-write.out  查看写测试结果 本地文件/tmp/DFSIO-write.out包含写测试性能报告：
hadoop@node50064:~$ cat /tmp/DFSIO-write.out ----- TestDFSIO ----- : write Date &amp;amp; time: Sat May 23 00:44:59 EDT 2015 Number of files: 10 Total MBytes processed: 1000.0 Throughput mb/sec: 3.458508276210305 Average IO rate mb/sec: 4.84163236618042 IO rate std deviation: 2.384178973806887 Test exec time sec: 170.605  DFSIO 写测试在HDFS上生成的数据：
hadoop@node50064:~$ hadoop fs -ls /benchmarks/TestDFSIO/ Found 3 items drwxr-xr-x - hadoop supergroup 0 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control drwxr-xr-x - hadoop supergroup 0 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data drwxr-xr-x - hadoop supergroup 0 2015-05-23 00:44 /benchmarks/TestDFSIO/io_write hadoop@node50064:~$ hadoop fs -ls /benchmarks/TestDFSIO/io_control Found 10 items -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_0 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_1 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_2 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_3 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_4 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_5 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_6 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_7 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_8 -rw-r--r-- 2 hadoop supergroup 112 2015-05-23 00:42 /benchmarks/TestDFSIO/io_control/in_file_test_io_9 hadoop@node50064:~$ hadoop fs -ls /benchmarks/TestDFSIO/io_data Found 10 items -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_0 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_1 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_2 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_3 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_4 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_5 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_6 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_7 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_8 -rw-r--r-- 2 hadoop supergroup 104857600 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data/test_io_9 hadoop@node50064:~$ hadoop fs -ls /benchmarks/TestDFSIO/io_write Found 2 items -rw-r--r-- 2 hadoop supergroup 0 2015-05-23 00:44 /benchmarks/TestDFSIO/io_write/_SUCCESS -rw-r--r-- 2 hadoop supergroup 79 2015-05-23 00:44 /benchmarks/TestDFSIO/io_write/part-00000  读10个100MB的文件 hadoop@node50064:~$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.X-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 100MB -resFile /tmp/DFSIO-read.out  查看读测试结果 本地文件/tmp/DFSIO-read.out包含读测试性能报告：
hadoop@node50064:~$ cat /tmp/DFSIO-read.out ----- TestDFSIO ----- : read Date &amp;amp; time: Sat May 23 00:59:25 EDT 2015 Number of files: 10 Total MBytes processed: 1000.0 Throughput mb/sec: 101.09179134654266 Average IO rate mb/sec: 160.2582244873047 IO rate std deviation: 126.10457558779005 Test exec time sec: 39.327  DFSIO 写测试在HDFS上生成的数据：
hadoop@node50064:~$ hadoop fs -ls /benchmarks/TestDFSIO Found 4 items drwxr-xr-x - hadoop supergroup 0 2015-05-23 00:58 /benchmarks/TestDFSIO/io_control drwxr-xr-x - hadoop supergroup 0 2015-05-23 00:44 /benchmarks/TestDFSIO/io_data drwxr-xr-x - hadoop supergroup 0 2015-05-23 00:59 /benchmarks/TestDFSIO/io_read drwxr-xr-x - hadoop supergroup 0 2015-05-23 00:44 /benchmarks/TestDFSIO/io_write hadoop@node50064:~$ hadoop fs -ls /benchmarks/TestDFSIO/io_read Found 2 items -rw-r--r-- 2 hadoop supergroup 0 2015-05-23 00:59 /benchmarks/TestDFSIO/io_read/_SUCCESS -rw-r--r-- 2 hadoop supergroup 80 2015-05-23 00:59 /benchmarks/TestDFSIO/io_read/part-00000  清除DFSIO在HDFS生成的数据 hadoop@node50064:~$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.X-tests.jar TestDFSIO -clean hadoop@node50064:~$ hadoop fs -ls /benchmarks  </content>
    </entry>
    
     <entry>
        <title>YAML</title>
        <url>https://mryqu.github.io/post/yaml/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>yaml</tag><tag>json</tag><tag>xml</tag><tag>sdl</tag><tag>数据序列化</tag>
        </tags>
        <content type="html">  简介 YAML是一个可读性高的数据序列化格式。YAML参考了其他多种语言，包括：XML、C语言、Python、Perl以及电子邮件格式RFC2822。ClarkEvans在2001年首次发表了这种语言 ，另外Ingy d?t Net与OrenBen-Kiki也是这语言的共同设计者。目前已经有数种编程语言或脚本语言支援（或者说解析）这种语言。 _YAML_是&amp;rdquo;YAML Ain&amp;rsquo;t a Markup Language&amp;rdquo;（YAML不是一种标记语言）的递回缩写。在开发的这种语言时，_YAML_的意思其实是：&amp;rdquo;Yet Another Markup Language&amp;rdquo;（仍是一种标记语言），但为了强调这种语言以数据做为中心，而不是以标记语言为重点，而用反向缩略语重新命名。
功能 YAML的语法和其他高阶语言类似，并且可以简单表达列表、哈希表，标量等数据形式。它使用空白符号缩排和大量依赖外观的特色，特别适合用来表达或编辑数据结构、各种配置文件、调试时的转储内容、文件标题（例如：许多电子邮件标题格式和YAML非常接近）。尽管它比较适合用来表达分层数据，不过也有紧凑的语法可以表示关联性数据。由于YAML使用空白字符和分行来分隔数据，使得它特别适合用grep／Python／Perl／Ruby操作。其让人最容易上手的特色是巧妙避开各种封闭符号，如：引号、各种括号等，这些符号在嵌套结构时会变得复杂而难以辨认。
范例 简单的文件 数据结构可以用类似大纲的缩排方式呈现
--- receipt: Oz-Ware Purchase Invoice date: 2007-08-06 customer: given: Dorothy family: Gale items: - part_no: A4786 descrip: Water Bucket (Filled) price: 1.47 quantity: 4 - part_no: E1628 descrip: High Heeled &amp;quot;Ruby&amp;quot; Slippers price: 100.27 quantity: 1 bill-to: &amp;amp;id001 street: | 123 Tornado Alley Suite 16 city: East Westville state: KS ship-to: *id001 specialDelivery: &amp;gt; Follow the Yellow Brick Road to the Emerald City. Pay no attention to the man behind the curtain. ...  注意在YAML中，字串不一定要用双引号标示。另外，在缩排中空白字符的数目并不是非常重要，只要相同阶层的元素左侧对齐就可以了。这个文件的的顶层由七个键值组成：其中一个键值&amp;rdquo;items&amp;rdquo;，是个两个元素构成的数组（或称列表），这列表中的两个元素同时也是包含了四个键值的哈希表。文件中重复的部分用这个方法处理：使用锚点（&amp;amp;）和参考（*）标签将&amp;rdquo;bill-to&amp;rdquo;哈希表的内容复制到&amp;rdquo;ship-to&amp;rdquo;哈希表。也可以在文件中加入可选的空行，以增加可读性。在一个文档中，可同时包含多个文件，并用&amp;rdquo;&amp;mdash;&amp;ldquo;分隔。可选的符号&amp;rdquo;&amp;hellip;&amp;ldquo;可以用来表示文档结尾（在利用串流的通讯中，这非常有用，可以在不关闭串流的情况下，发送结束讯号）。
语言的构成元素 YAML的基本元件 YAML提供缩排／区块以及内置（inline）两种格式，来表示列表和哈希表。以下展示几种YAML的基本原件。
列表（数组） 习惯上列表比较常用区块格式（block format）表示，也就是用短杠&#43;空白字符作为起始。
--- # 最喜爱的电影 - Casablanca - North by Northwest - Notorious  另外还有一种内置格式（inline format）可以选择──用方括号围住，并用逗号&#43;空白区隔（类似JSON的语法）
--- # 购物清单 [milk, pumpkin pie, eggs, juice]  哈希表 键值和数据由冒号及空白字符分开。
--- # 区块形式 name: John Smith age: 33 --- # 內置形式 {name: John Smith, age: 33}  区块的字符 再次强调，字串不需要包在引号之内。
保存新行(Newlines preserved) data: | #译者注：這是一首著名的五行民謠(limerick) There once was a man from Ealing #这里曾有一个来自伊灵的人 Who got on a bus bound to Darjeeling #他搭上一班往大吉岭的公車 It said on the door #门上这么说的 &amp;quot;Please don&#39;t spit on the floor&amp;quot; #&amp;quot;請勿在地上吐痰&amp;quot; So he carefully spat on the ceiling #所以他小心翼翼的吐在天花板上  根据设定，前方的引领空白符号（leading whitespace）必须排成条状，以便和其他数据或是行为（如范例中的缩排）明显区分。
折叠新行(Newlines folded) data: &amp;gt; Wrapped text #包裹的文字 will be folded #将会被收 into a single #进一个 paragraph #段落内 Blank lines denote #空白的行代表 paragraph breaks #分段符  和保存新行不同的是，换行字符会被转换成空白字符。而引领空白字符则会被自动消去。
元素的分层组合 于列表中使用哈希表 - {name: John Smith, age: 33} - name: Mary Smith age: 27  于哈希表中使用列表 men: [John Smith, Bill Jones] women: - Mary Smith - Susan Williams  YAML的进阶元件 这部分算是一个后续的讨论，在比较各种数数据列语言时，YAML最常被提到的特色有两个：结构和数据类型。
结构 YAML结构可以让多个文档存在一个文件中，为重复节点使用参考，使用任意节点作为键值。
数据锚节点和参考节点 为了维持文件的简洁，并避免数据输入的错误，YAML提供了结点锚节点(&amp;amp;)和参考节点(*)。参考会将结构加入锚点标记的内容，并可以在所有结构中可用（可以参考上面&amp;rdquo;ship-to&amp;rdquo;的范例） 下例是指令序列队列中，两个单步操作被重复使用而无需每次详细定义。
#眼部激光手术的序列器协议 --- - step: &amp;amp;id001 # 定义锚标签 &amp;amp;id001 instrument: Lasik 2000 pulseEnergy: 5.4 pulseDuration: 12 repetition: 1000 spotSize: 1mm - step: &amp;amp;id002 instrument: Lasik 2000 pulseEnergy: 5.0 pulseDuration: 10 repetition: 500 spotSize: 2mm - step: *id001 # 参考第一步 (使用锚标签 &amp;amp;id001) - step: *id002 # 参考第二部 - step: *id001 - step: *id002  数据类型 由于YAML自动监测简单数据类型，因此显式类型在大部分的YAML文件中很难看到。数据类型可以分成三大类：核心类型(core)、定义(defined)和用户定义(user-defined)。核心类型可自动被解析器分析（例如：浮点数，整数，字串，列表，哈希表，&amp;hellip;）。有一些高级的数据类型，例如二进制数据，在YAML规范中有被“定义”，但不是每一种解析器都支持。最后，YAML提供一种方式在本地扩展数据类型以接受用户自定义的类，结构或原型数据（例如：四倍精度的浮点数）。
强迫转型 YAML的自动判定实体是那种数据类型。但有时使用者会想要将数据显式转型成某种类型。最常见的状况是字串，有时候可能看起来像数字或布尔值，这种时候可以使用双引号，或是使用显式类型标签。
--- a: 123 # 整數 b: &amp;quot;123&amp;quot; # 字串（使用雙括號） c: 123.0 # 浮點數 d: !!float 123 # 浮點數，使用!!表达的显式数据类型 e: !!str 123 # 字串，使用显式数据类型 f: !!str Yes # 字串，使用显式数据类型 g: Yes # 布尔值&amp;quot;真&amp;quot; h: Yes we have No bananas # 字串（包含&amp;quot;Yes&amp;quot;和&amp;quot;No&amp;quot;）  其他特定数据类型 并非每个YAML实现都有每个特定数据类型。这些内建的类型需要在类型名称之前加上两个惊叹号前缀（!!）。包括集合（sets），有序映照（orderedmaps），时间戳（timestamps）以及十六进制数据（hexadecimal）等几种尤其有趣的特定数据类型在本文没有介绍。下面这个范例则是base64编码的二进制数据（binary）。
--- picture: !!binary | R0lGODlhDAAMAIQAAP//9/X 17unp5WZmZgAAAOfn515eXv Pz7Y6OjuDg4J&#43;fn5OTk6enp 56enmleECcgggoBADs=mZmE  使用者自定义数据类型扩充 许多YAML的实现允许使用者自定义数据类型。这是序列化对象的一个很不错的方法。本地数据类型不是通用数据类型，但是使用YAML解析器库定义在应用中。本地数据类型用单个惊叹号（!）表示。
--- myObject: !myClass { name: Joe, age: 15}  语法 在yaml.org（英文）可以找到简短的速查表及规范全本。下面的内容，是关于基本元件的摘要。
 YAML使用可打印的Unicode字符，UTF-8或UTF-16格式均可。 使用空白字符为文件缩排来表示结构；不过不能使用制表键(TAB)。 注解由井字号（ # ）开始，可以出现在一行中的任何位置，而且范围只有一行（也就是一般所谓的单行注解） 每个列表成员以单行表示，并用短杠 （ - ）起始。或使用方括号（ [] ），并用逗号&#43;空白（, ）分开成员。 每个哈希表的成员用冒号（ : ）分开键值和内容。或使用大括号（{ }），并用逗号&#43;空白（, ）分开。  哈希表的键值可以用问号 (? )起始，用来明确的表示多个词汇组成的键值。  字串平常并不使用引号，但必要的时候可以用双引号( &amp;ldquo; )或单引号( &amp;lsquo; )框住。  使用双引号表示字串时，可用倒斜线（ ** ）开始的转义字符（这跟C语言类似）表示特殊字符。  区块的字串用缩排和修饰词（非必要）来和其他数据分隔，有新行保留（preserve）（使用符号 |）或新行折叠（flod）（使用符号 &amp;gt; ）两种方式。 在单一数据流中，可用连续三个连字号（&amp;mdash;）区分多个文档。  另外，还有选择性的连续三个点号（ &amp;hellip; ）用来表示文档结尾。  重复的内容可使从参考标记星号 (***** )复制到锚点标记（ &amp;amp; ）。 指定格式可以使用两个惊叹号( !! )，后面接上名称。 文档中的单一文件可以使用指导指令，使用方法是百分比符号(% )。有两个指导指令在YAML1.1版中被定义：  %YAML 指令，用来识别文件的YAML版本。 %TAG 指令，被用作URI前缀的缩写。这个方法可能用于在节点类型的标签中。 YAML再使用逗号及冒号时，后面都必须接一个空白字符，这样包含标点的字串或数值（例如：5,280或http://www.wikipedia.org）就无需使用引号了。   另外还有两个特殊符号在YAML中被保留，有可能在未来的版本被使用：（@ ）和（ ` ）。
与其他数据序列化格式语言比较 虽然YAML是参考JSON，XML和SDL等语言，不过跟这些语言比起来，YAML仍有自己的特色。
JSON JSON的语法是YAML1.2版的基础，其发布就是为了使YAML全面兼容JSON，使JSON的语法称为YAML1.2版的子集。YAML之前的版本并不是严格兼容，但差异也不是很显著，因此大部分的JSON文件都可以被某些YAML解析器(例如Syck)解析。这是因为JSON的语法结构和YAML的内置格式相同。虽然扩展的分层也可以使用类似JSON的内置格式，除非其有助于增强文档可读性，否则YAML标准并不建议这样使用。YAML的许多扩展在JSON是找不到的，如：注释、高级数据类型、关系性锚节点、不带引号的字串、保留键值顺序的哈希表。
XML和SDL XML和SDL标签概念，在YAML中是找不到的。_对于数据结构序列化_，标签属性是一个存在问题的工具（尽管这是有争议的），将数据和元数据进行分离后当使用通用语言中的原生数据结构（如：哈希表、数组）进行表达时增加了复杂性。YAML则以数据类型的可扩展性作为替代（包括对象的类类型）。
YAML本身的规范中，并没有类似XML语言定义文档模式描述符（language-defined document schemadescriptors）──例如验证自己本身的结构是否正确的文件。 不过，用于YAML的外部定义的模式描述述语言（例如Doctrine、Kwalify和Rx）可以完成同样的功能。另外YAXML语言定义的类型声明提供的语义已经提供了足够的方式，来辨认YAML文件是否正确。YAXML──用XML描述YAML的结构──可以让XML模式与XSLT转换程式应用在YAML之上。
缩排划界 由于YAML的运作主要依赖大纲式的缩排来决定结构，这有效解决了界定符冲突（Delimitercollision）的问题。YAML的数据形态不依赖引号之特点，使的YAML文件可以利用区块，轻易的插入各种其他类型文件，如：XML、SDL、JSON，甚至插入另一篇YAML。
相反的，要将YAML置入XML或SDL中时，需要将所有空白字符和位势符号（potentialsigils，如：&amp;lt;、&amp;gt;和&amp;amp;）转换成实体语法；要将YAML置入JSON中，需要用引号框住，并转换内部的所有引号。
非分层的数据模型 跟SDL、JSON等，每个子结点只能有单一一个父节点的阶层是模型不同，YAML提供了一个简单的关系体制，可以从树状结构的其他地方，重复相同的数据，而不必显示那些冗余的结构。这点和XML中的IDRef类似。YAML解析器在将YAML转换成物件时，会自动将那些参考数据的结构展开，所以程式在使用时并不会查觉到哪些数据是解码自这种结构。XML则不会将这种结构展开。这种表示法可以增加程式的可读性，并且在那种‘大部分参数维持和上次相同，只有少数改变’的配置文件及通讯协议中可以减少数据输入错误。一个例子是：‘送货地点’和‘购买地点’在发票的纪录中几乎都是相同的数据。
实际的考量 YAML是“面向行的”，因此很容易将有程序的非结构化输出转换成YAML格式并保留大部分的原始文件之外观。因为无需匹配封闭的标签、括弧及引号，很容易通过一个不算精致的程序的输出命令生成格式良好的YAML文档。同样，空格分隔可让面向行的命令grep、Awk、perl、ruby和Python，在快捷地过滤YAML文件时更加方便。
特别是与标记语言不同的，连续的YAML区块往往本身都是格式良好的YAML文件。这使得很容易撰写那种“在开始提取的具体记录之前，不需要&amp;rsquo;读取全部文件内容&amp;rsquo;”的解析器（通常需要匹配的起始和关闭标签、寻找引号和转移字符）。当处理一个单一静态的，整个存在内存中的数据结构将很大，或为提取一个项目来重建的整个结构，代价相当昂贵的记录档，这种特性是相当方便的。
违反直观的是，尽管它的缩排方式似乎更加复杂化了嵌套层次，YAML将缩排视为一个单一的空白，这可能会取得比其他标记语言更好的压缩比。此外，可以通过下列方式避免极深的缩排：(1)使用“内置格式”（即简称类JSON格式）而无缩排；或(2)使用关联锚点展开阶层以形成一个摊平的格式，使得YAML解析器能透明地重组成完整的数据结构。
安全性 YAML是纯粹用来表达数据的语言，所以内部不会存代码注射的可执行命令。这代表解析器会相当（至少）安全的解析文件，而不用担心潜在与执行命令相关的安全漏洞。举例来说，JSON是JavaScript的子集，使用JavaScript本身的解析器是相当诱人的，不过也造成许多代码注射的漏洞。虽然在所有数据序列化格式语言中，安全解析本质上是可能的，但可执行性却正是这样一个恶名昭彰的缺陷；而YAML缺乏相关的命令语言，可能是一个相对安全的利益。
然而，YAML允许语言特定的标签，因此支持这些标签的解析器能够创建任意本地对象。任何允许复杂对象实例可被执行的YAML解析器将会受潜在的代码注射攻击。
数据处理和呈现 XML和YAML规范为数据结点的展现、处理及储存提供了不同的_逻辑_模型。 XML: XML文档中的主要逻辑结构是: 1) 元素，2)属性。对于这些基本逻辑结构，基础XML规范对于元素重复性或显示顺序等因素没有定义约束。在XML处理器的定义一致性上，XML规范仅将其归为两类:1) 有效，2) 无效。XML没有为API、处理模型或数据展现模型维护详细定义；尽管某些在用户或规范实现可以独立选择的其他规范中有定义，例如文档对象模型和XQuery。 对于定义有效XML内容的更丰富的模型是W3CXML模式标准。它允许对有效XML内容进行完整规范并被开源、免费和商业处理器和类库广泛支持。 YAML: YAML文档中的主要逻辑结构是: 1) 标量， 2) 列表， 3) 哈希表。YAML规范也指出了施用于这些基本罗结构的基本约束。例如，哈希表键值是无序的。当节点顺序很重要的时候，必须使用列表。 而且，在定义L处理器的定义一致性上，YAML规范定义了两种基本操作: 1) 转储； 2)加载。所有YAML兼容处理器必须提供至少一个操作，并可选地提供者两种操作。最后，YAML规范定义了在转储和加载操作处理时所必须创建的信息模型或“表现图”，尽管这些表现不需要通过API被用户访问。
实现与函数库 移植性 简单的YAML文档（例如：简单的键值对）不需要完整的YAML解析器，便可以被正则表达式解析。许多常用的编程语言──纯用某个语言，让函数库具有可携性──都有的YAML的产生器和解析器。当效能比较重要时，也有许多和C语言绑定的函数库可使用。
C语言函数库  libYAML2007-06时，这个YAML的函数库渐趋稳定，并被YAML格式作者推荐使用。 SYCK这个实现支援大部分1.0版的格式，并且被广泛的使用。它使用高阶interpretedlanguages进行最佳化。在2005之后，这个专案已经不再更新，不过仍可使用。  其他函数库 下面几种编程语言都有与YAML相关的函数库 - Perl - YAML::一个通用的界面，被数个YAML解析器使用。 - YAML::TinyYAML简化版的实现。拥有小巧轻快的优点──比完整功能的YAML实现快上许多──并用纯Perl写成。 - YAML::Syck与SYCK函数库绑定。提供快速，highly featured的YAML解析器。 - YAML::XS与LibYaml绑定。提供1.1版更好的相容性。 - PHP - Spyc 纯PHP的实现。 - PHP-Syck（与SYCK函数库绑定） - sfYaml为symfony项目重写的Spyc,可独立使用， 可以产生和解析YAML文件。 - Python - PyYaml纯Python，或可选用LibYAML的函数库。 - PySyck与SYCK绑定。 - Ruby（从1.8版开始，YAML解析器成为标准函数库之一。以SYCK为基础。） - Ya2YAMLwith full UTF-8support - Java - jvyaml 以Syck为基础，andpatterned off ruby-yaml - JYaml纯Java的实现。 - R - CRAN YAML 以SYCK为基础。 - JavaScript - 原生的JavaScript即可产生YAML，但不能解析。 - YAML JavaScript 产生和解析。 - .NET - [1] - OCaml - OCaml-Syck - C&#43;&#43; - 用C&#43;&#43;将libYaml包装 - Objective-C - Cocoa-Syck - Lua - Lua-Syck - Haskell - Haskell Reference wrappers - XMLYAXML (currently draft only) - Go - go-yaml 借鉴 C 库libyaml 设计的 Go 语言移植
</content>
    </entry>
    
     <entry>
        <title>[Gradle] 在子项目中共享项目属性</title>
        <url>https://mryqu.github.io/post/gradle_%E5%9C%A8%E5%AD%90%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%85%B1%E4%BA%AB%E9%A1%B9%E7%9B%AE%E5%B1%9E%E6%80%A7/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>ext</tag><tag>share</tag><tag>property</tag><tag>variable</tag>
        </tags>
        <content type="html"> build.gradle:
buildscript { repositories { mavenCentral() } } subprojects { apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 ext { HadoopVersion = &#39;2.7.x&#39; JUnitVersion = &#39;4.11&#39; ...... } }  HelloHadoopClient/build.gradle：
jar { baseName = &#39;hello-hadoopclient&#39; version = &#39;0.1.0&#39; } dependencies { compile &amp;quot;org.apache.hadoop:hadoop-common:${HadoopVersion}&amp;quot; testCompile &amp;quot;junit:junit:${JUnitVersion}&amp;quot; }  HelloMapReduce/build.gradle：
jar { baseName = &#39;hello-mapreduce&#39; version = &#39;0.1.0&#39; } dependencies { compile &amp;quot;org.apache.hadoop:hadoop-common:${HadoopVersion}&amp;quot; compile &amp;quot;org.apache.hadoop:hadoop-mapreduce-client-jobclient:${HadoopVersion}&amp;quot; testCompile &amp;quot;junit:junit:${JUnitVersion}&amp;quot; testCompile &amp;quot;org.apache.mrunit:mrunit:${MRUnitVersion}:hadoop2&amp;quot; }  通过在子项目共享项目属性HadoopVersion，所有子项目全都依赖一个版本的Hadoop库了。当Hadoop库版本需要更新时，仅修改根项目的build.gradle即可。
</content>
    </entry>
    
     <entry>
        <title>了解一下io域名</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8Bio%E5%9F%9F%E5%90%8D/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>io</tag><tag>域名</tag>
        </tags>
        <content type="html"> 对我来说最熟悉的io域名莫过于spring.io，最近发现很多开源项目的主页也是io域名。好奇了一把，对io域名稍作一点了解。
域名的种类其实是非常多的，我们大多最熟悉COM域名、CN域名、NET域名等常见的通用域名。但是随着通用域名中好的域名注册资源的日益减少，这就使得人们不得不扩大域名的注册范围了，这期间io域名的出现就很快的进入域名注册者的视线。
io是英属印度洋领地（British Indian OceanTerritory，BIOT）的简写，英属印度洋领地是英国在印度洋的海外領地，包含了查戈斯群岛（ChagosArchipelago）及总数达2300个大大小小的热带岛屿，总土地面积約60平方公里，周边水域面积54400平方公里。io域名为英属印度洋国家顶级域名后缀，于1997年分配使用。全球任何公司、团体和个人均可注册英属印度洋.io域名。
而且io还可以作为[inputoutput]理解，即“输入输出接口”的意思。同时，对全世界所有的人来说，.io也是互联网上能够用来表示信息、知识的最直接、最直观的符号。
这一系列的优势使得io域名价值正在与日俱增真正的全球域名新贵，为因为通用域名注册资源日益减少的拥挤不堪的域名世界开创了一片崭新的天地，为广大域名用户提供多样的选择，而io域名在注册资格上没有任何的限制，这就使得任何一个国家的企业或者个人都可以注册，这就更让io域名受到广大国际用户的支持。
</content>
    </entry>
    
     <entry>
        <title>[HBase] 查看ZooKeeper服务器</title>
        <url>https://mryqu.github.io/post/hbase_%E6%9F%A5%E7%9C%8Bzookeeper%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>zookeeper</tag><tag>zkservertool</tag>
        </tags>
        <content type="html"> 使用hbaseorg.apache.hadoop.hbase.zookeeper.ZKServerTool可以很方便查看HBase所使用的ZK服务器列表。 </content>
    </entry>
    
     <entry>
        <title>Apt-get代理配置</title>
        <url>https://mryqu.github.io/post/apt-get%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>apt-get</tag><tag>dns</tag><tag>source</tag><tag>proxy</tag><tag>ubuntu</tag>
        </tags>
        <content type="html">  在公司安装Ubuntu docker后使用apt-get update总是失败，经历了一番周折才成功。
DNS？ 一开始怀疑是DNS问题，可以学习了下面几个帖子： - Docker apt-get update fails - Docker - Network calls fail during image build on corporate network - How do I set my DNS on Ubuntu 14.04?
检查我ubuntu配置：
 cat /etc/resolv.conf  确认DNS没有问题。
Ubuntu官方服务器？ 是不是我的机器连不上欧美的Ubuntu官方服务器，换成中国服务器试试。尝试了Ubuntu 14.04服务器列表上的中国服务器还是不成。
Apt-get代理？ 照着how to install packages with apt-get on a system connected via proxy?设置一番，成功了
设置/etc/apt/apt.conf：
Acquire::http::proxy &amp;quot;http://yourServer:yourPort/&amp;quot;; Acquire::ftp::proxy &amp;quot;ftp://yourServer:yourPort/&amp;quot;; Acquire::https::proxy &amp;quot;https://yourServer:yourPort/&amp;quot;;  如需用户名、密码，则作如下修改：
Acquire::http::proxy &amp;quot;http://yourUsr:yourPwd@yourServer:yourPort/&amp;quot;; Acquire::ftp::proxy &amp;quot;ftp://yourUsr:yourPwd@yourServer:yourPort/&amp;quot;; Acquire::https::proxy &amp;quot;https://yourUsr:yourPwd@yourServer:yourPort/&amp;quot;;  最好将上述配置也存入/etc/apt/apt.conf.d/80proxy中，这样版本升级后这些变更也不会丢。
</content>
    </entry>
    
     <entry>
        <title>JS 库/UI 积累贴</title>
        <url>https://mryqu.github.io/post/js_%E5%BA%93ui_%E7%A7%AF%E7%B4%AF%E8%B4%B4/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>bootstrap</tag><tag>javascript</tag><tag>ui</tag>
        </tags>
        <content type="html"> Bootstrap库： jQuery File Upload Demo
Bootstrap table：示例不错
UI： codrops/TooltipStylesInspiration：工具提示做的很炫
OpenUI5： Welcome to 30 Days of UI5!
UI Framework related
</content>
    </entry>
    
     <entry>
        <title>在Ubuntu中强制Apt-get使用IPv4或IPv6</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8ubuntu%E4%B8%AD%E5%BC%BA%E5%88%B6apt-get%E4%BD%BF%E7%94%A8ipv4%E6%88%96ipv6/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>ubuntu</tag><tag>debian</tag><tag>apt-get</tag><tag>ipv4</tag><tag>ipv6</tag>
        </tags>
        <content type="html">  快速命令行选项 如果只想一次使apt-get使用IPv4或IPv6，使用下列步骤。该功能尽在apt-get的0.9.7.9~exp1版本后可用。首先，通过如下命令确认apt-get版本高于0.9.7.9~exp1：
apt-get --version  结果近似于:
apt 1.0.1ubuntu2 for amd64 compiled on Oct 28 2014 20:55:14  版本核实后，可以通过如下命令强制使用IPv4:
apt-get -o Acquire::ForceIPv4=true update  或IPv6:
apt-get -o Acquire::ForceIPv6=true update  这会将_sources.list_中的URL仅解析成IPv4并更新仓库。
持久化的选项 为了让设置持久化，在/etc/apt/apt.conf.d/下创建99force-ipv4文件。
sudoedit /etc/apt/apt.conf.d/99force-ipv4  在该文件放入如下内容：
Acquire::ForceIPv4 &amp;quot;true&amp;quot;;  保存文件即可。如果相反想强制使用IPv6，将文件名及其内容中的4改成6即可。
原文: https://www.vultr.com/docs/force-apt-get-to-ipv4-or-ipv6-on-ubuntu-or-debian
</content>
    </entry>
    
     <entry>
        <title>尝试boot2docker和Vagrant-boot2docker box</title>
        <url>https://mryqu.github.io/post/%E5%B0%9D%E8%AF%95boot2docker%E5%92%8Cvagrant-boot2docker_box/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>boot2docker</tag><tag>vagrant</tag><tag>docker</tag>
        </tags>
        <content type="html">  boot2docker boot2docker是基于Tiny Core Linux的轻量级Linux发布版本虚拟机，专用于运行Docker容器。 功能如下： - 3.18.5内核及AUFS文件系统、Docker 1.5.0 - 容器通过磁盘自动加载在/var/lib/docker目录持久化 - SSH密钥通过磁盘自动加载进行持久化 - 容易访问Docker映射端口的主机模式（Host-only）
Vagrant兼容的boot2docker box Vagrant创始人Mitchell Hashimoto使用boot2docker虚拟机创建了一个可被VirtualBox和VMware提供者支持的Vagrant box。当Vagrant被运行于Linux之外的操作系统时，Vagrant的Docker提供者默认使用boot2dockerbox提供Docker功能。 参考 boot2docker官网
GitHub:boot2docker
GitHub:boot2docker-cli
GitHub:mitchellh/boot2docker-vagrant-box
yungsang/boot2docker
GitHub:yungsang/boot2docker
Using Docker with Vagrant
Setting up a development environment using Docker and Vagrant
Docker in OSX via boot2docker or Vagrant: getting over the hump
</content>
    </entry>
    
     <entry>
        <title>了解一下Gerrit与BitBucket集成</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8Bgerrit%E4%B8%8Ebitbucket%E9%9B%86%E6%88%90/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>gerrit</tag><tag>bitbucket</tag><tag>atlassian</tag><tag>plugin</tag><tag>integration</tag>
        </tags>
        <content type="html"> 在《Gerrit插件配置》中没有找到有关BitBucket的信息。 通过https://marketplace.atlassian.com/search?q=Gerrit在Atlassian市场搜到几个跟Gerrit相关的控件，不过都是为JIRA/BitBucket/Bambooserve增加类似Gerrit的工作流的功能。 结论就是：目前没法用Gerrit为BitBucket进行代码审查。
</content>
    </entry>
    
     <entry>
        <title>了解用于Gerrit代码审查的GitHub插件</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3%E7%94%A8%E4%BA%8Egerrit%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5%E7%9A%84github%E6%8F%92%E4%BB%B6/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>gerrit</tag><tag>github</tag><tag>codereview</tag><tag>integration</tag><tag>win7</tag>
        </tags>
        <content type="html">  在网上看到了GitHub plugin for Gerrit，学习一下。
对比GitHub与Gerrit的代码审查机制 GitHub一派的代码审查机制主要通过fork一个远程分支，进行本地修改并提交到远程分支，然后通过PULL REQUEST来请求代码审查及合并回原上游远程分支。 Gerrit一派的代码审查机制主要通过checkout一个分支(refs/for/master)。从Gerrit克隆获得本地分支，进行修改并提交到Gerrit的refs/for/master分支，中间还可以通过Amend commit修改之前的提交，经过评审人批准后，代码会提交到&amp;rdquo;权威&amp;rdquo;仓库。 GitHub BitBucket GitLab Gitorious阵营 这一派的PULL REQUEST基于两个分支的合并，注释可能会乱一点，有点惹人烦。不考虑将所有原子/相关修改作为一个提交。除了写注释无法知道审查打分情况。 Gerrit GitBlit阵营 这一派的每个提交有其审查结果，可以清晰查看以往历史。Gerrit审查可以强制成仅接受快进（fast-worward）或可rebase的提交。 用于Gerrit代码审查的GitHub插件 https://gerrit-review.googlesource.com/#/admin/projects/plugins/github 优点： - 引入Pull Requests -&amp;gt;Gerrit改动/主题 - 使用Gerrit认证规则重用GitHub账户 - 复制: 代码继续存在于http://github.com 仓库 - 防止不可管理的fork激增 - 避免GitHub垃圾邮件 -&amp;gt;每个改动一封电邮
第一步：为Gerrit在GitHub上注册新的OAUTH应用 第二步：获取Client ID和Client Secret 第三步：下载并安装Gerrit 下载地址：https://gerrit-releases.storage.googleapis.com/index.html 为了确保安装成功，首先使用DEVELOPMENT_BECOME_ANY_ACCOUNT作为认证方式确保能登录进Gerrit。 使用Git Bash启动Gerrit。 登陆后，可以查看到当前安装的插件。 第四步：构建GitHub插件 git clone https://gerrit.googlesource.com/plugins/github &amp;amp;&amp;amp; cd github mvn install  第五步：安装OAUTH过滤器和GitHub插件 第六步：重新配置Gerrit 第七步：完成GitHub认证 参考 GitHub plugin for Gerrit
Gerrit vs Github: for code review and codebase management
GerritHub
Gerrit Code Review or Github’s fork and pull ? Take both !
Gerrit Code Review - Configuration
Gerrit Code Review - Plugin Install
GitHub configuration during Gerrit init
Config Gerrit Server Behind Apache Https Reverse-proxy
</content>
    </entry>
    
     <entry>
        <title>数据科学的战争：R vs Python</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%9A%84%E6%88%98%E4%BA%89r_vs_python/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>r</tag><tag>python</tag><tag>对比</tag><tag>数据科学</tag><tag>统计</tag>
        </tags>
        <content type="html"> R和Python都是用于数据分析任务的流行编程语言，都有各自的拥拓者和反对者。Python经常作为语法简单易懂的通用编程语言广受赞誉。在人们心中，R的功能是由统计学家开发的，因此具有特定领域优势，例如数据可视化上具有的大量功能。 DataCamp上有一篇帖子Choosing R or Python for data analysis? An infographic以信息图的方式从数据科学和统计的角度详细对比了R和Python这两种编程语言。 </content>
    </entry>
    
     <entry>
        <title>了解构件仓库管理器Artifactory和Nexus</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3%E6%9E%84%E4%BB%B6%E4%BB%93%E5%BA%93%E7%AE%A1%E7%90%86%E5%99%A8artifactory%E5%92%8Cnexus/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>artifactory</tag><tag>nexus</tag><tag>repository</tag><tag>仓库服务器</tag><tag>技术选型</tag>
        </tags>
        <content type="html">  使用Maven，可以从Maven中央仓库下载所需要的构件（artifact），但这通常不是一个好的做法，一般是在企业内部架设一个Maven仓库服务器，在代理远程仓库的同时维护本地仓库，以节省带宽和时间。企业仓库管理器一般可以提供高并发访问、浏览和查询、报表、访问控制、备份、对其他仓库进行代理、RESTAPI等特性 了解一下构件仓库管理器，市场上最好的是JFrog的Artifactory和Sonatype的Nexus，而且这两个产品既有商业版也有免费社区版。 以Artifactory为例，Ant&#43;Ivy、Maven和Gradle这些构建工具都可以自动下载Artifactory里的构件（artifact），此外Jenkins、Bamboo等CI工具也可以通过构建工具将生成的构件（artifact）部署到Artifactory上。 如果将构建结果部署到Artifactory，需要对Maven构建增加如下选项：
deploy -DaltDeploymentRepository=snapshots::default::http://svcartifact.yqu.com:8081/artifactory/snapshots  如果将release构建结果部署到Artifactory，需要对Maven构建增加如下选项：
deploy -DaltDeploymentRepository=release::default::http://svcartifact.yqu.com:8081/artifactory/release  或者在pom.xml中内嵌distributionManagement： 最近网上有一个不错的帖子 Maven Repository Manager Feature Matrix，对比了Archiva、Artifactory和Nexus的功能和价格，可供有需要做Maven仓库管理器技术选型的同学借鉴。
参考 JFrog Artifactory官网
Sonatype Nexus官网
Artifactory – 1 Min Setup
Apache Maven Deploy Plugin
</content>
    </entry>
    
     <entry>
        <title>Swagger实践和总结</title>
        <url>https://mryqu.github.io/post/swagger%E5%AE%9E%E8%B7%B5%E5%92%8C%E6%80%BB%E7%BB%93/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Swagger</category>
        </categories>
        <tags>
          <tag>swagger</tag><tag>rest</tag><tag>api</tag><tag>文档</tag><tag>自动化</tag>
        </tags>
        <content type="html">  Swagger学习和实践 最近安装并使用了一下Swagger-ui、Swagger-editor和Swagger-codegen，感觉还不错。 Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web服务。Swagger的目标是对RESTAPI定义一个标准的和语言无关的接口，可让人和计算机无需访问源码、文档或网络流量监测就可以发现和理解服务的能力。当通过Swagger进行正确定义，用户可以理解远程服务并使用最少实现逻辑与远程服务进行交互。与为底层编程所实现的接口类似，Swagger消除了调用服务时可能会有的猜测。 Swagger是一组开源项目，其中主要要项目如下： - Swagger-tools:提供各种与Swagger进行集成和交互的工具。例如模式检验、Swagger1.2文档转换成Swagger 2.0文档等功能。 - Swagger-core:用于、Servlets和Play框架进行集成。 - Swagger-js:用于JavaScript的Swagger实现。 - Swagger-node-express:Swagger模块，用于node.js的Express web应用框架。 - Swagger-ui：一个无依赖的HTML、JS和CSS集合，可以为Swagger兼容API动态生成优雅文档。 - Swagger-codegen：一个模板驱动引擎，通过分析用户Swagger资源声明以各种语言生成客户端代码。
 C:\tools\swagger-codegen&amp;gt;mvn package C:\tools\swagger-codegen\modules\swagger-codegen-cli&amp;gt;mvn package C:\tools\swagger-codegen\modules\swagger-generator&amp;gt;mvn package C:\tools\swagger-codegen&amp;gt;java -jar modules/swagger-codegen-cli/target/swagger-codegen-cli.jar generate -i http://petstore.swagger.io/v2/swagger.json -l spring-mvc -o yqu/petstore/spring-mvc C:\tools\swagger-codegen\yqu\petstore\spring-mvc&amp;gt;mvn package  上述操作通过底层使用SpringFox库，会创建带有Swagger注释的SpringMVC框架代码，包括Controller和DTO类。这样将Swagger-ui部署到Web应用内，就可以通过http://server:8002/v2/sdoc.jsp 在线访问API文档了。 - Swagger-editor：可让使用者在浏览器里以YAML格式编辑SwaggerAPI规范并实时预览文档。可以生成有效的SwaggerJSON描述，并用于所有Swagger工具（代码生成、文档等等）中。 除了Swagger项目自身支持的Java、Scala和JavaScript语言，Swagger社区中还提供了很多支持其他语言的第三方工具，覆盖了Clojure、ColdFusion/ CFML、Eiffel、Go、Groovy、.Net、Perl、PHP、Python、Ruby等各种编程语言。
Swagger总结 Swagger这类API文档工具可以满足下列需求： - 支持API自动生成同步的在线文档 - 这些文档可用于项目内部API审核 - 方便测试人员了解API - 这些文档可作为客户产品文档的一部分进行发布 - 支持API规范生成代码，生成的客户端和服务器端骨架代码可以加速开发和测试速度
跟下列其他API文档工具相比，Swagger各有优缺点，但它功能最多、也是最流行的。 - RESTful API Modeling Language (RAML) - apiary的API Blueprint - I/O Docs - Web Application Description Language (WADL)
参考 Swagger官网
GitHub：Swagger
Swagger规范
SpringFox官网
GitHub：SpringFox
Spring Boot &amp;amp; Swagger UI
[](http://blog.csdn.net/u010827436/article/details/44417637)
</content>
    </entry>
    
     <entry>
        <title>[Spring Boot] 访问JSP</title>
        <url>https://mryqu.github.io/post/spring_boot_%E8%AE%BF%E9%97%AEjsp/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>boot</tag><tag>jsp</tag><tag>tomcat-embed-jasper</tag><tag>jstl</tag>
        </tags>
        <content type="html">  需求 我的Spring Boot web应用中用到了JSP，可是访问始终404。
@Controller public class TestController { @RequestMapping(&amp;quot;/test&amp;quot;) public String webapp(Map model) { return &amp;quot;WEB-INF/index.jsp&amp;quot;; } }  解决方案是增加tomcat-embed-jasper依赖，此外可选性地增加了jstl依赖。
Gradle dependencies { ...... // jsps providedRuntime (&#39;org.apache.tomcat.embed:tomcat-embed-jasper&#39;) }  Maven </content>
    </entry>
    
     <entry>
        <title>定制Vagrant box主机</title>
        <url>https://mryqu.github.io/post/%E5%AE%9A%E5%88%B6vagrant_box%E4%B8%BB%E6%9C%BA/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>box</tag><tag>host</tag>
        </tags>
        <content type="html">  做个笔记，记录一下如何定制Vagrant box主机。 - 在Vagrantfile中通过一个Vagrant基础box定制box。 - 启动该定制box后，通过Vagrant package命令输出box文件 - 编写JSON格式的box元数据文件 - 将定制box文件及其元数据文件放到web服务器中 - 将此定制box作为基础box的Vagrantfile中，设置如下 - config.vm.box用于匹配上述定制box的名称 - config.vm.box_url为box文件的URL或box元数据文件的URL当config.vm.box_url为box文件的URL，该box文件即为基础box；当config.vm.box_url为box元数据文件的URL，可以使用config.vm.box指定名称的某一版本box文件（有config.vm.box_version参数，即使用其约束的特定版本；否则，使用最新版本box文件）作为基础box。 - config.vm.box_version指定box的特定版本。
参考 Custom Vagrant Cloud Versioned Box Host
Vagrant: CREATING A BASE BOX
Vagrant: MACHINE SETTINGS
</content>
    </entry>
    
     <entry>
        <title>找不到TTY而导致的Vagrant destroy失败</title>
        <url>https://mryqu.github.io/post/%E6%89%BE%E4%B8%8D%E5%88%B0tty%E8%80%8C%E5%AF%BC%E8%87%B4%E7%9A%84vagrant_destroy%E5%A4%B1%E8%B4%A5/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>tty</tag><tag>required</tag><tag>devops</tag>
        </tags>
        <content type="html"> 在Windows的Git Bash上想要够过vagrant destroy命令删除一个Vagrant虚拟机，结果碰到了这个错误：  Vagrant is attempting to interface with the UI in a way that requires a TTY. Most actions in Vagrant that require a TTY have configuration switches to disable this requirement. Please do that or run Vagrant with TTY. 
据说Windows上的Cygwin以一种奇怪的方式处理stdin导致Ruby以为没有TTY，没想到Git Bash所基于的MinGW也有这样的问题。 权变措施是使用vagrant destroy --force。
[](http://stackoverflow.com/questions/23633276/vagrant-is-attempting-to-interface-with-the-ui-in-a-way-that-requires-a-tty)
[](https://groups.google.com/forum/#!msg/vagrant-up/ExFet5jMomU/T9FiZluf4ggJ)
</content>
    </entry>
    
     <entry>
        <title>遭遇由VT-x设置导致的vagrant up失败</title>
        <url>https://mryqu.github.io/post/%E9%81%AD%E9%81%87%E7%94%B1vt-x%E8%AE%BE%E7%BD%AE%E5%AF%BC%E8%87%B4%E7%9A%84vagrant_up%E5%A4%B1%E8%B4%A5/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>vt-x</tag><tag>invalid_state</tag><tag>devops</tag>
        </tags>
        <content type="html">  我在同事申请的一台机器上安装Vagrant box，结果vagrant up失败，报如下错误：  The guest machine entered an invalid state while waiting for it to boot. Valid states are &amp;lsquo;starting, running&amp;rsquo;. The machine is in the &amp;lsquo;poweroff&amp;rsquo; state. Please verify everything is configured properly and try again. If the provider you&amp;rsquo;re using has a GUI that comes with it, it is often helpful to open that and watch the machine, since the GUI often has more helpful error messages than Vagrant can retrieve. For example, if you&amp;rsquo;re using VirtualBox, run vagrant up while the VirtualBox GUI is open. 
打开VirtualBox界面，重新执行vagrant up，然后在VirtualBox里获取日志，显示如下错误： Failed toopen a session for the virtual machine XXXXXXXXXXXXXXXX.VT-x is not available.(VERR_VMX_NO_VMX).
对于64位虚拟机，没有VT扩展的话，Virtualbox不支持64位客户机。解决方案是在BIOS使能Intel虚拟技术（VT-X）。 如果宿主机不支持VT-X，网上有一个解决方案（未经本人验证）：
config.vm.provider :virtualbox do |vb| vb.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--hwvirtex&amp;quot;, &amp;quot;off&amp;quot;] end  其作用等同于VirtualBox的vbox配置文件中的： 此外如果vagrant内部出现问题，一个很好的调试方法是：
VAGRANT_LOG=debug vagrant up  参考 Guest Machine Entered Invalid State #220
Vagrant Up Error In Headless Ubuntu: The guest machine entered an invalid state while waiting for it to boot
Error VT-x not available for Vagrant machine inside Virtualbox
Vagrant › VT-x is not available.
Enabling Intel VT-x and AMD-V virtualization hardware extensions in BIOS
Disable VT-X in Vagrantfile
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 示例：Accordion with all initial collapsed sections</title>
        <url>https://mryqu.github.io/post/openui5_%E7%A4%BA%E4%BE%8Baccordion_with_all_initial_collapsed_sections/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>accordion</tag><tag>coolapsed</tag><tag>section</tag>
        </tags>
        <content type="html"> sap.ui.commons.Accordion会设置一个默认展开的section。
sap.ui.commons.Accordion.prototype.addSection = function(oSection) { this.addAggregation(&amp;quot;sections&amp;quot;, oSection); //Add a default opened section id if ( (this.getOpenedSectionsId() == null || this.getOpenedSectionsId() == &amp;quot;&amp;quot; ) &amp;amp;&amp;amp; oSection.getEnabled()){ this.setOpenedSectionsId(oSection.getId()); } this.aSectionTitles.push(oSection.getTitle()); };  如果想让初始化所有section为折叠的，只要将openedSectionsId设为“-1”就可以了。 示例位置: http://jsbin.com/sajoba/1/edit?html,output </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 控件ID实践与总结</title>
        <url>https://mryqu.github.io/post/openui5_%E6%8E%A7%E4%BB%B6id%E5%AE%9E%E8%B7%B5%E4%B8%8E%E6%80%BB%E7%BB%93/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>control</tag><tag>id</tag><tag>practice</tag>
        </tags>
        <content type="html">  显式定义而不是自生成OpenUI5控件ID 为了便于开发和测试，为控件设置一个便于理解的ID尤为重要。我的博文《快速定位OpenUI5问题的一个方法》中的工具函数就是利用控件ID快速定位故障控件的。 在OpenUI5中，可在创建控件实例时使用JSON对象作为控件构造器参数。其中一个可选属性就是&amp;rdquo;id&amp;rdquo;，OpenUI5不仅用它（在&amp;rdquo;注册信息&amp;rdquo;中）追踪控件，也用在渲染控件的DOM输出。 如果没有显式指定一个控件的ID，OpenUI5框架就会使用基于实例数量的算法自生成控件ID。 自生成ID有两个缺点： - 调试的时候，不容易定位使用控件的代码位置。例如，异常跟某个控件相关，如果该类型控件实例很多，很难定位该控件定义在那个视图里。 - 测试代码相对显式定义ID更加难写。如果对控件使用显式定义ID，相应的测试代码可以很容易通过该ID进行控件查找或验证。
控件ID命名惯例 使用驼峰式写法、有意义且语法正确的ID来反映控件的本质。 例如： - 一个表单上的提交按钮，其id=&amp;ldquo;submit&amp;rdquo; - 到不同图形设置的导航控件，其id=&amp;ldquo;graphNav&amp;rdquo;
OpenUI5控件ID内幕 sap.ui.base.ManagedObject是OpenUI5框架包括控件在内的大部分类的父类，它的构造器里有对ID的处理：
if (!sId) { sId = this.getMetadata().uid() || jQuery.sap.uid(); } else { var preprocessor = ManagedObject._fnIdPreprocessor; sId = (preprocessor ? preprocessor.call(this, sId) : sId); var oType = DataType.getType(&amp;quot;sap.ui.core.ID&amp;quot;); if (!oType.isValid(sId)) { throw new Error(&amp;quot;\&amp;quot;&amp;quot; &#43; sId &#43; &amp;quot;\&amp;quot; is not a valid ID.&amp;quot;); } } this.sId = sId;  sap.ui.base.ManagedObjectMetadata的ID生成代码：
(function() { var mUIDCounts = {}; function uid(sId) { jQuery.sap.assert(!/[0-9]&#43;$/.exec(sId), &amp;quot;AutoId Prefixes must not end with numbers&amp;quot;); sId = sap.ui.getCore().getConfiguration().getUIDPrefix() &#43; sId; // 初始化计数器 mUIDCounts[sId] = mUIDCounts[sId] || 0; // 组合 前缀&#43;计数器 // 由于不许sId结尾为数字，因此sId和计数器的连接串是安全的! return (sId &#43; mUIDCounts[sId]&#43;&#43;); } ManagedObjectMetadata.uid = uid; ManagedObjectMetadata.prototype.uid = function() { var sId = this._sUIDToken; if ( typeof sId !== &amp;quot;string&amp;quot; ) { // 从全类名开始操作 sId = this.getName(); // 降为不带命名空间的类名 sId = sId.slice(sId.lastIndexOf(&#39;.&#39;)&#43;1); // 降为一个驼峰式写法, 多个词仅取最后一个 sId = sId.replace(/([a-z])([A-Z])/g, &amp;quot;$1 $2&amp;quot;).split(&amp;quot; &amp;quot;).slice(-1)[0]; // 删除不要的字符(结尾无数字!)并转换为小写 sId = this._sUIDToken = sId.replace(/([^A-Za-z0-9-_.:])|([0-9]&#43;$)/g,&amp;quot;&amp;quot;).toLowerCase(); } return uid(sId); }; }());  所有带元数据的ManagedObject对象，都采用这种生成方式。示例如下： 不带元数据的ManagedObject对象，采用jQuery.sap.uid()方法生成ID，比上一种缺少了类信息。
var iIdCounter = 0; jQuery.sap.uid = function uid() { return &amp;quot;id-&amp;quot; &#43; new Date().valueOf() &#43; &amp;quot;-&amp;quot; &#43; iIdCounter&#43;&#43;; };  控件ID最好包含父控件ID 例如在名为&amp;rdquo;Update.view.xml&amp;rdquo;的xml视图文件中定义一个图像控件： 图像相应的DOM输出为： DOM输出中的id和data-sap-id属性为父视图名和给定控件Id的连接串。同样在Javascript代码中，要巧妙利用creatId方法使控件ID为视图/UI组件/HTML片段的Id和给定控件Id的连接串。
sap.ui.core.Fragment.prototype.createId = function(sId) { // no ID Prefixing by Fragments! This is called by the template parsers, // but only if there is not a View which defines the prefix. var id = this._sExplicitId ? this._sExplicitId &#43; &amp;quot;--&amp;quot; &#43; sId : sId; if (this._oContainingView &amp;amp;&amp;amp; this._oContainingView != this) { // if Fragment ID is added to the control ID and Fragment ID already // contains the View prefix, the View prefix does not need to be added again // (this will now be checked inside the createId function already!) id = this._oContainingView.createId(id); } return id; }; sap.ui.core.mvc.View.prototype.createId = function(sId) { if (!this.isPrefixedId(sId)) { // views have 2 dashes as separator, components 3 and controls/elements 1 sId = this.getId() &#43; &amp;quot;--&amp;quot; &#43; sId; } return sId; }; sap.ui.core.mvc.Controller.prototype.createId = function(sId) { return this.oView ? this.oView.createId(sId): undefined; }; sap.ui.core.UIComponent.prototype.createId = function(sId) { if (!this.isPrefixedId(sId)) { // components have 3 dashes as separator, views 2 and controls/elements 1 sId = this.getId() &#43; &amp;quot;---&amp;quot; &#43; sId; } return sId; };  从上述代码可知，OpenUI5的惯例是： - UI组件使用两个连接符作为分隔符 - 视图/控件和HTML片段使用两个连接符作为分隔符 - 控件使用一个连接符作为分隔符在控件内部，可以手工将控件Id与子控件Id连接。下面以sap.ui.unified.FileUploader为例，其中的子控件Id为this.getId()&#43; &amp;ldquo;-fu_input&amp;rdquo;。
sap.ui.unified.FileUploader.prototype.init = function(){ // Instantiate browser-specific UI-Elements (IE8 only): // works fine with applySettings() after init() - most things are done in onAfterRendering // IE8 should render a native file uploader and the SAPUI5 controls should be exactly behind if (!!sap.ui.Device.browser.internet_explorer &amp;amp;&amp;amp; sap.ui.Device.browser.version == 8) { this.oFilePath = new sap.ui.commons.TextField(&amp;lt;font color=&amp;quot;#FF0000&amp;quot;&amp;gt;**this.getId() &#43; &amp;quot;-fu_input&amp;quot;**&amp;lt;/font&amp;gt;, {width: &amp;quot;225px&amp;quot;}); this.oBrowse = new sap.ui.commons.Button({enabled : this.getEnabled(), text: &amp;quot;Browse..&amp;quot;, width: &amp;quot;0px&amp;quot;, height: &amp;quot;0px&amp;quot;}); } else { //all other browsers will load the respective UI-Elements from the FileUploaderHelper this.oFilePath = sap.ui.unified.FileUploaderHelper.createTextField(&amp;lt;font color=&amp;quot;#FF0000&amp;quot;&amp;gt;**this.getId() &#43; &amp;quot;-fu_input&amp;quot;**&amp;lt;/font&amp;gt;); this.oBrowse = sap.ui.unified.FileUploaderHelper.createButton(); } this.oFilePath.setParent(this); this.oBrowse.setParent(this); this.oFileUpload = null; //retrieving the default browse button text from the resource bundle this.oBrowse.setText(this.getBrowseText()); };  控件ID最好包含父控件ID可以很好地表达层次关系，因此在调试时很有帮助！
定义聚合 当开发控件时，应该假定当其被使用时可在任何时候被销毁或重新创建。例如，控件的容器由于数据异步更新而销毁并重新创建，控件自身也要被销毁和重新创建。应此，对控件内使用的任何引用进行适当清理尤为重要。 上述场景中一个常见的错误是&amp;rdquo;duplicate ID&amp;rdquo;错误，例如
Uncaught Error: Error: adding element with duplicate id &#39;Slider-basic-slider-valueLabel&#39;  这可能是一个控件创建了另外一个控件实例(例如Label)并用变量引用它。当主控件进入销毁阶段，该引用没有自动清理。下次当控件重新创建时，它尝试使用相同ID创建Label实例从而导致上述错误。 在与此类似的场景中，该错误可以通过将Label定义为主组件元数据块内的&amp;rdquo;aggregation&amp;rdquo;来进行避免。子控件从而算是“被管理”了，OpenUI5将负责管理控件的生命周期，包括清除其引用。并且这种方式也提供了库内一致性和自文档。详情和示例请参见Creating Custom Controls in SAPUI5。
最佳实践是在开发中故意销毁并重新创建控件以验证其能正常清理资源。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 示例: Sorted, grouped and multi-selectable list</title>
        <url>https://mryqu.github.io/post/openui5_%E7%A4%BA%E4%BE%8B_sorted_grouped_and_multi-selectable_list/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>list</tag><tag>multi-selectable</tag><tag>sorter</tag><tag>grouper</tag>
        </tags>
        <content type="html">  做了一个可多选、使用定制分组和排序的list示例，示例位置：http://jsbin.com/jetena/1/edit?html,output
var fGrouper = function(oContext) { var v = oContext.getProperty(&amp;quot;workbook&amp;quot;); return { key: v, text: v }; } var oSorter = new sap.ui.model.Sorter(&amp;quot;&amp;quot;, false, fGrouper); oSorter.fnCompare = function(a, b) { // Determine the group and group order var agroup = a.workbook; var bgroup = b.workbook; // Return sort result, by group ... if (agroup &amp;lt; bgroup) return -1; if (agroup &amp;gt; bgroup) return 1; // ... and then within group (when relevant) if (a.worksheet &amp;lt; b.worksheet) return -1; if (a.worksheet == b.worksheet) return 0; if (a.worksheet &amp;gt; b.worksheet) return 1; } var sheetSelList = new sap.m.List({ id: &amp;quot;workSheetSelectionList&amp;quot;, mode: sap.m.ListMode.MultiSelect, headerToolbar: new sap.m.Toolbar({ content: [ new sap.m.CheckBox({ id:&amp;quot;workSheetSelectionList-selAll&amp;quot;, text: &amp;quot;All&amp;quot;, selected: true, select : function() { if(this.getSelected()) { sheetSelList.selectAll(); } } }) ] }), items:{ path: &#39;/modelData&#39;, template: new sap.m.StandardListItem({ title: &#39;{worksheet}&#39;, selected: &amp;quot;{selected}&amp;quot; }), sorter: oSorter, groupHeaderFactory: function (oGroup) { return new sap.m.GroupHeaderListItem({ title: oGroup.key, upperCase: false }); } }, selectionChange: function(ev) { if(!ev.getParameters().selected) { var oCB = sap.ui.getCore().byId(&amp;quot;workSheetSelectionList-selAll&amp;quot;); oCB.setSelected(false); } } }); var workbookInfo= [ {workbook:&amp;quot;1.xlsx&amp;quot;, worksheet:&amp;quot;sheet1&amp;quot;, selected:true}, {workbook:&amp;quot;2.xlsx&amp;quot;, worksheet:&amp;quot;sheet1&amp;quot;, selected:true}, {workbook:&amp;quot;1.xlsx&amp;quot;, worksheet:&amp;quot;sheet2&amp;quot;, selected:true}, {workbook:&amp;quot;1.xlsx&amp;quot;, worksheet:&amp;quot;sheet3&amp;quot;, selected:true} ]; var listModel = new sap.ui.model.json.JSONModel(); listModel.setData({modelData:workbookInfo}); sheetSelList.setModel(listModel); sheetSelList.placeAt(&amp;quot;content&amp;quot;);  参考 Custom Sorting and Grouping
</content>
    </entry>
    
     <entry>
        <title>非技术视角八卦一下docker</title>
        <url>https://mryqu.github.io/post/%E9%9D%9E%E6%8A%80%E6%9C%AF%E8%A7%86%E8%A7%92%E5%85%AB%E5%8D%A6%E4%B8%80%E4%B8%8Bdocker/</url>
        <categories>
          <category>Tool</category><category>Docker</category>
        </categories>
        <tags>
          <tag>docker</tag><tag>doucloud</tag><tag>容器</tag><tag>历史</tag><tag>竞争对手</tag>
        </tags>
        <content type="html">  2010年，几个大胡子年轻人在旧金山成立了一家做 PaaS平台的公司，起名为dotCloud。dotCloud是YCombinator（创投公司） S10的毕业生。创始人：Solomon Hykes - 2006年毕业于EPITECH - European Institute of Technology(硕士) - 2003-2004年做过个人IT教师 - 2006年曾经在SmartJog担任售后工程师 - 2010-2013年担任dotCloud的CEO - 2013年至今担任dotCloud的CTO
dotCloud主要是基于 PaaS 平台为开发者或开发商提供技术服务。PaaS的概念虽好，但是由于认知、理念和技术的局限性，市场的接受度并不高，市场的规模也不够大。除 此之外，还有巨头不断进场搅局，IBM的蓝云，微软的 Azure，Amazon 的 EC2，Google 的 GAE，VMware 的 Cloud Foundry等等，可谓强敌环伺，而且强敌都不差钱，想玩多久就玩多久，想玩多大玩多大。在这种情况下，虽然 dotCloud在2011年初拿到了1000万美元的融资，但依然举步维艰。 Solomon Hykes在这种情况下，决定将自己的核心引擎开源，并让团队的核心成员参与开源项目。这个引擎的名字叫做Docker，以Go语言写成。Docker一经开源立刻得到了「业界」的热烈吹捧。这个容器管理引擎大大降低了容器技术的使用门槛，轻量级，可移植，虚拟化，语言无关，写了程序扔上去做成镜像可以随处部署和运行，Docker迅速从单纯的云端虚机限定资源环境转变成新的代码或应用发布形式，方便有集成开发、快速迭代需求的用户实现多次更新的回退和版本管理，开发、测试和生产环境彻底统一了，还能进行资源管控和虚拟化。 从此以后，他们开始专心研发 Docker 产品和维护相关社区。2013年10月 dotCloud公司更名为Docker股份有限公司，2014年8月Docker宣布把PAAS的业务「dotCloud」出售给位于德国柏林的平台即服务提供商「cloudControl」，dotCloud的历史告一段落。同年8月，Docker内部员工 James Turnbull 发布了面向开发者、运维和系统管理员的 Docker电子书《The DockerBook》。2014年9月，Docker 宣布已获 4000 万美元的 C 轮融资。 2014年6月，Microsoft Open Technology （微软开放技术）宣布 Azure开始支持Docker部署；2014年10月，微软宣布下一个版本的WindowsServer将原生支持Docker；2014年11月，AWS加码押注Docker，推出了高性能容器管理服务EC2Container服务，用户可以在AWS上使用容器轻松地运行和管理分布式应用；2014年12月，Docker宣布发布跨容器的分布式应用编排服务，编排服务可以帮助开发者创建并管理新一代的可移植的分布式应用程序。 Docker的竞争对手是CoreOS公司的容器技术Rocket，现在Rocket得到谷歌、Red Hat以及 VMware等一批大公司的支持。
参考 Docker 传奇之 dotCloud
Docker，云时代的程序交付方式
Docker项目研究
Docker之父Solomon Hykes谈项目开发的初衷和挑战
解读2014之Docker篇：才气、勇气、运气
八个Docker的真实应用场景
Google支持Docker的竞争对手，云计算恩怨又起
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] check FSDataInputStream and its wrapped InputStream implementation</title>
        <url>https://mryqu.github.io/post/hadoop_check_fsdatainputstream_and_its_wrapped_inputstream_implementation/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>hdfs</tag><tag>hdfsdatainputstream</tag><tag>dfsinputstream</tag>
        </tags>
        <content type="html"> 打开一个HDFS文件，获得一个FSDataInputStream对象，其实现类到底是什么？小小探究一下。
package com.yqu.hadoop; import java.io.IOException; import java.io.InputStream; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; public class LearnFS { public static void main(String[] args) { Configuration config = new Configuration(); FSDataInputStream in = null; Path path = new Path(&amp;quot;/user/hadoop/input/access_log.txt&amp;quot;); try { FileSystem fs = FileSystem.get(config); System.out.println(&amp;quot;Scheme: &amp;quot; &#43; fs.getScheme()); System.out.println(&amp;quot;Uri: &amp;quot; &#43; fs.getUri().toString()); in = fs.open(path); if (in != null) { System.out.println(&amp;quot;FSDataInputStream impl:&amp;quot; &#43; in.getClass().getCanonicalName()); InputStream is = in.getWrappedStream(); if (is != null) { System.out.println(&amp;quot;InputStream impl:&amp;quot; &#43; is.getClass().getCanonicalName()); } } } catch (Throwable t) { t.printStackTrace(); } finally { if (in != null) { try { in.close(); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } } } } }  测试结果是org.apache.hadoop.hdfs.client.HdfsDataInputStream和org.apache.hadoop.hdfs.DFSInputStream： </content>
    </entry>
    
     <entry>
        <title>cURL速查表</title>
        <url>https://mryqu.github.io/post/curl%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
        <categories>
          <category>Tool</category><category>NetWork</category>
        </categories>
        <tags>
          <tag>curl</tag><tag>http</tag><tag>url</tag><tag>传输</tag><tag>调试</tag>
        </tags>
        <content type="html">  cURL是一个利用URL语法在命令行下工作的文件传输工具，1997年首次发行。它支持文件上传和下载，所以是综合传输工具，但按传统，习惯称cURL为下载工具。cURL还包含了用于程序开发的libcurl。 cURL支持的通訊协议有FTP、FTPS、HTTP、HTTPS、TFTP、SFTP、Gopher、SCP、Telnet、DICT、FILE、LDAP、LDAPS、IMAP、POP3、SMTP和RTSP。
请求多个资源curl http://api.test.com/pp/[123-321]
curl http://api.test.com/pp/{abc,def,ghi}/status允许重定向-L如果服务器返回3XX响应，使用-L选项可以让curl向新新地址发送请求。Cookies-b --cookie {name=data}发送原始Cookies或文件中的Cookies
范例：-b &#39;n1=v1; n2=v2&#39;-c / --cookie-jar {file name}将Cookies存入文件发送数据-d / --data {data}-d {data} 发送原始数据
-d {@filename} 发送文件中的数据。
如果不想对@进行解析，可以使用--data-raw。
JSON范例：
-d &#39;{&#34;firstName&#34;:&#34;yd&#34;, &#34;lastName&#34;:&#34;q&#34;}&#39;
原始数据范例：
-d &#39;name=yqu&#39; -d &#39;sex=male&#39;--data-ascii {data}等同于--data
--data-raw {data}几乎等同于--data，除了不对@进行解析。--data-binary {data}--data-binary {data} 发送原始二进制数据
--data-binary {@filename} 发送文件中的二进制数据。
在发送前对数据不做任何处理。发送表单-F / --form {name=content}以Content-Type: multipart/form-data方式发送数据
范例：
curl -F password=@/etc/passwd www.mypasswords.com允许&#34;不安全&#34;SSL-k / --insecure所有的SSL连接使用默认安装的CA证书捆绑试图保障其安全。除非使用-k/--insecure，否则所有被认为是“不安全”的连接将会失败。认证-u / --user {user:password}服务器认证
-U / --proxy-user {user:password}代理认证
代理-x / --proxyHTTP方法-X / --request {request}可以指定的方法为：POST、HEAD、PUT、GET、DELETE输出至文件-o / --output {file}输出到文件而不是标准输出输出至与远端文件同名的本地文件-O / --remote-name上传文件-T / --upload-file {file}curl -T &#34;{file1,file2}&#34; http://www.test.com
curl -T &#34;img[1-10].png&#34; ftp://ftp.test.com/img/最大操作时间-m / --max-time {seconds}该选项可以防止批量操作由于网络太慢或链路中断而被挂起几个小时。连接超时值--connect-timeout {seconds}连接允许的最长时间定制输出格式-w / --write-out {format}范例：
curl -w &#34;%{remote_ip} %{time_total} %{http_code}\n&#34; \
-s -L -o /dev/null www.test.com定制请求头部-H / --header {header}范例：
curl -X PUT \
-H &#39;Content-Type: application/json&#39; \
-H &#39;Authorization: OAuth 2c3455d1aeffc&#39; \
-d @example.json用户代理-A / --user-agent {agent string}-A &#34;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36&#34;显示响应头部-i / --include使用此选项可以使输出除了内容之外还包含头部HEAD请求-I / --head发送HEAD请求，响应将不包含内容而仅有头部详细输出模式-v / --verbose用于检查头部元素
以&amp;gt;开头表示curl发送的头部和数据
以&amp;lt;开头表示curl接受的头部和数据
以*开头表示curl提供的额外信息安静模式-s / --silent仅显示所要数据，不显示进步条或错误消息。显示错误-S / --show-error当使用-s时，该选项使curl显示错误消息。
不使-s时，该选项无意义。退出代码6无法解析主机7连接主机失败28操作超时55发送数据失败56接收数据失败 参考 cURL Man Page
curl cheat-sheet
[](https://github.com/tedyoung/curl-api-cheat-sheet/blob/master/cheat-sheet.md)
[](http://www.lornajane.net/posts/2008/curl-cheat-sheet)
[](http://blog.engelke.com/2009/10/20/curl-cheat-sheet/)
[](http://www.niclas-meier.de/2011/12/the-curl-cheat-sheet-to-myself/)
[](http://www.cantoni.org/2012/01/10/curl-cheat-sheet)
[](http://g33kinfo.com/info/archives/4872)
[](http://wenku.baidu.com/view/e02c69f4f61fb7360b4c65e9.html)
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 安装Hadoop 2.7.x 集群</title>
        <url>https://mryqu.github.io/post/hadoop_%E5%AE%89%E8%A3%85hadoop_2.7.x_%E9%9B%86%E7%BE%A4/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>cluster</tag><tag>install</tag><tag>yarn</tag>
        </tags>
        <content type="html">  集群规划 |节点|角色 |&amp;mdash;&amp;ndash; |node50064|NameNode RessourceManager |node50069|Datanode SecondNameNode |node51054|Datanade
准备工作 （在全部机器上）创建hadoop用户 $ sudo useradd -m hadoop -s /bin/bash $ sudo passwd hadoop $ sudo adduser hadoop sudo  （在全部机器上）配置/etc/hosts 10.120.12.135 node50064.mryqu.com node50064 10.120.11.201 node50069.mryqu.com node50069 10.120.14.226 node51054.mryqu.com node51054  （在全部机器上）禁止掉IPv6 参见之前的博文在Ubuntu中禁掉IPv6。
（在全部机器上）关闭防火墙 ufw disable //关闭 sudo apt-get remove ufw //卸载 sudo ufw status //查看  （在全部机器上）安装并配置Java JDK 安装Java JDK：
$ sudo apt-get update $ sudo apt-get install openjdk-7-jre openjdk-7-jdk  通过下列命令确定JDK安装路径为/usr/lib/jvm/java-7-openjdk-amd64： 通过sudovi /etc/profile添加如下内容：
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH  最后通过source/etc/profile刷新配置文件。
Java环境变量既可以在/etc/profile下配置，也可以在~/.profile或~/.bashrc中配置。区别在于/etc/profile对所有登陆用户都生效，在~/.profile或~./bashrc中配置仅对当前用户有效。~/.profile与~./bashrc二者的区别为，~/.profile可以设定本用户专有的路径、环境变量等，它只能登入的时候执行一次；~/.bashrc也是某用户专有设定文档，可以设定路径、命令别名，每次shellscript的执行都会对其使用一次。
设置无密码SSH登录 在node50064上生成公钥和私钥，公钥即为认证密钥：
 $ ssh-keygen -t dsa -P &#39;&#39; -f ~/.ssh/id_dsa $ cat ~/.ssh/id_dsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys  将认证密钥通过ssh-copy-id命令复制给node50069和node51054： 至此，可从node50064无密码SSH登录上node50069或node51054。
（在node50064上）安装Hadoop 下载并解压缩Hadoop wget http://apache.osuosl.org/hadoop/common/hadoop-2.7.x/hadoop-2.7.x.tar.gz tar -xzf hadoop-2.7.x.tar.gz sudo mv hadoop-2.7.x /usr/local/hadoop sudo chown -R hadoop /usr/local/hadoop cd /usr/local/hadoop mkdir tmp mkdir tmp/dfs mkdir tmp/dfs/name mkdir tmp/dfs/data  配置环境变量 通过vi~/.bashrc添加如下内容：
# Set HADOOP_HOME (deprecated) export HADOOP_HOME=/usr/local/hadoop # Set HADOOP_PREFIX export HADOOP_PREFIX=$HADOOP_HOME # Set HADOOP_CONF_DIR export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_PREFIX/lib/native export HADOOP_OPTS=&amp;quot;$HADOOP_OPTS -Djava.library.path=$HADOOP_PREFIX/lib/native&amp;quot; export JAVA_LIBRARY_PATH=$HADOOP_PREFIX/lib/native:$JAVA_LIBRARY_PATH export LD_LIBRARY_PATH=$HADOOP_PREFIX/lib/native # Add Hadoop bin and sbin directory to PATH export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin  最后通过source~/.bashrc刷新配置文件。
配置Hadoop 这些配置文件路径为/usr/local/hadoop的相对路径。
etc/hadoop/hadoop-env.sh export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64  etc/hadoop/core-site.xml etc/hadoop/hdfs-site.xml etc/hadoop/yarn-site.xml etc/hadoop/mapred-site.xml etc/hadoop/masters node50064  etc/hadoop/slaves node50069 node51054  复制Hadoop及配置到其他主机 在node50064上执行：
scp ~/.bashrc node50069:~/ scp ~/.bashrc node51054:~/ scp -r /usr/local/hadoop node50069:~/ scp -r /usr/local/hadoop node51054:~/  之后，在node50069和node51054上执行：
sudo mv ~/hadoop /usr/local/ sudo chown -R hadoop /usr/local/hadoop source ~/.bashrc  （在node50064上）启动Hadoop 首次使用需要格式化NameNode
hdfs namenode -format  启动dfs和yarn
start-dfs.sh start-yarn.sh  启动MapReduce JobHistory服务器
mr-jobhistory-daemon.sh start historyserver  检验安装结果 查看JVM进程状态 运行Hadoop范例 hdfs dfs -mkdir /user hdfs dfs -mkdir /user/hadoop hdfs dfs -put etc/hadoop input hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.x.jar grep input output &#39;dfs[a-z.]&#43;&#39; hdfs dfs -cat output/*  查看HDFS Web UI 查看YARN资管管理器Web UI 查看MapReduce作业历史 （在node50064上）关闭Hadoop mr-jobhistory-daemon.sh stop historyserver stop-dfs.sh stop-yarn.sh  附注 SecondNameNode可以在NameNode故障时快一点进行恢复，但它不是对NameNode的插入时更换，也没有提供自动故障切换的手段。更多HadoopHA配置可参考HDFS High Availability Using the Quorum Journal Manager。
参考 Hadoop: Setting up a Single Node Cluster.
Hadoop Cluster Setup
Hadoop Default Ports Quick Reference
</content>
    </entry>
    
     <entry>
        <title>阅读《Microservice Design Patterns》</title>
        <url>https://mryqu.github.io/post/%E9%98%85%E8%AF%BBmicroservice_design_patterns/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>微服务</tag><tag>架构</tag><tag>设计模式</tag><tag>组合</tag>
        </tags>
        <content type="html"> Java Code Geeks有一篇文章Microservice Design Patterns，提供了六种微服务架构的设计模式，用于组合微服务。
聚合器微服务设计模式
这是一种最常用也最简单的设计模式，如下图所示：
聚合器调用多个服务实现应用程序所需的功能。它可以是一个简单的Web页面，将检索到的数据进行处理展示。它也可以是一个更高层次的组合微服务，对检索到的数据增加业务逻辑后进一步发布成一个新的微服务，这符合DRY原则。另外，每个服务都有自己的缓存和数据库。如果聚合器是一个组合服务，那么它也有自己的缓存和数据库。聚合器可以沿X轴和Z轴独立扩展。
代理微服务设计模式
这是聚合器模式的一个变种，如下图所示：
在这种情况下，客户端并不聚合数据，但会根据业务需求的差别调用不同的微服务。代理可以仅仅委派请求，也可以进行数据转换工作。一个很好的实例就是针对不同设备的表现层可以封装在代理微服务内。
链式微服务设计模式
这种模式在接收到请求后会产生一个经过合并的响应，如下图所示：
在这种情况下，服务A接收到请求后会与服务B进行通信，类似地，服务B会同服务C进行通信。所有服务都使用同步消息传递。在整个链式调用完成之前，客户端会一直阻塞。因此，服务调用链不宜过长，以免客户端长时间等待。
分支微服务设计模式
这种模式是聚合器模式的扩展，允许同时调用两个微服务链，如下图所示：
数据共享微服务设计模式
自治是微服务的设计原则之一，就是说微服务是全栈式服务。但在重构现有的“单体应用（monolithicapplication）”时，SQL数据库反规范化可能会导致数据重复和不一致。因此，在单体应用到微服务架构的过渡阶段，可以使用这种设计模式，如下图所示：
在这种情况下，部分微服务可能会共享缓存和数据库存储。不过，这只有在两个服务之间存在强耦合关系时才可以。对于基于微服务的新建应用程序而言，这是一种反模式。
异步消息传递微服务设计模式
虽然REST设计模式非常流行，但它是同步的，会造成阻塞。因此部分基于微服务的架构可能会选择使用消息队列代替REST请求/响应，如下图所示：
感兴趣的读者可以参考《微服务中的耦合与自治》一文为自己的微服务选择合适的消息传递模式。
</content>
    </entry>
    
     <entry>
        <title>了解混合持久化（Polyglot Persistence）</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3%E6%B7%B7%E5%90%88%E6%8C%81%E4%B9%85%E5%8C%96polyglot_persistence/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>混合持久化</tag><tag>polyglot</tag><tag>persistence</tag><tag>优缺点</tag>
        </tags>
        <content type="html">  开发web程序的最早期时间，最被广泛使用的企业程序架构是将程序的服务器端组件打包为单个单元。很多企业Java应用程序由单个WAR或EAR文件组成。其它语言（比如Ruby，甚至C&#43;&#43;）编写的应用程序也大抵如此。这种单体架构模式(monolithicapplication architecturepattern)往往通过一个数据访问层与单一类型的数据库相连接，所有不同用途的数据存储都存储在该数据库内。 数据库环境在过去的十多年里增长巨大。每个新出现的数据库相较于其它数据库，都在某些方面有着优势，但同时它们也做了各种各样的折衷。事实上，根据CAP定理不可能拥有完美的数据库，我们需要根据应用程序来选择哪种折衷是可接受的。带有这些约束的工作场景符合Martin Fowler推广的混合持久化思路。混合持久化的思路是指，你应该根据工作的不同需求选择相应合适的数据库，这样我们就能两者兼得了。 混合持久化的优点显而易见：一个复杂的企业应用程序会使用很多类型的数据，对每种数据采用最适合的存储技术，可以带来性能的提升。随着微服务架构模式（Microservicearchitecturepattern）越来越受欢迎，复杂的企业应用程序将会被分解成很多的微服务，每个微服务内将对所操作的数据采用最适合的存储技术。
混合持久化的缺点也同样鲜明：以增加复杂性为代价。每种数据存储机制都有其学习成本，选择正确的数据存储也有决策成本。需要了解每种数据存储的性能瓶颈并不断迎接挑战。
Martin Fowler建议对战略性的项目采用混合持久化，这样才能通过新技术获得足够收益。
参考 Polyglot Persistence
The featuer is: Polyglot Persistence
NoSQL精粹 第13章混合持久化
[](http://sww.sas.com/saspedia/Polyglot_persistence)
</content>
    </entry>
    
     <entry>
        <title>Git&#43;Gerrit&#43;Gradle&#43;Jenkins持续集成</title>
        <url>https://mryqu.github.io/post/git&#43;gerrit&#43;gradle&#43;jenkins%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>git</tag><tag>gerrit</tag><tag>gradle</tag><tag>jenkins</tag><tag>持续集成</tag>
        </tags>
        <content type="html">  随着软件开发复杂度的不断提高，团队开发成员间如何更好地协同工作以确保软件开发的质量已经慢慢成为开发过程中不可回避的问题。尤其是近些年来，敏捷（Agile）在软件工程领域越来越红火，如何能再不断变化的需求中快速适应和保证软件的质量也显得尤其的重要。持续集成正是针对这一类问题的一种软件开发实践。它倡导团队开发成员必须经常集成他们的工作，甚至每天都可能发生多次集成。而每次的集成都是通过自动化的构建来验证，包括自动编译、发布和测试，从而尽快地发现集成错误，让团队能够更快的开发内聚的软件。
Gerrit Gerrit是基于GWT web应用的开源代码审查系统，为使用Git版本控制系统的项目提供在线代码审查。安卓开源项目（AOSP）用其来管理多代码库的庞大项目。Gerrit通过在自身代码库跟踪提交的Git变更集来提供代码审查的。它并排显示新旧文件，让审查者更容易对变更进行审查，并允许审查者添加内嵌注释。 提交的变更既可以被Jenkins这样的自动系统进行审查，也可以由同事进行审查。每个审查者检查代码变更、添加注释，然后将变更标记为“在我看来代码不错”(“没有打分”或“我期望你不要提交代码”)。验证者(例如Jenkins或其他人)通过构建和测试代码来验证变更。如果他们认为代码可行，则设置“在我看来代码不错”标记，Gerrit将尝试将变更合并到公开的“权威”代码库。文章Life of a Patch描述了这一工作流: Gradle 在Java构建工具的世界里，先有了Ant，然后有了Maven。Maven的CoC（约定优于配置）、依赖管理以及项目构建规则重用性等特点，让Maven几乎成为Java构建工具的事实标准。然而，冗余的依赖管理配置、复杂并且难以扩展的构建生命周期，都成为使用Maven的困扰。Gradle作为新的构建工具，是基于Groovy语言的构建工具，既保持了Maven的优点，又通过使用Groovy定义的DSL克服了Maven中使用XML繁冗以及不灵活等缺点，支持依赖管理和多项目，而且它有非常完善的说明文档。目前，SpringSource、Hibernate等都采用Gradle来构建。
Jenkins Jenkins，之前叫做Hudson，是一个开源项目，提供了一种易于使用的持续集成系统，使开发者从繁杂的集成中解脱出来，专注于更为重要的业务逻辑实现上。同时Jenkins能实施监控集成中存在的错误，提供详细的日志文件和提醒功能，还能用图表的形式形象地展示项目构建的趋势和稳定性。Jenkins通过Gerrit触发器插件可在新的Gerrit补丁集创建时开始使用Gerrit代码库中的代码进行构建项目，通过Gradle插件调用Gradle构建脚本，以帮助变更验证。
参考 Git权威指南-第5篇-第32章 Gerrit 代码审核服务器
Git&#43;Gerrit&#43;Gradle&#43;Jenkins持续集成设置
</content>
    </entry>
    
     <entry>
        <title>《The Art of Scalability》中的三维伸缩性模型</title>
        <url>https://mryqu.github.io/post/the_art_of_scalability%E4%B8%AD%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BC%B8%E7%BC%A9%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>scalability</tag><tag>伸缩性</tag><tag>三维</tag><tag>模型</tag>
        </tags>
        <content type="html"> 最近又搜集到Martin L. Abbott和Michael T. Fisher的两本好书：
 The Art of Scalability Scalability Rules：50 Principles for Scaling Web Sites  第一本书将近六百页，最近没时间看，就先粗略学习一下其中的三维伸缩性模型： 在该模型中，将应用程序水平复制，通过负载均衡运行应用程序的多个完全一样的副本的方式来实现应用程序伸缩性，这种方式称为X轴伸缩性。这是一种很好的方式来提高应用程序的容量和可用度。 当使用Z轴伸缩性，每个服务器运行代码的一个完全相同的副本。在该方面，它与X轴伸缩性很相似。最大的不同是每个服务器只负责数据的一个子集。该系统的一些组件负责将每个请求路由给适当的服务器。一个常见的路由规则是把请求的一个属性作为被访问的实体的主键，比如分区。另一个常见的路由规则是客户类型。例如，应用程序可以向付费用户提供比免费用户更高的SLA，实现方式是将付费用户的请求路由到具有更高容量的一组服务器上。 Z轴伸缩性与X轴伸缩性类似，提高了应用程序的容量和可用度。然而，没有任何一个方式能够解决不断增加的开发工作和程序复杂度的问题。解决这些问题需要Y轴伸缩性。 伸缩性的第三个维度是针对功能性分解的Y轴伸缩性。Y轴伸缩性与Z轴伸缩性分解事情的方式相似但有不同。在应用程序层级，Y轴伸缩性将单体应用程序拆分成一组服务。每个服务实现了一组相关的功能特性，例如订单管理，客户管理等。 决定如何将系统分割为一组服务更像是一门艺术，但是可借助于一些策略。一种方式是通过动词或使用情况拆分服务。另一个拆分方式是通过名词或资源分割系统。这种服务负责处理给定的实体/资源的所有操作。 Unix提供了大量的工具，比如grep，cat和find。每个工具只做一件事，效果往往非常好，并且可以使用shell脚本组合多个工具以执行复杂的任务。服务分割也应采用类似的策略。
</content>
    </entry>
    
     <entry>
        <title>阅读《Microservices, Monoliths, and NoOps》</title>
        <url>https://mryqu.github.io/post/%E9%98%85%E8%AF%BBmicroservices_monoliths_and_noops/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>微服务</tag><tag>单体应用</tag><tag>架构</tag><tag>模式</tag><tag>noops</tag>
        </tags>
        <content type="html">  Java Code Geeks有一篇文章Microservices, Monoliths, and NoOps，分析了单体应用与微服务的优缺点，并建议使用微服务重构现有的应用程序。
单体应用 通俗地讲，“单体应用（monolithapplication）”就是将应用程序的所有功能都打包成一个独立的单元，可以是JAR、WAR、EAR或其它归档格式。
单体应用优点 单体应用有如下优点： - 为人所熟知：现有的大部分工具、应用服务器、框架和脚本都是这种应用程序； - IDE 友好：像NetBeans、Eclipse、IntelliJ这些开发环境都是针对开发、部署、调试这样的单个应用而设计的； - 便于共享：单个归档文件包含所有功能，便于在团队之间以及不同的部署阶段之间共享； - 易于测试：单体应用一旦部署，所有的服务或特性就都可以使用了，这简化了测试过程，因为没有额外的依赖，每项测试都可以在部署完成后立刻开始； - 容易部署：只需将单个归档文件复制到单个目录下。
单体应用缺点 目前为止，单体应用已经很好地服务了我们，未来无疑还会继续发挥重要作用。但是，不管如何模块化，单体应用最终都会因为团队壮大、成员变动、应用范围扩展等出现问题。下面是单体应用的一些不足： - 不够灵活：对应用程序做任何细微的修改都需要将整个应用程序重新构建、重新部署。开发人员需要等到整个应用程序部署完成后才能看到变化。如果多个开发人员共同开发一个应用程序，那么还要等待其他开发人员完成了各自的开发。这降低了团队的灵活性和功能交付频率； - 妨碍持续交付：单体应用可能会比较大，构建和部署时间也相应地比较长，不利于频繁部署，阻碍持续交付。在移动应用开发中，这个问题会显得尤为严重； - 受技术栈限制：对于这类应用，技术是在开发之前经过慎重评估后选定的，每个团队成员都必须使用相同的开发语言、持久化存储及消息系统，而且要使用类似的工具，无法根据具体的场景做出其它选择； - 技术债务：“不坏不修（Not broken，don’tfix）”，这在软件开发中非常常见，单体应用尤其如此。系统设计或写好的代码难以修改，因为应用程序的其它部分可能会以意料之外的方式使用它。随着时间推移、人员更迭，这必然会增加应用程序的技术债务。
什么是微服务？ 而随着业务需求的快速发展变化，敏捷性、灵活性和可扩展性需求不断增长，迫切需要一种更加快速高效的软件交付方式。微服务就是一种可以满足这种需求的软件架构风格。单体应用被分解成多个更小的服务，每个服务有自己的归档文件，单独部署，然后共同组成一个应用程序。这里的“微”不是针对代码行数而言，而是说服务的范围限定到单个功能。
微服务的特征 微服务有如下特征： - 领域驱动设计：应用程序功能分解可以通过Eric Evans在《领域驱动设计》中明确定义的规则实现，领域驱动设计不是分解应用程序的唯一方法，但肯定是很常用的一种；每个团队负责与一个领域或业务功能相关的全部开发；团队遵循全栈开发方法拥有全系列的开发人员，具备用户界面、业务逻辑和持久化存储等方面的开发技能； - 单一职责原则：每个服务应该负责该功能的一个单独的部分，这是SOLID原则之一，Unix工具程序很好地证明这一原则的重要性； - 明确发布接口：每个服务都会发布一个定义明确的接口，而且保持不变；服务消费者只关心接口，而对于被消费的服务没有任何运行依赖；服务之间就业务模型、API、负载或其他契约达成一致并使用符合契约的方式进行通信。接口可能会产生新版本，但接口的老版本可以继续使用，且新服务保持后续兼容。不可以通过改变契约破坏兼容性。 - 独立部署、升级、扩展和替换：每个服务都可以单独部署及重新部署而不影响整个系统。这使得服务很容易升级，例如增加更多的功能点。每个服务都可以沿着《Art of Scalability》一书定义的X轴（水平复制）和Z轴（面向查询的分割，数据分区）进行独立扩展；由于其他服务仅依赖发布的接口，只要发布相同的契约，服务实现甚至是底层技术栈都可以修改； - 可以异构/采用多种语言：每个服务的实现细节都与其它服务无关，这使得服务之间能够解耦，团队可以针对每个服务选择最合适的开发语言、持久化存储、工具和方法；一个需要在关系型数据库存储数据的服务可以选择MySQL，另一个需要存储文档的服务可以选择MongoDB。不同的团队可以根据自己的需求选择JavaEE、NodeJS、Python、Vert.x或其他对本团队最有效的技术； - 轻量级通信：服务通信使用轻量级的通信协议，例如在HTTP上承载的REST。由于REST本质是同步的，可能会有某些潜在的瓶颈。另一个可选机制是使用支持异步消息的发布/订阅机制。任何符合需求的消息协议，例如AMQP、STOMP、MQTT或WebSocket，都可以使用。简单消息实现，例如ActiveMQ，提供了可靠的异步组构尤其适用于这种用途。每个开发团队可以根据服务的具体需求对同步还是异步消息做适宜的选择，当然也可以混用。
Netflix是微服务的一个典型代表，这里有几篇文章介绍他们对微服务的应用。 - Netflix&amp;rsquo;s Viewing Data: How We Know Where You Are in House of Cards - A Microscope on Microservices - Scalable Microservices at Netflix. Challenges and Tools of the Trade - Adopting Microservices at Netflix: Lessons for Architectural Design
很多增强Netflix微服务架构的工具可在netflix.github.io获得。
微服务的优点 微服务具有如下优点：
 易于开发、理解和维护：微服务中的代码仅限于业务的某一功能，因此更易于理解。IDE可以很轻松加载小的代码库，且使开发者保持高效。 比单体应用启动快：微服务的范围比单体应用小得多，应此会有较小的打包文件。其结果就是，更快的部署和启动使开发者保持高效。 局部修改很容易部署：每个服务独立于其他服务进行部署。服务的任何局部修改，例如更改底层实现使服务性能获得提升，无需同同其他组进行协调。其结果就是，保持了微服务敏捷性，同时也有利于持续集成和持续交付。 可独立扩展：每个服务可以根据需求给予X轴（克隆）和Z轴（分区）进行独立扩展。对于单体应用而言，这一点很难做到，且扩展必须一起部署。 故障隔离：一个应为异常的服务，例如内存溢出或数据库连接没有关闭，仅影响所提供的服务而不是整个应用，增强了故障隔离能力。 不会受限于任何技术栈：开发者可以自由选择对所开发服务最适合的开发语言和技术栈。  微服务看上去像一枚银弹，可以解决许多软件开发方面的问题。这看上去很美好，但并不易于实现。微服务会极大地增加运维工作量，InfoWorld在一篇文章中明确指出： &amp;gt; 使用微服务，一些技术债务势必从开发转到运维，因此，你最好有一个一流的开发运维团队。
微服务和NoOps 微服务对基础设施提出了一些额外的需求。通常，我们将它们总称为NoOps，本质上讲，就是一组服务，提供一个更好的应用程序部署流程并确保其运行。
 服务复制:每个服务都需要复制，一般使用X轴克隆或Z轴分区。 服务发现:可能需要多个服务协作提供一个应用的功能。这需要服务能够发现其他服务。在云环境下尤其棘手，因为其上的服务都是短暂的，很有可能扩展或缩减。服务解析是所有其他服务都需要的基础功能。服务需要向中央注册中心注册，其他服务需要查询注册来解析依赖关系。NetflixEureka、Etcd、 Zookeeper等都是这一领域的可选方案。 服务恢复:不管测试工作做得多努力，软件故障终会发生。关键问题不是如何避免故障而是如何解决故障，对于微服务这样的分布式服务尤其突出。对于服务很重要的一点是能够自动纠正故障，确保用户体验不受影响。 服务监控:分布系统最重要的一个方面就是服务监控和日志。  重构成微服务 不过，即使具备了上述条件，也并不是说就要抛弃现有的应用程序，在大多数情况下，我们无法做到。因此，我们要构建一种方法，依据它使用微服务重构现有的应用程序。虽然重构过程并不简单，但长远来看，重构一个单体应用可以一次性偿还所有的技术债务。
读后感 我基本赞同这篇文章所介绍的微服务特征及其优点，但还是有下列担忧： - 无论微服务与单体应用相比有多少优点，构建一个新的业务系统或重构一个老业务系统总归是有投入的。怎么选择一个投入/产出比阀值来确定启动微服务是否合理是一个很重要的问题。随着RESTful，云计算、Docker、持续交付等概念的深入人心，很多公司都想将业务迁移到云平台上去以适应今后的客户需求，微服务逐渐成为系统架构的一个代名词。我个人不觉得所有业务都应该一股脑地迁移到云平台上去，重构成微服务。还是要对每个业务进行定性/定量分析吧。 - 将一个单体应用分拆成微服务，微服务颗粒度的把握也是一个关键问题。颗粒度太粗的话，可能阻碍后继业务扩展，从而需要再次拆分。而过度的拆分服务，又会影响性能，例如原来一个JVM内部的调用就有可能变成微服务之间跨系统的通信，一个服务链上服务交互太多的话，响应会延迟很大。单体应用中仅某些关键点采用缓存，而微服务架构模式下每个服务都有可能采用缓存，对缓存的普遍使用跟分拆带来的性能下降有一定关系。 - 微服务架构模式下，监控相比单体应用也更加困难。服务的分散使得调用路径有时候会变得比较复杂，如何从业务维度对链路实行跟踪回溯，如何度量整体业务的吞吐，如何快速发现业务的瓶颈，是使用微服务必须解决的问题。 - 微服务采用多种开发语言，怎么进行代码复用？此外采用无论哪种架构，最关键的的还是能够胜任开发工作的人力资源。微服务可以异构/采用多种语言，说起来很容易。要一个团队负责的多个微服务采用异构&#43;多种开发语言，招聘起来就不容易，人员补充也不容易。就算没有招聘问题，一个开发者工作变动，开始负责另外一个微服务，还需要学习另一种系统或开发语言，也不是很容易立即上手。
其他阅读 Martin Fowler: Microservices Microservices - Not A Free Lunch!
</content>
    </entry>
    
     <entry>
        <title>了解OSM和Esri</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3osm%E5%92%8Cesri/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>geographic</tag><tag>mapping</tag><tag>openstreatmap</tag><tag>esri</tag>
        </tags>
        <content type="html">  学习SAS Visual Analytics Explorer时看到了地图提供程序模式竟然没有GoogleMap，而是OpenStreetMap和Esri。对没接触过的东东还比较好奇，搜了一下。
OpenStreetMap 开放街道地图（OpenStreetMap，简称OSM）是一个由地图制作爱好者组成的社区。这些爱好者提供并维护世界各地关于道路、小道、咖啡馆、铁路车站等各种各样的数据，目标是创造一个内容自由且能让所有人编辑的世界地图，并且让一般便宜的移动设备有方便的导航方案。OSM项目由英国人SteveCoast创立，概念启发自维基百科网站，以及英国以及其他地区私有地图数据占尽优势。一如维基百科等网站，OSM网站地图页有“编辑”按钮，亦有纪录修订历史。经注册的用户可上传GPS路径，及可编辑地图的矢量数据，包括使用OSM网站的编辑器或其他自由地理信息系统软件，如JOSM。OSM的地图由用户根据手持GPS设备、航空摄影照片、卫星视频、其他自由内容以至单靠用户由于对有关区域的熟悉而具有的本地知识绘制。地图的矢量数据以开放数据库授权方式授权。OSM网站由英国非营利组织OpenStreetMap基金会赞助维运。
既然有Google和Nokia这样的公司提供很好的地图商业产品，那为什么还需要一个像OpenStreetMap项目了？答案是简单的，作为一个社会，不应当有一家或几家公司在地理信息上进行垄断。地理信息是分享资源，当你将所有的这些权力给予一个单独的实体，你给予他们的权力就不止是告诉他们你的地理位置，更是在塑造它，从自己商业利益的角度显示地图上的内容。而在地图内容方面，OpenStreetMap即是中立的又是透明的。
Esri 美国环境系统研究所公司（Environmental Systems Research Institute,Inc.，简称Esri），是目前世界最大的地理信息系统技术供应商，其地理信息系统软件目前的全球市场占有率最高，公司最知名产品为ArcGIS。ArcGISOnline是一个面向全球用户的公有云GIS平台，是一种全新的GIS软件应用模式。ArcGISOnline包含了全球范围内的底图、地图数据、应用程序，以及可配置的应用模板和开发人员使用的 GIS 工具和 API，可用于创建Web 地图、发布GIS服务、共享地图、数据和应用程序等，以及管理组织的内容和多个用户。
SAS Visual Analysis Explorer里面在Esri模式下还需要选择ArcGISOnline的数据源（例如World Cities、World Street Map）。
参考 OpenStreetMap网站
Esri公司
Wiki: 开放街图
为什么世界需要 OpenStreetMap 开源道路地图
Wiki: 美国环境系统研究所公司
</content>
    </entry>
    
     <entry>
        <title>Jenkins超时设置</title>
        <url>https://mryqu.github.io/post/jenkins%E8%B6%85%E6%97%B6%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>jenkins</tag><tag>build</tag><tag>abort</tag><tag>timeout</tag><tag>configure</tag>
        </tags>
        <content type="html"> Jenkins在构建部署镜像时发生超时：
Build timed out (after 10 minutes). Marking the build as aborted. Build was aborted Finished: ABORTED  解决方法是在Jenkins当前项目下点击Configure菜单后，在BuildEnvironment配置项里修改超时策略。我把超时绝对值改大点就好了。 </content>
    </entry>
    
     <entry>
        <title>使用Vagrant Box</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8vagrant_box/</url>
        <categories>
          <category>Tool</category><category>Vagrant</category>
        </categories>
        <tags>
          <tag>vagrant</tag><tag>box</tag><tag>add</tag><tag>init</tag><tag>up</tag>
        </tags>
        <content type="html"> //Download a box to local disk $ vagrant box add my-box /path/to/the/new.box ... $ vagrant init my-box ... $ vagrant up ...  </content>
    </entry>
    
     <entry>
        <title>[Hadoop] 消除WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform.</title>
        <url>https://mryqu.github.io/post/hadoop_%E6%B6%88%E9%99%A4warn_util.nativecodeloader_unable_to_load_native-hadoop_library_for_your_platform/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>nativecodeloader</tag><tag>unable</tag><tag>native</tag><tag>library</tag>
        </tags>
        <content type="html"> 启动DFS或者执行hadoop fs命令总是得到告警util.NativeCodeLoader: Unable to load native-hadooplibrary for your platform&amp;hellip; using builtin-java classes whereapplicable：
hadoop@node50064:~$ start-dfs.sh 15/04/18 01:55:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [node50064.mryqu.com] node50064.mryqu.com: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-node50064.out node50069.mryqu.com: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node50069.out node51054.mryqu.com: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-node51054.out Starting secondary namenodes [node50069.mryqu.com] node50069.mryqu.com: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-node50069.out 15/04/18 01:55:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  解决方案为修改~/.bashrc：
export HADOOP_HOME=/usr/local/hadoop export HADOOP_PREFIX=$HADOOP_HOME export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_PREFIX/lib/native export HADOOP_OPTS=&amp;quot;$HADOOP_OPTS -Djava.library.path=$HADOOP_PREFIX/lib/native&amp;quot;  最后通过source~/.bashrc刷新配置文件。
</content>
    </entry>
    
     <entry>
        <title>[JPA] 重温CascadeType</title>
        <url>https://mryqu.github.io/post/jpa_%E9%87%8D%E6%B8%A9cascadetype/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>jpa</tag><tag>hibernate</tag><tag>cascadetype</tag>
        </tags>
        <content type="html"> 今天看了一篇帖子A beginner’s guide to JPA and Hibernate Cascade Types，对JPA/Hibernate中OneToOne、OneToMany、Many-To-Many关系下CascadeType的使用讲的很详尽，有代码示例也有SQL输出。作者VladMihalcea，著有High-Performance Java Persistence一书，应该淘一本学习学习。
</content>
    </entry>
    
     <entry>
        <title>[Gradle] 输出依赖包</title>
        <url>https://mryqu.github.io/post/gradle_%E8%BE%93%E5%87%BA%E4%BE%9D%E8%B5%96%E5%8C%85/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>dependencies</tag><tag>jar</tag><tag>output</tag>
        </tags>
        <content type="html"> 下面我以https://spring.io/guides/gs/spring-boot/中的gs-spring-boot项目为例，使用Gradle输出依赖包。
首先对build.gradle做如下修改：
buildscript { repositories { mavenCentral() } dependencies { classpath(&amp;quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.2.RELEASE&amp;quot;) } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &#39;spring-boot&#39; jar { baseName = &#39;gs-spring-boot&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 task copyToLib(type: Copy) { print configurations into &amp;quot;$buildDir/dep-libs&amp;quot; from configurations.runtime } build.dependsOn(copyToLib) dependencies { compile(&amp;quot;org.springframework.boot:spring-boot-starter-web&amp;quot;) // tag::actuator[] compile(&amp;quot;org.springframework.boot:spring-boot-starter-actuator&amp;quot;) // end::actuator[] // tag::tests[] testCompile(&amp;quot;org.springframework.boot:spring-boot-starter-test&amp;quot;) // end::tests[] }  首先可以在命令行中看到：  [configuration &amp;lsquo;:archives&amp;rsquo;, configuration &amp;lsquo;:compile&amp;rsquo;, configuration &amp;lsquo;:default&amp;rsquo;, configuration &amp;lsquo;:runtime&amp;rsquo;, configuration &amp;lsquo;:testCompile&amp;rsquo;, configur:clean&amp;rsquo;:testRuntime&amp;rsquo;, configuration &amp;lsquo;:versionManagement&amp;rsquo;] 
跟下面Java插件- 依赖配置相比，少了一些，可能是根据build.gradle生成的configurations。 此时在gs-spring-boot\complete\build\dep-libs目录下有32个jar文件；如果改成from configurations.testCompile，则该目录下会有48个jar文件。
最后要说的的是，我输出这些依赖包的目的是为了可以摆脱Gradle，通过java命令执行程序或进行测试。
</content>
    </entry>
    
     <entry>
        <title>[Gradle] 执行Java类</title>
        <url>https://mryqu.github.io/post/gradle_%E6%89%A7%E8%A1%8Cjava%E7%B1%BB/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>java</tag><tag>class</tag><tag>execute</tag><tag>run</tag>
        </tags>
        <content type="html">  需求 我想用Gradle脚本执行下列Java类Hello123.java：
import java.util.Arrays; public class Hello123 { public static void main(String[] args) { System.out.println(&amp;quot;args:&amp;quot;&#43; Arrays.toString(args)); } }  测试一：创建execute任务 build.gralde apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; task execute(type: { main = System.getProperty(&amp;quot;exec.mainClass&amp;quot;) classpath = sourceSets.main.runtimeClasspath systemProperties System.getProperties() if(System.getProperty(&amp;quot;exec.args&amp;quot;)) args System.getProperty(&amp;quot;exec.args&amp;quot;).split() } sourceCompatibility = 1.8 targetCompatibility = 1.8  测试结果 测试二：重写run任务 build.gralde apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; apply plugin: &amp;quot;application&amp;quot; mainClassName = &amp;quot;NonExistentClass&amp;quot; task run (type: { main = System.getProperty(&amp;quot;exec.mainClass&amp;quot;) classpath = sourceSets.main.runtimeClasspath systemProperties System.getProperties() if(System.getProperty(&amp;quot;exec.args&amp;quot;)) args System.getProperty(&amp;quot;exec.args&amp;quot;).split() } sourceCompatibility = 1.8 targetCompatibility = 1.8  测试结果 参考 Gradle to execute Java class (without modifying build.gradle)
What is the gradle equivalent of maven&amp;rsquo;s exec plugin for running Java apps?
gradle task pass arguments to java application
Problems passing system properties and parameters when running Java class via Gradle
[](https://docs.gradle.org/current/userguide/application_plugin.html)
</content>
    </entry>
    
     <entry>
        <title>[Gradle] 阻止build任务执行测试任务</title>
        <url>https://mryqu.github.io/post/gradle_%E9%98%BB%E6%AD%A2build%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E6%B5%8B%E8%AF%95%E4%BB%BB%E5%8A%A1/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>skip</tag><tag>task</tag>
        </tags>
        <content type="html"> 在执行gradle build时想要阻止执行测试任务，方法如下：
 第一种方法：如Gradle用户指南的14.8 Skipping tasks所说，在build.gradle里设置&amp;rdquo;test.enabled=false&amp;rdquo;，执行gradle build 第二种方法：在build.gradle里设置&amp;rdquo;check.dependsOn.remove(test)&amp;ldquo;，执行gradle build 第三种方法：执行gradle build -x test  </content>
    </entry>
    
     <entry>
        <title>[Gradle] 强制重新下载依赖</title>
        <url>https://mryqu.github.io/post/gradle_%E5%BC%BA%E5%88%B6%E9%87%8D%E6%96%B0%E4%B8%8B%E8%BD%BD%E4%BE%9D%E8%B5%96/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>dependency</tag><tag>refresh</tag><tag>update</tag><tag>--refresh-dependenci</tag>
        </tags>
        <content type="html"> 强制Gradle重新下载依赖的方式有两种： - 在Gradle命令中加入&amp;ndash;refresh-dependencies选项。该选项会让Gradle忽略已解析模块和构件的所有缓存项，对所配置的仓库重新进行解析，动态计算版本、更新模块和下载构件。 - 删除Gralde的缓存目录~/.gradle/caches。这个有点过于粗暴。
示例：
gradlew clean --refresh-dependencies build bootRun  </content>
    </entry>
    
     <entry>
        <title>学习Gradle</title>
        <url>https://mryqu.github.io/post/%E5%AD%A6%E4%B9%A0gradle/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag>
        </tags>
        <content type="html">  当前参与的SAS Workflow2.0原型开发项目，美国那边的项目组经营将代码编译脚本从Ant切换成Gradle了。此外我司已经在去年就将Gradle列入了技术雷达的正式采用象限，感觉有必要学习一下Gradle，扩充自己的开发能力。 Gradle官网上提供的图书信息链接中有两本免费O&amp;rsquo;Reilly出版社图书： - Building and Testing With Gradle - Gradle Beyond the Basics一般软件官网文档质量不错的情况下，我优先阅读最新的官网文档，所以我先看了《Gradle入门》和《Gradle用户指南》。
参考 Gradle官网
Gradle文档
Spring提供的Gradle入门
Gradle用户指南
Groovy官网
Groovy&#43;&#43;
Java Build Tools: Ant vs Maven vs Gradle
</content>
    </entry>
    
     <entry>
        <title>尝试Bootply和Codeply</title>
        <url>https://mryqu.github.io/post/%E5%B0%9D%E8%AF%95bootply%E5%92%8Ccodeply/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>bootply</tag><tag>codeply</tag><tag>bootstrap</tag><tag>编辑器</tag><tag>代码仓库</tag>
        </tags>
        <content type="html">  Bootply Bootply被称为是Bootstrap的活动平台。它不但是一个Bootstrap的编辑器和生成器，同时也拥有非常广泛的代码库。Bootply编辑器可以让你拖拽Bootstrap组件并可以编辑你自己的代码。 Bootply同时整合了其他流行的Bootstrap插件、微型代码库和框架。你也可以借助其他工具的使用像FontAwesome, jQuery &#43; jQuery UI, Bootstrap Select, FuelUX, AngularJS,Google Maps 等等。 Codeply Codeply是 - 一个HTML/CSS/JavaScript编辑器 - 一个响应式设计活动平台和开发工具 - 一个Web设计者和前段开发者的社区 - 一个代码片段和示例的代码仓库
Codeply编辑器也可以让你拖拽Bootstrap组件并可以编辑你自己的代码。 Codeply也整合了很多响应式框架和代码库。 响应式框架 Bootstrap3 Foundation Kube MaterializeCSS NoFramework PureCSS SemanticUI  Skeleton Unsemantic 可用的JS和CSS库 Angular AngularAnimate AngularAria AngularMaterial AngularUI AngularUI Bootstrap AnimateCSS Backbone BootstrapDatepicker BootstrapSelect DropZone Ember Ember.js FastClick FontAwesome FullCalendar GoogleMaps API Hammer.js Handlebars.js Ionic Isotope Jasny jQueryUI Knockout Masonry Minicolors PrototypeJs Raphael RequireJs UnderscoreJs 此外，Codeply还能针对不同大小的屏幕进行测试。 参考 Bootply网站
Codeply网站
15 Best Bootstrap Tools for Designers
</content>
    </entry>
    
     <entry>
        <title>GoJS国际化和本地化支持</title>
        <url>https://mryqu.github.io/post/gojs%E5%9B%BD%E9%99%85%E5%8C%96%E5%92%8C%E6%9C%AC%E5%9C%B0%E5%8C%96%E6%94%AF%E6%8C%81/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>gojs</tag><tag>国际化</tag><tag>本地化</tag><tag>i18n</tag><tag>i10n</tag>
        </tags>
        <content type="html"> 查了一下GoJS国际化支持文档，藏的还挺深，放在GoJS 部署里了。
GoJS应用可以显示非拉丁语文本。示例可见Japanese Family Tree. GoJS不操作货币值、日期值或地址，因此对这些数据类型没有本地化问题；GoJS不包含任何自己的图标（图像）或光标。 GoJS不显示任何内建文本字符串，因此无需转换工作。用于往控制台输出的错误和告警消息仅用于程序员调试，而不会面向最终用户。当读写JSON、几何路径字符串或CSS颜色时，其中的数值读写仅用于内部使用且为非本地化格式。 所有用户可见文本都完全在程序员的控制之下。为了本地化，可以很方便地使用Binding中的转换函数。TextEditingTool使用HTMLTextArea元素实现原地文本输入和编辑，从而利用浏览器对输入法编辑器的支持。
GoJS不像OpenUI5那样根据Locale相应从I18Nproperties文件获取本地化文本，而是通过下列方式提供国际化和本地化支持： - 提供显示非拉丁语文本的能力 - 将自身摘出来，确保自身实现没有国际化和本地化的要求 - 将一切国际化的工作推出去，程序员可以直接设置国际化显示文本，也可以实现Binding(targetprop,sourceprop, conv)中的转换工具方法在客户端进行本地化。
</content>
    </entry>
    
     <entry>
        <title>GoJS中的类</title>
        <url>https://mryqu.github.io/post/gojs%E4%B8%AD%E7%9A%84%E7%B1%BB/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>gojs</tag><tag>class</tag><tag>diagram</tag><tag>类图</tag><tag>javascript</tag>
        </tags>
        <content type="html">  GoJS中的类 GoJS API文档介绍了GoJS中的类，不过在调试GoJS时发现有3个类不是通过go对象访问的。此外有14个类有介绍，但没有显示在左侧导航栏里。GoJS的很多类、方法和属性名都是经过混淆的，不过起码这一层还是很好对上号的。 Diagram ClassesAdornmentAdornmentAnimationManager&amp;nbsp;CommandHandlerCommandHandlerDiagramDiagramDiagramEventDiagramEventGraphObjectGraphObjectGroupGroupInputEventInputEventLayerLayerLinkLinkNodeNodeOverviewOverviewPalettePalettePanelPanelPartPartPicturePicturePlaceholderPlaceholderRowColumnDefinitionRowColumnDefinitionShapeShapeTextBlockTextBlockGeometry ClassesBrushBrushGeometryGeometryMarginMarginPathFigurePathFigurePathSegmentPathSegmentPointPointRectRectSizeSizeSpotSpotModel ClassesBindingBindingChangedEventChangedEventGraphLinksModelGraphLinksModelModelModelTransactionTransactionTreeModelTreeModelUndoManagerUndoManagerLayout ClassesCircularLayoutCircularLayoutCircularNetworkCircularVertexCircularEdgeForceDirectedLayoutForceDirectedLayoutForceDirectedNetworkForceDirectedVertexForceDirectedEdgeGridLayoutGridLayoutLayeredDigraphLayoutLayeredDigraphLayoutLayeredDigraphNetworkLayeredDigraphVertexLayeredDigraphEdgeLayoutLayoutLayoutNetworkLayoutNetworkLayoutVertexLayoutEdgeTreeLayoutTreeLayoutTreeNetworkTreeVertexTreeEdgeTool ClassesActionToolActionToolClickCreatingToolClickCreatingToolClickSelectingToolClickSelectingToolContextMenuToolContextMenuToolDraggingToolDraggingToolDragSelectingToolDragSelectingToolLinkingBaseToolLinkingBaseToolLinkingToolLinkingToolLinkReshapingToolLinkReshapingToolPanningToolPanningToolRelinkingToolRelinkingToolResizingToolResizingToolRotatingToolRotatingToolTextEditingToolTextEditingToolToolToolToolManagerToolManagerCollection ClassesIterableIteratorListListMapMapSetSet
GoJS类图 </content>
    </entry>
    
     <entry>
        <title>图表工具JS库</title>
        <url>https://mryqu.github.io/post/%E5%9B%BE%E8%A1%A8%E5%B7%A5%E5%85%B7js%E5%BA%93/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>图表</tag><tag>diagram</tag><tag>javascrip</tag><tag>html5</tag><tag>libarary</tag>
        </tags>
        <content type="html"> 由于目前工作用到GoJS，所有也关注一下图表工具JS库。10 JavaScript libraries to draw your own diagrams一文给出了10个JS库的功能分析和比较图表。
|库|许可|语言 / 基础架构|高/低级|内建编辑器|Github (04/02/2015) |&amp;mdash;&amp;ndash; |JointJS|MPL|HTMLJavascriptSVG|高|无|1388星265分支（fork） |Rappid|商业1 500,00 €|HTMLJavascriptSVG|高|有|&amp;nbsp; |Mxgraph|商业4300.00 €|HTMLJavascriptSVG|高|有|&amp;nbsp; |GoJS|商业$1,350.00|HTMLCanvasJavascript|高|有|&amp;nbsp; |Raphael|MIT|HTMLJavascriptSVG|低|无|7105星1078分支（fork） |Draw2D|GPL2商业|HTMLJavascriptSVG|中|无|&amp;nbsp; |D3|BSD|HTMLJavascriptSVG|低|无|36218星9142分支（fork） |FabricJS|MIT|HTMLCanvasjavasript|低|无|4127星705分支（fork） |paperJS|MIT|HTMLCanvasjavascript|低|无|4887星496分支（fork） |JsPlumb|MIT/GPL2|HTMLJavascript|中|无|2161星563分支（fork）
这里面D3在数据科学领域成绩比计突出，是数据可视化的一个重要工具。此外我在接触过的开源项目有几个用到了Raphael（包括Activiti）。其他库还没有接触过。 我为什么选择 D3.js一文中将D3和Raphael进行了对比。Raphael是一个矢量图的API，专注于对矢量图形的操作。D3是一个数据可视化展示的API，通过数据与图形进行绑定。 图表工具JS库除了上面帖子提及的外，还有很多。如果有机会的话，我会在GoJS之外更多关注D3和Raphael。
</content>
    </entry>
    
     <entry>
        <title>[Gradle] 设置项目属性的三种方式</title>
        <url>https://mryqu.github.io/post/gradle_%E8%AE%BE%E7%BD%AE%E9%A1%B9%E7%9B%AE%E5%B1%9E%E6%80%A7%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>project</tag><tag>property</tag><tag>set</tag>
        </tags>
        <content type="html">  命令行 gradle bootRun -PyquPropKey=yquPropValue  build.properties yquPropKey=yquPropValue  gradle.properties 添加ext块：
ext { yquPropKey=yquPropValue }  </content>
    </entry>
    
     <entry>
        <title>Python: installing pymongo with Anaconda</title>
        <url>https://mryqu.github.io/post/python_installing_pymongo_with_anaconda/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>anaconda</tag><tag>pymongo</tag><tag>mongodb</tag><tag>安装</tag>
        </tags>
        <content type="html"> 在Anaconda发行版Python上通过conda install pymongo安装MongoDBpython驱动，结果失败了，最终通过conda install -c https://conda.binstar.org/anaconda pymongo安装成功。
BinstarBinstar和Anaconda都是同一家的产品，是Continuum Analytics推出的一个包管理服务，托管公开的pip和conda。 安装记录如下：
C:\tools\Anaconda&amp;gt;conda list # packages in environment at C:\tools\Anaconda: # _license 1.1 py27_0 anaconda 2.0.1 np18py27_0 argcomplete 0.6.7 py27_0 astropy 0.3.2 np18py27_0 atom 0.3.7 py27_0 backports.ssl-match-hostname 3.4.0.2 beautiful-soup 4.3.1 py27_0 beautifulsoup4 4.3.1 binstar 0.5.3 py27_0 bitarray 0.8.1 py27_1 blaze 0.5.0 np18py27_1 blz 0.6.2 np18py27_0 bokeh 0.4.4 np18py27_1 boto 2.28.0 py27_0 casuarius 1.1 py27_0 cdecimal 2.3 py27_1 chaco 4.4.1 np18py27_0 colorama 0.2.7 py27_0 conda 3.5.5 py27_0 conda-build 1.3.5 py27_0 configobj 5.0.5 py27_0 cubes 0.10.2 py27_4 cython 0.20.1 py27_0 datashape 0.2.0 np18py27_1 dateutil 2.1 py27_2 docutils 0.11 py27_0 dynd-python 0.6.2 np18py27_0 enable 4.3.0 np18py27_2 enaml 0.9.1 py27_1 flask 0.10.1 py27_1 future 0.12.1 py27_0 gevent 1.0.1 py27_0 gevent-websocket 0.9.3 py27_0 greenlet 0.4.2 py27_0 grin 1.2.1 py27_2 h5py 2.3.0 np18py27_0 ipython 2.1.0 py27_2 ipython-notebook 2.1.0 py27_0 ipython-qtconsole 2.1.0 py27_0 itsdangerous 0.24 py27_0 jdcal 1.0 py27_0 jinja2 2.7.2 py27_0 kiwisolver 0.1.2 py27_0 launcher 0.1.5 py27_0 libpython 1.0 py27_1 llvmpy 0.12.6 py27_0 lxml 3.3.5 py27_0 markupsafe 0.18 py27_1 matplotlib 1.3.1 np18py27_2 menuinst 1.0.3 py27_0 mingw 4.7 1 mock 1.0.1 py27_0 multipledispatch 0.4.3 py27_0 networkx 1.8.1 py27_0 nltk 2.0.4 np18py27_0 nose 1.3.3 py27_0 numba 0.13.2 np18py27_0 numexpr 2.3.1 np18py27_0 numpy 1.8.1 py27_0 openpyxl 1.8.5 py27_0 pandas 0.14.0 np18py27_0 patsy 0.2.1 np18py27_0 pep8 1.5.6 py27_0 pil 1.1.7 py27_0 pip 1.5.6 py27_0 ply 3.4 py27_0 psutil 2.1.1 py27_0 py 1.4.20 py27_0 pycosat 0.6.1 py27_0 pycparser 2.10 py27_0 pycrypto 2.6.1 py27_1 pyface 4.4.0 py27_0 pyflakes 0.8.1 py27_0 pygments 1.6 py27_0 pyparsing 2.0.1 py27_0 pyqt 4.10.4 py27_0 pyreadline 2.0 py27_0 pytables 3.1.1 np18py27_0 pytest 2.5.2 py27_0 python 2.7.7 1 python-dateutil 1.5 pytz 2014.3 py27_0 pywin32 218.4 py27_0 pyyaml 3.11 py27_0 pyzmq 14.3.0 py27_0 requests 2.3.0 py27_0 rope 0.9.4 py27_1 runipy 0.1.0 py27_0 scikit-image 0.10.0 np18py27_0 scikit-learn 0.14.1 np18py27_2 scipy 0.14.0 np18py27_0 setuptools 3.6 py27_0 six 1.6.1 py27_0 sphinx 1.2.2 py27_0 spyder 2.3.0rc1 py27_0 spyder-app 2.3.0rc1 py27_0 sqlalchemy 0.9.4 py27_0 ssl_match_hostname 3.4.0.2 py27_0 statsmodels 0.5.0 np18py27_1 sympy 0.7.5 py27_0 tables 3.1.1 tornado 3.2.1 py27_0 traits 4.4.0 py27_0 traitsui 4.4.0 py27_0 ujson 1.33 py27_0 werkzeug 0.9.6 py27_0 wsgiref 0.1.2 xlrd 0.9.3 py27_0 xlsxwriter 0.5.5 py27_0 xlwings 0.1.0 py27_0 xlwt 0.7.5 py27_0 C:\tools\Anaconda&amp;gt;conda install pymongo Fetching package metadata: .Could not connect to http://repo.continuum.io/pkgs/pro/win-64/ .Could not connect to http://repo.continuum.io/pkgs/free/win-64/ Error: No packages found matching: pymongo C:\tools\Anaconda&amp;gt;conda install -c https://conda.binstar.org/anaconda pymongo Fetching package metadata: .Could not connect to http://repo.continuum.io/pkgs/pro/win-64/ .Could not connect to http://repo.continuum.io/pkgs/free/win-64/ . Solving package specifications: . Package plan for installation in environment C:\tools\Anaconda: The following packages will be downloaded: package | build ---------------------------|----------------- conda-3.9.1 | py27_0 206 KB conda-env-2.1.3 | py27_0 54 KB pymongo-2.8 | py27_0 241 KB requests-2.5.3 | py27_0 588 KB ------------------------------------------------------------ Total: 1.1 MB The following packages will be UN-linked: package | build ---------------------------|----------------- conda-3.5.5 | py27_0 requests-2.3.0 | py27_0 The following packages will be linked: package | build ---------------------------|----------------- conda-3.9.1 | py27_0 hard-link conda-env-2.1.3 | py27_0 hard-link pymongo-2.8 | py27_0 hard-link requests-2.5.3 | py27_0 hard-link Proceed ([y]/n)? y Fetching packages ... conda-3.9.1-py 100% |###############################| Time: 0:00:01 137.71 kB/s conda-env-2.1. 100% |###############################| Time: 0:00:00 109.51 kB/s pymongo-2.8-py 100% |###############################| Time: 0:00:01 136.74 kB/s requests-2.5.3 100% |###############################| Time: 0:00:13 44.13 kB/s Extracting packages ... [ COMPLETE ] |#################################################| 100% Unlinking packages ... [ COMPLETE ] |#################################################| 100% Linking packages ... [ COMPLETE ] |#################################################| 100% C:\tools\Anaconda&amp;gt;conda info Current conda install: platform : win-64 conda version : 3.9.1 conda-build version : 1.3.5 python version : 2.7.7.final.0 requests version : 2.5.3 root environment : C:\tools\Anaconda (writable) default environment : C:\tools\Anaconda envs directories : C:\tools\Anaconda\envs package cache : C:\tools\Anaconda\pkgs channel URLs : http://repo.continuum.io/pkgs/free/win-64/ http://repo.continuum.io/pkgs/free/noarch/ http://repo.continuum.io/pkgs/pro/win-64/ http://repo.continuum.io/pkgs/pro/noarch/ config file : None is foreign system : False  </content>
    </entry>
    
     <entry>
        <title>[Hadoop] Windows平台编译Hadoop2.6.0笔记</title>
        <url>https://mryqu.github.io/post/hadoop_windows%E5%B9%B3%E5%8F%B0%E7%BC%96%E8%AF%91hadoop2.6.0%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>windows</tag><tag>hadoop</tag><tag>2.6.0</tag><tag>build</tag>
        </tags>
        <content type="html">  环境 64位虚拟机及64位 Windows Server 2008 R2
所需工具  JDK7 Maven .NET Framework 4 Microsoft Windows SDK 7.1 安装前一定要先卸载比Microsoft Visual C&#43;&#43; 2010 x86Redistributable - 10.0.30319 更高的版本。 Microsoft Visual C&#43;&#43; 2010 Service Pack 1 Compiler Update for the Windows SDK 7.1 Cygwin (x64) Protocol Buffers 2.5.0 CMake 3.2.1 安装时选择添加CMake到所有用户的PATH环境变量。 hadoop-2.6.0源文件压缩包 解压至c:\hadoop-2.6.0-src   编译Hadoop2.6.0  进入Windows SDK 7.1 Command Prompt 在c:\执行buildHadoop.bat，其内容如下：
setlocal set Platform=x64 set CYGWIN_ROOT=C:\cygwin64 set JAVA_HOME=C:\tools\Java\jdk7 set M2_HOME=C:\tools\apache-maven set MS_BUILD_PATH=C:\Windows\Microsoft.NET\Framework64\v4.0.30319 set MS_SDK=C:\Program Files\Microsoft SDKs\Windows\v7.1 set CMAKE_PATH=C:\tools\CMake set PROTOBUF_PATH=C:\tools\protoc-2.5.0-win32 set PATH=%PROTOBUF_PATH%;%CMAKE_PATH%\bin;%MS_BUILD_PATH%;%MS_SDK%\bin\;%MS_SDK%\Include\;%M2_HOME%\bin;%CYGWIN_ROOT%\bin;%PATH% set HADOOP_SRC=hadoop-2.6.0-src pushd hadoop-2.6.0-src attrib * -R /S icacls * /grant Everyone:(OI)(CI)F set log=..\mvn.log mvn package -Pdist,native-win -DskipTests -Dtar -e -X &amp;gt;&amp;gt; %log% 2&amp;gt;&amp;amp;1 popd   编译结果： - C:\hadoop-2.6.0-src\hadoop-dist\target/hadoop-2.6.0.tar.gz - C:\hadoop-2.6.0-src\hadoop-dist\target\hadoop-dist-2.6.0-javadoc.jar
解决故障 缺ammintrin.h文件 c:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\include\intrin.h(26): fatal error C1083: Cannot open include file: &#39;ammintrin.h&#39;: No such file or directory [C:\hadoop-2.6.0-src\hadoop-common-project\hadoop-common\src\main\native\native.vcxproj]  解决方法： 将此ammintrin.m下载改名为ammintrin.h，置于C:\ProgramFiles (x86)\Microsoft Visual Studio 10.0\VC\INCLUDE\目录下。
参考 Hadoop 2.6.0 Installation Guide (Windows)
Yet Another Hadoop Build Tutorial for Windows
Build instructions for Hadoop
Fix problem when mex .cpp file
[](http://stackoverflow.com/questions/30485525/missing-ammintrin-h-when-compiling-hadoop-on-windows)
</content>
    </entry>
    
     <entry>
        <title>粗览基于Eclipse RCP架构的Activiti Designer实现(图文版)</title>
        <url>https://mryqu.github.io/post/%E7%B2%97%E8%A7%88%E5%9F%BA%E4%BA%8Eeclipse_rcp%E6%9E%B6%E6%9E%84%E7%9A%84activiti_designer%E5%AE%9E%E7%8E%B0%E5%9B%BE%E6%96%87%E7%89%88/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>designer</tag><tag>bpmn</tag><tag>eclipse</tag><tag>rcp</tag>
        </tags>
        <content type="html">  org.activiti.designer.feature contains an Eclipse feature definition, which groups the variousprojects into the installable Activiti Designer feature.
org.activiti.designer.eclipse plug-in contains the main extensions of Eclipse extension points and a lotof shared code for working with the model, saving resources andutilities.
org.activiti.designer.eclipse.extension.ExportMarshallerextension-point Activiti Designer Export Marshaller: Use this extension point toprovide custom output marshallers when Activiti diagrams areexported.
org.activiti.designer.eclipse.extension.ProcessValidatorextension-point Activiti Designer Process Validator: Use this extension point toprovide validation when Activiti diagrams are validated.
org.activiti.designer.eclipse.extension.IconProviderextension-point Icon Provider: Use this extension point to provide custom iconproviders to be used for displaying icons at certain places inActiviti Designer.
org.activiti.designer.eclipse.extension.PaletteExtensionProviderextension-point Palette extension Provider: Use this extension point to providecustom palette in diagram editor of Activiti Designer.
org.eclipse.ui.perspectives extension To add Activiti perspective factory to the workbench. org.eclipse.ui.newWizards extension To register Activiti project/diagram creation wizardextensions. org.eclipse.ui.editors extension To add &amp;ldquo;Activiti Diagram Editor&amp;rdquo; to the workbench. org.eclipse.ui.views extension To define &amp;ldquo;Activiti Explorer&amp;rdquo; and &amp;ldquo;Activiti Cloud Editor&amp;rdquo; view forthe workbench.
org.eclipse.ui.navigator.viewer extension To define the viewer within the &amp;ldquo;Activiti Explorer&amp;rdquo; and &amp;ldquo;ActivitiCloud Editor&amp;rdquo; view - Defines popup menu for the viewer - Binds content and actions to the viewer - Additionally defines filter extensions for the Designer filtersto the default Project Explorer
org.eclipse.ui.navigator.navigatorContent extension To add &amp;ldquo;Activiti Diagram Contents&amp;rdquo;, &amp;ldquo;Activiti Project Contents&amp;rdquo; and&amp;rdquo;Activiti Cloud Editor content&amp;rdquo; to the navigator.
org.eclipse.ui.preferencePages extension To declare Activiti cloud editor page to the preference dialogbox. org.eclipse.ui.commands extension To declare refreh command to Activiti Cloud Editor Navigator.
org.eclipse.ui.handlers extension a To declare download handler to Activiti Cloud Editor Navigatorwhich will trigger a download dialog.
org.eclipse.ui.menus extension To declare &amp;ldquo;Download&amp;rdquo; menu to Activiti Cloud EditorNavigator. org.eclipse.ui.popupMenus extension To declare &amp;ldquo;Activiti Editor&amp;rdquo; popup menu with two actions: - Upload new version - Download latest version
org.eclipse.graphiti.ui.imageProviders extension To register Activiti&amp;rsquo;s image provider for Activiti Designer? The source location of these icons are Activiti-Designer\org.activiti.designer.eclipse\icons.
org.eclipse.wst.xml.core.catalogContributions extension To register XML schemas in the catalog from a codecontribution - BPMN20.xsd - activiti-bpmn-extensions-5.4.xsd
org.eclipse.core.resources.natures extension To register Activiti project nature org.eclipse.core.resources.markers extension Activiti Marker?
org.eclipse.core.resources.markers extension Activiti General Marker?
org.eclipse.core.resources.markers extension Activiti Marshaller Marker?
org.eclipse.core.resources.markers extension Activiti Validator Marker?
org.eclipse.core.contenttype.contentTypes extension To declare &amp;ldquo;Activiti Diagram Editor File&amp;rdquo; content type which havefile extension &amp;ldquo;.bpmn&amp;rdquo;.
org.eclipse.core.runtime.preferences extension To set the default values of &amp;ldquo;Activiti&amp;rdquo; and &amp;ldquo;Activiti cloud editor&amp;rdquo;preferences
org.eclipse.core.exp_ressions.definitions extension To check if the selected node is mode node in Activiti Cloud EditorNavigator.
org.activiti.designer.libs plug-in Provide APIs in the below libraries/projects to other plug-ins: - activiti-bpmn-converter - activiti-image-generator - activiti-simple-workflow - activiti-simple-workflow-alfresco - Apache Chemistry: Content Management InteroperabilityServices - Apache Commons Logging - Apache HttpComponents - Joda-Time: provides a quality replacement for the Java date andtime classes
org.activiti.designer.gui plug-in contains most of the UI code, mainly functionality that utilizesGraphiti for drawing diagrams.
org.eclipse.graphiti.ui.diagramTypes extension Activiti BPMN diagram type: as Activiti create a diagram typeprovider for a diagram type which does not exist in the repository,it is necessary to provide the information about the new diagramtype.
org.eclipse.graphiti.ui.diagramTypeProviders extension To register Activiti&amp;rsquo;s diagram type provider for Activiti BPMNeditor.
org.eclipse.graphiti.ui.imageProviders extension To register Activiti&amp;rsquo;s image provider for Activiti BPMNeditor.For each diagram at org.activiti.designer.PluginImage, it has aicon at palette.These keys and paths of icons are defined at classorg.activiti.designer.PluginImage, and source location of theseicons are Activiti-Designer\org.activiti.designer.gui\icons.
org.eclipse.ui.views.properties.tabbed.propertyContributorextension Property Contributor for Activiti tabbed properties view. It mostimportantly identifies the unique contributor identifier forActiviti tabs and sections.Activiti tabbed properties view apply to whole process or aspecified BPMN element such as StartEvent, UserTask. org.eclipse.ui.views.properties.tabbed.propertyTabsextension To describes the tabs for Activiti contributor. For example,Process, Data Object and Listener tabs in tabbed properties viewfor Activiti process.
org.eclipse.ui.views.properties.tabbed.propertySectionsextension To describes the sections for Activiti contributor. OnepropertySection will create a property sheet page (form) on aspecified tab for a process or a BPMN element.
org.eclipse.ui.preferencePages extension To describes Activiti pages to the preference dialog box such as&amp;rdquo;Activiti&amp;rdquo;, &amp;ldquo;Activiti languages&amp;rdquo;, &amp;ldquo;Alfresco settings&amp;rdquo;, &amp;ldquo;Editor&amp;rdquo; and&amp;rdquo;Save Actions&amp;rdquo; page. org.eclipse.ui.popupMenus extension To add add new actions to context menus: - &amp;ldquo;Create deployment artifacts&amp;rdquo; action at popup menu for aActiviti project - &amp;ldquo;Generate unit test&amp;rdquo; action at popup menu for a bpmn a Activitidiagram editor file
org.activiti.designer.eclipse.extension.IconProviderextension Activiti Designer GUI Icon Provider: provide icon providers to beused for displaying icons at certain places in the Designer.The source location of these icons are Activiti-Designer\org.activiti.designer.gui\icons.
org.activiti.designer.integration plug-in provides APIs to developers creating CustomServiceTaskextensions.
org.activiti.designer.help plug-in org.eclipse.help.toc extension help files about usage and preferences for end users
org.activiti.designer.validation.bpmn20 plug-in org.activiti.designer.eclipse.extension.ProcessValidatorextension Validate diagrams such as ScriptTask, ServiceTask, UserTask,SequenceFlow and SubProcess according to BPMN 2.0 rules.
org.activiti.designer.util plug-in Provoide utility APIs under org.activiti.designer.util to otherplug-ins
org.activiti.designer.kickstart.eclipse plug-in org.eclipse.ui.newWizards extension To register Activiti kickstart project/process diagram/formcreation wizard extensions. org.eclipse.ui.editors extension To add &amp;ldquo;Kickstart Process Editor&amp;rdquo; and &amp;ldquo;Kickstart Form Editor&amp;rdquo; tothe workbench. org.eclipse.ui.views extension To define &amp;ldquo;CMIS Navigator&amp;rdquo; view for the workbench.
org.eclipse.ui.navigator.viewer extension To define the viewer within the &amp;ldquo;CMIS Navigator&amp;rdquo; view - Defines popup menu for the viewer - Binds content to the viewer
org.eclipse.ui.navigator.navigatorContent extension To add &amp;ldquo;CMIS Navigator Content&amp;rdquo; to the navigator.
org.eclipse.ui.preferencePages extension To declare &amp;ldquo;Kickstart settings&amp;rdquo; page to the preference dialogbox. org.eclipse.ui.popupMenus extension To declare &amp;ldquo;kickstart&amp;rdquo; popup menu with &amp;ldquo;Synchronize withrepository&amp;rdquo; action org.eclipse.ui.menus extension To declare &amp;ldquo;Download&amp;rdquo;, &amp;ldquo;Delete&amp;rdquo; and &amp;ldquo;Rename&amp;rdquo; menu to CMISNavigator. org.eclipse.ui.commands extension To declare refreh command to CMIS Navigator.
org.eclipse.ui.handlers extension To declare download handler to CMIS Navigator.
org.eclipse.ui.handlers extension To declare delete handler to CMIS Navigator.
org.eclipse.ui.handlers extension To declare rename handler to CMIS Navigator.
org.eclipse.ui.exportWizards extension To register Activiti kickstart process export wizardextension org.eclipse.graphiti.ui.imageProviders extension The classorg.activiti.designer.kickstart.eclipse.common.ActivitiEclipseImageProvidernot found!!!
org.eclipse.core.contenttype.contentTypes extension To declare &amp;ldquo;Kickstart Process Editor File&amp;rdquo; content type which havefile extension &amp;ldquo;.kickproc&amp;rdquo;. To declare &amp;ldquo;Kickstart Form Editor File&amp;rdquo; content type which havefile extension &amp;ldquo;.kickform&amp;rdquo;.
org.eclipse.core.exp_ressions.definitions extension To check if the selected node is folder or document node in CMISNavigator.
org.eclipse.core.runtime.preferences extension To set the default values of &amp;ldquo;Kickstart settings&amp;rdquo; preference.
org.activiti.designer.kickstart.gui.form plug-in org.eclipse.graphiti.ui.diagramTypes extension Kickstart Form diagram type: as Activiti Kickstart Form create adiagram type provider for a diagram type which does not exist inthe repository, it is necessary to provide the information aboutthe new diagram type.
org.eclipse.graphiti.ui.diagramTypeProviders extension To register a diagram type provider for Activiti KickstartForm.
org.eclipse.graphiti.ui.imageProviders extension To register Activiti&amp;rsquo;s image provider for Activiti KickstartForm.These keys and paths of icons are defined at classorg.activiti.designer.kickstart.form.KickstartFormPluginImage, andsource location of these icons are Activiti-Designer\org.activiti.designer.kickstart.gui.form\icons.
org.activiti.designer.eclipse.extension.IconProviderextension The method getIcon of classorg.activiti.designer.kickstart.form.diagram.KickstartFormIconProvideralways return null.
org.eclipse.ui.views.properties.tabbed.propertyContributorextension Property Contributor for Activiti Kickstart Form tabbed propertiesview. It most importantly identifies the unique contributoridentifier for Activiti tabs and sections. org.eclipse.ui.views.properties.tabbed.propertyTabsextension To describes the tabs for Activiti Kickstart Formcontributor.
org.eclipse.ui.views.properties.tabbed.propertySectionsextension To describes the sections for Activiti contributor. OnepropertySection will create a property sheet page (form) on aspecified tab for a element in Activiti Kickstart Form.
org.activiti.designer.kickstart.gui.process plug-in org.eclipse.graphiti.ui.diagramTypes extension Kickstart Process diagram type: as Activiti Kickstart Processcreate a diagram type provider for a diagram type which does notexist in the repository, it is necessary to provide the informationabout the new diagram type.
org.eclipse.graphiti.ui.diagramTypeProviders extension To register a diagram type provider for Activiti KickstartProcess.
org.eclipse.graphiti.ui.imageProviders extension To register Activiti&amp;rsquo;s image provider for Activiti KickstartProcess.These keys and paths of icons are defined at classorg.activiti.designer.kickstart.process.KickstartProcessPluginImage,and source location of these icons are Activiti-Designer\org.activiti.designer.kickstart.gui.process\icons.
org.eclipse.ui.views.properties.tabbed.propertyContributorextension Property Contributor for Activiti Kickstart Process tabbedproperties view. It most importantly identifies the uniquecontributor identifier for Activiti tabs and sections. org.eclipse.ui.views.properties.tabbed.propertyTabsextension To describes the tabs for Activiti Kickstart Processcontributor.
org.eclipse.ui.views.properties.tabbed.propertySectionsextension To describes the sections for Activiti contributor. OnepropertySection will create a property sheet page (form) on aspecified tab for a element in Activiti Kickstart Process.
org.activiti.designer.kickstart.util plug-in Provoide utility APIs under org.activiti.designer.kickstart.util toother plug-ins
</content>
    </entry>
    
     <entry>
        <title>粗览基于Eclipse RCP架构的Activiti Designer实现（思维导图版）</title>
        <url>https://mryqu.github.io/post/%E7%B2%97%E8%A7%88%E5%9F%BA%E4%BA%8Eeclipse_rcp%E6%9E%B6%E6%9E%84%E7%9A%84activiti_designer%E5%AE%9E%E7%8E%B0%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE%E7%89%88/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>designer</tag><tag>bpmn</tag><tag>rcp</tag><tag>思维导图</tag>
        </tags>
        <content type="html"> </content>
    </entry>
    
     <entry>
        <title>函数式编程笔记</title>
        <url>https://mryqu.github.io/post/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>函数式编程</tag><tag>闭包</tag><tag>lambda</tag>
        </tags>
        <content type="html">  澄清概念 面向对象编程(OOP)对应的是面向过程编程(POP)。其区别在于模块化和如何对事物进行抽象： - POP中程序可被分解成函数，OOP中程序可被分解成对象； - POP侧重于过程及其执行顺序，OOP侧重于数据而不是过程； - POP自顶向下解决问题，OOP自底向上解决问题； - POP可以自由地在函数之间传递数据，OOP中对象需要通过成员方法传递数据和彼此通信； - OOP更容易添加新数据和函数，有POP所没有的（public、private、protected等）访问修饰符、数据隐藏、方法覆写等特性。
函数式编程(Functional Programming)对应的是命令式编程(Imperativeprogramming)。
普林斯顿的科学家阿隆左·丘奇（Alonzo Church）、阿兰·图灵（Alan Turing）、约翰·冯·诺依曼（Johnvon Neumann）和库尔特·冈特（KurtGodel）都对形式系统很感兴趣，致力于解决抽象的数学难题。这些难题的共同之处就是计算：如果计算机能有无限的计算能力，哪些问题可以被解决？哪些问题可以被自动解决？哪些问题依旧无法解决？为什么不能被解决？基于不同设计的各种计算机是否具有相同的计算能力？ 阿隆左·丘奇提出了一个被称为lambda演算的形式系统。这个系统本质上是一种程序设计语言。它可以运行在具有无限计算能力的机器上。lambda演算由一些函数构成，这些函数的输入输出也是函数。函数用希腊字母lambda标识，因此整个形式系统也叫lambda。通过这一形式系统，阿隆左就可以对上述诸多问题进行推理并给出结论性的答案。在同一时间，阿兰·图灵也在进行着相似的工作。他提出了一个完全不同的形式系统（现在被称为图灵机），并使用这一系统得出了和阿隆左相似的结论。事后证明，图灵机和lambda的演算能力是等价的。 1949年，第一台电子计算机EDVAC被推出并获得了巨大的成功。这是冯·诺依曼架构的第一个具体实现，实际上也是图灵机的第一个实现。而与此同时，阿隆左·丘奇则没有那么幸运。直到二十世纪五十年代，一位MIT的教授JohnMcCarthy对阿隆左·丘奇的工作产生了兴趣。1958年，他发布了Lisp语言。Lisp的不同之处在于，它在冯·诺依曼计算机上实现了阿隆左·丘奇的lambda演算！很多计算机科学家开始意识到Lisp的表达能力。1973年，MIT人工智能实验室的一帮程序员开发了被称为Lisp机器的硬件，于是阿隆左的lambda演算系统终于在硬件上实现了！函数式编程更加现代一些的例子包括scheme、Haskell、Clean、Erlang和Miranda等。1980年代末期，集函数式编程研究成果于大成的Haskell发布。命令式编程是图灵机思想的一种实现，对应地函数式编程则是lambda演算思想的一种实现。但并非所有的lambda演算都被实现了，因为lambda演算原本不是为有物理限制的计算机设计的。
|特征|命令式编程|函数式编程 |&amp;mdash;&amp;ndash; |程序员关注点|任务（算法）如何被执行及状态变化如何被追踪|所要得到的信息及所需转换 |状态变化|很重要|不存在状态变化 |执行顺序|很重要|不是很重要 |基本流控制|循环、条件分支和函数调用|包括递归在内的函数调用 |基本操作单元|结构体或类的对象|作为头等对象的函数、数据集合
定义 在维基百科中，已经对函数式编程有了很详细的介绍，其定义如下：
函数式编程或称函数程序设计，又称泛函编程，是一种编程范型，它将电脑运算视为数学上的函数计算，并且避免使用程序状态以及易变对象。函数编程语言最重要的基础是λ演算（lambda calculus）。而且λ演算的函数可以接受函数当作输入（引数）和输出（传出值）。  概念 支持闭包和高阶函数 函数在函数式编程中是所谓的&amp;rdquo;头等公民&amp;rdquo;，函数与其他数据类型一样处于平等地位，有时称为闭包或者仿函数（functor）对象。实质上，闭包是起函数的作用并可以像对象一样操作的对象。与此类似，函数式编程语言支持高阶函数。高阶函数可以用另一个函数（间接地，用一个表达式）作为其输入参数，在某些情况下，它甚至返回一个函数作为其输出参数。这两种结构结合在一起使得可以用优雅的方式进行模块化编程，这是使用函数式编程的最大好处。
纯函数 所谓“纯”函数式(或表达式)就是实现了lambda演算并且不包含与Church范式矛盾的特性，它没有(内存或I/O)副作用。 - 变量的不可变性：函数式编程的变量都是不可变的，函数保持独立，不能修改外部变量的值，所有功能就是返回一个新的值。在命令式编程中，变量往往用来保存&amp;rdquo;状态&amp;rdquo;（state）,变量会影响函数的输出。在函数式编程中，不能修改变量，意味着状态不能保存在变量中。函数式编程使用函数参数保存状态，函数参数是影响函数返回值的唯一途径。如果一个编程语言中变量都是不可变的好处是： - 可以去掉很多情况的锁操作, 并发处理速度会更快. - 可以简化垃圾回收GC - 函数的确定性或引用透明性（Referentialtransparency）：指的是函数的运行不依赖于外部变量或&amp;rdquo;状态&amp;rdquo;，只依赖于输入的参数，任何时候只要参数相同，引用函数所得到的返回值总是相同的。这使您可以从形式上推断程序行为，因为表达式的意义只取决于其子表达式而不是计算顺序或者其他表达式的副作用。这有助于验证正确性、简化算法，甚至有助于找出优化它的方法。
递归 函数式编程还有一个特点是用递归做为控制流程的机制。递归最大的好处就简化代码，他可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓。
懒惰计算 懒惰计算需要编译器的支持。表达式不是在绑定到变量时立即计算，而是在求值程序需要产生表达式的值时进行计算。延迟的计算使您可以编写可能潜在地生成无穷输出的函数。因为不会计算多于程序的其余部分所需要的值，所以不需要担心由无穷计算所导致的out-of-memory错误。一个懒惰计算的例子是生成无穷Fibonacci列表的函数，但是对第n个Fibonacci数的计算相当于只是从可能的无穷列表中提取一项。 C类和ML类的语言都是非懒惰的（饥饿求值），而Haskell和Miranda都是懒惰的。OCaml是缺省非懒惰，但是在需要的时候支持懒惰的风格。
模式匹配 模式匹配不是什么新特性。事实上它和函数式编程的关系不大。为什么总是把它当做函数式编程的一个特性呢？这是因为函数式语言已经支持模式匹配一段时间了，而现代命令式语言还不行。 用一个例子来进一步了解模式匹配。下面是Java实现的斐波那契函数：
int fib(int n) { if(n == 0) return 1; if(n == 1) return 1; return fib(n - 2) &#43; fib(n - 1); }  如果用我们上文构造的并且支持模式匹配的Java来写，实现如下
int fib(0) { return 1; } int fib(1) { return 1; } int fib(int n) { return fib(n - 2) &#43; fib(n - 1); }  两者有什么区别？编译器为我们实现了分支。
科里化(currying) 把一个函数的多个参数分解成多个函数，然后把函数多层封装起来，每层函数都返回一个函数去接收下一个参数这样，可以简化函数的多个参数。 这里有一个适配器模式的例子：
int pow(int i, int j); int square(int i) { return pow(i, 2); }  上面的代码把一个整数幂运算接口转换成为了一个平方接口。在学术圈中，这样的用法被称之为currying（得名于逻辑学家HaskellCurry，他曾将相关的数学理论形式化）。因为在函数式编程中，函数而不是类作为参数进行传递，因此可以用currying把函数适配到其他接口。又因为函数的接口就是其参数列表，所以currying可以减少参数的数量(如上例所示)。 因为函数式语言内建了这一技术，所以不用手动地创建一个包装了原函数的类。函数式语言会为你代劳。和之前一样，我们来扩展一下我们的语言来支持这个技术：
square = int pow(int i, 2);  尾递归优化(tail call optimization) 传统地递归过程就是函数调用，涉及返回地址、函数参数、寄存器值等压栈（在x86-64上通常用寄存器保存函数参数），其缺点是 - 效率低，占内存 - 如果递归链过长，可能会堆栈溢出
与普通递归相比，由于尾递归的调用处于方法的最后，因此方法之前所积累下的各种状态对于递归调用结果已经没有任何意义，因此完全可以把本次方法中留在堆栈中的数据完全清除，把空间让给最后的递归调用。这样的优化便使得递归不会在调用堆栈上产生堆积，意味着即时是“无限”递归也不会让堆栈溢出”。这其实才是尾递归的“正统”优化方式。 因为Python、Java等无法在语言中实现尾递归优化，所以采用了循环迭代等方式代替递归的表述。
总结 函数式编程在某些场景下比命令式编程具有优势。鼓吹函数式编程的人说函数式编程比命令式编程代码精简、表达直观，其实不能一概而论。此外要让函数式编程来做CRUD，来做我们传统的逻辑性很强的Web编程，就有些免为其难了。实用最好！
参考 WIKI：函数编程语言
函数式程序设计的另类指南
函数式编程初探
IBM developerWorks：Java 语言中的函数编程
MSDN: Functional Programming vs. Imperative Programming
函数式编程
编程的宗派
[](http://www.zhihu.com/question/19732025)
</content>
    </entry>
    
     <entry>
        <title>Activiti Designer开发环境配置</title>
        <url>https://mryqu.github.io/post/activiti_designer%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>designer</tag><tag>bpmn</tag>
        </tags>
        <content type="html">   Download eclipse-modeling-juno-SR2-win32-x86_64.zip,eclipse-rcp-juno-SR2-win32-x86_64.zip and extract to the sameeclipse folder Use Eclipse juno instead of kepler which can easily consume your day!!! Add Eclipse Graphiti SDK 0.10.1 from update site http://archive.eclipse.org/graphiti/updates/0.10.1/ Add the following location as source code repository: https://github.com/Activiti/Activiti-Designer  Import Maven project - Activiti Designer  Open a console and navigate to the directory of your Eclipseworkspace and then into the org.activiti.designer.parent directory,which is of course the directory of the project with the same name.Now perform the following Maven commands:  mvn clean eclipse:clean mvn eclipse:eclipse  Go back to Eclipse, select all of the projects, right click andchoose Refresh. The information set by Maven will now be picked upby Eclipse. Solve issue at Activiti-Designer\examples\text-export-marshaller\pom.xml then update Maven projects:change &amp;ldquo;5.15.0-SNAPSHOT&amp;rdquo; to&amp;rdquo;5.16.0-SNAPSHOT&amp;rdquo; Now you have a environment without compilation errors.  The Activiti Designer still can&amp;rsquo;t run successfully by now. To solve the issue Eclipse plug-in unable to instantiate class &amp;ldquo;org.activiti.designer.eclipse.editor.ActivitiDiagramEditor&amp;rdquo;,add the missing jars inActiviti-Designer\org.activiti.designer.libs\META-INF\MANIFEST.MF re-written by the mvn eclipse:eclipse process  To run Designer, right click the org.activiti.designer.eclipseproject and choose Run as -&amp;gt; Eclipse application.  Reference Activiti Designer Developer Guide
Compiling github activiti-designer project
Eclipse plug-in unable to instantiate class &amp;ldquo;org.activiti.designer.eclipse.editor.ActivitiDiagramEditor&amp;rdquo;
</content>
    </entry>
    
     <entry>
        <title>在Ubuntu中禁掉IPv6</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8ubuntu%E4%B8%AD%E7%A6%81%E6%8E%89ipv6/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>ubuntu</tag><tag>ipv6</tag><tag>disable</tag>
        </tags>
        <content type="html"> 为了禁止掉IPv6，需要在/etc/sysctl.conf做如下修改：
net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1  如果IPv6仍没有禁掉，是由于sysctl.conf没有激活造成的。为了解决上述问题，执行下面的命令：
sudo sysctl -p  之后，运行:
$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6  它将返回1，这表示IPv6被成功禁止掉。 </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] set required field in form element</title>
        <url>https://mryqu.github.io/post/openui5_set_required_field_in_form_element/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>form</tag><tag>required</tag><tag>field</tag>
        </tags>
        <content type="html"> 对FormElement中的sap.m.Input设置了required属性，但是界面上的标签并没有显示星号*。
new FormElement({ label: &amp;quot;name&amp;quot;, fields: [ new sap.m.Input({ id: sFormId&#43;&amp;quot;-name&amp;quot;, type: sap.m.InputType.Text, value: &amp;quot;{/name}&amp;quot;, required: true, layoutData: new GridData({span: &amp;quot;L3 M5 S6&amp;quot;}) }) ] })  通过阅读Q: UI5 Setting field as required得知，需要对label属性赋值一个带有required为true的Label控件。
new FormElement({ label: new sap.m.Label({ text:&amp;quot;name&amp;quot;, required:true }), fields: [ new sap.m.Input({ id: sFormId&#43;&amp;quot;-name&amp;quot;, type: sap.m.InputType.Text, value: &amp;quot;{/name}&amp;quot;, required: true, layoutData: new GridData({span: &amp;quot;L3 M5 S6&amp;quot;}) }) ] })  </content>
    </entry>
    
     <entry>
        <title>[HBase] 使用ImportTsv命令导入数据</title>
        <url>https://mryqu.github.io/post/hbase_%E4%BD%BF%E7%94%A8importtsv%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>importtsv</tag><tag>put</tag><tag>bulk</tag>
        </tags>
        <content type="html">  ImportTsv简介 ImportTsv是一款用于将TSV格式数据导入HBase的工具。它有两种用法： - 通过Put将TSV格式数据导入HBase - 通过批量导入数据的方式生成用于加载进HBase的存储文件
下面看一下ImportTsv的使用说明： ImportTsv参数 -Dimporttsv.skip.bad.lines=false - 若遇到无效行则失败 &amp;lsquo;-Dimporttsv.separator=|&amp;rsquo; - 使用特定分隔符| -Dimporttsv.timestamp=currentTimeAsLong - 使用导入时的时间戳 -Dimporttsv.mapper.class=my.Mapper -使用用户自定义Mapper类替换TsvImporterMapper -Dmapreduce.job.name=jobName - 对导入使用特定mapreduce作业名 -Dcreate.table=no - 避免创建表，注：如设为为no，目标表必须存在于HBase中 -Dno.strict=true - 忽略HBase表列族检查。默认为false
ImportTsv测试 准备数据 hadoop@node50064:~$ hadoop fs -cat /user/hadoop/tsv_input/sales2013.csv Name,Sex,Age,Height,Weight Alfred,M,14,69,112.5 Alice,F,13,56.5,84 Barbara,F,13,65.3,98 Carol,F,14,62.8,102.5 Henry,M,14,63.5,102.5 James,M,12,57.3,83 Jane,F,12,59.8,84.5 Janet,F,15,62.5,112.5 Jeffrey,M,13,62.5,84 John,M,12,59,99.5 Joyce,F,11,51.3,50.5 Judy,F,14,64.3,90 Louise,F,12,56.3,77 Mary,F,15,66.5,112 Philip,M,16,72,150 Robert,M,12,64.8,128 Ronald,M,15,67,133 Thomas,M,11,57.5,85 William,M,15,66.5,112  准备目标表 hbase(main):001:0&amp;gt; create &#39;sales2013&#39;, &#39;info&#39; 0 row(s) in 4.5730 seconds =&amp;gt; Hbase::Table - sales2013 hbase(main):002:0&amp;gt; create &#39;sales2013bulk&#39;, &#39;info&#39; 0 row(s) in 2.2790 seconds =&amp;gt; Hbase::Table - sales2013bulk  通过Put方式导入到HBase表sales2013 bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv &#39;-Dimporttsv.separator=,&#39; -Dimporttsv.columns=&#39;HBASE_ROW_KEY,info:Sex,info:Age,info:Height,info:Weight&#39; sales2013 /user/hadoop/tsv_input  通过批量导入方式导入到HBase表sales2013bulk bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv &#39;-Dimporttsv.separator=,&#39; -Dimporttsv.columns=&#39;HBASE_ROW_KEY,info:Sex,info:Age,info:Height,info:Weight&#39; -Dimporttsv.bulk.output=/user/hadoop/tsv_output sales2013bulk /user/hadoop/tsv_input # completebulkload工具用于将生成的存储文件移入一个HBase表 bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /user/hadoop/tsv_output sales2013bulk  ImportTsv相关源码 ImportTsv.java TsvImporterMapper.java
LoadIncrementalHFiles.java
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] sap.ui.core.format.DateFormat使用</title>
        <url>https://mryqu.github.io/post/openui5_sap.ui.core.format.dateformat%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>date</tag><tag>format</tag>
        </tags>
        <content type="html"> 使用javascript的Date类型，想要输出国际化的字符串，可以使用toLocaleString函数，但是需要自己往里设locale，并且输出结果随操作系统和浏览器不同而变化。
最后还是用OpenUI5的DateFormat，既可以固定格式有可以自动国际化。
var oDateFormat = sap.ui.core.format.DateFormat.getDateTimeInstance({ pattern: &amp;quot;EEEE, MMMM d, yyyy HH:mm:ss a z&amp;quot; }); oDateFormat.format(new Date());  </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 将布尔型数据在数据表中显示为checkbox</title>
        <url>https://mryqu.github.io/post/openui5_%E5%B0%86%E5%B8%83%E5%B0%94%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%9C%A8%E6%95%B0%E6%8D%AE%E8%A1%A8%E4%B8%AD%E6%98%BE%E7%A4%BA%E4%B8%BAcheckbox/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>datatables</tag><tag>javascript</tag><tag>boolean</tag><tag>checkbox</tag>
        </tags>
        <content type="html"> 看了一下OpenUI5、Datatables和Vaadin中将布尔型数据在数据表中显示为checkbox的示例： OpenUI5 example: DataTable
Datatables example: Always shown checkbox Vaadin table: How to display Boolean as checkboxes with editable=false
感觉还是OpenUI5更灵活，不过小项目用OpenUI5又太重了！
</content>
    </entry>
    
     <entry>
        <title>Retrieve the status of an process instance in Activiti</title>
        <url>https://mryqu.github.io/post/retrieve_the_status_of_an_process_instance_in_activiti/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>process_instance</tag><tag>status</tag><tag>retrieve</tag>
        </tags>
        <content type="html">  Retrieve the status of an running process instance Use the executionQuery() or processInstanceQuery() to find out ifany result is returned or not when querying execution/process byId.
runtimeService.createProcessInstanceQuery().processInstanceId(specifiedProcessInstanceId).singleResult() runtimeService.createExecutionQuery().executionId(specifiedExecutionId).singleResult()  Specially when the history capabilities of Activiti engine isenabled: - Specified process/execution instance id:
 ProcessInstance inst = historyService.createHistoricProcessInstanceQuery().processInstanceId(specifiedProcessInstanceId).singleResult(); inst.isEnd()   Unspecified process/execution instance id:  historyService.createHistoricProcessInstanceQuery().finished().list() historyService.createHistoricProcessInstanceQuery().unfinished().list()   Retrieve the active node of an running process instance RuntimeService provide the API to retrieve the active node of anrunning process instance:
 // Finds the activity ids for all executions that are waiting in activities. // This is a list because a single activity can be active multiple times. // // @param executionId // id of the execution, cannot be null. // @throws ActivitiObjectNotFoundException // when no execution exists with the given executionId. List getActiveActivityIds(String executionId);  Its implementation work as below: The API usage in Activiti demo: - org.activiti.rest.diagram.services.BaseProcessDefinitionDiagramLayoutResource: dead code org.activiti.rest.diagram.services.ProcessDefinitionDiagramLayoutResource doesn&amp;rsquo;t satisfy the condition to run getActiveActivityIds method. org.activiti.rest.diagram.services.ProcessInstanceDiagramLayoutResourceis not used in Activiti demo. - org.activiti.rest.diagram.services.ProcessInstanceHighlightsResource: used to high light current active node and sequence flow - org.activiti.explorer.ui.process.ProcessDefinitionImageStreamResourceBuilder: used to high light current active node - org.activiti.rest.service.api.runtime.process.ExecutionActiveActivitiesCollectionResource: no usage in Activiti demo - org.activiti.rest.service.api.runtime.process.ProcessInstanceDiagramResource: no usage in Activiti demo
Reference Activiti user guide: history confiugration
Activiti Forums: ProcessInstance.isEnded() returns false
Activiti Forums: How to determine when process is complete after signal()?
Activiti Forums: any api to get the current node?
</content>
    </entry>
    
     <entry>
        <title>粗览Activiti Explorer源代码</title>
        <url>https://mryqu.github.io/post/%E7%B2%97%E8%A7%88activiti_explorer%E6%BA%90%E4%BB%A3%E7%A0%81/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>explorer</tag><tag>代码</tag><tag>分析</tag>
        </tags>
        <content type="html"> ActivitiExplorer的应用程序为org.activiti.explorer.ExplorerApp，其界面配置文件为Activiti\modules\activiti-webapp-explorer2\src\main\resources\activiti-ui-context.xml。通过该配置文件创建主窗口org.activiti.explorer.ui.MainWindow类实例，并通过org.activiti.explorer.ViewManagerFactoryBean创建实现org.activiti.explorer.ViewManager接口的org.activiti.explorer.DefaultViewManager或org.activiti.explorer.ui.alfresco.AlfrescoViewManager类实例。 在Activiti演示中采用的是org.activiti.explorer.DefaultViewManager类实例，它对主窗口进行视图管理，完成视图切换、对应导航栏和功能菜单的设置。 主窗口org.activiti.explorer.ui.MainWindow类实例通过org.activiti.explorer.ui.mainlayout.MainLayout类进行界面布局。
下面列举了Activiti Explorer两级导航栏所对应的页面实现类。 - Tasks - Inboxorg.activiti.explorer.ui.task.InboxPage - My Tasksorg.activiti.explorer.ui.task.TasksPage - Queuedorg.activiti.explorer.ui.task.QueuedPage - Involvedorg.activiti.explorer.ui.task.InvolvedPage - Archivedorg.activiti.explorer.ui.task.ArchivedPage - Processes - My Instancesorg.activiti.explorer.ui.process.MyProcessInstancesPage - Deployed process definitionsorg.activiti.explorer.ui.process.ProcessDefinitionPage - Model workspaceorg.activiti.editor.ui.EditorProcessDefinitionPage - Reports - Generate reportsorg.activiti.explorer.ui.reports.RunReportsPage - Saved reportsorg.activiti.explorer.ui.reports.SavedReportsPage - Manage - Databaseorg.activiti.explorer.ui.management.db.DatabasePage - Deploymentsorg.activiti.explorer.ui.management.deployment.DeploymentPage - Active Processesorg.activiti.explorer.ui.management.processdefinition.ActiveProcessDefinitionPage - Suspend Processesorg.activiti.explorer.ui.management.processdefinition.SuspendedProcessDefinitionPage - Jobsorg.activiti.explorer.ui.management.job.JobPage - Usersorg.activiti.explorer.ui.management.identity.UserPage - Groupsorg.activiti.explorer.ui.management.identity.GroupPage - Administrationorg.activiti.explorer.ui.management.admin.AdministrationPage - Crystalballorg.activiti.explorer.ui.management.crystalball.CrystalBallPage
</content>
    </entry>
    
     <entry>
        <title>GoJS BPMN元素界面实现分析</title>
        <url>https://mryqu.github.io/post/gojs_bpmn%E5%85%83%E7%B4%A0%E7%95%8C%E9%9D%A2%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>gojs</tag><tag>bpmn</tag><tag>element</tag><tag>javascript</tag><tag>html5</tag>
        </tags>
        <content type="html"> GoJS BPMN里面的BPMN元素采用过不是PNG/JPEG这样的静态图标，而是通过GoJS在Cavas绘出来的。 由BPMN.js可知，一个BPMN元素是一个go.Node，内部大致包含go.Panel、go.Shape、go.TextBlock等对象，用于绘制外层的正方形、填充内部颜色、添加图标和文字。 如果创建自己定制的BPMN元素，最麻烦的就是图标了。现在我来看一下GoJS BPMN扩展里面的图标是怎么保存和绘制的。 GoJS BPMN扩展里面大部分的图标都已经在go.js里面以源代码的形式定义了，只有四个是在BPMN.js里面通过go.Shape.defineFigureGenerator方法定制的，分别是Empty、Annotation、BpmnTaskManual和BpmnTaskService。这四个图标的内容是GoJS geometry的格式保存的。 所有这些图标可以通过与go.Shape的figure进行绑定，由GoJS驱动完成底层绘制。我自己做了一个简单样例，对这些内嵌图标和BPMN定制图标稍微玩了点花样 http://jsfiddle.net/mryqu/mywy0nhz/ 显示效果如下： </content>
    </entry>
    
     <entry>
        <title>了解Apache Accumulo</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3apache_accumulo/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>apache</tag><tag>accumulo</tag><tag>nosql</tag><tag>gemfire</tag><tag>mongodb</tag>
        </tags>
        <content type="html">  Apache Accumulo是一个可靠的、可伸缩的、高性能的排序分布式的Key-Value存储解决方案，提供了基于单元的访问控制以及可在数据处理过程中多个控制点修改键值对的服务器端编程机制。使用GoogleBigTable设计思路，基于ApacheHadoop、Zookeeper和Thrift构建。 Accumulo由美国国家安全局（NSA）于2008年开始研发，2011年捐赠给Apache基金会。Accumulo现已成为Apache基金会的顶级项目，并被评过第三大最受欢迎的NoSQL数据库引擎。 目前，Accumulo技术已经得到美国政府层面的全面认可，NSA已将该技术作为内部组织架构运行的核心部分，在对来源于各方面的庞大海量数据进行分析处理时，所应用的运算程序基本都运行在Accumulo技术上，即NSA“大多数监控和分析应用程序的后台都是Accumulo技术”。基于Hadoop的Accumulo技术已在实质上被视为美国国家安全战略的关键。 据2014年9月份的文章介绍，已经有几十家不同类型的美国企业安装了Accumulo技术系统，其中，美国20强企业中已有3家安装，50强企业中有5家安装，还有不少企业已表示对此有兴趣。 我一碰到KV存储方案，总想跟我用过的GemFire和MongoDB做个比较。vsChart.com - The Comparision Wiki上已经有现成的比较(见参考3和4)，值得学习。
参考  Apache Accumulo官网
 Accumulo: Why The World Needs Another NoSQL Database
 Apache Accumulo vs. GemFire
 Apache Accumulo vs. MongoDB
 NOSQL中文网：Apache Accumulo用户手册
  </content>
    </entry>
    
     <entry>
        <title>MongoDB 3.0新特性、提升与兼容性改变</title>
        <url>https://mryqu.github.io/post/mongodb_3.0%E6%96%B0%E7%89%B9%E6%80%A7%E6%8F%90%E5%8D%87%E4%B8%8E%E5%85%BC%E5%AE%B9%E6%80%A7%E6%94%B9%E5%8F%98/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mongodb</tag><tag>新特性</tag><tag>兼容性</tag><tag>nosql</tag><tag>3</tag>
        </tags>
        <content type="html"> MONGODB 3.0
WHAT’S NEW IN MONGODB 3.0
Release Notes for MongoDB 3.0
Compatibility Changes in MongoDB 3.0
MongoDB3.0发布&amp;ndash;新特性
</content>
    </entry>
    
     <entry>
        <title>粗览Activiti Modeler属性显示和设置代码</title>
        <url>https://mryqu.github.io/post/%E7%B2%97%E8%A7%88activiti_modeler%E5%B1%9E%E6%80%A7%E6%98%BE%E7%A4%BA%E5%92%8C%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%A0%81/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>modeler</tag><tag>代码</tag><tag>分析</tag>
        </tags>
        <content type="html"> 我的博文粗览Activiti Modeler操作和源代码介绍了Activiti ModelEditor组件的常用操作和对应源代码分析，本文接着介绍一下ActivitiModeler里BPMN元素属性的显示和设置。 粗览Activiti Modeler操作和源代码里提到的stencilset.json除了含有每个BPMN元素应该使用什么图标，也定义了每个BPMN元素的所有属性。以UserTask元素为例，它含有以下的属性包： - &amp;ldquo;overrideidpackage&amp;rdquo; - &amp;ldquo;namepackage&amp;rdquo; - &amp;ldquo;documentationpackage&amp;rdquo; - &amp;ldquo;asynchronousdefinitionpackage&amp;rdquo; - &amp;ldquo;exclusivedefinitionpackage&amp;rdquo; - &amp;ldquo;executionlistenerspackage&amp;rdquo; - &amp;ldquo;multiinstance_typepackage&amp;rdquo; - &amp;ldquo;multiinstance_cardinalitypackage&amp;rdquo; - &amp;ldquo;multiinstance_collectionpackage&amp;rdquo; - &amp;ldquo;multiinstance_variablepackage&amp;rdquo; - &amp;ldquo;multiinstance_conditionpackage&amp;rdquo; - &amp;ldquo;isforcompensationpackage&amp;rdquo; - &amp;ldquo;usertaskassignmentpackage&amp;rdquo; - &amp;ldquo;formkeydefinitionpackage&amp;rdquo; - &amp;ldquo;duedatedefinitionpackage&amp;rdquo; - &amp;ldquo;prioritydefinitionpackage&amp;rdquo; - &amp;ldquo;formpropertiespackage&amp;rdquo; - &amp;ldquo;tasklistenerspackage&amp;rdquo;
[tomcat]\webapps\activiti-explorer\editor-app\configuration\properties.js里提供了对各类BPMNstencil的所需要包含的HTML页面。 [tomcat]\webapps\activiti-explorer\editor-app\stencil-controller.js中对ORYX.CONFIG.EVENT_SELECTION_CHANGED事件注册了匿名函数监听器。当某一BPMN元素被选中后，该监听器会设置$scope.selectedItem和$scope.selectedShape，其中包含BPMN元素的所有属性，每个属性都有对应的stencilreadModeTemplateUrl、writeModeTemplateUrl和templateUrl。 [tomcat]\webapps\activiti-explorer\editor-app\editor.html是ActivitiModelEditor组件的显示页面，其ID为propertiesHelpWrapper的DIV元素内对所选择的BPMN元素的每个属性放置templateUrl、readModeTemplateUrl或writeModeTemplateUrl对应的HTML子页面用于显示和设置stencil属性。 用于设置BPMNstencil属性的页面都位于[tomcat]\webapps\activiti-explorer\editor-app\configuration\properties\下。 这些页面和Anjugar.JS控制器为： </content>
    </entry>
    
     <entry>
        <title>GoJS对浏览器和移动设备的支持</title>
        <url>https://mryqu.github.io/post/gojs%E5%AF%B9%E6%B5%8F%E8%A7%88%E5%99%A8%E5%92%8C%E7%A7%BB%E5%8A%A8%E8%AE%BE%E5%A4%87%E7%9A%84%E6%94%AF%E6%8C%81/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>gojs</tag><tag>browser</tag><tag>mobile</tag><tag>support</tag>
        </tags>
        <content type="html"> 在其官网上没有发现GoJS对浏览器和移动设备的支持的详细报告，仅在http://www.nwoods.com/products/gojs/ 有粗略介绍： &amp;gt; GoJS takes advantage of the HTML Canvas to supporthigh-performance diagrams. For creating static documents andprintable resources, GoJS supports exporting Diagrams to images and SVG. &amp;gt; &amp;gt; GoJS supports all modern browsers (IE9&#43;), including mobilebrowsers. &amp;gt; </content>
    </entry>
    
     <entry>
        <title>Activiti与GoJS BPMN支持的BPMN元素对比</title>
        <url>https://mryqu.github.io/post/activiti%E4%B8%8Egojs_bpmn%E6%94%AF%E6%8C%81%E7%9A%84bpmn%E5%85%83%E7%B4%A0%E5%AF%B9%E6%AF%94/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>gojs</tag><tag>bpmn</tag><tag>support</tag><tag>comparision</tag>
        </tags>
        <content type="html"> 首先抱怨一下,GoJS BPMN扩展中有两个图标和标题是一样的,我看了又看还是分不清. ActivitiGoJS BPMNStartEventsStart Event Start Timer Event Start Signal Event Start Message Event Start Error Event ActivitiesAll Activiti activities support compensation optionAll GoJS BPMN activities support &amp;ldquo;Add Email Event&amp;rdquo;, &amp;ldquo;Add TimerEvent&amp;rdquo;, &amp;ldquo;Add Escalation Event&amp;rdquo; and &amp;ldquo;Add Error Event&amp;rdquo; options.User Task Service Task Looping Service TaskScript Task Business Rule Task Receive Task Manual Task Mail Task Camel Task Mule Task Generic TaskStructuralSub Process Event Sub Process Call Activity GatewaysExclusive Gateway Parallel Gateway Inclusive Gateway Event Gateway BoundaryEventsBoundary Error Event Boundary Timer Event Boundary Signal Event Boundary Message Event Intermediate Catching EventsIntermediate Timer Catching Event Intermediate Signal Catching Event Intermediate Message Catching Event Intermediate Throwing EventsIntermediate None Throwing Events Intermediate Signal Throwing Events EndEventsEnd Event End Error Event ????SwimlanesPool Lane ArtifactsText annotationDataObjectPrivate Process
</content>
    </entry>
    
     <entry>
        <title>浏览器的本地存储在GoJS BPMN样例中的使用</title>
        <url>https://mryqu.github.io/post/%E6%B5%8F%E8%A7%88%E5%99%A8%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8%E5%9C%A8gojs_bpmn%E6%A0%B7%E4%BE%8B%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>localstorage</tag><tag>gojs</tag><tag>bpmn</tag><tag>javascript</tag><tag>browser</tag>
        </tags>
        <content type="html">  GoJS BPMN样例中可以存储BPMN模型，它采用的存储媒体是浏览器的本地存储。Web Storage(W3C WebApps Working Group)中定义了如下的Storage接口：
interface Storage { readonly attribute unsigned long length; DOMString key(unsigned long index); getter DOMString getItem(DOMString key); setter creator void setItem(DOMString key, DOMString value); deleter void removeItem(DOMString key); void clear(); };  GoJS BPMN样例采用的方法如下：
function checkLocalStorage() { return (typeof (Storage) !== &amp;quot;undefined&amp;quot;) &amp;amp;&amp;amp; (window.localStorage !== undefined); } window.localStorage.setItem(key, value) window.localStorage.getItem(key) window.localStorage.removeItem(key)  我的测试是存储一个名为yqu_GoJSBPMN_Samp1的模型。 如果想清除我的小测试所用的本地存储，可以通过chrome://settings/cookies#cont页面来完成： 参考 MDN：DOM Storage guide
DOM Storage
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 编译OpenSSL和libCurl</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E7%BC%96%E8%AF%91openssl%E5%92%8Clibcurl/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>curl</tag><tag>libcurl</tag><tag>openssl</tag><tag>build</tag><tag>library</tag>
        </tags>
        <content type="html">  准备工作 登录一台Linux服务器，并完成下列工作： - 在目录/home/mryqu/创建子目录out，在out目录下创建子目录ssl和curl； - 从OpenSSL项目下载openssl-1.0.2.tar.gz，并解压； - 从curl项目下载curl-7.40.0.tar.gz，并解压
编译OpenSSL  进入openssl-1.0.2目录; 完成OpenSSL配置，仅支持静态库不支持动态库：  ./config no-shared --openssldir=/home/mryqu/out/ssl  对Makefile文件中的FGLAG和DEPFLAG变量进行修改，增加-fPIC。 编译：  make depend make make install   编译产生如下内容： 编译libCurl  进入curl-7.40.0目录; 首先设定pkg-config路径，指定为上一步OpenSSL编译结果。由于我们的OpenSSL编译结果不在编译器/链接器默认搜索路径，通过pkg-config路径和&amp;ndash;with-ssl让libCurl查找到OpenSSL。通过&amp;ndash;without-zlib禁止掉即时解压缩。  export PKG_CONFIG_PATH=/home/mryqu/out/ssl/lib/pkgconfig ./configure --prefix=/home/mryqu/out/curl --with-ssl --without-zlib make make install   编译产生如下内容： 参考 OpenSSL Compilation and Installation
how to install curl and libcurl
OpenSSL Cookbook
Everything curl
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] Grid layout for responsive design</title>
        <url>https://mryqu.github.io/post/openui5_grid_layout_for_responsive_design/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>layout</tag><tag>grid</tag><tag>responsive</tag><tag>html5</tag>
        </tags>
        <content type="html">  OpenUI5的Grid机制位于sap.ui.layout库内，它在12列流布局中定位子控件位置。取决于当前屏幕尺寸，子控件可以指定可变的列数，从而实现响应式设计。 在上图示例中，无论屏幕大小，子控件1都占满12列，从而其他子控件无法跟它位于同一行内。 在大屏幕和中等屏幕尺寸下，子控件2和3共同占满12列，可以置于一行内；而在小屏幕尺寸下，二者需要列数超过12，只能分置于两行了。
参考 Responsive Web Design
UI5 features for building responsive Fiori apps
jsDoc: sap.ui.layout.GridData
MDN: CSS Grid Layout
MDN: Using CSS multi-column layouts
</content>
    </entry>
    
     <entry>
        <title>Activiti Modeler中sid生成机制</title>
        <url>https://mryqu.github.io/post/activiti_modeler%E4%B8%ADsid%E7%94%9F%E6%88%90%E6%9C%BA%E5%88%B6/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>modeler</tag><tag>sid</tag><tag>机制</tag><tag>gojs</tag>
        </tags>
        <content type="html"> 昨天跟同事说Activiti Modeler中的sid比GoJS中的元素ID讲究，估计是由时戳和随机数混合生成的。 今天看了一下，发现原来就是一个纯随机数。
[tomcat]\webapps\activiti-explorer\editor-app\editor.html里对类为stencil-item的HTML元素设置的拖拽处理函数在[tomcat]\webapps\activiti-explorer\editor-app\stencil-controller.js中定义。 sid生成方法在[tomcat]\webapps\activiti-explorer\editor-app\editor\oryx.debug.js中定义。
ORYX.Editor.provideId = function() { var res=[], hex=&#39;0123456789ABCDEF&#39;; for(var i=0; i&amp;lt;36; i ) res[i]=Math.floor(Math.random()*0x10); res[14]=4; res[19]=(res[19] &amp;amp; 0x3) | 0x8; for(var i=0; i&amp;lt;36; i ) res[i] = hex[res[i]]; res[8] = res[13] = res[18] = res[23] = &#39;-&#39;; return &amp;quot;sid-&amp;quot; res.join(&#39;&#39;); };  当然，GoJSBPMN样例中的ID就更简单的不得了，全都是预定义的简单数字。例如，userTask的key预定义为7，当一个BPMN元素加入GoJSmodel时，GoJS会让model中的key变成唯一的（代码混淆过，我猜估计没混淆前叫makeUniqueKeyFunction）。在我的小测试中，第一个userTask的key仍然为7，第二个userTask的key被改成了-3。 感觉要是借鉴MongoDB的ObjectId生成机制，ID的冲撞概率可能会更低。
</content>
    </entry>
    
     <entry>
        <title>将GoJS和Activiti Explorer熬成一锅粥</title>
        <url>https://mryqu.github.io/post/%E5%B0%86gojs%E5%92%8Cactiviti_explorer%E7%86%AC%E6%88%90%E4%B8%80%E9%94%85%E7%B2%A5/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>gojs</tag><tag>activiti</tag><tag>bpmn</tag><tag>modeler</tag><tag>integration</tag>
        </tags>
        <content type="html"> 熬粥时，一开始创建一个跟editor-app平行的独立目录gojs-editor-app，结果我的Chrome浏览器报“Resourceinterpreted as stylesheet but transferred with MIME typetext/html”错误，GoJS的所有Javascript和stylesheet文件(例如BPMN.js和BPMN.css)都被加了料从Tomcat发给浏览器变成了html格式。 看了下面两个帖子，没有丝毫头绪： http://stackoverflow.com/questions/3467404/chrome-says-resource-interpreted-as-script-but-transferred-with-mime-type-tex
thttp://stackoverflow.com/questions/22631158/resource-interpreted-as-stylesheet-but-transferred-with-mime-type-text-html-see
搜了一下ActivitiExplorer的配置，看到org.activiti.explorer.filter.ExplorerFilter里有六个路径(&amp;ldquo;/ui&amp;rdquo;、&amp;rdquo;/VAADIN&amp;rdquo;、&amp;rdquo;/modeler.html&amp;rdquo;、&amp;rdquo;/editor-app&amp;rdquo;、&amp;rdquo;/service&amp;rdquo;、&amp;rdquo;/diagram-viewer&amp;rdquo;)走了特殊的过滤器。只好将GoJS的内容都转到Activiti已有的editor-app目录，这下齐活了。
</content>
    </entry>
    
     <entry>
        <title>试用GoJS BPMN生成Activiti支持的process.bpmn.xml</title>
        <url>https://mryqu.github.io/post/%E8%AF%95%E7%94%A8gojs_bpmn%E7%94%9F%E6%88%90activiti%E6%94%AF%E6%8C%81%E7%9A%84process.bpmn.xml/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>gojs</tag><tag>activiti</tag><tag>process.bpmn.xml</tag><tag>生成</tag>
        </tags>
        <content type="html"> 我在Activiti中建立一个简单的仅有startEvent、userTask和endEvent的BPMN模型，导出的process.bpmn.xml内容如下： 前一博文玩玩GoJS BPMN样例中我给出了类似 BPMN模型的JSON数据。通过分析可知，除了两者的ID生成机制不同（GoJSBPMN生成的ID太简单，很容易重复），完全可以通过GoJS的JSON数据构造上面的process.bpmn.xml文件内容。 试了一下在Javascript中生成上面的process.bpmn.xml文档，大致可行。从MDN查到的资料可知，仅支持IE9&#43;浏览器。不支持低版本IE浏览器，估计现在不算什么问题。目前生成的XML文档有些瑕疵，第一个使用createElement_x方法创建的节点会自动添加命名空间xmlns=http://www.w3.org/1999/xhtml，但是应该可以避免。
|Chrome|Firefox (Gecko)|Internet Explorer|Opera|Safari |&amp;mdash;&amp;ndash; |(Yes)|1.0 (1.7 or earlier)|9.0|(Yes)|(Yes)
JS代码如下： </content>
    </entry>
    
     <entry>
        <title>玩玩GoJS BPMN样例</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E7%8E%A9gojs_bpmn%E6%A0%B7%E4%BE%8B/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>gojs</tag><tag>bpmn</tag><tag>svg</tag><tag>png</tag><tag>json</tag>
        </tags>
        <content type="html"> 玩一玩GoJS，GoJS是Northwoods Software的产品。Northwoods Software创立于1995年，专注于交互图控件和类库。旗下四款产品： - GoJS：用于在HTML上创建交互图的纯javaSCript库，GoJS支持复杂的模板定义和数据绑定。 - GoDiagram：用于WinForms的.NET图控件。 - GoXam：用于WPF/Silverlight的图控件。 - JGo：用于Swing/SWT中创建交互图的java库。
试了一下GoJS的BPMN样例，很容易导出SVG或PNG/JPEG等格式的图像数据。其中： - 由于GoJS是经过代码混淆的，不能对makeSVG方法进行确定的分析，大概是采用HTMLCanvasElement.getContext(&amp;lsquo;2d&amp;rsquo;).drawImage(&amp;hellip;).getImageData(&amp;hellip;)获得的 - makeImage和makeImageData方法通过HTMLCanvasElement.toDataURL()方法实现的
在控制台执行window.myDiagram.model.toJson()，返回如下结果：
{ &amp;quot;class&amp;quot;: &amp;quot;go.GraphLinksModel&amp;quot;, &amp;quot;linkFromPortIdProperty&amp;quot;: &amp;quot;fromPort&amp;quot;, &amp;quot;linkToPortIdProperty&amp;quot;: &amp;quot;toPort&amp;quot;, &amp;quot;nodeDataArray&amp;quot;: [ { &amp;quot;category&amp;quot;: &amp;quot;activity&amp;quot;, &amp;quot;item&amp;quot;: &amp;quot;User task&amp;quot;, &amp;quot;key&amp;quot;: 7, &amp;quot;loc&amp;quot;: &amp;quot;388.33645784919577 140.35229369949943&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;User Task&amp;quot;, &amp;quot;taskType&amp;quot;: 2, &amp;quot;boundaryEventArray&amp;quot;: [ ], &amp;quot;size&amp;quot;: &amp;quot;120 80&amp;quot; }, { &amp;quot;category&amp;quot;: &amp;quot;event&amp;quot;, &amp;quot;item&amp;quot;: &amp;quot;End&amp;quot;, &amp;quot;key&amp;quot;: 104, &amp;quot;loc&amp;quot;: &amp;quot;569.86545617508 140.90913111767696&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;End&amp;quot;, &amp;quot;eventType&amp;quot;: 1, &amp;quot;eventDimension&amp;quot;: 8 }, { &amp;quot;category&amp;quot;: &amp;quot;event&amp;quot;, &amp;quot;item&amp;quot;: &amp;quot;start&amp;quot;, &amp;quot;key&amp;quot;: 101, &amp;quot;loc&amp;quot;: &amp;quot;183.42028795985397 135.34075693590137&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Start&amp;quot;, &amp;quot;eventType&amp;quot;: 1, &amp;quot;eventDimension&amp;quot;: 1 } ], &amp;quot;linkDataArray&amp;quot;: [ { &amp;quot;from&amp;quot;: 101, &amp;quot;to&amp;quot;: 7, &amp;quot;fromPort&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;toPort&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;points&amp;quot;: [ 204.92028795985, 135.3407569359, 214.92028795985, 135.3407569359, 261.62837290452, 135.3407569359, 261.62837290452, 140.3522936995, 308.3364578492, 140.3522936995, 328.3364578492, 140.3522936995 ] }, { &amp;quot;from&amp;quot;: 7, &amp;quot;to&amp;quot;: 104, &amp;quot;fromPort&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;toPort&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;points&amp;quot;: [ 448.3364578492, 140.3522936995, 458.3364578492, 140.3522936995, 492.60095701214, 140.3522936995, 492.60095701214, 140.90913111768, 526.86545617508, 140.90913111768, 546.86545617508, 140.90913111768 ] } ] }  (newXMLSerializer()).serializeToString(window.myDiagram.makeSvg())可以获得SVG的字符串表示。
</content>
    </entry>
    
     <entry>
        <title>[HBase] 启动内置ZooKeeper过程</title>
        <url>https://mryqu.github.io/post/hbase_%E5%90%AF%E5%8A%A8%E5%86%85%E7%BD%AEzookeeper%E8%BF%87%E7%A8%8B/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>zookeeper</tag><tag>hbase_manages_zk</tag><tag>start</tag><tag>process</tag>
        </tags>
        <content type="html">  start-hbase.sh  hbase-daemons.sh start zookeeper zookeepers.sh --hosts /usr/local/hbase/conf/regionservers--config /usr/local/hbase/conf cd /usr/local/hbase;/usr/local/hbase/bin/hbase-daemon.sh --config /usr/local/hbase/confstart zookeeper 检查配置HBASE_MANAGES_ZK，若不为true则终止处理。  ssh node51054 cd /usr/local/hbase;/usr/local/hbase/bin/hbase-daemon.sh --config /usr/local/hbase/confstart zookeeper hbase zookeeper start  java org.apache.hadoop.hbase.zookeeper.HQuorumPeer     </content>
    </entry>
    
     <entry>
        <title>粗览Activiti Modeler操作和源代码</title>
        <url>https://mryqu.github.io/post/%E7%B2%97%E8%A7%88activiti_modeler%E6%93%8D%E4%BD%9C%E5%92%8C%E6%BA%90%E4%BB%A3%E7%A0%81/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>modeler</tag><tag>代码</tag><tag>分析</tag>
        </tags>
        <content type="html">  Activiti Model Editor组件 我的了解Activiti Explorer及其Vaadin实现方式博文里提到ActivitiExplorer使用的是Vaadin架构，但是Activiti 模型编辑器组件却没用使用Vaadin架构，而是采用Angular.JS的MVC模式。Activiti模型编辑器组件的客户端代码位于Activiti\modules\activiti-webapp-explorer2\src\main\webapp\editor-app\。 该目录下的editor.html是Activiti Modeler Editor的主界面HTML代码 其中palette区是通过Angular.JS使用stencilsets\bpmn2.0\icons下多个子目录内的PNG图像形成的多组列表。其节点层次关系获取相关代码为： - stencil-controller.js - Activiti\modules\activiti-modeler\src\main\java\org\activiti\rest\editor\main\StencilsetRestResource.java - Activiti\modules\activiti-webapp-explorer2\src\main\resources\stencilset.json
editor.html中的视图与两个控制器进行了绑定: - stencil-controller.js：处理对canvas中BPMN元素的操作，很多处理是通过editor目录下的QRYX库完成的 - toolbar-controller.js：处理对工具栏的操作，很多处理由configuration\toolbar-default-actions.js完成
保存模型操作 保存模型操作，是通过toolbar-default-actions.js中的SaveModel方法完成的，它需要将三部分信息传给服务器： - 模型的元数据：例如模型名称、分类、创建时间、最后一次更新时间等等 - 模型JSON数据：将canvas内的图像数据转换成JSON数据UTF8字符串
 { &amp;quot;resourceId&amp;quot;: 53, &amp;quot;properties&amp;quot;: { &amp;quot;process_id&amp;quot;: &amp;quot;process&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;process_author&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;process_version&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;process_namespace&amp;quot;: &amp;quot;http://www.activiti.org/processdef&amp;quot;, &amp;quot;executionlisteners&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;eventlisteners&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;stencil&amp;quot;: { &amp;quot;id&amp;quot;: &amp;quot;BPMNDiagram&amp;quot; }, &amp;quot;childShapes&amp;quot;: [ { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-4F7484B9-11EC-4FCE-8950-FEFFB723D88B&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;overrideid&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;executionlisteners&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;initiator&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;formkeydefinition&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;formproperties&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;stencil&amp;quot;: { &amp;quot;id&amp;quot;: &amp;quot;StartNoneEvent&amp;quot; }, &amp;quot;childShapes&amp;quot;: [], &amp;quot;outgoing&amp;quot;: [ { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-B589A0D9-FA79-4C12-95B7-253E72480384&amp;quot; } ], &amp;quot;bounds&amp;quot;: { &amp;quot;lowerRight&amp;quot;: { &amp;quot;x&amp;quot;: 259, &amp;quot;y&amp;quot;: 139 }, &amp;quot;upperLeft&amp;quot;: { &amp;quot;x&amp;quot;: 229, &amp;quot;y&amp;quot;: 109 } }, &amp;quot;dockers&amp;quot;: [] }, { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-1A762474-62B9-4F3D-A81C-1ADD46AF7D2F&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;overrideid&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;asynchronousdefinition&amp;quot;: &amp;quot;false&amp;quot;, &amp;quot;exclusivedefinition&amp;quot;: &amp;quot;false&amp;quot;, &amp;quot;executionlisteners&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;multiinstance_type&amp;quot;: &amp;quot;None&amp;quot;, &amp;quot;multiinstance_cardinality&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;multiinstance_collection&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;multiinstance_variable&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;multiinstance_condition&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;isforcompensation&amp;quot;: &amp;quot;false&amp;quot;, &amp;quot;usertaskassignment&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;formkeydefinition&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;duedatedefinition&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;prioritydefinition&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;formproperties&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;tasklisteners&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;stencil&amp;quot;: { &amp;quot;id&amp;quot;: &amp;quot;UserTask&amp;quot; }, &amp;quot;childShapes&amp;quot;: [], &amp;quot;outgoing&amp;quot;: [ { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-4134C10E-B589-42FF-AACC-463D35D52016&amp;quot; } ], &amp;quot;bounds&amp;quot;: { &amp;quot;lowerRight&amp;quot;: { &amp;quot;x&amp;quot;: 746, &amp;quot;y&amp;quot;: 172 }, &amp;quot;upperLeft&amp;quot;: { &amp;quot;x&amp;quot;: 646, &amp;quot;y&amp;quot;: 92 } }, &amp;quot;dockers&amp;quot;: [] }, { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-B22A5CAB-94D0-419E-BB1E-E8538C6A7283&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;overrideid&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;executionlisteners&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;stencil&amp;quot;: { &amp;quot;id&amp;quot;: &amp;quot;EndNoneEvent&amp;quot; }, &amp;quot;childShapes&amp;quot;: [], &amp;quot;outgoing&amp;quot;: [], &amp;quot;bounds&amp;quot;: { &amp;quot;lowerRight&amp;quot;: { &amp;quot;x&amp;quot;: 1089, &amp;quot;y&amp;quot;: 138 }, &amp;quot;upperLeft&amp;quot;: { &amp;quot;x&amp;quot;: 1061, &amp;quot;y&amp;quot;: 110 } }, &amp;quot;dockers&amp;quot;: [] }, { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-B589A0D9-FA79-4C12-95B7-253E72480384&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;overrideid&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;conditionsequenceflow&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;executionlisteners&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;defaultflow&amp;quot;: &amp;quot;false&amp;quot; }, &amp;quot;stencil&amp;quot;: { &amp;quot;id&amp;quot;: &amp;quot;SequenceFlow&amp;quot; }, &amp;quot;childShapes&amp;quot;: [], &amp;quot;outgoing&amp;quot;: [ { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-1A762474-62B9-4F3D-A81C-1ADD46AF7D2F&amp;quot; } ], &amp;quot;bounds&amp;quot;: { &amp;quot;lowerRight&amp;quot;: { &amp;quot;x&amp;quot;: 645.5626565925471, &amp;quot;y&amp;quot;: 131.10730365650525 }, &amp;quot;upperLeft&amp;quot;: { &amp;quot;x&amp;quot;: 259.12484340745283, &amp;quot;y&amp;quot;: 124.26769634349473 } }, &amp;quot;dockers&amp;quot;: [ { &amp;quot;x&amp;quot;: 15, &amp;quot;y&amp;quot;: 15 }, { &amp;quot;x&amp;quot;: 50, &amp;quot;y&amp;quot;: 40 } ], &amp;quot;target&amp;quot;: { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-1A762474-62B9-4F3D-A81C-1ADD46AF7D2F&amp;quot; } }, { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-4134C10E-B589-42FF-AACC-463D35D52016&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;overrideid&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;conditionsequenceflow&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;executionlisteners&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;defaultflow&amp;quot;: &amp;quot;false&amp;quot; }, &amp;quot;stencil&amp;quot;: { &amp;quot;id&amp;quot;: &amp;quot;SequenceFlow&amp;quot; }, &amp;quot;childShapes&amp;quot;: [], &amp;quot;outgoing&amp;quot;: [ { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-B22A5CAB-94D0-419E-BB1E-E8538C6A7283&amp;quot; } ], &amp;quot;bounds&amp;quot;: { &amp;quot;lowerRight&amp;quot;: { &amp;quot;x&amp;quot;: 1060.676003953202, &amp;quot;y&amp;quot;: 130.93202152143962 }, &amp;quot;upperLeft&amp;quot;: { &amp;quot;x&amp;quot;: 746.595480421798, &amp;quot;y&amp;quot;: 124.30235347856038 } }, &amp;quot;dockers&amp;quot;: [ { &amp;quot;x&amp;quot;: 50, &amp;quot;y&amp;quot;: 40 }, { &amp;quot;x&amp;quot;: 14, &amp;quot;y&amp;quot;: 14 } ], &amp;quot;target&amp;quot;: { &amp;quot;resourceId&amp;quot;: &amp;quot;sid-B22A5CAB-94D0-419E-BB1E-E8538C6A7283&amp;quot; } } ], &amp;quot;bounds&amp;quot;: { &amp;quot;lowerRight&amp;quot;: { &amp;quot;x&amp;quot;: 1200, &amp;quot;y&amp;quot;: 1050 }, &amp;quot;upperLeft&amp;quot;: { &amp;quot;x&amp;quot;: 0, &amp;quot;y&amp;quot;: 0 } }, &amp;quot;stencilset&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;stencilsets/bpmn2.0/bpmn2.0.json&amp;quot;, &amp;quot;namespace&amp;quot;: &amp;quot;http://b3mn.org/stencilset/bpmn2.0#&amp;quot; }, &amp;quot;ssextensions&amp;quot;: [] }   模型的SVG图像数据：将canvas中的SVG图像数据经过过滤处理而得  服务器侧保存模型的代码位于 Activiti\modules\activiti-modeler\src\main\java\org\activiti\rest\editor\model\ModelSaveRestResource.java。 - 通过RepositoryService的saveModel方法将模型的元数据存入数据库的ACT_RE_MODEL表 - 通过RepositoryService的addModelEditorSource方法将模型JSON数据UTF8字符串存入数据库的ACT_GE_BYTEARRAY表 - 通过Apache™ Batik SVG Toolkit将模型的SVG图像数据转换成PNG格式，通过RepositoryService的addModelEditorSourceExtra方法将PNG图像存入数据库的ACT_GE_BYTEARRAY表
Activiti Explorer操作已保存模型 对模型的编辑操作是在Activiti Model Editor组件里实现的，对已保存模型的其他操作还是在ActivitiExplorer里基于Vaadin架构实现的。 客户端代码位于：Activiti\modules\activiti-explorer\src\main\java\org\activiti\editor\ui\。 下图的HTML界面由EditorProcessDefinitionDetailPanel.java实现。 显示已保存模型  选择模型，会调用EditorProcessDefinitionPage类的showProcessDefinitionDetail方法 EditorProcessDefinitionDetailPanel类的initUI方法调用initProcessDefinitionInfo方法，它会加入EditorProcessDefinitionInfoComponent实例 在构造EditorProcessDefinitionInfoComponent实例时，其initImage方法会被调用，通过RepositoryService的getModelEditorSourceExtra方法获得PNG格式图像，最终被显示到浏览器界面上。  部署已保存模型 EditorProcessDefinitionDetailPanel类的deployModel方法处理部署已保存模型的操作。 - 通过RepositoryService的getModelEditorSource方法获得模型JSON数据的UTF8字符串 - 通过FasterXML/jackson-databind转换成Java对象树 - 通过Activiti\modules\activiti-json-converter\src\main\java\org\activiti\editor\language\json\converter\BpmnJsonConverter.java将模型JSON数据的Java对象树转换成BpmnModel实例 - 通过Activiti\modules\activiti-bpmn-converter\src\main\java\org\activiti\bpmn\converter\BpmnXMLConverter.java将BpmnModel实例转成BPMN XML数据 - 通过RepositoryService的createDeployment方法将BPMN XML数据进行部署
导出已保存模型 EditorProcessDefinitionDetailPanel类的exportModel方法处理导出已保存模型的操作。 - 通过RepositoryService的getModelEditorSource方法获得模型数据的JSON字符串 - 通过FasterXML/jackson-databind转换成Java对象树 - 通过Activiti\modules\activiti-json-converter\src\main\java\org\activiti\editor\language\json\converter\BpmnJsonConverter.java将模型JSON数据的Java对象树转换成BpmnModel实例 - 通过Activiti\modules\activiti-bpmn-converter\src\main\java\org\activiti\bpmn\converter\BpmnXMLConverter.java将BpmnModel实例转成BPMN XML数据
编辑已保存模型 EditorProcessDefinitionDetailPanel类内注册了EditModelClickListener监听器用于处理导入BPMN模型操作。EditModelClickListener的showModeler会生成访问模型编辑器组件的URL地址，打开指定的模型。 - Activiti\modules\activiti-webapp-explorer2\src\main\webapp\editor-app\app.js中的监听器处理$includeContentLoaded事件，调用了fetchModel方法 - Activiti\modules\activiti-modeler\src\main\java\org\activiti\rest\editor\model\ModelEditorJsonRestResource.java处理该REST请求，返回由RepositoryService的getModel和getModelEditorSource方法获得Activiti模型元数据和JSON数据
导入BPMN模型 EditorProcessDefinitionDetailPanel类内注册了ImportModelClickListener监听器用于处理导入BPMN模型操作。ImportPopupWindow界面完成BPMN模型操作后，ImportUploadReceiver类的deployUploadedFile方法处理上传的BPMNXML数据。 - 通过Activiti\modules\activiti-bpmn-converter\src\main\java\org\activiti\bpmn\converter\BpmnXMLConverter.java将BPMN XML数据转换成BpmnModel实例 - 通过BpmnModel实例生成模型的元数据，通过RepositoryService的saveModel方法将模型的元数据存入数据库的ACT_RE_MODEL表 - 通过Activiti\modules\activiti-json-converter\src\main\java\org\activiti\editor\language\json\converter\BpmnJsonConverter.java将BpmnModel实例转换成模型JSON数据的Java对象树，通过RepositoryService的addModelEditorSource方法将模型JSON数据UTF8字符串存入数据库的ACT_GE_BYTEARRAY表
一些疑惑和想法： - 这里BpmnXMLConverter和BpmnJsonConverter用的比较频繁，而且成对出现。为什么不跳过中间的BpmnModel？ - 导入BPMN模型为什么不生成PNG图像？ - 数据库存储的模型数据不采用BPMNXML格式而是采用JSON格式，很灵活，可以随意添加Activiti扩展内容。但是如果没有现成的JSONschema，分析起来够麻烦。
</content>
    </entry>
    
     <entry>
        <title>Activiti 5.17 JNDI数据源配置</title>
        <url>https://mryqu.github.io/post/activiti_5.17_jndi%E6%95%B0%E6%8D%AE%E6%BA%90%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>mysql</tag><tag>jndi</tag><tag>datasource</tag><tag>postgres</tag>
        </tags>
        <content type="html">  Activiti演示环境采用的是h2内存数据库。为了便于研究代码，所以将其迁移到我已有的MySQL/PostgreSQL数据库上去。
MySQL MySQL配置 activiti数据库DDL文件位于activiti-engine-5.17.0.jar\org\activiti\db\create\，MySQL 5.6.4及其之后版本与之前的版本使用的是不同的DDL文件。将下列用于MySQL5.6.4&#43;的DDL文件提取保存到某一目录下。 - activiti.mysql.create.engine.sql - activiti.mysql.create.identity.sql - activiti.mysql.create.history.sql
MySQL命令如下：
create database ActivitiDB character set utf8 collate utf8_general_ci; use ActivitiDB; source c:/activiti.mysql.create.engine.sql; source c:/activiti.mysql.create.identity.sql; source c:/activiti.mysql.create.history.sql;  Tomcat配置 删除下列MyBatis配置文件： - apache-tomcat-7\webapps\activiti-explorer\WEB-INF\classes\db.properties - apache-tomcat-7\webapps\activiti-rest\WEB-INF\classes\db.properties
修改下列Spring配置文件： - apache-tomcat-7\webapps\activiti-explorer\WEB-INF\classes\activiti-custom-context.xml - apache-tomcat-7\webapps\activiti-rest\WEB-INF\classes\activiti-custom-context.xml
去掉XMl注释，删除&amp;rdquo;dbProperties&amp;rdquo;bean，将&amp;rdquo;dataSource&amp;rdquo;bean改成JNDI数据源。 修改下列Tomcat上下文，配置Tomcat JNDI资源： - apache-tomcat-7\webapps\activiti-explorer\META-INF\context.xml- apache-tomcat-7\webapps\activiti-rest\META-INF\context.xmlPostgreSQL PostgreSQL配置 activiti数据库DDL文件位于activiti-engine-5.17.0.jar\org\activiti\db\create\，将下列用于PostgreSQL的DDL文件提取保存到某一目录下。 - activiti.postgres.create.engine.sql - activiti.postgres.create.identity.sql - activiti.postgres.create.history.sql
PostgreSQL命令如下：
CREATE DATABASE ActivitiDB WITH ENCODING &#39;UTF8&#39; TEMPLATE=template0; \c ActivitiDB; \i c:/activiti.postgres.create.engine.sql; \i c:/activiti.postgres.create.identity.sql; \i c:/activiti.postgres.create.history.sql;  Tomcat配置 删除下列MyBatis配置文件： - apache-tomcat-7\webapps\activiti-explorer\WEB-INF\classes\db.properties - apache-tomcat-7\webapps\activiti-rest\WEB-INF\classes\db.properties
修改下列Spring配置文件： - apache-tomcat-7\webapps\activiti-explorer\WEB-INF\classes\activiti-custom-context.xml - apache-tomcat-7\webapps\activiti-rest\WEB-INF\classes\activiti-custom-context.xml
去掉XMl注释，删除&amp;rdquo;dbProperties&amp;rdquo;bean，将&amp;rdquo;dataSource&amp;rdquo;bean改成JNDI数据源。 修改下列Tomcat上下文，配置Tomcat JNDI资源： - apache-tomcat-7\webapps\activiti-explorer\META-INF\context.xml - apache-tomcat-7\webapps\activiti-rest\META-INF\context.xml </content>
    </entry>
    
     <entry>
        <title>Activiti模型编辑器之前前身：Oryx editor</title>
        <url>https://mryqu.github.io/post/activiti%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%BE%91%E5%99%A8%E4%B9%8B%E5%89%8D%E5%89%8D%E8%BA%ABoryx_editor/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>oryx</tag><tag>signavio</tag><tag>bpmn</tag><tag>modeler</tag>
        </tags>
        <content type="html">  我的The history and commercial version of Activiti Modeler博文里提到Activiti建模器组件是由Signavio捐赠的Signavio Core Components project， 可以说Activiti模型组件的前身是Signavio Core Components project， 那么Signavio Core Components project的前身又是什么呢？ Signavio (wiki)里面提到了Signavio是德国波茨坦大学哈索普莱特纳研究院（HPI）毕业生团队创建的。在Signavio之前，Signavio的这些创始人在HPI从2006年开始开发了世界上第一个用于BPMN的web建模器Oryx，一个学术性开源项目。Oryx就是Signavio Core Components project开源版和Signavio Process Editor商业版的蓝图。随着Signavio公司的产生，Oryx项目被废弃了、不再进行维护。 Oryx开发团队
答案就是：Signavio Core Componentsproject的前身，即Activiti模型编辑器之前前身是Oryx。Oryx里面有一些设计文档，对学习Activiti建模器组件仍然有一定的帮助。
参考 http://en.wikipedia.org/wiki/Signavio
http://bpt.hpi.uni-potsdam.de/Oryx/News
https://github.com/Activiti
http://code.google.com/p/signavio-core-components/
https://code.google.com/p/oryx-editor/
</content>
    </entry>
    
     <entry>
        <title>The history and commercial version of Activiti Modeler</title>
        <url>https://mryqu.github.io/post/the_history_and_commercial_version_of_activiti_modeler/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>modeler</tag><tag>history</tag><tag>signavio</tag><tag>kisbpm</tag>
        </tags>
        <content type="html">  The history of Activiti Modeler Activiti Modeler component is as part of the Activiti projectfrom the start. This Modeler was donated by Signavio as part of the Signavio Core Components project. This project was not maintained anymoresince 2011 (Activiti 5.7) and Activiti team no longer considered it‘core’ to Activiti. Tijs Rademakers (the author of Activiti in Action) has spentquite some in his spare time to master, enhance and tweak the ‘oldcode base’, fork the Signavio Core Components project and create acompletely new, maintainable codebase. The Modeler is now perfectly integrated with Activiti Explorer and has all the specific coveragefor Activiti (Activiti 5.11)!
The commercial version of Activiti Modeler Activiti Modeler is KIS BPM process solution&amp;rsquo;s open sourceversion. KIS means keep it simple. Below a feature comparisonbetween the KISBPM product and the Activiti Modeler. Reference Activiti Modeler: Resurrected and Revamped
BPMN 2.0 / Activiti in Action
What should you know about Activiti BPM?
KIS BPM
KIS BPM feature
</content>
    </entry>
    
     <entry>
        <title>业务流程建模标注工具比较</title>
        <url>https://mryqu.github.io/post/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E5%BB%BA%E6%A8%A1%E6%A0%87%E6%B3%A8%E5%B7%A5%E5%85%B7%E6%AF%94%E8%BE%83/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>bpmn</tag><tag>建模</tag><tag>标注</tag><tag>工具</tag><tag>比较</tag>
        </tags>
        <content type="html">  原文http://en.wikipedia.org/wiki/Comparison_of_Business_Process_Modeling_Notation_tools ，留着学习参考、技术选型。
General NameCreatorPlatform / OSBPMN VersionFeaturesFirst ReleaseLatest ReleaseSoftware licenseActiviti ModelerAlfresco&amp;nbsp;and theActiviti communityCross-platformBPMN 2.0Modeler,&amp;nbsp;Simulation,&amp;nbsp;Execution2010-05-17[1]2014-10-16[2]Apache License&amp;nbsp;2.0[3]ActiveVOSInformaticaWindows,&amp;nbsp;LinuxBPMN 2.0Modeling, Testing and Execution with open standards.20052014ProprietaryADONIS (software)BOC Information Technologies Consulting AGWindowsBPMN 2.0Business Process Analysis (BPA) tool supporting businessprocess management allowing process modeling, analysis, simulation,evaluation, publishing and automation. Freeware Community Editionavailable.19952012Proprietary/FreewareAgiles BPMS &amp;amp; ECMIMAGE Technology S.A.Windows,&amp;nbsp;Linux,MacBPMN 2.0Modeler,&amp;nbsp;Execution,2003-Oct2013-SepProprietaryAltova&amp;nbsp;UModelAltovaWindowsBPMN&amp;nbsp;1.1, 2.0Includes BPMN,&amp;nbsp;UML,&amp;nbsp;SysML, C#and&amp;nbsp;Java&amp;nbsp;round trip code generation,documentation, collaboration (includingwith&amp;nbsp;MetaTeam) and database modeling20052013-06-12Proprietary[4]ARCWAY CockpitARCWAY AGWindows,&amp;nbsp;Mac&amp;nbsp;(Linuxunofficially)BPMN 2.0BPMN Collaboration Diagrams, EPC, Petri Nets, integratedwith&amp;nbsp;FMC&amp;nbsp;Blockdiagramsfor business and IT architecture,&amp;nbsp;UML&amp;nbsp;Class diagramsfor data models and&amp;nbsp;Requirements management20052014Proprietary, free single user (Designer) editionARIS ExpressSoftware AGWindows&amp;nbsp;(andLinux,&amp;nbsp;Macunofficially)BPMN 2.02009-07-282012-12-19Freeware (registration needed)AuraPortalAuraPortalWindowsBPMN 2.0Modeler,&amp;nbsp;Execution,&amp;nbsp;Simulation. BPM (BusinessProcess Management) Suite for automating the operating processes ofan organization, including free BPMN tool: AuraPortal HeliumModeler.2001Suite Proprietary or Cloud and Modeler FreewareBizagi BPM SuiteBizagiWindowsBPMN 2.0Simulation,&amp;nbsp;ExecutionProprietaryBizagi Process ModelerBizagiWindowsBPMN 2.0ModelerFreewareBiZZdesign ArchitectBiZZdesignWindowsBPMN 2.0Modeler, Integrate with&amp;nbsp;ArchiMate, User collaboration, Support forCollaboration diagrams, BPMN 2.0 XML Export &amp;amp; Import, Webreporting, Paper reporting, Touchscreen interface available20122014ProprietaryBPMN Visio ModelerTrisotechWindowsBPMN 2.0Complete BPMN 2.0 file format import and export within Visio,live teamwork support, migration BPMN 2.0 tool, process repository,process simulator.2010Bi-yearlyProprietary, shareware, 1 month free trialBPMN Web ModelerTrisotechCloudBPMN 2.0Process Animator to learn about the dynamic behaviour of themodel, live teamwork support, migration BPMN 2.0 tool, processrepository, Interchange capability, process simulator.2012Bi-monthly[5]Proprietary, shareware, 1 month free trialBonita BPMBonitaSoftWindows,&amp;nbsp;Linux,MacBPMN 2.0Bonita Open Solution combines three solutions in one: a Studiofor process modeling, a BPM &amp;amp; Workflow engine, and a userinterface， enabling to create standalone process-basedapplications.20012014-07-02GPLv2[6]Borland TogetherBorlandWindows,&amp;nbsp;Linux,Mac,&amp;nbsp;Solaris2009-07-04Proprietary/SharewareCamunda ModelerCamundaCross-platformBPMN 2.0BPMN 2.0 modeling plugin for Eclipse20132014-03-31Apache License&amp;nbsp;2.0Cubettosemture GmbHiOS,&amp;nbsp;AndroidBPMN 2.0Auto-layout, Process-Flow-Wizard, BPMN 2.0 XML Export2012-05[7]2013-08ProprietaryCubetto Toolsetsemture GmbHWindows,&amp;nbsp;Mac,LinuxBPMN 2.0Typing can be changed, further support for ARIS/EPC, UML, BSC,Flow Charts20032013-08FreewareEclipse BPMN2 ModelerEclipse.org. Eclipse SOA project.Cross-platformBPMN 2.0BPMN 2.0 modeling. Included in jBPM.20112014Eclipse Public License 1.0jBPMNNetBeans Community project.Cross-platformBPMN 2.0BPMN 2.0 Process and Conversation Flow.20132014Apache License&amp;nbsp;2.0Enterprise ArchitectSparx SystemsWindows,&amp;nbsp;Linux,MacBPMN 2.0Includes BPMN, BPEL, UML, SysML, BPMN 2.0 XML Import/Export,XSD, WSDL and supports BPMN Simulation20002015-01Proprietary[8]Genexus WorkFlowArtechWindowsBPMN 2.0Model BPMN 2.0 import and explort XPDL2012-03ProprietaryYaoqiang BPMN Editor[9]史耀强 (Blenta)
(Sourceforge ID)Java&amp;nbsp;/&amp;nbsp;Windows,Linux,&amp;nbsp;Mac,SolarisBPMN 2.0Supports all BPMN 2.0 elements; real-time syntax-checking;built-in Version Control; Diagram Interchange support with multiplegraphical formats supported.2010-05-272014-11-16GPL (v4.0)
Proprietary (V3.0)
GPL (V2.2 and earlier)HP Process Automation[10]
HPJava&amp;nbsp;/&amp;nbsp;WindowsBPMN 2.0Powerful as a standalone BPM application. Integrated suite ofapplications to provide end-to-end Solutions for Human and ContentCentric Processes. Single vendor solution from HP MFPs to DocumentCapture to Process Automation to Content Management to RecordsRetention to Legal Holds to Content Distribution20002013ProprietaryIBM BlueWorks LiveIBMCloud (browser based)BPMN 2.0ProprietaryIBM Process DesignerIBMeclipse based tool for creating executable processesBPMN 2.0&#43;Business process design tool with support for executableenvironment, simulation, analysis and optimization. Includesprocess portal web interface for executable business processes andcollaboration with other process users and experts.ProprietaryIBM&amp;nbsp;Rational System ArchitectIBMEnterprise Architecture toolBPMN 2.0&#43;Enterprise Architecture tool supporting BPMN 2.0 notation (andbidirectional BPMN 2.0 interchange) integrated within EA frameworkssuch as TOGAF, DoDAF, Archimate, and others. A thin-client sisterproduct, SA/XT, allows creation of BPMN 2.0 models on the web. AnSA Design Manager extension publishes the EA with BPMN to web in[RDF] format allowing [SPARQL] queries to produceopen-social-gadget dashboards.2014, SeptemberProprietaryiGrafx Flowcharter, iGrafx ProcessiGrafxWindows&amp;nbsp;andCross-platform(via iGrafx Cloud)BPMN 2.0iGrafx supports BPMN diagramming throughout its entire toolsetand incorporates validation tools to ensure compliance with thestandard. There is a collaborative environment available(on-premise and cloud).19882012ProprietaryINNOVATOR for Business AnalystsMID GmbHWindowsBPMN 2.0process modeling, documentation, collaboration, simulation,analysis,20102012Proprietary, free Personal EditionjBPMRedhatCross-platformBPMN 2.0Workflow Engine and Tools2014-08-02Apache LicenseLogizian[11]Visual ParadigmWindows,&amp;nbsp;Linux,OS X,&amp;nbsp;SolarisBPMN 2.0BPMN Modeling, Business Rules, Simulation, Central Vocabulary,Data Modeling, Team Collaboration, BPMN 2.0 XML Export, GenerateBPEL, WSDL, and JPDL2013-12-162014-04-03ProprietaryLucidChartLucid Software IncCross-platform(browser based)BPMN 2.0Google App / Google Drive integration, supports Visio files,JIRA integration, Confluence integration2011updated twice a monthProprietary - 14 day free trial on team accounts / Freeprofessional accounts for educators and students / FreeversionMagicDrawNo MagicWindows,&amp;nbsp;Linux,MacBPMN 2.02007-09-242014-06-02Proprietary/SharewarePega SystemsPega BPMWindowsBPMN 2.0Supports Process modelling, business analysis and simulation,with publication ad-hoc reporting and advanced collaboration andanalysis features. The most advanced method-based businessarchitecture solution available, fully integrated with enterpriseArchitecture and Compliance and risk.19912014ProprietaryMicrosoft Visio2013MicrosoftWindows2013ProprietaryModelioModeliosoftWindows,&amp;nbsp;Linux,Mac OSBPMN 2.0Includes BPMN, UML, SysML, Java round trip code generation,documentation, TOGAF, XSD, WSDL20092014-10-01Open sourceOmniGraffleOmni GroupMac2010ProprietaryProcessCraft[12]Tabtou LtdiOS,&amp;nbsp;Android,Windows,&amp;nbsp;Linux,OS XBPMN 2.0Automatic Syntax Checking, Context Sensitive Help, Gestures,Intelligent Menu, Auto-resizing Pools, User Collaboration, InfiniteCanvas Size, BPMN 2.0 XML Export2012-05-10ProprietaryProcess Modeler for Microsoft Visio[13]itp commerce agWindowsBPMN 2.0Provides&amp;nbsp;Microsoft Visio&amp;nbsp;integrationinto&amp;nbsp;SharePoint, full support for Method&amp;amp;Style,Process Maps, Modeling Wizard (Business friendly Modeling), ContextSensitive Help, Auto-resizing Pools, Simulation, UserCollaboration, BPMN 2.0 XML Export, Reports in various formats,Enterprise Repository20032014-09-18ProprietaryQPR ProcessDesignerQPR Software Plc.WindowsBPMN 2.0Document Management,2002-Jan2014-JanProprietaryQUAMLINTRA Solutions GmbHMicrosoftSharePointBPMN 2.0Provides&amp;nbsp;Microsoft Visio&amp;nbsp;integrationinto&amp;nbsp;SharePoint. Support Business Process ModelingNotation (BPMN) and freehand charts.20072013ProprietaryRunaWFERuna Consulting GroupCross-platformBPMN 2.0Graphical Process Designer, Task Notifier, Engine (the coreworkflow processor)20042014-08-01[14]Free and open-source software&amp;nbsp;(LGPL)SemTalkSemtation GmbHWindowsSharePointMicrosoft VisioBPMN 2.0Syntax check, SharePoint integration with shared repositories,publication in HTML/ Silverlight/ as process portal, export in XML,multi-language, extensive &amp;amp; flexible reports, adds OO modeling&amp;amp; simulation capabilities to Microsoft Visio (as aplug-in)20012014-05ProprietarySignavio Process EditorSignavioCloud or
On-premise (Windows,Linux)server),Client-side browserBPMN 2.0Primarily a BPMN2.0 collaborative modeling platform includingprocess portal and simulation. Also supports ArchiMate2.1, ValueChains, EPC and other notations. BPMN2.0 XML Import/Export.QuickModel forms based BPM model creation. Google Sites,Confluence, Sharepoint integration.[15]2009updated monthlyProprietarySoftware Ideas ModelerDusan RodinaWindows,&amp;nbsp;LinuxBPMN 2.02009-Aug2014-FebProprietaryStagesMethod ParkCross-Platform, Cloud and On-PremiseProcess Definition (BPMN, EPC, SIPOC, Turtle, customerspecific), Process Sharing (HTML, XML, MS Project, ...), ProcessExecution (BPMN Engine, IBM Rational Team Concert), ProcessControl, Compliance Assurance (SPICE, Automotive SPICE, CMMI, ISO26262, IEC 62304, EN 50128, DO-178, DO-254, etc.)2002March 2014ProprietarySYDLE SEED CommunitySYDLE SystemsCloud (browser based)Supports modeling and execution of processes2012-072012-07FreewareTIBCO ActiveMatrixTIBCO Software Inc.Linux,&amp;nbsp;AIX,&amp;nbsp;HP-UX,&amp;nbsp;Solaris,WindowsBPMN 2.0Modeler,&amp;nbsp;Simulation,&amp;nbsp;Execution2010-05[16]2013-02[17]Proprietary[18]TriasterTriasterWindowsTriaster Solution supports Business process modeling,Analysis,Simulation, Search, Reporting, Advanced Process LibraryarchitectureSeptember 2014ProprietaryVisible Analyst[19]Visible SystemsWindows, CitrixBPMNBPMN diagramming; interactive data repository support;multi-user support.20072013ProprietaryW4 BPMN&#43;[20]W4 SoftwareWindows,&amp;nbsp;Mac,Linux/UnixBPMN2.0BPMN diagramming; BPMN2.0 Execution; Extensions for UI andData.2014-012014-01ProprietaryyEdyWorksWindows,&amp;nbsp;Mac,Linux/UnixBPMN 2.0Stores graphics in internal format. Cannot load/save BPMN 2.0files.2013-03FreewareSparx Enterprise ArchitectSparx SystemsWindows,&amp;nbsp;Mac,Linux/UnixBPMN 2.0Useful to map State Machines, Interaction (Sequence diagrams,Activities, BPMN. Written in C&#43;&#43;20002013-03ProprietaryIntellileap SolutionsIntelliPROBPMSWindows&amp;nbsp;Cloud &amp;amp; On-premiseBPMN 2.0BPM Software20132013-14Proprietary References   Alfresco. &amp;ldquo;Alfresco launches Activiti BPMN 2.0 Business Process Engine&amp;rdquo;.Retrieved 17September 2013.  Activiti. &amp;ldquo;Activiti Readme&amp;rdquo;.Retrieved   Activiti. &amp;ldquo;Activiti 5.13 User Guide&amp;rdquo;. Retrieved 17September 2013.  Altova GmbH. &amp;ldquo;ALTOVA? END-USER LICENSE AGREEMENT&amp;rdquo;. Retrieved 17September 2013.  Trisotech. &amp;ldquo;BPMN Web Modeler Releases&amp;rdquo;.  Bonitasoft. &amp;ldquo;Download BPM Software And Documentation&amp;rdquo;.Retrieved 17September 2013.  semture News. &amp;ldquo;News&amp;rdquo;.Retrieved 12March 2014.  Sparx System&amp;rsquo;s EnterpriseArchitect. &amp;ldquo;Enterprise Architect End User License Agreement (EULA)&amp;rdquo;.Retrieved 18September 2013.  http://sourceforge.net/projects/bpmn  http://www.autonomy.com/products/process-automation  http://www.visual-paradigm.com/product/lz/  http://www.showgen.com  http://www.itp-commerce.com  RunaWFE. &amp;ldquo;RunaWFE on sourceforge&amp;rdquo;. Retrieved 26December 2013.  Feature Overview Signavio ProcessEditor; http://www.signavio.com/products/process-editor/process-modeling/  TIBCO ActiveMatrix&amp;reg;BPM. &amp;ldquo;Readme 1.0.1&amp;rdquo;. Retrieved 18September 2013.  TIBCO ActiveMatrixBPM. &amp;ldquo;Readme 2.1.0&amp;rdquo;. Retrieved 18September 2013.  TIBCO Software Inc. &amp;ldquo;END USER LICENSE AGREEMENT (&amp;ldquo;AGREEMENT&amp;rdquo;)&amp;rdquo;.Retrieved 17September 2013.  http://www.visible.com/Products/Analyst/  http://www.w4software.com/produit/bpmn-plus.htm  </content>
    </entry>
    
     <entry>
        <title>了解Activiti Explorer及其Vaadin实现方式</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3activiti_explorer%E5%8F%8A%E5%85%B6vaadin%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>explorer</tag><tag>vaadin</tag><tag>gwt</tag><tag>web</tag>
        </tags>
        <content type="html">  通过ActivitiModeler架构图可知，Activiti Explorer采用的是Vaadin框架。 Vaadin 是一种 Java Web 应用程序的开发框架, 其设计目标是便利地创建和维护高质量的 Web UI 应用程序.Vaadin 支持两种不同的开发模式: 服务器端开发和客户端开发. 服务器端开发方式是这二者中更为强大的一种. 它能帮助开发者忘记Web 程序的各种实现细节, 使得 Web 应用程序的开发变得就象过去使用便利的Java开发工具(如AWT, Swing,SWT)来开发桌面应用程序一样, 甚至更简单。 Vaadin 应用程序中基本上所有的逻辑都是运行在服务器端的 Java Servlet API 上的，如下图中Vaadin的运行时结构图所示，Vaadin运行时结构主要由服务器端框架和客户端引擎两部分构成。服务器端框架包含了用来与客户端引擎通讯的服务器端集成层以及一系列的 server端 UI 组件。客户端引擎则由 Google Web toolkit(GWT) 页面渲染模块和客户端集成层两部分组成。 ActivitiExplorer的代码位于Activiti\modules\activiti-explorer下： 参考资料 vaadin官方网站
book of vaadin 中文版
Vaadin - 来自北欧的 Web 应用开发利器，第 1 部分: Vaadin 的基本概况和基础开发
Vaadin - 来自北欧的 Web 应用开发利器，第 2 部分: Vaadin 的体系结构和功能扩展
</content>
    </entry>
    
     <entry>
        <title>搭建Activiti的IntelliJ IDEA开发调试环境</title>
        <url>https://mryqu.github.io/post/%E6%90%AD%E5%BB%BAactiviti%E7%9A%84intellij_idea%E5%BC%80%E5%8F%91%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>intellij</tag><tag>idea</tag><tag>开发</tag><tag>调试</tag>
        </tags>
        <content type="html">  准备工作 在搭建Activiti的IntelliJ IDEA开发调试环境前，确保下列软件已经安装： - JDK 1.6 - ant - Maven - IntelliJ IDEA
导入Activiti Import Project 通过Activiti的Maven pom.xml导入IntelliJ IDEA项目。 Import Modules 导入IntelliJ IDEA项目后，仅有几个Activiti模块变成了IntelliJIDEA项目中的模块。为了可以调试所有模块，通过ProjectStructure菜单简单粗暴地将剩余的Activiti模块导入IntelliJ模块。 对每个手工导入的模块设置源代码、资源、测试代码和测试资源。 Frameworks Dection 手工导入所有模块后，重启IntelliJ IDEA，它会自动检测所有模块的类型，例如Spring、WEB、JPA和GWT等。 构建Activiti Activiti项目可以通过下面的ant命令构建：
.....\wfgitws\Activiti\distro&amp;gt;ant -Dnodocs=true clean distro  也可以在IntelliJ IDEA IDE中构建： 本地调试 To make sure that the necessary application server plugin isenabled 本测试中使用Apache Tomcat服务器。 Defining application servers in IntelliJ IDEA Check and configure artifacts in IntelliJ IDEA Add run/local configuration in IntelliJ IDEA Run in IntelliJ IDEA Add Jetbrain IDE plugin in Chrome 远程调式 Remote Debugging Configuration 通过Run-Edit Configuration菜单命令，添加一个远程调试配置。 该调试配置需要同Tomcat保持一致 Remote Debugging 设置断点，在Activiti Explorer上的操作触发断点，该Activiti的IntelliJIDEA开发调试环境已经可以工作了。 </content>
    </entry>
    
     <entry>
        <title>了解Activiti组件</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3activiti%E7%BB%84%E4%BB%B6/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>bpmn</tag><tag>process</tag><tag>flow</tag><tag>component</tag>
        </tags>
        <content type="html">  Activiti 简介 Activiti作为一个遵从 Apache许可的工作流和业务流程管理开源平台，其核心是基于 Java 的超快速、超稳定的 BPMN 2.0流程引擎，强调流程服务的可嵌入性和可扩展性，同时更加强调面向业务人员。Activiti可以运行在任何JAVA程序中：单机、服务器、集群或云上。
Activiti 组件 - Activiti Modeler—建模器Activiti建模器是基于开源的Signavio流程编辑器的一个定制版本，可以使用浏览器图形化地编辑BPMN2.0兼容流程，流程文件被存储在数据库模型仓库内。Activiti团队已经停止Activiti建模器的活跃开发，目前仍保留在ActivitiExplorer内。- Activiti Designer—设计器Activiti设计器是Eclipse插件形式的，除了可以建模BPMN2.0流程，还支持Activiti特定的扩展，可以发挥Activiti流程和引擎的全部潜能。- Activiti Kickstart—基于表格的流程设计在Activiti Explorer的Model workspace里新建模型时选择Table-drivendefination时就是所谓的ActivitiKickstart，现在Activiti Designer里也包含这一功能。 - Activiti ExplorerExplorer是流程引擎的Web用户控制台。使用它来进行任务管理、流程实例检查、管理和基于历史统计数据查看报表等等。它不是一个的最终客户可用的成熟监控和管理应用程序，仅用于演示Activiti如何用于用户的应用中。 - Activiti RestActiviti引擎的 REST API。此外，曾经存在过下面两个组件（至少 Activiti 5.0-alpha4版本）： - ActivitiProbe：一个对流程引擎运行期实例提供管理及监控的web应用程序。包含部署的管理、流程定义的管理、数据库表的检视、日志查看、事务的平均执行时间、失败多次的工作等功能。已经变成ActivitiExplorer的一部分 - Activiti Cycle：BPM协作用具，改用camunda Cycle了
参考 Activiti官方网站
Activiti用户指南
Activiti组件
Activiti开发指南
Activiti Javadoc
Activiti源代码(GitHub)
Adhoc workflow with Activiti: introducing Activiti KickStart
Activiti Cycle explained
Easy Workflows - Activiti Kickstart
Activiti - 新一代的开源 BPM 引擎
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 第三方JavaScript库加载</title>
        <url>https://mryqu.github.io/post/openui5_%E7%AC%AC%E4%B8%89%E6%96%B9javascript%E5%BA%93%E5%8A%A0%E8%BD%BD/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>3rd</tag><tag>library</tag><tag>loading</tag>
        </tags>
        <content type="html"> SAP often put 3rd JavaScript libraries at \resources\sap\ui\thirdparty, then load as below:
jQuery.sap.require(&amp;quot;sap/ui/thirdparty/d3&amp;quot;);  样例： OpenUI5: D3.js based custom control and table
Custom SAPUI5 Visualization Controls with D3.js
</content>
    </entry>
    
     <entry>
        <title>Activiti相关帖子汇总</title>
        <url>https://mryqu.github.io/post/activiti%E7%9B%B8%E5%85%B3%E5%B8%96%E5%AD%90%E6%B1%87%E6%80%BB/</url>
        <categories>
          <category>workflow</category>
        </categories>
        <tags>
          <tag>activiti</tag><tag>汇总</tag><tag>博文</tag><tag>帖子</tag><tag>资料</tag>
        </tags>
        <content type="html">  咖啡兔 :《Activiti实战》作者闫洪磊Activiti相关博文 iteye - pyzheng的博客工作流相关博文 csdn - howareyoutodaysoft的博客Activiti相关博文 新浪博客 - 微笑浆糊-xerllentbpm工作流相关博文 网易博客 - homeland520工作流引擎相关博文 csdn - fanfan159357的专栏Activiti designer源码研究相关博文 csdn - 白乔专栏工作流activiti的一些概念Activiti源码浅析：Activity与TaskActiviti源码浅析：Activiti的活动授权机制activiti 源码笔记之startProcess[转]Activiti源码分析（框架、核心类。。。）[转]activiti源码解读之心得整编在Activiti官方源码上提交的两个bugfix csdn - 宋三丝的专栏Activiti相关博文 iteye - jhaij的博客集成activiti-modeler 到 自己的业务系统(集成流程跟踪-完美支持IE)activiti taskservice addComment Provided id is nullactiviti 用户任务 - - - iteye站内Activiti相关博文
csdn站内Activiti相关博文
开源中国社区（oschina.net）站内Activiti相关博文
  </content>
    </entry>
    
     <entry>
        <title>[算法] 实证分析</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_%E5%AE%9E%E8%AF%81%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>algorithm</tag><tag>exmpirical</tag><tag>analytics</tag><tag>power-law</tag><tag>relationship</tag>
        </tags>
        <content type="html"> 最近又重温了一下算法课中的实证分析。 - 首先针对不同大小的输入获取运行时长。 - 可以通过标准坐标图或双对数坐标图查看运行时常与输入大小的关系。 - 通过成倍增加输入量，可以更便利地估算T(N)与N之间的幂指数关系。 lg( T(N) ) = b lg( N ) &#43; c 即 T(N) = a Nb, 其中 a = 2c b = ( lg( T(2N1) ) - lg( T(N1)) ) ) / ( lg(2N1)) - lg( N1) ) ) = lg( T(2N1)) ) -lg( T(N1)) ) a = T(2N1)) / (2N1))b 可以教程上第二行就可以计算出b，这就有点不对头了。时常为零，意味着取对数的结果是-∞。返回数值是以毫秒为单位的，因此一般时长也应该是为毫秒为单位的。估计教程上时常实际精度为三位而显示精度为一位，导致不一致的。 下图是我用Excel根据教程显示数据计算的结果： </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 数据绑定模式</title>
        <url>https://mryqu.github.io/post/openui5_%E6%95%B0%E6%8D%AE%E7%BB%91%E5%AE%9A%E6%A8%A1%E5%BC%8F/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>data</tag><tag>binding</tag><tag>model</tag><tag>源代码</tag>
        </tags>
        <content type="html">  OpenUI5数据绑定模式概念 OpenUI5开发指南-数据绑定模式介绍了OpenUI5数据绑定模式概念和不同模型的默认值。
绑定模式 绑定模式定义了数据源如何绑定。不同模型实现需要特定绑定模式。例如资源模型仅支持模型到视图的一次性绑定。 SAPUI5提供如下绑定模式： - 单向绑定：单向绑定意味着模型到视图的绑定；模型中的数据变化将更新相应的绑定和视图。 - 双向绑定：双向绑定意味着模型到视图及视图到模型的绑定；模型/视图中的数据变化将更新相应的绑定和视图/模型。 - 一次性绑定：一次性绑定意味着模型到视图的绑定。
下表展示了不同模型分别支持的绑定模式：
|模型|一次性绑定|双向绑定|单向绑定 |&amp;mdash;&amp;ndash; |资源模型|&amp;ndash;|&amp;ndash;|X |JSON模型|X|X|X |XML模型|X|X|X |OData模型|X|X|X
资源模型仅处理静态文本，所以仅支持一次性绑定模式
模型的默认绑定模式 当模型实例被创建后，该实例具有一个默认绑定模式。该模型实例的所有绑定会采用他们自己默认绑定模式。 下表展示了不同模式实现的默认绑定模式。
|模型|默认绑定模式 |&amp;mdash;&amp;ndash; |资源模型|一次性绑定 |JSON模型|双向绑定 |XML模型|双向绑定 |OData模型|单向绑定
OpenUI5数据绑定模式范例 OpenUI5开发指南-数据绑定入门介绍了数据绑定使用范例。
OpenUI5数据绑定模式源代码研究 数据绑定模式在sap.ui.model.BindingMode中定义。 通过如上类图可知，JSON模型类和XML模型类继承自客户端模型类，资源模型和OData模型直接继承自模型类。 客户端模型具有额外的setData方法。客户端模型相对模型类多了一层客户端数据，可以存储视图属性变化相应的数据，应此能够在不跟服务器端交互的情况下实现双向绑定。网上的很多演示采用客户端模型，就是因为无需搭建服务器，易于实现。 模型类原型有一个checkUpdate方法，用于在模型数据发生变化后，检查模型的所有绑定是否需要更新以实现模型到视图的绑定。其调用情况如下： - JSON模型和XML模型：被setData和setProperty方法调用 - OData模型：被loadData、setProperty和refresh方法调用 - 资源模型：无调用
视图到模型的绑定，主要在sap.ui.base.ManagedObject类实现。 ManagedObject是所有视图控件的祖宗类，ManagedObject原型的_bindProperty方法判别绑定模式是否是一次性绑定，是的话就将绑定上的事件和模型数据变化处理程序卸载掉。 ManagedObject原型的updateModelProperty方法判别绑定模式是否是双向绑定，是的话就将视图属性变化写入绑定，从而将数据写入模型。
参考 OpenUI5 API参考指南
sap.ui.model包源代码 - GitHub
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] YARN中的AuxiliaryService</title>
        <url>https://mryqu.github.io/post/hadoop_yarn%E4%B8%AD%E7%9A%84auxiliaryservice/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>yarn</tag><tag>auxiliaryservice</tag><tag>mapreduce</tag><tag>shufflehandler</tag>
        </tags>
        <content type="html">  一个附属服务（AuxiliaryService）是由YARN中节点管理器（NM）启动的通用服务。该服务由YARN配置&amp;ldquo;yarn.nodemanager.aux-services&amp;rdquo;定义。默认值为mapreduce_shuffle，即MRv2中的ShuffleHandler。 AuxiliaryService是节点管理器内的服务，接收应用/容器初始化和停止事件并作相应处理。 MRv2提供了一个叫做org.apache.hadoop.mapred.ShuffleHandler的内建AuxiliaryService，用于将节点内map输出文件提供给reducer(上图中除ShuffleHandler之外的其他AuxiliaryService子类均为测试类)。 节点管理器可能有多个AuxiliaryService，类AuxServices用于处理此类服务集合。 当AuxServices对象启动，它从YarnConfiguration.NM_AUX_SERVICES（即&amp;rdquo;yarn.nodemanager.aux-services&amp;rdquo;）获得附属服务名，从YarnConfiguration.NM_AUX_SERVICE_FMT（即&amp;rdquo;yarn.nodemanager.aux-services.%s.class&amp;rdquo;）获得对应的服务类名。例如&amp;rdquo;yarn.nodemanager.aux-services.mapreduce_shuffle.class&amp;rdquo;对应ShuffleHandler类。之后它将服务置入serviceMap并调用init()方法对服务进行初始化。 Hadoop实现是一个事件驱动系统。AuxServices既是ServiceStateChangeListener也是EventHandler，用于处理AuxServicesEventType事件。
public enum AuxServicesEventType { APPLICATION_INIT, APPLICATION_STOP, CONTAINER_INIT, CONTAINER_STOP } public class AuxServicesEvent extends AbstractEvent { private final String user; private final String serviceId; private final ByteBuffer serviceData; private final ApplicationId appId; private final Container container; } public abstract class AbstractEvent&amp;gt; implements Event { private final TYPE type; private final long timestamp; }  在handle(AuxServicesEventevent)方法中，每个事件与AuxiliaryService中的一个API调用相关连。例如，只要AuxServices收到一个APPLICATION_INIT事件，对应AuxiliaryService的initializeApplication()方法就会被调用。 那一个事件如何被传递给AuxServices的？ NodeManager类包含一个ContainerManagerImpl对象变量，而ContainerManagerImpl类包含一个AuxServices对象变量。此外ContainerManagerImpl类有自己的AsyncDispatcher,它会向AuxServices分发所有AuxServicesEventType类型事件。 AuxServicesEventType.APPLICATION_STOP事件在ApplicationImpl类中被创建，节点管理器中应用表述的状态机触发。 其他三个的AuxServicesEventType事件，例如APPLICATION_INIT、CONTAINER_INIT和CONTAINER_STOP，在ContainerImpl类中随着容器的生命周期被创建。
参考 AuxiliaryService in Hadoop 2
Implementing a Custom Shuffle and a Custom Sort
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] Open/SaveAs File</title>
        <url>https://mryqu.github.io/post/javascript_%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E5%92%8C%E5%8F%A6%E5%AD%98%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>web</tag><tag>html5</tag><tag>javascript</tag><tag>file</tag><tag>save</tag>
        </tags>
        <content type="html"> 看了一下HTML5应用中如何打开文件或另存文件。与Swing/EclipseRCP应用不同，有些操作由于安全的原因无法在HTML5应用内使用，而是浏览器与客户交互。例如HTML5应用往本地写文件。下面的显示了在新窗口打开文件、在当前窗口打开文件以及a标签的download属性。 学习了下面链接中的代码和文章，其中FileSaver.js是一个跨浏览器的JS库，但是在各个浏览器上保存文件的用户体验却不相同。目前为止，我还没发现更好的跨浏览器/设备的另存文件解决方案。 Google HTML5 Download Demo
An HTML5 saveAs() FileSaver implementation
New HTML5 Attributes for Hyperlinks: download, media, and ping
Save files on disk using JavaScript or JQuery!
JavaScript Question:Opening Save As Dialog
Internet media type
</content>
    </entry>
    
     <entry>
        <title>[Git] Create patch with untracked files</title>
        <url>https://mryqu.github.io/post/git_create_patch_with_untracked_files/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>add</tag><tag>untracked</tag><tag>diff</tag><tag>reset</tag>
        </tags>
        <content type="html"> 前一博文Create patch with untracked files using Git format-patch/diff/stash中的方案比较绕，今天有了一个更好一点的法子:
git add . git diff --cached &amp;gt; yqu.patch git reset origin/master  </content>
    </entry>
    
     <entry>
        <title>[Git] Create patch with untracked files using Git format-patch/diff/stash</title>
        <url>https://mryqu.github.io/post/git_create_patch_with_untracked_files_using_git_format-patch_diff_stash/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>patch</tag><tag>format-patch</tag><tag>diff</tag><tag>stash</tag>
        </tags>
        <content type="html">  Setup testing environment I created 123.txt at branch master, then modified 123.txt and added321.txt at branch yqu
C:\test&amp;gt;mkdir GitTest C:\test&amp;gt;cd GitTest C:\test\GitTest&amp;gt;git init Initialized empty Git repository in C:/test/GitTest/.git/ C:\test\GitTest&amp;gt;echo &amp;quot;this is a file at mast branch&amp;quot; &amp;gt; 123.txt C:\test\GitTest&amp;gt;git add 123.txt C:\test\GitTest&amp;gt;git commit -m &amp;quot;initial commit&amp;quot; [master (root-commit) f140825] initial commit 1 file changed, 1 insertion(&#43;) create mode 100644 123.txt C:\test\GitTest&amp;gt;git push origin HEAD:master C:\test\GitTest&amp;gt;git checkout -b yqu Switched to a new branch &#39;yqu&#39; C:\test\GitTest&amp;gt;echo &amp;quot;bye&amp;quot; &amp;gt;&amp;gt; 123.txt C:\test\GitTest&amp;gt;echo &amp;quot;this is new file at branch yqu&amp;quot; &amp;gt; 321.txt  Try patch-format C:\test\GitTest&amp;gt;git format-patch master  patch-format command only works for committed files.
Try diff &amp;rdquo;git diff&amp;rdquo; is used for unstaged changes, &amp;ldquo;gitdiff &amp;ndash;cached&amp;rdquo; is used for staged changes, neither includeuntracked files.
Try stash The above &amp;ldquo;git stash&amp;rdquo; and &amp;ldquo;git show&amp;rdquo; commands can satisfy the requirement.
Solution git stash show -p &amp;gt; patch git show fd056bfee6b7129f916f81247f39a4d39d27b466 &amp;gt;&amp;gt;patch  Reference How to create and apply a patch with Git
Create a git patch from the changes in the current working directory
Can I use git diff on untracked files?
In git, is there a way to show untracked stashed files without applying the stash?
Git stash to patch with untracked files
How to recover from “git stash save &amp;ndash;all”?
</content>
    </entry>
    
     <entry>
        <title>[C] 了解printf中的%.s</title>
        <url>https://mryqu.github.io/post/c_%E4%BA%86%E8%A7%A3printf/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C</tag><tag>printf</tag><tag>width</tag><tag>precision</tag><tag>string</tag>
        </tags>
        <content type="html"> 偶尔看到C代码printf(&amp;quot;%.*s&amp;quot;,dataL,data);，对printf中的格式化字符串&amp;rdquo;%.*s&amp;rdquo;有点不解。
查看了http://www.cplusplus.com/reference/cstdio/printf/文档后，有所理解。
| width |description |&amp;mdash;&amp;ndash; | (number) |Minimum number of characters to be printed. If the value tobe printed is shorter than this number, the result is padded withblank spaces. The value is not truncated even if the result islarger. | * |The width is not specified in the format string, but as an additional integer value argument preceding theargument that has to be formatted.
| .precision |description |&amp;mdash;&amp;ndash; | .number |For integer specifiers (d, i, o,u, x, X): precision specifies theminimum number of digits to be written. If the value to be written is shorter than this number, the result is padded with leading zeros. The value is not truncated even if the result is longer. A precision of 0 means that no character is written for the value 0.
For a, A, e, E, f and F specifiers: this is the number of digits to be printed after the decimal point (by default, this is 6).
For g and G specifiers: This is the maximumnumber of significant digits to be printed.
For s: this is the maximum number of characters to beprinted. By default all characters are printed until the ending null character is encountered.
If the period is specified without an explicit value for precision , 0 is assumed. | .* |The precision is not specified in the format string, but as an additional integer value argument preceding the argument that has to be formatted.
个人理解如下： - 格式化字符串没有指定固定宽度/精确度，而是使用*，代表接受整数参数。 - 对于字符串打印，宽度代表最小打印字符个数。如果字符串相对短的话，则填充空格。如果字符串相对长的话，也不会截断。 - 对于字符串打印，精度代表最大打印字符个数。如果字符串相对短的话，不会填充；如果字符串相对长的话，截断。
示例代码testPrintf.cpp如下： 测试结果：
Hello123| Hello123| Hello123 | Hello123| Hello123| Hello123| Hello123| Hello| Hello| Hello123| Hello|  我的理解是正确的。此外光指定精度时，使用&amp;rdquo;-&amp;ldquo;标志指定右对齐没起作用。
</content>
    </entry>
    
     <entry>
        <title>WebDAV Javascript库</title>
        <url>https://mryqu.github.io/post/webdav_javascript%E5%BA%93/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>web</tag><tag>webdav</tag><tag>library</tag><tag>client</tag>
        </tags>
        <content type="html"> 需要用JS库对WebDAV进行CRUD操作，找了一堆备选JS库。 - IT Hit WebDAV Ajax Library：http://www.webdavsystem.com/ajax/programming - https://github.com/sandro-pasquali/jquery.dav - https://github.com/evert/davclient.js - https://github.com/matthewp/webdav - https://github.com/aslakhellesoy/webdavjs - https://github.com/dom111/webdav-js - https://github.com/sara-nl/js-webdav-client
</content>
    </entry>
    
     <entry>
        <title>Spring3 REST can&#39;t solve list of object generated by Javascript</title>
        <url>https://mryqu.github.io/post/spring3_rest_cant_solve_list_of_object_generated_by_javascript/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>rest</tag><tag>json</tag><tag>javascript</tag><tag>解析</tag>
        </tags>
        <content type="html">  最近遭遇Spring3REST无法解析对象数组这么一个问题。为了排除客户端Javascript代码嫌疑，我通过GET操作从Spring RestfulWeb服务获取一个复杂对象，然后通过POST操作将其原封不动返给Spring Restful Web服务，问题依旧重现。
客户端代码 var meatadata=&#39;[{&amp;quot;varName&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;varTitle&amp;quot;:&amp;quot;The Id&amp;quot;,&amp;quot;varIndex&amp;quot;:1},{&amp;quot;varName&amp;quot;:&amp;quot;name&amp;quot;,&amp;quot;varTitle&amp;quot;:&amp;quot;The Name&amp;quot;,&amp;quot;varIndex&amp;quot;:2},{&amp;quot;varName&amp;quot;:&amp;quot;age&amp;quot;,&amp;quot;varTitle&amp;quot;:&amp;quot;The Age&amp;quot;,&amp;quot;varIndex&amp;quot;:3}]&#39;; $.ajax({ url: &amp;quot;configure&amp;quot;, type: &amp;quot;POST&amp;quot;, data: metadata, dataType: &amp;quot;json&amp;quot;, contentType: &amp;quot;application/json&amp;quot;, success: function (res) { $(&#39;#cfgContent&#39;).text(JSON.stringify(res)); $(&#39;#cfgError&#39;).text(&amp;quot;&amp;quot;); }, error: function (res) { $(&#39;#cfgContent&#39;).text(&amp;quot;&amp;quot;); $(&#39;#cfgError&#39;).text(res.responseText); } });  中间层代码 package com.yqu.rest; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.http.HttpStatus; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.*; import org.springframework.web.servlet.ModelAndView; import java.util.ArrayList; import java.util.List; @RestController public class ConfigurationController { @RequestMapping(value = &amp;quot;/&amp;quot;, method = RequestMethod.GET) public ModelAndView home(Model m){ System.out.println(&amp;quot;home&amp;quot;); return new ModelAndView(&amp;quot;index&amp;quot;); } @RequestMapping(value = &amp;quot;/configure&amp;quot;, method = RequestMethod.GET) public @ResponseBody List getConfiguration() { List columns = new ArrayList(); columns.add(new ColumnVO(&amp;quot;id&amp;quot;,&amp;quot;The Id&amp;quot;,1)); columns.add(new ColumnVO(&amp;quot;name&amp;quot;,&amp;quot;The Name&amp;quot;,2)); columns.add(new ColumnVO(&amp;quot;age&amp;quot;,&amp;quot;The Age&amp;quot;,3)); System.out.println(&amp;quot;getConfiguration:&amp;quot;&#43;columns); return columns; } @RequestMapping(value = &amp;quot;/configure&amp;quot;, method = RequestMethod.POST) public @ResponseBody List&amp;lt;ColumnVO&amp;gt; setConfiguration(@RequestBody List&amp;lt;ColumnVO&amp;gt; columns) { System.out.println(columns); return columns; } }  中间层异常 java.lang.ClassCastException: java.util.LinkedHashMap cannot be cast to com.yqu.rest.ColumnVO  问题原因 通过阅读参考帖子，可知：
public @ResponseBody List&amp;lt;ColumnVO&amp;gt; setConfiguration(@RequestBody List&amp;lt;ColumnVO&amp;gt; columns)在编译后经过泛型擦除就变成了public @ResponseBody List setConfiguration(@RequestBody List columns)而Jackson为List解编的默认类型是LinkedHashMap。
解决方案 Spring4已经解决了这一问题。在Spring3中，需要增加一个封装类class ColumnVOList extends ArrayList {}
参考 receiving json and deserializing as List of object at spring mvc controller
</content>
    </entry>
    
     <entry>
        <title>Spring REST can&#39;t solve nested object array generated by JavaScript</title>
        <url>https://mryqu.github.io/post/spring_rest_cant_solve_nested_object_array_generated_by_javascript/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>rest</tag><tag>javascript</tag><tag>json</tag><tag>解析</tag>
        </tags>
        <content type="html">  最近遭遇SpringREST无法解析嵌套对象数组这么一个问题。为了排除客户端Javascript代码嫌疑，我通过GET操作从Spring RestfulWeb服务获取一个复杂对象，然后通过POST操作将其原封不动返给Spring Restful Web服务，问题依旧重现。
所操作的复杂对象 客户端POST响应 中间层代码 package com.yqu.rest; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.http.HttpStatus; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.*; import org.springframework.web.servlet.ModelAndView; import java.util.ArrayList; import java.util.List; @RestController public class ConfigurationController { @RequestMapping(value = &amp;quot;/&amp;quot;, method = RequestMethod.GET) public ModelAndView home(Model m){ System.out.println(&amp;quot;home&amp;quot;); return new ModelAndView(&amp;quot;index&amp;quot;); } @RequestMapping(value = &amp;quot;/configure&amp;quot;, method = RequestMethod.GET) public @ResponseBody SheetVO getConfiguration() { List columns = new ArrayList(); columns.add(new ColumnVO(&amp;quot;id&amp;quot;,&amp;quot;The Id&amp;quot;,1)); columns.add(new ColumnVO(&amp;quot;name&amp;quot;,&amp;quot;The Name&amp;quot;,2)); columns.add(new ColumnVO(&amp;quot;age&amp;quot;,&amp;quot;The Age&amp;quot;,3)); SheetVO metadata = new SheetVO(SheetVO.TITLE_MATCHING, 1,0, columns); System.out.println(&amp;quot;getConfiguration:&amp;quot;&#43;metadata); return metadata; } @RequestMapping(value = &amp;quot;/configure&amp;quot;, method = RequestMethod.POST) public @ResponseBody SheetVO setConfiguration(@ModelAttribute(&amp;quot;metadata&amp;quot;) SheetVO metadata) { System.out.println(metadata); return metadata; } }  中间层异常 org.springframework.beans.InvalidPropertyException: Invalid property &#39;columns[0][varIndex]&#39; of bean class [com.yqu.rest.SheetVO]: Property referenced in indexed property path &#39;columns[0][varIndex]&#39; is neither an array nor a List nor a Map; returned value was [1] at org.springframework.beans.BeanWrapperImpl.setPropertyValue(BeanWrapperImpl.java:1058) at org.springframework.beans.BeanWrapperImpl.setPropertyValue(BeanWrapperImpl.java:927) at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:95) at org.springframework.validation.DataBinder.applyPropertyValues(DataBinder.java:749) at org.springframework.validation.DataBinder.doBind(DataBinder.java:645) at org.springframework.web.bind.WebDataBinder.doBind(WebDataBinder.java:189) at org.springframework.web.bind.ServletRequestDataBinder.bind(ServletRequestDataBinder.java:106) at org.springframework.web.servlet.mvc.method.annotation.ServletModelAttributeMethodProcessor.bindRequestParameters(ServletModelAttributeMethodProcessor.java:150) at org.springframework.web.method.annotation.ModelAttributeMethodProcessor.resolveArgument(ModelAttributeMethodProcessor.java:110) at org.springframework.web.method.support.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:77) at org.springframework.web.method.support.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:162) at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:129) at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:776) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:705) at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966) at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868) at javax.servlet.http.HttpServlet.service(HttpServlet.java:644) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842) at javax.servlet.http.HttpServlet.service(HttpServlet.java:725) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90) at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:516) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1086) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:659) at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:223) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1558) at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1515) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745)  问题原因 通过阅读两篇参考帖子，可知对于下列对象：
{ &amp;quot;columnMatchMethod&amp;quot;: 2, &amp;quot;ignoredHeaderCount&amp;quot;: 1, &amp;quot;ignoredFooterCount&amp;quot;: 0, &amp;quot;columns&amp;quot;: [{ &amp;quot;varName&amp;quot;: &amp;quot;id&amp;quot;, &amp;quot;varTitle&amp;quot;: &amp;quot;The Id&amp;quot;, &amp;quot;varIndex&amp;quot;: 1 }, { &amp;quot;varName&amp;quot;: &amp;quot;name&amp;quot;, &amp;quot;varTitle&amp;quot;: &amp;quot;The Name&amp;quot;, &amp;quot;varIndex&amp;quot;: 2 }, { &amp;quot;varName&amp;quot;: &amp;quot;age&amp;quot;, &amp;quot;varTitle&amp;quot;: &amp;quot;The Age&amp;quot;, &amp;quot;varIndex&amp;quot;: 3 } ] }  JQuery是进行如下：
columnMatchMethod: 2, ignoredHeaderCount: 1, ignoredFooterCount: 0, columns[0][varName]: &amp;quot;id&amp;quot;, columns[0][varTitle]: &amp;quot;The Id&amp;quot;, columns[0][varIndex]: 1 columns[1][varName]: &amp;quot;name&amp;quot;, columns[1][varTitle]: &amp;quot;The Name&amp;quot;, columns[1][varIndex]: 2 columns[2][varName]: &amp;quot;age&amp;quot;, columns[2][varTitle]: &amp;quot;The Age&amp;quot;, columns[2][varIndex]: 3  但是Spring MVC期望的是如下参数格式：
columnMatchMethod: 2, ignoredHeaderCount: 1, ignoredFooterCount: 0, columns[0].varName: &amp;quot;id&amp;quot;, columns[0].varTitle: &amp;quot;The Id&amp;quot;, columns[0].varIndex: 1 columns[1].varName: &amp;quot;name&amp;quot;, columns[1].varTitle: &amp;quot;The Name&amp;quot;, columns[1].varIndex: 2 columns[2].varName: &amp;quot;age&amp;quot;, columns[2].varTitle: &amp;quot;The Age&amp;quot;, columns[2].varIndex: 3  解决方案 通过资料查找，采用参考帖子中的第二种方案解决问题。
- 客户端代码：
 setConf = function (event) { $.ajax({ url: &amp;quot;configure&amp;quot;, type: &amp;quot;POST&amp;quot;, data: JSON.stringify(metadata), dataType: &amp;quot;json&amp;quot;, contentType: &amp;quot;application/json&amp;quot;, success: function (res) { $(&#39;#cfgContent&#39;).text(JSON.stringify(res)); $(&#39;#cfgError&#39;).text(&amp;quot;&amp;quot;); }, error: function (res) { $(&#39;#cfgContent&#39;).text(&amp;quot;&amp;quot;); $(&#39;#cfgError&#39;).text(res.responseText); } }); };   中间层代码：
   package com.yqu.rest; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.http.HttpStatus; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.*; import org.springframework.web.servlet.ModelAndView; import java.util.ArrayList; import java.util.List; @RestController public class ConfigurationController { @RequestMapping(value = &amp;quot;/&amp;quot;, method = RequestMethod.GET) public ModelAndView home(Model m){ System.out.println(&amp;quot;home&amp;quot;); return new ModelAndView(&amp;quot;index&amp;quot;); } @RequestMapping(value = &amp;quot;/configure&amp;quot;, method = RequestMethod.GET) public @ResponseBody SheetVO getConfiguration() { List columns = new ArrayList(); columns.add(new ColumnVO(&amp;quot;id&amp;quot;,&amp;quot;The Id&amp;quot;,1)); columns.add(new ColumnVO(&amp;quot;name&amp;quot;,&amp;quot;The Name&amp;quot;,2)); columns.add(new ColumnVO(&amp;quot;age&amp;quot;,&amp;quot;The Age&amp;quot;,3)); SheetVO metadata = new SheetVO(SheetVO.TITLE_MATCHING, 1,0, columns); System.out.println(&amp;quot;getConfiguration:&amp;quot;&#43;metadata); return metadata; } @RequestMapping(value = &amp;quot;/configure&amp;quot;, method = RequestMethod.POST) public @ResponseBody SheetVO setConfiguration(@RequestBody SheetVO metadata) { System.out.println(metadata); return metadata; } }  参考 Spring MVC 3: Property referenced in indexed property path is neither an array nor a List nor a Map
Post Nested Object to Spring MVC controller using JSON
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 示例: open dialog which content is a form defined in another view</title>
        <url>https://mryqu.github.io/post/openui5_%E7%A4%BA%E4%BE%8B_open_dialog_which_content_is_a_form_defined_in_another_view/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>dialog</tag><tag>form</tag><tag>view</tag>
        </tags>
        <content type="html"> 使用OpenUI5做了一个例子，在一个JSVIEW中定义的dialog的内容是另外一个JSVIEW中定义的form。 示例位置: http://jsbin.com/fotepu/1/edit?html,output 此外，通过学习http://stackoverflow.com/questions/25510090/sapui5-attach-chart-to-dialog ，了解到dialog内容为图表时有可能需要使用invalidate()函数。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 获得当前页面语言</title>
        <url>https://mryqu.github.io/post/openui5_%E8%8E%B7%E5%BE%97%E5%BD%93%E5%89%8D%E9%A1%B5%E9%9D%A2%E8%AF%AD%E8%A8%80/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>jquery</tag><tag>locale</tag><tag>lang</tag>
        </tags>
        <content type="html"> 获得当前页面语言的方法：
 javascript:document.getElementsByTagName(&#39;html&#39;)[0].getAttribute(&#39;lang&#39;) jQuery: $(&#39;html&#39;).attr(&#39;lang&#39;) OpenUI5: sap.ui.getCore().getConfiguration().getLanguage()  示例： </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 快速定位OpenUI5问题的一个方法</title>
        <url>https://mryqu.github.io/post/openui5_%E5%BF%AB%E9%80%9F%E5%AE%9A%E4%BD%8Dopenui5%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B9%E6%B3%95/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>html</tag><tag>javascript</tag><tag>debugging</tag><tag>杂谈</tag>
        </tags>
        <content type="html"> sap.ui.base.Object是所有OpenUI5对象的父类，它的某些方法对快速定位OpenUI5问题很有帮助。我写了一个小函数通过OpenUI5对象的元数据获得类名，并且获得OpenUI5对象的ID信息。
 traceUI5Object: function(obj) { if(obj instanceof sap.ui.base.Object) console.log(obj.getMetadata().getName()&#43;&amp;quot;{id:\&#39;&amp;quot;&#43;obj.getId()&#43;&amp;quot;\&#39;}&amp;quot;); } traceUI5EventProviders: function(obj) { var that = obj; while (that &amp;amp;&amp;amp; that instanceof sap.ui.base.EventProvider) { console.log(that.getMetadata().getName()&#43;&amp;quot;{id:\&#39;&amp;quot;&#43;that.getId()&#43;&amp;quot;\&#39;}&amp;quot;); that = that.getEventingParent(); } }  traceUI5EventProviders函数运行结果示例： sap.ui.commons.CheckBox{id:&#39;check1&#39;} sap.ui.commons.Panel{id:&#39;panel1&#39;} sap.ui.core.mvc.JSView{id:&#39;leftView&#39;} sap.ui.commons.Splitter{id:&#39;Splitter1&#39;} sap.ui.core.mvc.JSView{id:&#39;__jsview0&#39;} sap.ui.core.UIArea{id:&#39;content&#39;}  在编写和调试OpenUI5时，有时会有Exception抛出。 假定上面图中代码会抛出Exception，通过this我们看到的的是一个Factory，通过sId我们可以找到发生问题的定义了ID的控件。但是如果控件ID是自生成的，就不太容易了。我们可以通过监视表达式获取（组件链上所有的）组件类名及ID，这样就可以更快定位导致抛出Exception的OpenUI5视图/控件了。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] MVC和EventBus示例</title>
        <url>https://mryqu.github.io/post/openui5_mvc%E5%92%8Ceventbus%E7%A4%BA%E4%BE%8B/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>mvc</tag><tag>eventbus</tag><tag>消息总线</tag><tag>通信</tag>
        </tags>
        <content type="html"> 昨天发了一个帖子[OpenUI5] MVC：访问其他View/Controller的方法，里面的示例是用违反MVC原则的方式演示一下效果，今天又在jsbin上做了个OpenUI5MVC &amp;amp; EventBus示例：http://jsbin.com/nixomo/1/edit?html,output。 sap.ui.core.EventBus使用起来很简单。
 通过var bus = sap.ui.getCore().getEventBus() 获得消息总线 接收方首先在某个消息通道上订阅消息时间并注册消息监听器listener 发送方在这个消息通道上发布消息，接收方就会去处理  通过阅读代码可知，EventBus一个实例对应一个消息通道，EventBus的_defaultChannel和_mChannels都是sap.ui.base.EventProvider实例，用于事件注册与分发、将数据与事件的绑定/解绑。上图中就是消息通道&amp;rdquo;rightViewChannel&amp;rdquo;对应的EventBus实例，已经注册了两个事件setRightPanelVisible和doSomething。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] MVC：访问其他View/Controller的方法</title>
        <url>https://mryqu.github.io/post/openui5_mvc%E8%AE%BF%E9%97%AE%E5%85%B6%E4%BB%96view%E6%88%96controller%E7%9A%84%E6%96%B9%E6%B3%95/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>mvc</tag><tag>controller</tag><tag>交互</tag><tag>通信</tag>
        </tags>
        <content type="html">  访问其他视图/控件的方法 在创建视图/控件实例时，设置ID：
var oViewLeft = sap.ui.jsview(&amp;quot;leftView&amp;quot;, &amp;quot;com.yqu.view.Left&amp;quot;); var oPanelRight = new sap.ui.commons.Panel(&amp;quot;panel2&amp;quot;);  通过sap.ui.getCore().byId(&amp;ldquo;compID&amp;rdquo;)获取上述视图/控件。控制器可以通过getView()函数获取自身对应的视图，但是该视图内部的控件还得通过这种方式获取：
var refViewLeft = sap.ui.getCore().byId(&amp;quot;leftView&amp;quot;); var refPanelRight = sap.ui.getCore().byId(&amp;quot;panel2&amp;quot;);  需要注意的是，上面讲的是JS视图最简单的一种情况。 对于使用了静态视图ID的XML、HTML和JSON视图，其内部的控件ID会自动添加视图ID做前缀。JS视图中，在动态实例化控件时通过oController.createId(&amp;ldquo;ID&amp;rdquo;)也可以生成用视图ID做前缀的唯一ID。
var refSubView = oViewParent.byId(&amp;quot;subViewId&amp;quot;); refSubView.byId(&amp;quot;ctrId&amp;quot;);  JS、XML、HTML和JSON片断(Fragment)是更轻量级的分割和UI重用单元，每个片段示例化时为了保证唯一性，即使没有定义片段ID也会自动生成，其内部的控件ID会自动添加视图ID和片段ID做前缀。 - 当没有给定片段ID：myControl = sap.ui.getCore().byId(&amp;ldquo;myControl&amp;rdquo;) - 当给定片段ID &amp;ldquo;myFrag&amp;rdquo; ：myControl =sap.ui.core.Fragment.byId(&amp;ldquo;myFrag&amp;rdquo;, &amp;ldquo;myControl&amp;rdquo;)
访问其他Controller的方法  最简单的方法是使用一个全局变量引用所需控制器 不推荐 通过获取其他控制器对应的视图来访问该控制器的函数：sap.ui.getCore().byId(&amp;ldquo;viewId&amp;rdquo;).getController().method(); 直接调用控制器的函数：sap.ui.controller(&amp;ldquo;namespace.Controllername&amp;rdquo;).method(); 最推荐的是在控制器(或应用组件)之间的通信使用sap.ui.core.EventBus，这种事件/消息总线模式可以更好进行解耦。  在jsbin上做了个Retrive other component示例：http://jsbin.com/xufeyo/1/edit?html,output 这个示例演示了一个视图如何控制另外一个视图的控件是否显示，一个视图调用了另外一个视图控制器的方法。但是它完全违反了我以前有篇学习帖子重温MVC:一个很好的MVC中的规则，不可以在实际工作中使用！
此外通过调试找到了sap.ui.core.Core里面ID与组件的映射表。</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] MVC示例:JSView embedding JSVIEW</title>
        <url>https://mryqu.github.io/post/openui5_mvc%E7%A4%BA%E4%BE%8Bjsview_embedding_jsview/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>mvc</tag><tag>jsview</tag><tag>embedding</tag><tag>json</tag>
        </tags>
        <content type="html"> OpenUI5 SDK的演示程序里面有一个视图嵌套另外一个视图，但是通过Componentcontainer和Component.js实现的。一直对视图直接嵌套另外一个视图觉得理所当然但是有点顾虑，此外也担心外层视图的数据模型如何传递给内部视图。当然内外两层视图可以使用不同的数据模型，但是如果不知道共享一份数据视图是否可行?
在jsbin上做了一个示例：http://jsbin.com/jirogo/1/edit?html,output，结果显示担忧是多余的
</content>
    </entry>
    
     <entry>
        <title>两个HTML线上工具：jsbin和jsfiddle</title>
        <url>https://mryqu.github.io/post/%E4%B8%A4%E4%B8%AAhtml%E7%BA%BF%E4%B8%8A%E5%B7%A5%E5%85%B7jsbin%E5%92%8Cjsfiddle/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>jsbin</tag><tag>jsfiddle</tag><tag>线上工具</tag><tag>sap.ui.table.table</tag><tag>示例</tag>
        </tags>
        <content type="html"> 最近做HTML5开发，关注了两个HTML线上工具，都可以编辑、测试、验证、存档和分享HTML、JavaScript和CSS代码，还可以引入一些常用的外部JS库，例如jQuery、Bootstrap、YUI、AngularJS&amp;hellip;,感觉都很不错。
 http://jsbin.com/ http://jsfiddle.net/  这两个线上工具使用方便简单，网上也有很详细的介绍贴：介紹好用工具：JS Bin ( 網站前端工程師的學習利器 )、介紹好用工具：jsFiddle - Online Editor for the Web。
对于OpenUI5演示，jsbin更好用，因为它没有jsfiddle那些限制（HTML区域不允许有header、meta，JavaScript区域不允许有scipt标签和参数等等）。
附上一个我在jsbin上做的sap.ui.table.TableMVC示例：http://jsbin.com/jojeta/1/edit?html,output
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 调节元素间距</title>
        <url>https://mryqu.github.io/post/openui5_%E8%B0%83%E8%8A%82%E5%85%83%E7%B4%A0%E9%97%B4%E8%B7%9D/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag>
        </tags>
        <content type="html"> 在使用OpenUI5时，有时两个元素间距不合预期，我大体可用两种方式进行改进：
 一种方式是添加自己定制的CSS类，然后通过addStyleClass方法对控件设置自己定制的CSS类 ​另一种方法土点，就是对需要调整间距的两个元素上增加一个HBox/VBox控件，然后在两个之间加一个定宽/高的控件调节间距。 //在oControl1和oControl2之间增加15px的间距 new VBox({ items: [ oControl1, new HBox({ height: &amp;quot;15px&amp;quot;, fitContainer: true }), oControl2 ] })  ​  </content>
    </entry>
    
     <entry>
        <title>HTML Busy Indicator</title>
        <url>https://mryqu.github.io/post/html_busy_indicator/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>busy.indicator</tag><tag>html</tag><tag>openui5</tag><tag>bootstrap</tag><tag>jquery</tag>
        </tags>
        <content type="html"> 看了一下OpenUI5的LocalBusyIndicator效果，感觉跟自己想的转圈圈的那种spinner不一样:https://sapui5.hana.ondemand.com/sdk/test-resources/sap/ui/core/demokit/LocalBusyIndicator.html 想看看bootstrap的busy indicator，竟然没有，不过找到了开发组的讨论：https://github.com/twbs/bootstrap/issues/12598 不止一次有人建议开发busy indicator，不过Mark Otto（Bootstrap是Mark Otto和JacobThornton共同开发的）没同意。因为满足不了下列条件： - It needs to be retina-ready - Needs to work in IE8&#43; - Needs to work on light backgrounds and dark—alphatransparencywould be bomb - Would be cool if it was a font, but PNG or GIF is fine,too - Available in multiple sizes
开发一款满意的控件容易吗！！！还好我就用用而已 下面是我找到的一些Busy Indicator资源： http://fgnass.github.io/spin.js/
http://semantic-ui.com/elements/loader.html
http://w3lessons.info/2014/01/26/showing-busy-loading-indicator-during-an-ajax-request-using-jquery/
</content>
    </entry>
    
     <entry>
        <title>YCSB相关博文</title>
        <url>https://mryqu.github.io/post/ycsb%E7%9B%B8%E5%85%B3%E5%8D%9A%E6%96%87/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>ycsb</tag><tag>mongodb</tag><tag>hbase</tag><tag>性能测试</tag><tag>基准</tag>
        </tags>
        <content type="html"> 使用YCSB测试MongoDB的微分片性能
使用YCSB测试Mongodb性能的方法简介
NoSQL数据库压力测试工具YCSB使用
使用YCSB对HBase进行测试
性能测试：SequoiaDB vs. MongoDB vs. Cassandra vs. HBase
</content>
    </entry>
    
     <entry>
        <title>Java的Base64编解码</title>
        <url>https://mryqu.github.io/post/java%E7%9A%84base64%E7%BC%96%E8%A7%A3%E7%A0%81/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>base64</tag>
        </tags>
        <content type="html">  Base64编码是网络上最常见的用于传输8Bit字节代码的编码方式之一，用基于64个可打印字符[大小写字母52个字符、数字10个字符、&#43;和/2个字符(对于URL为-和_)，补全用=]来表示二进制数据的一种表示方法。相关协议可见：
 RFC4648 The Base16, Base32, and Base64 Data Encodings RFC2045 MIME Part One: Format of Internet Message Bodies RFC2046 MIME Part Two: Media Types RFC2047 MIME Part Three: Message Header Extensions for Non-ASCII Text RFC2048 MIME Part Four: Registration Procedures RFC2049 MIME Part Five: Conformance Criteria and Examples  因为有些网络传送渠道不支持所有的字节，例如传统的邮件只支持可见字符的传送，像ASCII码的控制字符就不能通过邮件传送。通过Base64编码可以把不可打印的字符也能用可打印字符来表示。
Java6之前 在Java6之前，JDK核心类一直没有Base64的实现类。除了使用Sun内部实现sun.misc.BASE64Encoder、sun.misc.BASE64Decoder或com.sun.org.apache.xerces.internal.impl.dv.util.Base64外，就需要使用第三方类库了。 Java6 Java6中添加了Base64的实现：javax.xml.bind.DatatypeConverter两个静态方法parseBase64Binary和 printBase64Binary。
import javax.xml.bind.DatatypeConverter; public class HelloBase64 { public static void main(String[] args) { String me = &amp;quot;blog.sina.com.cn/yandongqu&amp;quot;; byte[] plainContent; String base64Str = DatatypeConverter.printBase64Binary(me.getBytes()); System.out.println(base64Str); plainContent = DatatypeConverter.parseBase64Binary(base64Str); System.out.println(new String(plainContent)); } }  测试结果：
YmxvZy5zaW5hLmNvbS5jbi95YW5kb25ncXU= blog.sina.com.cn/yandongqu  Java8 Java8中添加了另一个Base64的实现java.util.Base64类。该类提供三种编解码器：
 基本编解码器：64个字符包括&#43;和/； URL编解码器：为了处理URL里面的反斜线“/”，64个字符包括-和_； MIME编解码器：使用基本的字母数字产生Base64输出，而且对MIME格式友好：每一行输出不超过76个字符且以“\r\n”结束；  import java.util.Base64; public class HelloBase64 { public static void main(String[] args) { String me = &amp;quot;blog.sina.com.cn/yandongqu&amp;quot;; byte[] plainContent; byte[] base64Content = Base64.getEncoder().encode(me.getBytes()); System.out.println(new String(base64Content)); plainContent = Base64.getDecoder().decode(base64Content); System.out.println(new String(plainContent)); } }  测试结果：
YmxvZy5zaW5hLmNvbS5jbi95YW5kb25ncXU= blog.sina.com.cn/yandongqu  第三方类库  commons-codec.jar中的org.apache.commons.codec.binary.Base64类(Apache License)； google-guava.jar中的com.google.common.io.BaseEncoding.base64()静态方法 (Apache License)； iHarder.net的Base64类(I have released this software into the Public Domain. That meansyou can do whatever you want with it. Really. You don&amp;rsquo;t have tomatch it up with any other open source license &amp;em;just use it. You can rename the files, move the Java packages,whatever you want. If your lawyers say you have to have a license,contact me, and I&amp;rsquo;ll make a special release to you under whateverreasonable license you desire: MIT, BSD, GPL, whatever.)； MiGBase64的util.Base64类(BSD License)；  参考 从原理上搞定编码（四）&amp;ndash; Base64编码
</content>
    </entry>
    
     <entry>
        <title>[CSS] 图片叠加效果</title>
        <url>https://mryqu.github.io/post/css_%E5%9B%BE%E7%89%87%E5%8F%A0%E5%8A%A0%E6%95%88%E6%9E%9C/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>web</tag><tag>html5</tag><tag>css</tag><tag>image</tag><tag>图片叠加效果</tag>
        </tags>
        <content type="html"> 今天接着折腾OpenUI5。我们原有的客户端是EclipseRCP富客户端，有些菜单上的图标是叠加出来的，很不幸sap.ui.commons.MenuItem仅支持一个图片文件的URL，不支持多个图片进行叠加。 玩一下使用CSS做图片叠加效果。代码如下： </content>
    </entry>
    
     <entry>
        <title>[Swing]图片叠加效果</title>
        <url>https://mryqu.github.io/post/swing_%E5%9B%BE%E7%89%87%E5%8F%A0%E5%8A%A0%E6%95%88%E6%9E%9C/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>swing</tag><tag>overlay</tag><tag>image</tag><tag>图片叠加效果</tag>
        </tags>
        <content type="html"> 玩一下使用Java做图片叠加效果。 代码如下：
package com.yqu.swing.img; import java.awt.Component; import java.awt.Graphics; import java.awt.image.BufferedImage; import java.io.File; import java.io.IOException; import java.util.ArrayList; import java.util.List; import javax.imageio.ImageIO; import javax.swing.GrayFilter; import javax.swing.Icon; import javax.swing.ImageIcon; public class OverlayIcon implements Icon{ private int maxWidth = -1, maxHeight = -1; private List icons; public OverlayIcon(String[] iconPaths) { if(iconPaths != null &amp;amp;&amp;amp; iconPaths.length &amp;gt;0) { icons = new ArrayList(iconPaths.length); for (String iconPath : iconPaths) { Icon icon = makeIcon(iconPath, false); icons.add(icon); int width = icon.getIconWidth(); int height = icon.getIconHeight(); if (width &amp;gt; maxWidth) maxWidth = width; if (height &amp;gt; maxHeight) maxHeight = height; } if (maxWidth == -1) maxWidth = 16; if (maxHeight == -1) maxHeight = 16; } } private Icon makeIcon(String iconPath, boolean makeDisabled) { ImageIcon icon = new ImageIcon(iconPath); if (makeDisabled) { icon = new ImageIcon( GrayFilter.createDisabledImage(icon.getImage())); } return icon; } private BufferedImage getBufferedImage() { BufferedImage bi = new BufferedImage(maxWidth, maxHeight, BufferedImage.TYPE_INT_ARGB); for (Icon icon : icons) { icon.paintIcon(null, bi.getGraphics(), 0, 0); } return bi; } public Icon getIcon() { if(icons != null) { BufferedImage bi = getBufferedImage(); return new ImageIcon(bi); } return null; } public void saveIcon(String flName) { if(icons != null) { BufferedImage bi = getBufferedImage(); try { ImageIO.write(bi, &amp;quot;gif&amp;quot;, new File(flName)); } catch (IOException e) { e.printStackTrace(); } } else { throw new IllegalStateException(&amp;quot;Can&#39;t generate overlay icon file&amp;quot;); } } @Override public void paintIcon(Component c, Graphics g, int x, int y) { if(icons != null) { for (Icon icon : icons) { icon.paintIcon(c, g, x, y); } } } @Override public int getIconWidth() { return maxWidth; } @Override public int getIconHeight() { return maxHeight; } public static void main(String[] args) { String[] iconPaths = {&amp;quot;Cycle.gif&amp;quot;, &amp;quot;New_overlay.gif&amp;quot;}; OverlayIcon icon = new OverlayIcon(iconPaths); icon.saveIcon(&amp;quot;New_Cycle.gif&amp;quot;); } }  </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 打开web应用调试模式的方法</title>
        <url>https://mryqu.github.io/post/openui5_%E6%89%93%E5%BC%80web%E5%BA%94%E7%94%A8%E8%B0%83%E8%AF%95%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%96%B9%E6%B3%95/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>web</tag><tag>openui5</tag><tag>debug</tag><tag>mode</tag><tag>调试</tag>
        </tags>
        <content type="html"> OpenUI5 web应用调试模式无需服务器端支持，完全可在浏览器上进行设置。下面列举了四种打开调试模式的方法： - URL指定参数sap-ui-debug=true例如：http://localhost:8080/fmwebstudio/?sap-ui-debug=true - 在浏览器控制台执行jQuery.sap.debug(true) - 在加载了OpenUI5 web应用页面执行CTRL-SHIFT-ALT-P快捷键调出技术信息进行设置。- 在加载了OpenUI5web应用页面执行CTRL-SHIFT-ALT-S快捷键调出OpenUI5诊断页面进行设置。</content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 与Eclipse的Back/Forward等同功能</title>
        <url>https://mryqu.github.io/post/intellij_%E4%B8%8Eeclipse%E7%9A%84back%E5%92%8Cforward%E7%AD%89%E5%90%8C%E5%8A%9F%E8%83%BD/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>navigate</tag><tag>goback</tag><tag>goforward</tag>
        </tags>
        <content type="html"> 当查找一个方法的调用者后，可能还想退回到该方法进行研究，Eclipse的Back/Forward功能相应快捷键Alt &#43; Left 和 Alt &#43; Right是很便利的。
使用IntelliJ IDEA进行开发时，可以使用Navigate | Back和Navigate |Forward菜单或快捷键Ctrl &#43; Alt &#43; Left 和 Ctrl &#43; Alt &#43; Right实现相同的功能。 不过在我的机器中存在热键冲突，Intel显卡控制面板也使用相同的快捷键，禁掉Intel显卡控制面板的快捷键就好了。 </content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 与Eclipse Open Type Hierarchy等同功能</title>
        <url>https://mryqu.github.io/post/intellij_%E4%B8%8Eeclipse_open_type_hierarchy%E7%AD%89%E5%90%8C%E5%8A%9F%E8%83%BD/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>intellij_idea</tag><tag>type_hierarchy</tag><tag>ctrlh</tag><tag>f4</tag>
        </tags>
        <content type="html"> 使用Eclipse进行开发时，我喜欢用F4快捷键打开类型层次视图查看类层次关系，或者用Ctrl &#43; T快捷键打开快速类型层次对话框查看类层次关系。 使用IntelliJ IDEA进行开发时，可以使用Navigate | Type Hierarchy菜单或Ctrl &#43; H快捷键打开类层次视图，此外使能了“UMLSupport”插件后还可以使用Ctrl &#43; Alt &#43; U快捷键打开类型层次关系UML图。
</content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 导入项目</title>
        <url>https://mryqu.github.io/post/intellij_%E5%AF%BC%E5%85%A5%E9%A1%B9%E7%9B%AE/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>import</tag><tag>project</tag><tag>导入项目</tag>
        </tags>
        <content type="html"> 如果在IntelliJ IDEA没有打开任何项目的话，欢迎界面里有Import Project和Check out fromVersion Control两个菜单。
有一段时间为了导入新项目，我都老老实实关闭所有已打开的项目，去欢迎界面里操作。终于有一天觉得自己太老土，才搜了搜，发现原来这两个功能当有项目打开的时候是二级菜单而已。
 File | New | Project from Existing Sources File | New | Project from Version Control   </content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 与Eclipse Organize Imports等同功能</title>
        <url>https://mryqu.github.io/post/intellij_%E4%B8%8Eeclipse_organize_imports%E7%AD%89%E5%90%8C%E5%8A%9F%E8%83%BD/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>intellij_idea</tag><tag>imports</tag><tag>organize</tag><tag>optimize</tag>
        </tags>
        <content type="html"> 使用Eclipse进行开发时，我喜欢用Ctrl&#43; Shift &#43; O快捷键管理Java类的导入，它可以导入所需的Java类，去除不需要的Java类。 Eclipse的Organize Imports偏好配置： 使用IntelliJ IDEA进行开发时，可以使用Code | OptimizeImports菜单或 Ctrl &#43; Alt &#43; O快捷键优化Java类的导入。它仅能去除不需要的Java类，无法像Eclipse那样自动导入所需的Java类。 IntelliJ IDEA的Optimize Imports偏好配置： </content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 处理Unhandled Exception</title>
        <url>https://mryqu.github.io/post/intellij_%E5%A4%84%E7%90%86unhandled_exception/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>unhandled</tag><tag>exception</tag>
        </tags>
        <content type="html"> 当在IntelliJ IDEA出现&amp;rdquo;Unhandled Exception&amp;rdquo;时，处理方法很简单。在有问题的代码行按下快捷键Ctrl&#43; ANTER即会出现下列处理选项： - Add exception to method signature - Surround with try/catch</content>
    </entry>
    
     <entry>
        <title>Web调试工具:Fiddler</title>
        <url>https://mryqu.github.io/post/web%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7fiddler/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>fiddler</tag><tag>web</tag><tag>调试</tag><tag>工具</tag>
        </tags>
        <content type="html"> 今天看网上有介绍Web调试工具Fiddler的，顺风搂一眼。比我原来用wireshark调试客户端与服务器端web通信强，值得学习。 Fiddler是最强大最好用的Web调试工具之一，它能记录所有客户端和服务器的http和https请求，允许你监视，设置断点，甚至修改输入输出数据.使用Fiddler无论对开发还是测试来说，都有很大的帮助。
资料： 慕课网：Fiddler工具使用
Fiddler 教程
Fiddler Script 用法
Fiddler Composer创建和发送HTTP Request
Fiddler 实现手机的抓包
</content>
    </entry>
    
     <entry>
        <title>[IntelljJ] 文件修改提示和自动保存功能</title>
        <url>https://mryqu.github.io/post/intelljj_%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9%E6%8F%90%E7%A4%BA%E5%92%8C%E8%87%AA%E5%8A%A8%E4%BF%9D%E5%AD%98%E5%8A%9F%E8%83%BD/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>修改提示</tag><tag>自动保存</tag><tag>eclipse</tag>
        </tags>
        <content type="html"> Eclipse中文件修改后没有保存前文件都会有星号提示，IntelljJ IDEA默认没有提示，但是可以通过如下设置完成：Settings -&amp;gt; Editor -&amp;gt; General -&amp;gt; Editor Tabs: Check &amp;ldquo;Markmodified tabs with asterisk&amp;rdquo;IntelljJ IDEA关于文件自动保存功能主要有两种方式： - 切换到其他应用时保存变化（默认使能）设置路径：Settings -&amp;gt; Apperance &amp;amp; Behavior -&amp;gt; Save files onframe deactivation - 如果应用空闲则自动保存变化（默认禁止）设置路径：Settings -&amp;gt; Apperance &amp;amp; Behavior -&amp;gt; Save filesautomatically if application is idle for &amp;hellip; sec.</content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 与Eclipse工作集近似的功能</title>
        <url>https://mryqu.github.io/post/intellij_%E4%B8%8Eeclipse%E5%B7%A5%E4%BD%9C%E9%9B%86%E8%BF%91%E4%BC%BC%E7%9A%84%E5%8A%9F%E8%83%BD/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>intellij</tag><tag>idea</tag><tag>workset</tag><tag>modulegroup</tag>
        </tags>
        <content type="html"> Eclipse鼓励将不同的功能模块划分为独立的项目存在，这样不但结构清晰，组织起来还非常灵活，因为我们可以用feature对这些项目进行不同的组合，输出后得到具有不同功能的产品。 不过这样一来项目浏览器里的项目会以更快的速度增加，当你面对几十上百个项目时，工作效率必然大打折扣。幸好Eclipse提供了工作集（WorkingSet）的功能，它可以用来对项目进行分组，并且可以项目浏览器里指定显示所有项目或者特定工作集下的项目。 具体操作可以参考Eclipse帮助文档工作集概念和项目浏览器显示/隐藏文件。
IntelliJ IDEA与Eclipse术语对比如下：
|Eclipse|IntelliJ IDEA |&amp;mdash;&amp;ndash; |A number of projects, a workspace|Project |Project|Module |Project-specific JRE|Module SDK |User library|Global library |Classpath variable|Path variable |Project dependency|Module dependency |Library|Module library
由此可知在IntelliJ IDEA中近似功能应该在module一层，就我查找的资料来看最近似的功能就是模块组（modulegroup）了。 具体操作可以参考IntelliJ IDEA帮助文档对模块分组。Eclipse可选择对某个工作集下的所有项目进行集中编译；同样IntelliJ IDEA也可选择对模块组下的所有模块集中编译。Eclipse可以显示工作空间下所有项目，或仅显示某个工作集下的项目以隐藏其他项目；IntelliJIDEA只能对模块组进行折叠来隐藏其下的模块。这一点两者的行为有一定差异。
</content>
    </entry>
    
     <entry>
        <title>调试Javascript</title>
        <url>https://mryqu.github.io/post/%E8%B0%83%E8%AF%95javascript/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>调试</tag>
        </tags>
        <content type="html"> 为了调试Javascript，下载了Firefox developer edition，但是没感觉有什么不同，接着下载Firebug，使用感觉有点说不出来的别扭。 还是接着用Chrome调试吧，感觉挺好的，这次conditional break出了不少力！ https://developer.chrome.com/devtools/docs/javascript-debugging </content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 与Eclipse Quick Outline等同功能</title>
        <url>https://mryqu.github.io/post/intellij_%E4%B8%8Eeclipse_quick_outline%E7%AD%89%E5%90%8C%E5%8A%9F%E8%83%BD/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>intellij_idea</tag><tag>quick outline</tag><tag>ctrl&#43;o</tag>
        </tags>
        <content type="html"> 使用Eclipse进行开发时，我喜欢用Ctrl&#43; O快捷键打开快速概要对话框查找或浏览当前类变量和方法。
使用IntelliJ IDEA进行开发时，可以使用Navigate | FileStructure菜单或Ctrl &#43; F12快捷键打开文件结构视图查找或浏览当前类的变量或方法。 此外如果在整个项目内查找变量或方法，可以使用Navigate | Symbol菜单或 Ctrl &#43; Alt &#43; Shift &#43; N快捷键打开符号查找对话框进行查找。
</content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 与Eclipse关于Call Hierarchy和Find Reference功能比较</title>
        <url>https://mryqu.github.io/post/intellij_%E4%B8%8Eeclipse%E5%85%B3%E4%BA%8Ecall_hierarchy%E5%92%8Cfind_reference%E5%8A%9F%E8%83%BD%E6%AF%94%E8%BE%83/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>eclispe</tag><tag>intellij_idea</tag><tag>call_hierarchy</tag><tag>find_reference</tag><tag>analyze_data_flow</tag>
        </tags>
        <content type="html">  &amp;ldquo;Call Hierarchy&amp;rdquo;功能比较 Eclipse的&amp;rdquo;CallHierarchy&amp;rdquo;可以查看一个Java方法或类成员变量的调用树（caller和callee两个方向）。 IntelliJ IDEA中可以在主菜单中选择Navigate | CallHierarchy命令查看一个Java方法调用树（caller和callee两个方向），但是不像Eclipse那样可以查看类成员变量的调用树。 IntelliJ IDEA中可以在主菜单中选择Analyze | Dataflow from/toHere两个命令查看表达式、变量和方法参数的传递关系树。 Eclipse的&amp;rdquo;Call Hierarchy&amp;rdquo;命令的功能，在IntelliJIDEA中被划分到了三个命令，增加了一点点记忆成本，不过IntelliJ IDEA中的处理范围更广，相对功能更强一些。
&amp;ldquo;Find Reference&amp;rdquo;功能比较 Eclipse的&amp;rdquo;Find Reference&amp;rdquo;可以查看一个Java类、方法或变量的直接使用情况。 IntelliJ IDEA的&amp;rdquo;Find Usage&amp;rdquo;具有相同的功能。在我的体验中，IntelliJIDEA中的功能更强一些，可以分析Sping配置文件中对Java类或方法的使用情况。
参考 https://www.jetbrains.com/idea/help/building-call-hierarchy.html
https://www.jetbrains.com/idea/help/analyzing-data-flow.html
</content>
    </entry>
    
     <entry>
        <title>[IntelliJ] Javascript code inspection</title>
        <url>https://mryqu.github.io/post/intellij_javascript_code_inspection/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>javascript</tag><tag>code</tag><tag>inspection</tag>
        </tags>
        <content type="html"> </content>
    </entry>
    
     <entry>
        <title>接触字体图标(Icon Font)</title>
        <url>https://mryqu.github.io/post/%E6%8E%A5%E8%A7%A6%E5%AD%97%E4%BD%93%E5%9B%BE%E6%A0%87icon_font/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>icon</tag><tag>font</tag><tag>html</tag><tag>字体图标</tag>
        </tags>
        <content type="html">  最近玩SAP的OpenUI5，碰到了sap-icon://协议，接触了字体图标。 字体图标流行了有两年了，现在已经不是什么新鲜概念啦。主要是因为 CSS3 增加了一个非常实用的属性@font-face。传统的网页中的字体设置，使用font-family属性来定义，而且受限于浏览者电脑上所安装的字体，如果浏览者电脑上没有安装对应字体，那么网页渲染起来就会使用其他字体来代替。而新增的@font-face改变了这一现状，使用该属性，可以指定服务器上的一个字体，当浏览者访问的时候，会优先下载服务器上的字体，然后再使用该字体渲染网页。这样就可以发挥设计师的想象，灵活的任意应用字体，同时不需要考虑不同平台的差异。该属性的兼容性也非常好。详细兼容性见http://caniuse.com/#feat=fontface 。 @font-face功能不仅仅可以用在改变文章的字体样式上，还可以来做字体图标。字体其实就是一种图标，把对应的基础的文字，渲染成有棱有角的文字。如果某个文字的字体，并不设计成那个文字的变形，而设计成截然不同的图标，那么当网页中出现这个文字，就会渲染出一个图标。
字体图标与像素位图的对比 优点： - 兼容性：各个平台浏览器基本都可以使用，而且在某些老版本浏览器中，效果比图片更好。 - 轻量性：相对于同效果的位图相比，体积要小。一旦图标字体加载了，图标就会马上渲染出来，不需要下载一个图像。可以减少HTTP请求，增强前端性能，还可以配合HTML5离线存储做性能优化。 - 灵活性：图标字体可以用过font-size属性设置其任何大小，还可以加各种文字效果，包括颜色、Hover状态、透明度、阴影和翻转等效果。可以在任何背景下显示。使用位图的话，必须得为每个不同大小和不同效果的图像输出一个不同文件。
劣势： - 图标字体只能被渲染成单色或者CSS3的渐变色。 - 免费开源的精美字体图标资源还是不够多。 - 创作自已的字体图标很费时间，重构人员后期维护的成本偏高。
常用字库文件格式  TTF(TrueTypeFont)格式：TTF是Apple公司和Microsoft公司推出的字体文件格式,随着windows的流行,已经变成最常用的一种字体文件表示方式。truetype字体的最大优点是可以很方便地把字体轮廓转换成曲线，可以对曲线进行填充，制成各种颜色和效果，字款丰富。 OTF(OpenType Font)格式：OpenType，是一种可缩放字型（scalablefont），微软公司与Adobe公司联合开发，用来替代TrueType字型的新字型。 WOFF格式：Web开放字体格式（Web Open FontFormat，简称WOFF），是一种网页所采用的字体格式标准。此字体格式不但能够有效利用压缩来减少档案大小，并且不包含加密。WOFF得到许多主要字体制造公司的支持。 EOT格式：EOT是一种压缩字库，目的是解决在网页中嵌入特殊字体的难题。例如：网页前端开发人员在网页中使用了很多种特殊的精美的字体，当网友浏览时，却因没有安装相应的字库，只能看到默认的宋体字，效果惨不忍睹。利用EOT字库即可解决此难题。  网上一些字体图标资源 OpenUI5 Icon Explorer
confont.cn：由阿里巴巴UX部门推出的矢量图标管理网站，也是国内首家推广Webfont形式图标的平台。
Font Awesome：An iconic font and CSS framework project at GitHub
在OpenUI5里使用字体图标 SAPUI5提供了sap.ui.core.icon控件和sap.ui.core.IconPool力提供的一套预定义图标。通过学习https://github.com/SAP/openui5/blob/master/src/sap.ui.core/src/sap/ui/core/IconPool.js ，大致可以找到OpenUI5里的字体库。
/resources/sap/ui/core/themes/base/fonts/SAP-icons.eot /resources/sap/ui/core/themes/base/fonts/SAP-icons.ttf /resources/sap/ui/core/themes/sap_bluecrystal/fonts/bluecrystal_icons.ttf /resources/sap/ui/core/themes/sap_bluecrystal/fonts/SAP-icons.eot /resources/sap/ui/core/themes/sap_bluecrystal/fonts/SAP-icons.ttf /resources/sap/ui/core/themes/sap_goldreflection/fonts/SAP-icons.eot /resources/sap/ui/core/themes/sap_goldreflection/fonts/SAP-icons.ttf /resources/sap/ui/core/themes/sap_hcb/fonts/SAP-icons.eot /resources/sap/ui/core/themes/sap_hcb/fonts/SAP-icons.ttf  </content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 与Eclipse Link with Editor等价功能设置</title>
        <url>https://mryqu.github.io/post/intellij_%E4%B8%8Eeclipse_link_with_editor%E7%AD%89%E4%BB%B7%E5%8A%9F%E8%83%BD%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>link_with_editor</tag><tag>intellij_idea</tag><tag>autoscroll_from_sour</tag>
        </tags>
        <content type="html"> Link With Editor是Eclipse内置功能中十分小巧，但却异常实用的一个功能。这个开关按钮 (ToggleButton) 出现在各式导航器视图 ( 例如 Resource Explorer, Package Explorer 等 )的右上角。点击时能根据当前打开的文件，相应地展开导航器视图，并迅速关联到该结点。 IntelliJ IDEA中也有等价功能，即项目视图中&amp;rdquo;Autoscroll from source&amp;rdquo;设置。 </content>
    </entry>
    
     <entry>
        <title>WebStorm与IntelliJ IDEA的区别</title>
        <url>https://mryqu.github.io/post/webstorm%E4%B8%8Eintellij_idea%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>jetbrains</tag><tag>webstorm</tag><tag>intellij</tag><tag>idea</tag><tag>区别</tag>
        </tags>
        <content type="html"> WebStorm与IntelliJ IDEA都被很多JS开发者誉为“Web前端开发神器”、“最强大的HTML5编辑器”、“最智能的JavaScriptIDE”等，用了一点IntelliJ IDEA，还真不知道两者有什么联系和区别。网上搜了一下，WebStorm FAQ和PhpStorm FAQ解答了我的疑问。JetBrains旗下的产品： - IntelliJ IDEA偏重于Java开发，旗舰产品，它可以通过（捆绑的或可下载的）插件的方式提供WebStorm和PhpStorm所有的功能。支持Scala和Groovy开发，也可以通过插件支持Ruby和Python语言。 - PhpStorm侧重于PHP开发 - WebStorm侧重于JS开发 - RubyMine侧重于Ruby和Rails开发 - PyCharm侧重于Python和Djanjo开发WebStorm与IntelliJ IDEA相比，功能少，集中于JS开发这一块，更加轻量级，新项目配置起来更简单。
下面两个链接是WebStorm和IntelliJ IDEA的官方报价，WebStorm的价格大概是IntelliJIDEA的五分之一。 - https://www.jetbrains.com/webstorm/buy/ - https://www.jetbrains.com/idea/buy/
</content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 添加Plugin更新URL并安装</title>
        <url>https://mryqu.github.io/post/intellij_%E6%B7%BB%E5%8A%A0plugin%E6%9B%B4%E6%96%B0url%E5%B9%B6%E5%AE%89%E8%A3%85/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>plugin</tag><tag>安装</tag><tag>jetbrains</tag>
        </tags>
        <content type="html"> 1) 选择菜单 File -&amp;gt; Setting2) Setting对话框内选择plugins，点击Browse Repositories -&amp;gt; ManageRepositories -&amp;gt; Add Repository添加完所要安装的插件更新URL，然后就可以安装了。
</content>
    </entry>
    
     <entry>
        <title>[IntelliJ] 格式化代码</title>
        <url>https://mryqu.github.io/post/intellij_%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%BB%A3%E7%A0%81/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>format</tag><tag>code</tag><tag>eclipse</tag><tag>intellij</tag><tag>idea</tag>
        </tags>
        <content type="html"> 用了多年的Eclipse后，转而用IntelliJ IDEA，觉得很多简单的功能都不会了，感觉是一个囧呀。 刚碰到的就是不知道如何去格式化代码。
作为一个非快捷键达人的码农，在Eclipse中都是右键菜单，然后找Source-&amp;gt;Format。
但是在IntelliJ中，右键菜单里面真没这功能。后来还是找了一会，终于在菜单里发现了！
</content>
    </entry>
    
     <entry>
        <title>我的IntelliJ IDEA配置</title>
        <url>https://mryqu.github.io/post/%E6%88%91%E7%9A%84intellij_idea%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>intellij</tag><tag>idea</tag><tag>配置</tag><tag>内存</tag><tag>代理</tag>
        </tags>
        <content type="html">  安装完IntelliJIDEA，我首先切换到64位模式： 安装文件生成的快捷方式默认指向的是idea.exe，更改为idea64.exe。 更改默认内存配置：修改IntelliJ&amp;rdquo;bin&amp;rdquo;目录下的idea64.exe.vmoptions文件。 &amp;ldquo;-Xms&amp;rdquo;和&amp;rdquo;-Xmx&amp;rdquo;定义了Java分配给IntelliJ的最小和最大堆空间，将其更改为&amp;rdquo;-Xms256m&amp;rdquo;和&amp;rdquo;-Xmx1600m&amp;rdquo; 启动IntelliJ IDEA，通过File-&amp;gt;Setting菜单做如下修改：  更改编译器的构建过程堆大小为1024 设置代理为总动检测   </content>
    </entry>
    
     <entry>
        <title>Eclipse用户IntelliJ IDEA入门指南</title>
        <url>https://mryqu.github.io/post/eclipse%E7%94%A8%E6%88%B7intellij_idea%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/</url>
        <categories>
          <category>Tool</category><category>IntelliJ</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>intellij</tag><tag>idea</tag><tag>入门指南</tag><tag>jetbrains</tag>
        </tags>
        <content type="html"> 除了IntelliJ IDEA帮助文档 （https://www.jetbrains.com/idea/help/intellij-idea.html），发现一个IntelliJIDEA不错的入门教程： Getting Started with IntelliJ IDEA as an Eclipse User（http://zeroturnaround.com/rebellabs/getting-started-with-intellij-idea-as-an-eclipse-user/） - Introduction: Why IntelliJ IDEA? - Chapter I: Getting your first IntelliJ IDEA project set up - Chapter II: Getting comfortable with IDEA’s Keymap, Navigation and Settings - Chapter III: Getting productive with Tests, Deployments and Artifacts - Chapter IV: Summary, Conclusion and Goodbye Comic ;-)
其他参考: IntelliJ Idea 常用快捷键列表
</content>
    </entry>
    
     <entry>
        <title>玩一会IPython Notebook</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E4%B8%80%E4%BC%9Aipython_notebook/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>ipython</tag><tag>notebook</tag><tag>python</tag><tag>ide</tag>
        </tags>
        <content type="html"> 晚上想写两行python代码，懒得打开eclipse，也不想玩SublimeText2，就试用了一下Anaconda带的IPython Notebook。 支持函数自动补全，很多快捷键，功能还真不错！以后打算作为主力用了。
资料： IPython Notebook简介1
ipython notebook——超级强大的工具
第三课 Ipython &amp;amp; Ipython Notebook
IPython: Python at your fingertips
</content>
    </entry>
    
     <entry>
        <title>[社交网络分析课] 数据集预处理及Gephi导入</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90%E8%AF%BE_%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E5%A4%84%E7%90%86%E5%8F%8Agephi%E5%AF%BC%E5%85%A5/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>经验性网络分析</tag><tag>r</tag><tag>gephi</tag><tag>数据预处理</tag>
        </tags>
        <content type="html"> 紧赶慢赶忙完了社交网络分析课的编程大作业，选择的是经验性网络分析。
这次的大作业没有指定数据集，自己找数据集做分析。我在网上搜到以前一个同学做的移民分析，感觉很不错。然后，就茫然了。最怕这种自己找数据集的，以前有的课程介绍了一些比较好的开放数据集，可惜当时没觉得有什么太大的价值，没记呀！！！
费了半天，找了一个大不列颠的公路流量数据集，公路就是edge了，每个公路的两端就是node了，感觉还可以。数据集在线地址: http://data.dft.gov.uk/gb-traffic-matrix/Traffic-major-roads-miles.csv.数据集手册在线地址: http://data.dft.gov.uk/gb-traffic-matrix/all-traffic-data-metadata.pdf
这次作业主要使用Gephi对数据集进行分析，所以我先使用R语言对原始数据集进行预处理，然后使用Gephi导入生成的nodes.csv和edges.csv。
if (!file.exists(&amp;quot;./Traffic-major-roads-miles.csv&amp;quot;)) { download.file(&amp;quot;http://data.dft.gov.uk/gb-traffic-matrix/Traffic-major-roads-miles.csv&amp;quot;, destfile = &amp;quot;./Traffic-major-roads-miles.csv&amp;quot;) } data &amp;lt;- read.csv(&amp;quot;Traffic-major-roads-miles.csv&amp;quot;, sep = &amp;quot;,&amp;quot;) data2013 &amp;lt;- subset(data, Year == 2013 &amp;amp; AllMV &amp;gt; 0, select = c(A.Junction, B.Junction, AllMV)) data2013 &amp;lt;- data.frame(A.Junction = toupper(data2013$A.Junction), B.Junction = toupper(data2013$B.Junction), Weight = data2013$AllMV, stringsAsFactors = FALSE) A.junctions &amp;lt;- as.vector(data2013[, 1]) B.junctions &amp;lt;- as.vector(data2013[, 2]) junctions &amp;lt;- sort(unique(c(A.junctions, B.junctions))) # write nodes.csv nodes_print &amp;lt;- cbind(c(&amp;quot;Nodes&amp;quot;, junctions), c(&amp;quot;Id&amp;quot;, 1:length(junctions)), c(&amp;quot;Label&amp;quot;, junctions)) nodes_print &amp;lt;- t(nodes_print) write(nodes_print, file = &amp;quot;nodes.csv&amp;quot;, ncolumns = 3, sep = &amp;quot;;&amp;quot;) # edges &amp;lt;- data.frame(Source=c(&#39;Source&#39;), Target=c(&#39;Target&#39;), # Weight=c(&#39;Weight&#39;), Id=c(&#39;Id&#39;)) for(i in 1:nrow(data2013)) { edges &amp;lt;- # rbind(edges, # data.frame(Source=as.character(which(junctions==data2013[i,1])), # Target=as.character(which(junctions==data2013[i,2])), # Weight=data2013[i,3], Id = as.character(i) ) ) } edges &amp;lt;- data.frame(Source = c(&amp;quot;Source&amp;quot;, match(data2013[, 1], junctions)), Target = c(&amp;quot;Target&amp;quot;, match(data2013[, 2], junctions)), Weight = c(&amp;quot;Weight&amp;quot;, data2013[, 3]), Id = c(&amp;quot;Id&amp;quot;, 1:nrow(data2013)), stringsAsFactors = FALSE) # write edges.csv edges_print &amp;lt;- as.matrix(edges) edges_print &amp;lt;- t(edges_print) write(edges_print, file = &amp;quot;edges.csv&amp;quot;, ncolumns = 4, sep = &amp;quot;;&amp;quot;)  代码里首先将所有公路起始和终止端点合并、去冗余、排序，用于生成nodes.csv。然后搜索每个公路端点在公路端点集合的下标，用于生成edges.csv。第一版的代码被迫使用了一次for循环，主要是只对which函数印象很深刻。在R语言里使用for循环慢的要死，感觉那是相当的屌丝。后来找到了match函数，感觉非常好。
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] logging</title>
        <url>https://mryqu.github.io/post/openui5_logging/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>javascript</tag><tag>log</tag>
        </tags>
        <content type="html"> jQuery.sap.log是客户端Javascript日志API。 通过上图可知，其日志级别分别为ALL、DEBUG、ERROR、FATAL、INFO、NONE、TRACE和WARNING，默认日志级别为ERROR。
如果要显示所有日志信息，可以执行:
jQuery.sap.log.setLevel(6)  </content>
    </entry>
    
     <entry>
        <title>了解Google Closure Tools</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3google_closure_tools/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>web</tag><tag>javascript</tag><tag>google</tag><tag>closurecompiler</tag><tag>闭包工具集</tag>
        </tags>
        <content type="html">  hello一个html5-openui5项目，公司的编译系统在googlecc.xml（ant脚本）报了一个错，用googlecc做关键词搜了半天没弄清是什么东西，后来才发现是GoogleClosure Compiler。 不同于个人的小项目，企业级Web应用里面可能存在大量的Javascript代码。JS文件很多，文件块头还不小。不管是静态引入还是GoogleClosureLibrary/require.js这种模块化动态异步加载，下载时间长了，都会给Web用户带来不好的感知性能体验。很多Javascript压缩工具可以帮助减小JS文件大小，GoogleClosure Compiler就是其中一款。 谷歌2009年开源了其内部使用的JavaScript开发工具，Google Closure Tools，希望帮助程序员更高效地开发出富客户端Web应用程序。该工具集由如下工具组成： - Closure Compiler:该优化器将JavaScript优化成紧凑、高性能的代码。它通过去除无用死代码、空格和注释、缩短长的局部变量名等方法压缩代码，检查语法、变量引用和变量类型，并对常见的JavaScript陷阱给出警告。 - Closure Library：功能广泛的，经过良好测试的，模块化的，跨浏览器的JavaScript库 - Closure Templates：客户端和服务器端模板系统，可以有助于动态生成可重用的HTML和UI元素。ClosureTemplates摒弃了一个页面使用一个(大)模板，而是针对单个小组件使用(小)模板，以便复用。该模板可生成JavaScript或Java代码，因此同一模板可在客户端或者服务端使用。 - Closure Linter：按照《谷歌JavaScript编程风格指南》 里面的指导方针对JavaScript代码进行编程风格检查和修复的工具 - Closure Stylesheets：支持很多谷歌扩展的增强格式表语言系统。可以定义和使用变量、函数、条件，以使格式表可读性增强、更易于维护。内建的工具可以将其编译成标准CSS。
阅读列表： 闭包：权威指南(Closure：The Definitive Guide) 部分翻译 前言 1 2 3 4 5
Google Closure Compiler &amp;ndash;js压缩优化
Closure Compiler vs. YUICompressor
应用 closure compiler 高级模式
Closure Compiler 高级模式及更多思考
知乎为什么要选择 Closure Library 来作为 JavaScript 库，而不选择更流行的 jQuery 之流呢？
Google Closure Library介绍
</content>
    </entry>
    
     <entry>
        <title>[OpenUI5] JSView的createContent和Controller的onInit孰先孰后？</title>
        <url>https://mryqu.github.io/post/openui5_jsview%E7%9A%84createcontent%E5%92%8Ccontroller%E7%9A%84oninit%E5%AD%B0%E5%85%88%E5%AD%B0%E5%90%8E/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>createcontent</tag><tag>oninit</tag><tag>mvc</tag><tag>javascript</tag>
        </tags>
        <content type="html"> 首先在这个两个函数设置断点，很容易知道JSView的createContent先于Controller的onInit被调用。 通过sap.ui.core.mvc.View源码片段可知，View的_initCompositeSupport函数中首先调用createAndConnectController函数创建Controller,之后调用的onControllerConnected函数会调用createContent函数，最后调用的fireAfterInit函数会触发Controller的onInit函数回调。
View.prototype._initCompositeSupport = function(mSettings) { // init View with constructor settings // (e.g. parse XML or identify default controller) // make user specific data available during view instantiation this.oViewData = mSettings.viewData; // remember the name of this View this.sViewName = mSettings.viewName; // remember the preprocessors this.mPreprocessors = mSettings.preprocessors || {}; //check if there are custom properties configured for this view, //and only if there are, create a settings preprocessor applying these if (sap.ui.core.CustomizingConfiguration &amp;amp;&amp;amp; sap.ui.core.CustomizingConfiguration.hasCustomProperties( this.sViewName, this)) { var that = this; this._fnSettingsPreprocessor = function(mSettings) { var sId = this.getId(); if (sap.ui.core.CustomizingConfiguration &amp;amp;&amp;amp; sId) { if (that.isPrefixedId(sId)) { sId = sId.substring((that.getId() &#43; &amp;quot;--&amp;quot;).length); } var mCustomSettings = sap.ui.core.CustomizingConfiguration. getCustomProperties(that.sViewName, sId, that); if (mCustomSettings) { // override original property initialization // with customized property values mSettings = jQuery.extend(mSettings, mCustomSettings); } } }; } if (this.initViewSettings) { this.initViewSettings(mSettings); } createAndConnectController(this, mSettings); // the controller is connected now =&amp;gt; notify the view implementations if (this.onControllerConnected) { this.onControllerConnected(this.oController); } this._preprocessViewContent(); // notifies the listeners that the View is initialized this.fireAfterInit(); };  通过sap.ui.core.mvc.JSView源码片段可知，JSView的onControllerConnected函数中会调用createContent函数。
JSView.prototype.onControllerConnected = function(oController) { var that = this; var oPreprocessors = {}; // when auto prefixing is enabled we add the prefix if (this.getAutoPrefixId()) { oPreprocessors.id = function(sId) { return that.createId(sId); }; } oPreprocessors.settings = this._fnSettingsPreprocessor; // unset any preprocessors (e.g. from an enclosing JSON view) sap.ui.base.ManagedObject.runWithPreprocessors(function() { that.applySettings({ content : that.createContent(oController) }); }, oPreprocessors); };  通过sap.ui.core.mvc.Controller源码片段可知，Controller的connectToView方法就是将回调注册到View的生命周期事件监听器上。
Controller.prototype.connectToView = function(oView) { this.oView = oView; if (this.onInit) { oView.attachAfterInit(this.onInit, this); } if (this.onExit) { oView.attachBeforeExit(this.onExit, this); } if (this.onAfterRendering) { oView.attachAfterRendering(this.onAfterRendering, this); } if (this.onBeforeRendering) { oView.attachBeforeRendering(this.onBeforeRendering, this); } //oView.addDelegate(this); };  </content>
    </entry>
    
     <entry>
        <title>[OpenUI5] 十分钟了解sap.ui.table.Table</title>
        <url>https://mryqu.github.io/post/openui5_%E5%8D%81%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3sap.ui.table.table/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>web</tag><tag>openui5</tag><tag>sap.ui.table.table</tag><tag>sap</tag><tag>table</tag>
        </tags>
        <content type="html">  转发一篇SAP community network的好帖子：http://scn.sap.com/docs/DOC-54075
Introduction _sap.ui.table.Table_ is commonly used inOpenUI5 desktop application. Many questions (related to thiscontrol) that are posted in this group, it is evident thatdocumentation for this control is lacking and we (developers) haveto dive deep into debugging its source code to figure things out.It is fortunate that Javascript source code is always available;modern browsers provide debugging capability and personally, I amfortunate to have the opportunity to work with someone in SAPUI5team while using this control. Hence it is an opportunity for me tocontribute back to the community on what I have learned about thiscontrol.
I am assuming that reader of this document has some basicunderstanding of OpenUI5. The person understands what is OpenUI5control, model and model binding. Please ask questions if you haveany and I (the community experts) will try answer them. I am hopingthat I can keep this as a live document where I can constantlyreceive feedback and update it. In this document, we show - A sample JSON model where we are use it our sample code; - Some code snippet on how to instantiate a table - Creating columns and binding of rows - Getting context of selected row(s), - Two ways binding of model and control
Sample JSON Model  var naughtyList = [ {lastName: &amp;quot;Dente&amp;quot;, name: &amp;quot;Al&amp;quot;, stillNaughty: true}, {lastName: &amp;quot;Friese&amp;quot;, name: &amp;quot;Andy&amp;quot;, stillNaughty: true}, {lastName: &amp;quot;Mann&amp;quot;, name: &amp;quot;Anita&amp;quot;, stillNaughty: false} ]; var oModel = new sap.ui.model.json.JSONModel(); oModel.setData(naughtyList);  Instantiate a table It can be as simple as
var oTable = new sap.ui.table.Table();  where we inherit all the default properties from the control.Please refer to the official doc for reference. There are a few properties that I hope to explain a littlemore.
SelectionMode (API) Example
var oTable = new sap.ui.table.Table({ selectionMode : sap.ui.table.SelectionMode.Single });  Here you can control the selection mode such as None (disableselection), Single (maximum one row can be selected at any time)and Multi (one or more rows can be selected) . Default is Multi
SelectionBehavior (API) Example
var oTable = new sap.ui.table.Table({ selectionMode : sap.ui.table.SelectionMode.Row });  Here you can control how the row can be selected such as RowOnly(mouse click on the row only), Row (mouse click on the row and rowselector) and RowSelector (mouse click on the row selector only).Default is RowSelector. Create columns and bind rows Here is an example of howyou can do these.
 // instantiate the table var oTable = new sap.ui.table.Table({ selectionMode : sap.ui.table.SelectionMode.Single, selectionBehavior: sap.ui.table.SelectionBehavior.Row }); // define the Table columns and the binding values oTable.addColumn(new sap.ui.table.Column({ label: new sap.ui.commons.Label({text: &amp;quot;Last Name&amp;quot;}), template: new sap.ui.commons.TextView({text:&amp;quot;{lastName}&amp;quot;}) })); oTable.addColumn(new sap.ui.table.Column({ label: new sap.ui.commons.Label({text: &amp;quot;First Name&amp;quot;}), template: new sap.ui.commons.TextField({value: &amp;quot;{name}&amp;quot;}) })); oTable.addColumn(new sap.ui.table.Column({ label: new sap.ui.commons.Label({text: &amp;quot;Still Naughty&amp;quot;}), template: new sap.ui.commons.CheckBox({checked: &#39;{stillNaughty}&#39;}) })); oTable.setModel(oModel); oTable.bindRows(&amp;quot;/&amp;quot;); oTable.placeAt(&amp;quot;content&amp;quot;);  Observe that we do bindRows(&amp;ldquo;/&amp;rdquo;), this is because we have theJSON model containing an array.
oModel.setData(naughtyList);  But if your model is created in this manner,
oModel.setData({&#39;mydata&#39; : naughtyList});  Then you need to do bindRows(&amp;lsquo;/mydata&amp;rsquo;); And if your data is modified to
 var naughtyList = [ {lastName: &amp;quot;Dente&amp;quot;, name: &amp;quot;Al&amp;quot;, stillNaughty: &#39;true&#39;}, {lastName: &amp;quot;Friese&amp;quot;, name: &amp;quot;Andy&amp;quot;, stillNaughty: &#39;true&#39;}, {lastName: &amp;quot;Mann&amp;quot;, name: &amp;quot;Anita&amp;quot;, stillNaughty: &#39;false&#39;} ];  where _stillNaugthy_ is astring and not a boolean. We can adapt to this with this change inthe column definition
 oTable.addColumn(new sap.ui.table.Column({ label: new sap.ui.commons.Label({text: &amp;quot;Still Naughty&amp;quot;}), template: new sap.ui.commons.CheckBox({ checked: { path: &#39;stillNaughty&#39;, formatter: function(v) { return (v === &#39;true&#39;) } } }) }));  There are many things that you can do with formatter function.For instance if you want to display the first and last nametogether uppercase the last name. we can do this. (Example)
 oTable.addColumn(new sap.ui.table.Column({ label: new sap.ui.commons.Label({text: &amp;quot;Name&amp;quot;}), template: new sap.ui.commons.TextView( {text: { parts : [&#39;name&#39;, &#39;lastName&#39;], formatter: function(n, l) { if (n &amp;amp;&amp;amp; l) { return n &#43; &#39;, &#39; &#43; l.toUpperCase(); } } } } ) }));  Getting context of selected row(s), Here come the interesting part of this document because manyquestions are asked related to this topic.
Single Selection Mode When a row is selected or deselected,a _rowSelectionChange_ eventis fired. And here is how wedeal with it. And below this is main code.
 var oTable = new sap.ui.table.Table({ selectionMode : sap.ui.table.SelectionMode.Single, rowSelectionChange: function(e) { var idx = e.getParameter(&#39;rowIndex&#39;); if (oTable.isIndexSelected(idx)) { var cxt = oTable.getContextByIndex(idx); var path = cxt.sPath; var obj = oTable.getModel().getProperty(path); console.log(obj); } } });  Here (in the code), we can get the selected or deselected row;then we usethe _isIndexSelected_ functionto check if it is selected or deselected. And by getting thecontext and path, we are able to get the binding object itself. Please note that if row 1 is already selected and now userselect row #2, this event will not fire for that deselection of row#1, an event will be fired for selection of row #2.
Multi Selection Mode When one or more rows are selected or deselected, thesame _rowSelectionChange_ eventwill be fired. And here is how wedeal with it. And below this is main code.
 var oTable = new sap.ui.table.Table({ selectionMode : sap.ui.table.SelectionMode.Multi, rowSelectionChange: function(e) { var indices = e.getParameter(&#39;rowIndices&#39;); for (var i = 0; i &amp;lt; indices.length; i&#43;&#43;) { var idx = indices[i]; if (oTable.isIndexSelected(idx)) { var cxt = oTable.getContextByIndex(idx); var path = cxt.sPath; var obj = oTable.getModel().getProperty(path); console.log(obj); } } } });  Here (in the code), we can get the selected or deselected rows;then we iterate through the array and usethe_isIndexSelected_ function to check if itis selected or deselected. And by getting the context and path, weare able to get the binding object itself. This piece of code isalmost identical to the code snippet in the single selection modeexcept for getting an array of selected indices.
Selection None Mode and Select by control in row You can also identify the row and context with button, checkbox,etc. in the row. Here is an example.
 oTable.addColumn(new sap.ui.table.Column({ label: new sap.ui.commons.Label({text: &amp;quot;Still Naughty&amp;quot;}), template: new sap.ui.commons.CheckBox({ checked: { path: &#39;stillNaughty&#39;, formatter: function(v) { return (v === &#39;true&#39;) } }, change: function(e) { var path = e.getSource().getBindingContext().sPath; var obj = oTable.getModel().getProperty(path); alert(JSON.stringify(obj)); var index = parseInt(path.substring(path.lastIndexOf(&#39;/&#39;)&#43;1)); alert(index); } }) }));  Two ways binding of model and control One of the beauty of OpenUI5 is the binding between model andcontrol. In our example here (Checkand uncheck the checkbox and then click on Show button to see thatcurrent data in the model), we are able to bind the checkbox valueto the model effortlessly. You can also bind any controls such astextfield, radio button, etc in the table. However if you use the formatter function to alter the valuebefore displaying it then you need to update the model manuallylike this. And below this is maincode.
 oTable.addColumn(new sap.ui.table.Column({ label:new sap.ui.commons.Label({text: &amp;quot;Still Naughty&amp;quot;}), template: new sap.ui.commons.CheckBox({ checked: { path: &#39;stillNaughty&#39;, formatter: function(v) { return (v === &#39;true&#39;) } }, change: function(e) { var path = e.getSource().getBindingContext().sPath; var obj = oTable.getModel().getProperty(path); obj.stillNaughty = this.getChecked() } }) }));  </content>
    </entry>
    
     <entry>
        <title>Web安全</title>
        <url>https://mryqu.github.io/post/web%E5%AE%89%E5%85%A8/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>web安全</tag><tag>xss</tag><tag>clickjacking</tag><tag>csrf</tag><tag>session_fixation</tag>
        </tags>
        <content type="html">  学习OpenUI5程序员指南里关于Web安全的章节，在网上也搜了一下资料，下面是这次的学习笔记。
浏览器安全 跨站脚本攻击（Cross-Site Scripting， XSS） XSS指的是恶意攻击者往Web页面里插入恶意html代码，当用户浏览该页之时，嵌入其中Web里面的html代码会被执行，从而达到恶意用户的特殊目的。例如，如果某些无需登录即可访问的公共页面区域有提交表单，攻击者可以在提交内容里面注入恶意javascript脚本（原理类似与SQL注入攻击），提交后的页面内容可能就含有这样的可执行恶意脚本。当其他登录用户访问这个页面后，就有可能泄漏自己的会话cookie信息。一般的防御手段是使用HTMLSanitization工具对用户输入信息做检查，过滤其提交内容；或者cookie记录登录ip，仅允许该ip使用此cookie。
点击劫持（clickjacking） 点击劫持指的是通过欺骗用户点击看似正常的恶意网页来获得机密信息或远程控制其电脑。例如，攻击者想要攻击某网页，可以发布一个网址与原网址及其近似的恶意网页，恶意网页通过iframe嵌入原网页。当用户被欺骗到恶意网页，界面和操作都与原网页一模一样，但操作实际上是与恶意网页交互。另外一个例子是攻击者发布一个恶意flash游戏，当用户玩游戏时，点击会被引导到恶意链接上，进而控制用户的摄像头和麦克风，用户的个人隐私遭到泄漏。一般的防御手段是Framebusting：正常网页中添加X-FRAME-OPTIONS=DENY(拒绝任何域加载)的http头，以及通过javascript脚本判别顶级frame是否被修改并将覆盖在自身上面的&amp;rdquo;恶意&amp;rdquo;frame重新定向会self.location。
HTML5 localStorage 所有浏览器都提供本地存储API，可用于存储有限数量的数据。仅运行在与存储数据相同域上的JavaScript代码可被执行进行数据访问。浏览器的本地存储不是安全存储，所以仅能用于存储静态数据，不应该存储应用数据。
WEBGL 越来越多的浏览器默认支持WEBG，WEBG允许访问计算机的底层图形API，这可能导致底层漏洞。
WebSockets WebSockets为web应用的客户端/服务器通信提供了新的方式，但很多浏览器厂商的第一版WebSockets实现就暴露了很多安全问题。RGC6455中的WebSockets标准化已经稳定，并且Chrome16、Firefox 11和IE10都实现了WebSockets。即使浏览器自身的WebSockets实现被证明是安全的，使用WebSockets时客户端仍需要额外的安全措施。
Postmessage/Onmessage postMessage允许浏览器窗口之间的跨域通信，这可能会导致安全问题。应用程序应该检查消息的发送域并仅处理来自受信域的消息。
会话安全 HTTP通信是无状态的和非加密的，客户端和服务器之间的数据传输是不安全的，因此有必要使用SSL进行加密，并使用cookie或URL地址重写进行会话处理。即使使用SSL对数据传输进行加密，仍有可能发生会话劫持。跨域请求伪造和会话固定攻击就是这类攻击中突出的两个例子。
WEB认证基础知识  用户访问需要认证的网站。 用户提供一个用户名和口令进行验证。 网站验证用户口令，如果通过，则准许用户登录进入，并将一个cookie提供给用户的浏览器。此cookie用于唯一的标识会话。 用户继续访问网站。在用户请求一个新网页时，浏览器都会发送cookie和用户请求，提醒Web服务器：该请求是前面的认证连接的一部分。在多数情况下，Web开发人员和网站管理员都会使用HTTPS加密来保护这个过程的第二步，他们都知道如果其它人员能够访问其他用户的用户名和口令，就可以轻易地获得访问权。在许多情况下，他们会转而使用一个不加密的HTTP连接，以便于实现Web通信的其余部分，其中也包括cookie的交换。  火羊（Firesheep） Firesheep是一个火狐（Firefox）插件，它在不安全的无线网络中自动操作会话劫持攻击。这个插件（plug-in）本质上是一个数据包嗅探器，它监测并分析Wi-Fi上终端用户之间的流量，并获取正在交换的cookie。西雅图的软件开发工程师EricButler研发了Firesheep并在2010年10月的ToorCon黑客会议上宣布了它的发行。Firesheep非常容易使用，下载插件，登陆公用的WIFI点，按一下按钮就可以获取网络中各种人的用户名和图像（例如：Facebook、Twitter、Flickr、bit.ly、Google和Amazon），双击图像攻击者就可以用受害人的身份登陆了。一般的防御手段是仅允许通过SSL来发送cookie；要限制能够利用cookie的应用程序；限制cookies仅能使用HTTPS。
跨域请求伪造(Cross-site request forgery, CSRF或XSRF) 跨域请求伪造指的是恶意攻击者往Web页面里插入恶意html代码，当用户浏览该页之时，在用户毫不知情的情况下以用户名义伪造请求发送给受攻击站点，从而在并未授权的情况下执行在权限保护之下的操作。例如，恶意网页中有转账请求的恶意链接，当有用户被骗访问网站并点击了带有恶意链接的图片或广告，该请求会附带用户浏览器的cookie一起发往银行。大多数情况下，该请求会失败，因为它要求用户的认证信息。但是如果用户当时恰巧刚访问他的银行后不久，他的浏览器与银行网站之间的会话尚未过期，浏览器的cookie之中含有用户的认证信息。这时，转账请求就被处理。一般的防御手段是验证HTTP的Referer字段；在所有页面加入同步令牌模式(Synchronizer tokenpattern)并要求在请求地址中添加令牌并验证；在强调安全的操作前重复用户认证。
会话固定攻击(session fixation) 会话固定攻击是利用服务器的session不变机制，借他人之手获得认证和授权，然后冒充他人。例如，攻击者访问http://vulnerable.example.com/ ，并从服务器响应(Set-Cookie:SID=0D6441FEA4496C2)获得会话ID，他发送给受害者一个链接http://vulnerable.example.com/?SID=0D6441FEA4496C2。受害者登录的话，会使用这个固定会话标识符SID=0D6441FEA4496C2。攻击者这时访问http://vulnerable.example.com/?SID=0D6441FEA4496C2就可以不受限地访问受害者的账户了。一般的防御手段是用户登录后更改会话ID；仅接受服务器生成的会话ID；超时或退出登录后删除会话；强调安全的操作使用SSL会话ID；每次请求更新会话ID。
参考  Busting Frame Busting: a Study of Clickjacking Vulnerabilities on Popular Sites
 使用 HTML5 WebSocket 构建实时 Web 应用
 应用 HTML5 的 WebSocket 实现 BiDirection 数据交换
 HTML5 postMessage 和 onmessage API 详细应用
 urlrewrite使用小结
 怎样应对会话劫持：以Firesheep为例
 CSRF 攻击的应对之道
 Spring Security如何防止会话固定攻击(session fixation attack)
 黑客攻防技术宝典: Web实战篇
  </content>
    </entry>
    
     <entry>
        <title>[社交网络分析课] 小世界网络</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90_%E5%B0%8F%E4%B8%96%E7%95%8C%E7%BD%91%E7%BB%9C/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>sna</tag><tag>coursera</tag><tag>笔记</tag><tag>小世界网络</tag>
        </tags>
        <content type="html">  本文为Social NetworkAnalysis学习笔记，课程地址为https://www.coursera.org/course/sna 。
小世界网络 在网络理论中，小世界网络是一类特殊的复杂网络结构，在这种网络中大部份的节点彼此并不相连，但绝大部份节点之间经过少数几布就可到达。 在日常生活中，有时你会发现，某些你觉得与你隔得很“遥远”的人，其实与你“很近”。小世界网络就是对这种现象（也称为小世界现象）的数学描述。用数学中图论的语言来说，小世界网络就是一个由大量顶点构成的图，其中任意两点之间的平均路径长度比顶点数量小得多。除了社会人际网络以外，小世界网络的例子在生物学、物理学、计算机科学等领域也有出现。 二十世纪60年代，美国哈佛大学社会心理学家斯坦利·米尔格伦（StanleyMilgram）做了一个连锁信实验。他将一些信件交给自愿的参加者，要求他们通过自己的熟人将信传到信封上指明的收信人手里，他发现，20%的信件最终送到了目标人物手中。而在成功传递的信件中，平均只需要6.5次转发，就能够到达目标。也就是说，在社会网络中，任意两个人之间的“距离”是6。这就是所谓的“六度分隔”理论。
全局集聚系数(Global Clustering Coefficient) 全局集聚系数基于节点的三点组。一个三点组由三个节点组成，其中可以两边连接(为闭三点组)或三边连接(开三点组)，统称连通三点组。 全局集聚系数是闭三点组个数(或三倍三角形个数)除以全部连通三点组个数。 该方法首次由Luce和Perry使用 (1949)。该指标指示了整个网络的集聚程度，可被用于无方向和有向网络。 定义： 局部集聚系数(Local Clustering Coefficient, Watts&amp;amp;Strogatz1998) 对于每个节点i，ni是节点i的邻居节点个数。 平均本地集聚系数： 示例1： 示例2： 示例3： 我用Gephi做了一个例子，Gephi不计算全局集聚系数，它会计算平均集聚系数和每个节点的局部集聚系数。 平均集聚系数 = (1&#43;0.667&#43;0.667&#43;1)/4 = 0.833，同软件所得一致。
下面手动算一下全局集聚系数： 闭三点组有6个： B-&amp;gt;A&amp;lt;-C; A-&amp;gt;B&amp;lt;-C; A-&amp;gt;C&amp;lt;-B; C-&amp;gt;B&amp;lt;-D;B-&amp;gt;C&amp;lt;-D; B-&amp;gt;D&amp;lt;-C 开三点组有2个： A-&amp;gt;C&amp;lt;-D; A-&amp;gt;B&amp;lt;-D 全局集聚系数=6/(6&#43;2)=3&amp;frasl;4=0.75
强联系(A strong tie)  联系频繁 亲密 许多共同的联系人  连接边嵌入性(edge embeddeness) 嵌入性(embeddeness)：两个端点所共同拥有的邻居节点个数。 邻居节点重叠(neighborhood overlap)： 分解局部结构：网络基序(Network motif) 网络的复杂性本质上就是关系的复杂性。但是，研究者通过对真实网络的分析，发现各种关系种类的出现频率是非随机性的。某些特定的关系种类在网络中反复出现，形成网络的典型连接方式；不同类型的网络具有不同的典型连接方式。研究者把这些特定的关系种类称为“网络基序”，认为它们是一个网络的基本构造单元。 基序是从功能的角度来分析网络的构成，着眼于网络内各种成分之间连接的模式或关系。 所有三节点基序 网络基序示例(三节点) 网络基序示例(四节点) 真实网络与随机网络的网络基序对比 网络基序探测 跟随机网络相比，一些网络基序更容易在真实网络中出现。
技术： 使用相同数量的节点和边构造随机网络(相同的节点度分布？) 计算图中的基序个数 计算Z-评分：给定基序个数偶然在真实网络出现的概率
基序软件 Uri AlonLab mfinder： 网络基序探测工具 Uri AlonLab mDraw： 网络基序可视化工具 FANMOD：网络基序快速探测工具(开发者S. Wernicke和F. Rasche) MAVisto：网络基序分析和可视化工具 小网络现象  高集聚系数  低平均最短路径长度   例子： 秀丽隐杆线虫的神经网络 语言的语义网络 演员协作图 食物网
Watts/Strogatz模型：生成小世界图 同许多网络生成算法一样： - 禁止自连接 - 禁止多连接
示例：每个节点有K&amp;gt;=4最近邻居节点（局部） 可调：改变重连接给定边的概率p 小p：规则网格 大p：经典随机图 通过数字模拟，将p从0增加到1：
 平均距离快速下降 集聚系数缓慢增长  当重连接增加时的集聚系数和平均最短路径(ASP): WS模型集聚系数 连接的三点组重连接后仍然连接的概率： - 三个边没有一个重连接的概率(1-p)3 - 边重连接回原有状态的概率很小，可以忽略
集聚系数= C(p) = C(p=0)*(1-p)3 对比随机网络判断真实网络是否是“小世界网络” WS模型：遗漏  远程连接不可能同短程连接一样 分层结构/组 社交中心(hub)  Kleinberg的地理小世界模型 可通性(Navigability) 没看懂的说 小世界的起源：群体关系 分层小世界模型：Kleinberg 分层小世界模型：WDN 生成小世界网络  向节点分配属性(例如空间位置、组的成员关系等) 根据一些规则添加或重连接边  针对特定属性进行优化(模拟退火法) 取决于已有节点、边（优先连接、链接复制）以一定概率添加边 将节点模拟成代理决定是否添加或重连接边    小世界起源：高效网络示例 连接与连通性之间的权衡 优化的网络 另一视图看优化的网络 从零开始优化 引人深思 参考 小世界网络
Clustering in Weighted Networks
Clustering coefficient
小世界网络模型
Social Network Analysis - 2012 - 001
</content>
    </entry>
    
     <entry>
        <title>开玩OpenUI5</title>
        <url>https://mryqu.github.io/post/%E5%BC%80%E7%8E%A9openui5/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>openui5</tag><tag>sap</tag><tag>html5</tag><tag>javascript</tag><tag>sapui5</tag>
        </tags>
        <content type="html"> OpenUI5是SAP推出的开源HTML5Javascript用户界面库，网址为http://sap.github.io/openui5/index.html 前不久SAP宣布我司成了签署OpenUI5企业贡献者许可协议的第一个组织(BjornGoerkee的Tweet)，从此我司产品中的Flex就要纷纷下岗，让位HTML5了。 据说SAP在UI框架上的选择纠结了十多年了，甚至投奔过微软的silverlight，后来才成为HTML5的拥拓。 我司的富客户端技术用过Swing，中间打算换成Eclipse RCP，再后来决定全面采用Flex技术，兜了一圈又一圈，花了时间费了钱，最后决定采用HTML5。 虽然各浏览器厂商还在HTML5上进行利益博弈，W3C与WHATWG分道扬镳，Facebook和Linkedin抛弃HTML5转投原生App应用，但是为了将我司的产品转向云应用，HTML5对于我们这种企业级应用来说还算是很靠谱的了。 今天下载了openui5-sdk-1.24.3.zip，直接解压到Tomcat的webapps目录下，开始学文档做demo。 </content>
    </entry>
    
     <entry>
        <title>JavaScript中的点符号和方括号符号</title>
        <url>https://mryqu.github.io/post/javascript%E4%B8%AD%E7%9A%84%E7%82%B9%E7%AC%A6%E5%8F%B7%E5%92%8C%E6%96%B9%E6%8B%AC%E5%8F%B7%E7%AC%A6%E5%8F%B7/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>dot</tag><tag>square_bracket</tag><tag>notation</tag>
        </tags>
        <content type="html"> JavaScript中对象可以通过点符号(dot notation)或方括号符号(square bracketnotation)访问属性。
a = {}; b = function() { alert(&amp;quot;Thanks!&amp;quot;); }; c = function() { alert(&amp;quot;Bye!&amp;quot;); }; a[&amp;quot;Hello&amp;quot;] = b; a.bye = c; a.hello(); a.bye();  两者相同之处: 当属性不存在时返回undefined。两者的区别是: - 点符号访问方式更快，代码阅读起来更清晰。 - 方括号符号访问方式可以访问包含特殊字符的属性，属性选择可以使用变量。JSLint会对方括号符号访问进行告警。
</content>
    </entry>
    
     <entry>
        <title>又被Mining Massive Datasets的老师伤了！</title>
        <url>https://mryqu.github.io/post/%E5%8F%88%E8%A2%ABmining_massive_datasets%E7%9A%84%E8%80%81%E5%B8%88%E4%BC%A4%E4%BA%86/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>大数据挖掘</tag><tag>kmeans</tag><tag>作业</tag>
        </tags>
        <content type="html"> Mining Massive Datasets这周的课讲聚类和计算广告学：二分图匹配。课后的作业好几个都是一眼看不出来，只好写程序算。
其中有一道题是这个样子： We wish to cluster the following set of points: into 10 clusters. We initially choose each of the green points(25,125), (44,105), (29,97), (35,63), (55,63), (42,57), (23,40),(64,37), (33,22), and (55,20) as a centroid. Assign each of thegold points to their nearest centroid. (Note: the scales of thehorizontal and vertical axes differ, so you really need to applythe formula for distance of points; you can&amp;rsquo;t just &amp;ldquo;eyeball&amp;rdquo; it.)Then, recompute the centroids of each of the clusters. Do any ofthe points then get reassigned to a new cluster on the next round?Identify the true statement in the list below. Each statementrefers either to a centroid AFTER recomputation of centroids(precise to one decimal place) or to a point that getsreclassified.
第一个念头就是用R捯饬：
points &amp;lt;- matrix(c(28, 65, 50, 25, 55, 38, 44, 29, 50, 63, 43, 35, 55, 50, 42, 23, 64, 50, 33, 55, 145, 140, 130, 125, 118, 115, 105, 97, 90, 88, 83, 63, 63, 60, 57, 40, 37, 30, 22, 20), ncol = 2) colnames(points) &amp;lt;- c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;) centroids &amp;lt;- matrix(c(25, 44, 29, 35, 55, 42, 23, 64, 33, 55, 125, 105, 97, 63, 63, 57, 40, 37, 22, 20), ncol = 2) dimnames(centroids)[[2]] &amp;lt;- c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;) cl &amp;lt;- kmeans(points, centroids, trace = TRUE)  ## KMNS(*, k=10): iter= 1, indx=10 ## QTRAN(): istep=20, icoun=13 ## QTRAN(): istep=40, icoun=14 ## KMNS(*, k=10): iter= 2, indx=20  plot(points, col = cl$cluster) points(cl$centers, col = 1:2, pch = 8, cex = 2)  cl  ## K-means clustering with 10 clusters of sizes 3, 3, 5, 1, 2, 1, 1, 1, 1, 2 ## ## Cluster means: ## x y ## 1 30.33 128.3 ## 2 56.67 129.3 ## 3 45.80 92.6 ## 4 35.00 63.0 ## 5 52.50 61.5 ## 6 42.00 57.0 ## 7 23.00 40.0 ## 8 64.00 37.0 ## 9 33.00 22.0 ## 10 52.50 25.0 ## ## Clustering vector: ## [1] 1 2 2 1 2 1 3 3 3 3 3 4 5 5 6 7 8 10 9 10 ## ## Within cluster sum of squares by cluster: ## [1] 559.3 359.3 900.0 0.0 17.0 0.0 0.0 0.0 0.0 62.5 ## (between_SS / total_SS = 94.4 %) ## ## Available components: ## ## [1] &amp;quot;cluster&amp;quot; &amp;quot;centers&amp;quot; &amp;quot;totss&amp;quot; &amp;quot;withinss&amp;quot; ## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot; &amp;quot;size&amp;quot; &amp;quot;iter&amp;quot; ## [9] &amp;quot;ifault&amp;quot;  运行结果和问题的四个选择项都配不上。kmeans做了两轮计算，返回的是最终结果，而第一轮结果即使加了trace参数也弄不出来。最后只好默默抄起了Java，很受伤:(
</content>
    </entry>
    
     <entry>
        <title>[社交网络分析课] 社区结构</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90_%E7%A4%BE%E5%8C%BA%E7%BB%93%E6%9E%84/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>sna</tag><tag>coursera</tag><tag>笔记</tag><tag>社区</tag>
        </tags>
        <content type="html">  本文为Social NetworkAnalysis学习笔记，课程地址为https://www.coursera.org/course/sna 。
为什么关注群体凝聚力？ 观点的形成和一致性：如果节点采用邻居节点的主流观点，就有可能不同有凝聚力的子组有不同的观点。 什么导致社区的形成？  联系的相互关系(mutuality of ties)：组内每个人都认识其他人。 成员间联系的频率：组内每个人都同组内的其他人存在至少k个连接。 子组成员的亲密性或可达性：个体间被最多n跳分割。 子组成员间相对非成员的相对联系频率。  归属网络(Affiliation networks)  成员网络，例如董事会 超网络或超图(hypergraph) 二分图(bipartite graphs) 连锁网络?(interlocks)  小集团/帮派(cliques) 组内的每个成员都同其他成员有连接。小集团可以重叠。 小集团的意义  不健壮：缺失一个连接将毁掉小集团 无趣：每个人都同其他人连接；没有核心-边缘结构；无法使用中心性指标 小集团如何重叠比自身更有趣  k-cores 每个节点同组内k个其他节点相连。与小集团相比，简单且不那么严格。 即使如此，对于识别自然社区还是过于严苛的要求。 n – cliques 基于可达性和直径进行子组划分。子组内每两个节点最大距离为n。 问题： - 直径可能大于n - n-clique可能是断开的(路径通过子组外的节点进行)
解决方法：n-club:直径2的最大子图 {1,2,3,4},{1,2,3,5} and {2,3,4,5,6}
p-cliques 将网络分割成子组，其中节点至少有概率p的邻居节点在子组内。 有向加权网络的凝聚力 查找强连通分量 在查找强连通分量仅保留部分联系：相互的联系；边权重大于一定阀值。
分层聚类 处理流程： - 计算所有节点对的距离 - 从所有断开的n个节点开始 - 为了减少权重，为节点对逐个添加边 - 结果：递归分量，可在树的任一级切片
介数聚类 算法： - 计算所有边的介数 - 当(任一边的介数大于阀值)： - 移除带有最高介数的边 - 重新计算介数
介数在每一步都需要重新计算 - 移除一个边会对其他边的介数造成影响 - 代价昂贵：所有节点对最短路径– O(N3) - 可能需要重复N次 - 即使使用最快的算法也无法扩大到几百个节点的网络
移除带有最高介数的边后，网络被分隔成分量。 模块度（Modularity） 考虑了社区内的边和社区与网络其他部分之间的边 模块度定义： 假设有两个node v和w，它们的度数分别是Kv和Kw，那么如果随机的话，两个节点间有一条边的概率是(kv * kw)/2m。其中2m是所有节点的度数之和。 对于随机网络，Q=0。
算法： - 起始将所有节点当作鼓励的对待 - 执行贪婪策略： - 一个接一个地连接带有模块度最大增长DQ的聚类 - 当连接任意两个聚类时最大的DQ&amp;lt;=0时停止
参考 Cohesive Subgroups
Community Detection – Modularity的概念
</content>
    </entry>
    
     <entry>
        <title>[数据分析与统计推断] 多元线性回归</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD_%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据分析</tag><tag>统计推断</tag><tag>coursera</tag><tag>笔记</tag><tag>多元线性回归</tag>
        </tags>
        <content type="html">  本文为Data Analysis and StatisticalInference学习笔记，课程地址为https://www.coursera.org/course/statistics。
adjusted R squared 共线性(collinearity)：当两个预测变量彼此相关就可以说共线的。预测变量也称为独立变量，因此它们应该彼此独立。引入共线的预测变量(多重共线性，multicollinearity)使模型估计更加复杂。 简约法(parsimony)：避免添加互相关联的预测变量，因为这些变量不会带来新信息；优选最简单最好的模型，例如精简模型；加入共线的变量会导致回归参数有偏估计；当无法避免观察数据中的共线性，经常设计实验来控制关联的预测变量。
多元线性回归推断 模型选择 逐步模型选择 - 后向消除：从全模型(包含所有预测变量)开始，每次丢弃一个预测模型，直到获得精简模型 - 前向选择：从空模型开始，每次添加一个预测模型，直到获得精简模型
标准： 调整变量的标准有两种： 1. 使adjusted R square最大化； 2. 使p-value最小化
如果存在反映同一个性质的一组分类变量，要么全部保留，要么全部去除，不可以保留不完整的一部分 诊断 (1)(数字变量)x和y的线性关系 每个(数字)解释变量线性关联响应变量； 使用残差图检查残差是否随机分布0附近；不使用x和y的散点图，因为这样可以考虑模型内的其他变量，而不是仅仅x和y之间的二元变量关系。 (2)近似均值为0的正态分布残差 某些残差为正，某些为负；在残差图中残差随机分布0附近； 使用直方图或正态概率图检查是否近似均值为0的正态分布残差 (3)残差的恒定可变性 残差应该对预测的响应变量的低值和高值有相等的可变性 使用residuals vs. predicted残差图检查： residuals vs. predicted而不是residuals vs.x，允许一次考虑(带有所有解释变量的)整个模型；残差随机分布在0附件的常量带内；值得观察residuals vs.predicted的绝对值，轻松识别异常观察值。 (4)独立的残差 独立残差-&amp;gt;独立观察值 如果是可疑的时间序列结构，对比检查残差和数据采集顺序，否则，思考数据如何采样的。 </content>
    </entry>
    
     <entry>
        <title>igraph包的cliques函数总也不返回</title>
        <url>https://mryqu.github.io/post/igraph%E5%8C%85%E7%9A%84cliques%E5%87%BD%E6%95%B0%E6%80%BB%E4%B9%9F%E4%B8%8D%E8%BF%94%E5%9B%9E/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>igraph</tag><tag>cliques</tag><tag>hang</tag><tag>R语言</tag>
        </tags>
        <content type="html"> 做社交网络分析课的作业时碰到一个小麻烦，igraph包的cliques函数总也不返回，最后只能强行终止但是数据量也不大，而且largest.cliques和clique.number都是立刻返回，不解呀！
&amp;gt; library(igraph) &amp;gt; g = read.graph(&amp;quot;wikipedia.gml&amp;quot;,format=&amp;quot;gml&amp;quot;) &amp;gt; cliques(as.undirected(g)) &amp;gt; largest.cliques(as.undirected(g)) [[1]] [1] 26526 247 370 2119 6625 7826 8277 10019 11773 11801 13289 15758 [13] 16845 16885 16937 18925 22144 22644 23318 24585 24654 25487 &amp;gt; clique.number(as.undirected(g)) [1] 22  </content>
    </entry>
    
     <entry>
        <title>[社交网络分析课] 网络中心性</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90_%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%BF%83%E6%80%A7/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>sna</tag><tag>coursera</tag><tag>笔记</tag><tag>网络中心性</tag>
        </tags>
        <content type="html">  本文为Social NetworkAnalysis学习笔记，课程地址为https://www.coursera.org/course/sna 。
对于中心性(centrality)的不同观点 在下面每一个网络中，X都相对Y具有更高的中心性。 定量度中心性 在每个节点上标注节点度。例如，拥有朋友越多的节点其中心性越高。 其标准化就是用节点度除以最大的连接可能(N-1)。 集中度(centralization) 节点间的集中度分数存在多少差异？Freeman的集中度通用公式(可使用其他指标，如基尼系数或标准差) 度集中度示例： 介数(betweenness) 直观：多少个节点对必须经由本节点实现最小跳数互达。 定义： 非标准化版本示例1： 非标准化版本示例2： 非标准化版本示例3： 非标准化版本示例4： 经由B的最短路径有(A,C)、(A,E)、(A,D)，此外(C,D)的最短路径分配给B和E各0.5，因此B为3.5，E为0.5。
紧密性(closeness) 紧密性基于节点与网络所有其他节点的平均最短路径长度计算而得。 示例： 特征向量中心性(eigenvector centrality) 当前节点的中心性取决于邻居节点们的中心性。 Bonacich特征向量中心性 - a是一标准化常数 - b决定邻居节点对中心性的重要性 - A是相邻矩阵(可被权重) - I是单位矩阵 - 1是全一的矩阵
衰减因子b： 小b -&amp;gt; 高衰减：仅直接朋友起作用，且其重要性仅被考虑进去一点点。 高b -&amp;gt; 低衰减：全局网络(朋友，朋友的朋友等等)起作用 =0 产生简单的度中心性 如果b&amp;gt;0,节点连接其他中心节点具有更高中心性。 如果b&amp;lt;0, 节点连接其他非中心节点具有更高中心性。 示例： 有向网络的介数中心性 唯一的修改：当标准化时将使用(N-1)(N-2)而不是(N-1)(N-2)/2。 有向网络的紧密性中心性 选择一个方向：入紧密性、出紧密性。
有向网络的特征向量中心性 讲了pagerank和teleport。
中心性应用 以Java论坛为例，介绍了各种分析。
幂定律分布 在log-log图中是一条直线：ln(p(x))=c-aln(x) 参考 中心性(centrality)
Bonacich’s Centrality
</content>
    </entry>
    
     <entry>
        <title>A Short Tutorial on Graph Laplacians, Laplacian Embedding, and Spectral Clustering</title>
        <url>https://mryqu.github.io/post/a_short_tutorial_on_graph_laplacians_laplacian_embedding_and_spectral_clustering/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>拉普拉斯矩阵</tag><tag>laplacian</tag><tag>谱聚类</tag>
        </tags>
        <content type="html"> http://csustan.csustan.edu/~tom/Lecture-Notes/Clustering/GraphLaplacian-tutorial.pdf
</content>
    </entry>
    
     <entry>
        <title>[数据分析与统计推断] 线性回归</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据分析</tag><tag>统计推断</tag><tag>coursera</tag><tag>笔记</tag><tag>线性回归</tag>
        </tags>
        <content type="html">  本文为Data Analysis and StatisticalInference学习笔记，课程地址为https://www.coursera.org/course/statistics 。
相关性(correlation) 相关性描述了两个变量之间线性关联的强度，表示符号为R。属性： - 相关系数的幅度(绝对值)测量两个数字变量之间线性关联的强度 - 相关系数的正负指示关联的方向 - 相关系数总是介于-1(完美负线性关联)和1(完美正线性关联)之间，R=0指示没有线性关系 - 相关系数没有单位，不受变量中心点和比例变化的影响 - X与Y的相关性等同于Y与X的相关性 - 相关系数对异常点敏感
残差(residuals) 测量线性匹配度 方案1：最小化残差的幅度和 方案2：最小化残差平方和。更常使用，易于计算，残差增幅更大。 使用线性模型对解释变量给定值预测响应变量值称之为预测(prediction)。 使用模型估计原有数据域外的值称之为外推法(extrapolation)。优势预测可能是外推法。 线性回归条件 (1)线性 解释变量和响应变量值间的关系必须是线性的； 存在匹配非线性关系模型的方法； 使用数据散点图scatterplot或残差图residuals plot检查 (2)近似正态残差 残差必须近似正态分布，中心点为0；如果有异常观察值不遵循正常数据的趋势，有可能不满足该条件使用残差的直方图或正态概率图检查(3)恒定可变性 点围绕最小平方和线(the least squaresline)的可变性应该大概恒定，暗指残差围绕0的可变性应该大概恒定，这也称为同方差性(homoscedasticity)。 使用残差图residuals plot检查 R squared 评估线性模型拟合度更常使用，通过相关系数平方计算而得； 可以获知线性模型解释响应变量可变性的百分比，剩余可变性无法由模型解释； 介于0和1之间。
使用分类解释变量的回归 异常点类型 线性回归推断 由于我们经常检查解释变量和响应变量之间是否存在关系，对斜率虚假设值经常为0；很少对截距进行推断。 对线性回归的每个估计参数都会损失一个自由度。 我们必须了解所工作的数据：随机样本、非随机样本或总体。如果已有总体数据，假设推断及其p-value结果就毫无意义。如果样本是非随机（有偏）的，结果将不可信。 变异分解(variability partitioning) t检验是评估x和y线性关系斜率假设检验的证据力度的一种方式。将y可变性分解为可解释和无法解释的可变性，需要使用方差分析ANOVA。 再学习 R sqared Rsqared是模型可以解释y可变性的比例。很大，即x和y之间存在线性关系；小，则x和y之间存在线性关系的证据不令人信服。 两种结算方式：相关性，相关系数平方；定义，总可变性中可解释可变性的比例。 </content>
    </entry>
    
     <entry>
        <title>[社交网络分析课] 随机网络模型</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90_%E9%9A%8F%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>sna</tag><tag>coursera</tag><tag>笔记</tag><tag>随机网络模型</tag>
        </tags>
        <content type="html">  本文为Social NetworkAnalysis学习笔记，课程地址为https://www.coursera.org/course/sna。
网络模型 为什么建立网络模型？ - 复杂网络的简单表达 - 可以通过数学推导出属性 - 可以预测属性和输出 - 可以对比真实网络与假设模型的不同之处，从中获得顿悟
随机图模型：Erdös-Renyi模型 假设  节点随机连接 网络是无向网络  除了节点数N之外的关键参数：p或M  p=任意两个节点连接的概率 M=图中边的总数  度分布 (N,p)模型：对每个潜在的边，概率p添加边，概率(1-p)不添加边。 每节点边数 每个节点最多可能（N-1）条边，每条边以概率p的可能性存在。二项分布提供了节点具有度K的概率： 度的平均数为：z = (n-1)*p度的方差为：s2=(n-1)p(1-p)近似值：ER图模型认知 社交中心(hub) 该模型下无法获得大的社交中心。
巨分量 上述实验中，每个点有可变的概率P表示渗透。当P等于1/2的时候，图中会出现一个明显的巨分量。
平均最短路径 每对节点的平均跳数是多少？假如你的朋友与z（=度平均值）个你的其他朋友相连，忽略回路的话与你距离l跳的人有zl个。 介绍模型(Introduction Model) 任意两个节点连接的概率prob-link仍然是p，此外增加了连接邻居节点的邻居节点概率prob-intro，不再完全随机。 与ER模型相比，介绍模型具有： - 更多的闭三点组（朋友将他的朋友介绍给你） - 更长的平均最短距离（边更局部连接） - 更多不均匀的度分布（有更多连接的节点更容易增加边） - 低概率p的情况更小的巨分量
静态地理模型(Static Geographical model) 每个节点与最接近的节点中的num-neighbors个相连。 与ER模型相比，静态地理模型具有： - 更窄的度分布(每个节点邻居节点目标一样，num-neighbors) - 更长的平均最短距离（地理局部连接） - 更小的巨分量(更少的机会进行随机连接)
随机遭遇模型(Random encounter) 节点随机移动，与偶遇的节点相连。 与ER模型相比，随机遭遇模型具有： - 更长的平均最短距离 - 更小的巨分量 - 更多的闭三点组
生长模型(Growth model) 不是一开始就是固定数目节点，节点数随着时间增加。 与ER模型相比，生长模型具有： - 很多节点度为1 - 没有很多闭三点组 - 更小的巨分量 - 老节点比新节点有更多的边
真实网络度分布 幂律分布分布 度值为k的概率：p(k)= C * k ^ (-α)。C是标准化常数以使所有k的概率和为1。log(p(k)) = c - α *log(k)。
泊松分布 偏好性连接模型（preferential attachment） 节点更倾向与有更多边的节点连接，也被称为积累优势、富者愈富、马太效应。偏好性连接加剧了度分布的偏斜，使其具有幂律特性，指数α =2&#43;1/m
Barabasi-Albert模型 开始用于描述万维网倾斜度分布。每个节点同其他节点以和对方度正比的概率相连。 - 该过程起始有某个初始子图 - 每个新节点将会有m个边 - 连接节点i的概率结果是幂律分布其指数α =3
BA模型属性  分布是尺度无关的，其指数α =3P(k) = 2 m2/k3 图是连通的。一开始的图是连通的， 新节点一出现就会有一到数个边（取决于m），它会同老节点相连，同时也会通过介绍与其他节点相连 老节点更富有节点随着时间积累连接，从而使老节点更有优势，因为新节点偏好性连接而老节点会有更高的度值。  BA模型中新老节点的对比 </content>
    </entry>
    
     <entry>
        <title>创建MySQL表失败，“show tables”命令显示表存在却无法删除</title>
        <url>https://mryqu.github.io/post/%E5%88%9B%E5%BB%BAmysql%E8%A1%A8%E5%A4%B1%E8%B4%A5show_tables%E5%91%BD%E4%BB%A4%E6%98%BE%E7%A4%BA%E8%A1%A8%E5%AD%98%E5%9C%A8%E5%8D%B4%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mysql</tag><tag>error1146</tag><tag>42s02</tag><tag>创建</tag><tag>issue</tag>
        </tags>
        <content type="html"> 在MySQL表中创建一个表table_c失败了，返回错误ERROR 1146(42S02)；结果发现MySQL显示有这个表，却无法查询和删除。```
mysql&amp;gt; create table table_c (&amp;hellip;&amp;hellip;&amp;hellip;); ERROR 1146 (42S02): Table &amp;lsquo;yqutesting.table_c&amp;rsquo; doesn&amp;rsquo;t exist mysql&amp;gt; show tables; &#43;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;&#43; | Tables_in_yqutesting | &#43;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;&#43; | table_a | | table_b | | table_c | &#43;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;&#43; 3 rows in set (0.00 sec)
mysql&amp;gt; select * from table_c; ERROR 1146 (42S02): Table &amp;lsquo;yqutesting.table_c&amp;rsquo; doesn&amp;rsquo;t exist mysql&amp;gt; drop table table_c; ERROR 1051 (42S02): Unknown table &amp;lsquo;table_c&amp;rsquo;
``` 结果还是drop掉yqutesting数据库，修正了table_c的定义重新创建数据库和所有表完事。 参考：MySQL Create Table Error - Table Doesn&amp;rsquo;t ExistMySQL &amp;gt; Table doesn&amp;rsquo;t exist. But it does (or it should)Can&amp;rsquo;t create table, but table doesn&amp;rsquo;t existHowto: Clean a mysql InnoDB storage engine?
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] Map Reduce Slot</title>
        <url>https://mryqu.github.io/post/hadoop_map_reduce_slot/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>slot</tag>
        </tags>
        <content type="html">  MR1 在MR1中，每个节点可以启动的并发map和reduce任务数(即slot数)由管理员通过mapred-site.xml中mapred.tasktracker.map.tasks.maximum (MR2中为mapreduce.tasktracker.map.tasks.maximum )和mapred.tasktracker.reduce.tasks.maximum (MR2中为mapreduce.tasktracker.reduce.tasks.maximum )配置指定。(下面的参考帖子提到过作业级参数mapred.map.tasks.maximum和mapred.reduce.tasks.maximum，但是在HADOOP-4295并没有通过。)
此外，管理员通过mapred.child.配置设置mapper或reducer默认的内存分配量。
</content>
    </entry>
    
     <entry>
        <title>[社交网络分析课] 网络介绍</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90_%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>sna</tag><tag>coursera</tag><tag>笔记</tag>
        </tags>
        <content type="html">  本文为Social NetworkAnalysis学习笔记，课程地址为https://www.coursera.org/course/sna。
网络定义 网络就是由边(edge)所连接的一些点(node)。
&#34;网络&#34;≡&#34;图&#34;
点线verticesedges, arcs数学nodeslinks计算机科学sitesbonds物理学actorsties, relations社会学![社交网络分析：网络介绍](/images/2014/10/0026uWfMgy6O7qbbpnuec.png) 边 方向： - 单向的 A-&amp;gt;B (例如A likesB; A gave agift to B; Ais B’s child) - 双向的 A&amp;lt;-&amp;gt;B 或 A-B (例如A and Blike each other; A and B aresiblings; Aand B are co-authors)
属性： - 权重(例如通信频率) - 排名(第一好友、第二好友…) - 类型(朋友、亲戚、同事) - 取决于剩余图结构的属性：例如betweenness
图的数据表达方式 |相邻矩阵(adjacency matrix)|边列表
(edge list)|相邻列表(adjacency list) |&amp;mdash; ||2, 3
2, 4
3, 2
3, 4
4, 5
5, 2
5, 1|如果网络很大或疏松的话，相邻列表易于实现。
可以为某个节点快速获取所有邻居节点。
1:
2: 3 4
3: 2 4
4: 5
5: 1 2
网络指标：节点度 (degree) 节点度是指和该节点相关联的边的个数。特别地，对于有向图，节点的入度是指进入该节点的边的个数；节点的出度是指从该节点出发的边的个数。
网络指标：节点度序列 每个节点的(入、出)度有序列表。
|度列表|图 |&amp;mdash; |入度序列:[2, 2, 2, 1, 1, 1, 1, 0]
出度序列:[2, 2, 2, 2, 1, 1, 1, 0]
度序列:[3, 3, 3, 2, 2, 1, 1, 1]|网络指标：节点度分布 每个节点度分布频度。
|度分布|图 |&amp;mdash; |入度分布:[(2,3) (1,4) (0,1)]
出度分布:[(2,4) (1,3) (0,1)]
度分布:[(3,3) (2,2) (1,3)]|网络指标-连通分量(connectedcomponent) 强连通分量：分量中每个节点都可以通过有向链接彼此可达。 示例中存在的强连通分量： - B C D E - A - G H - F
弱连通分量：分量中每个节点都可以通过任意方向链接彼此可达。 示例中存在的弱连通分量： - A B C D E - G H F
无向图中一般简单称之为连通分量。
巨分量(Giant component) 如果最大的分量占整个图的显著部分，可被称为巨分量。 </content>
    </entry>
    
     <entry>
        <title>[数据分析与统计推断] 分类变量推断</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD_%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%E6%8E%A8%E6%96%AD/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据分析</tag><tag>统计推断</tag><tag>coursera</tag><tag>笔记</tag><tag>分类变量推断</tag>
        </tags>
        <content type="html">  本文为Data Analysis and StatisticalInference学习笔记，课程地址为https://www.coursera.org/course/statistics。
抽样可变性 用于概率的大树极限定理 概率的置信区间 计算符合误差幅度的样本个数 概率的假设检验 两个概率差的置信区间 两个概率差的假设检验 小样本概率 &amp;amp; 比较两个小样本概率 模拟方法进行假设检验： - 假设检验的最终目标是p-value，P(observed or more extreme outcome | H0true) - 假定虚假设为真，设计模拟方法 - 多次重复模拟，记录相关样本统计量 - 计算倾向备选假设的模拟概率作为p-value。
卡方拟合度检验(chi-square GOF test) 拟合度(goodness of fit)检验：评估观察数据有多拟合期望分布单个类别变量，变量值大于2级 卡方检验的p-value 用于卡方检验的p-value定义为高于计算的检验统计量的尾部区域由于检验统计量总是正数，更高的检验统计量意味着与虚假设偏离更大。 卡方独立检验 独立检验：评估两个类别变量之间的关系两个类别变量，类别值大于两级 </content>
    </entry>
    
     <entry>
        <title>[社交网络分析课] SNA工具</title>
        <url>https://mryqu.github.io/post/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90_sna%E5%B7%A5%E5%85%B7/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>社交网络分析</tag><tag>sna</tag><tag>工具</tag><tag>gephi_netlogo_igraph</tag><tag>r_python_java_js</tag>
        </tags>
        <content type="html">  本文为Social NetworkAnalysis学习笔记，课程地址为https://www.coursera.org/course/sna。
Gephi - https://gephi.github.io/ - 用于网络、复杂系统和动态封层图形的交互式可视化及研究平台，支持度、介数、紧密性等网络中心性指标以及密度、路径长度、网络直径、模块度、集聚系数等指标，支持GDF(GUESS)、GraphML (NodeXL)、GML、NET (Pajek)、GEXF等文件格式。 - 开源，支持Windows、Linux和Mac OS X平台 - Gephi指南 - 使用Gephi可视化twitter网络 - Twitter上的埃及革命
NetLogo - https://ccl.northwestern.edu/netlogo/index.shtml - 多主体仿真建模工具。可用于模拟各种社会现象和自然现象，通过设置个体行为并使多个个体自由运行来研究个体行为对于复杂系统的影响和变化。 - 开源，支持Windows、Linux和Mac平台 - NetLogo帮助文档 - Lada的多个特定网络属性演示
iGraph - http://igraph.org/ - 网络分析工具库，侧重于执行效率、可移植性和易用性，可被R、Python和C/C&#43;&#43;调用。 - 开源，支持Windows、Linux和Mac OS X平台 - R iGraph帮助文档 - Python iGraph帮助文档 - C iGraph帮助文档
Pajek - http://pajek.imfm.si/doku.php - 网络分析和可视化工具，功能丰富，通过下拉菜单进行各种操作。 - 免费，支持Windows平台，也可以在Linux（64）和Mac平台上仿真（Wine）运行 - Pajek参考手册
UCINet - https://sites.google.com/site/ucinetsoftware/ - 社交网络数据分析软件包，功能丰富。 - 商业软件，支持Windows平台 - UCINet文档
NodeXL - http://nodexl.codeplex.com/ - 交互式网络可视化和分析工具，以MS Excel模板的形式利用MSExcel作为数据展示和分析平台。可以定制图像外观、无损缩放、移动图像，动态过滤顶点和边，提供多种布局方式，查找群和相关边，支持多种数据格式输入和输出。 - 开源，支持Windows Excel 2007/2010/2013 - NodeXL文档
其他 R  R的SNA库(见统计软件杂志上关于sna包的文章)：功能丰富，偏于统计 如果使用Gephi的话，可以看一下用于读写Gephi gexf图形文件的rgexf库。  Python  NetworkX：开源Python包，用于复杂网络的创建、操作和复杂网络的结构、动力学和功能方面的研究。 Sage：开源基于Web的数学计算环境，包含NetworkX以及自己的图形库。这里有三篇重要文档，通用图参考文档、 无向图参考文档 和 有向图参考文档。 graph-tool：高效python模块，用于图/网络的操作和统计分析。 Newt  Java  Jung（Java Universal Network/Graph Framework）：Java平台网络/图应用开发的一种通用基础架构。其目的在于为开发关于图或网络结构的应用程序提供一个易用、通用的基础架构。使用JUNG功能调用，可以方便的构造图或网络的数据结构，应用经典算法（如聚类、最短路径，最大流量等），编写和测试用户自己的算法，以及可视化的显示数据的网络图。 Neo4j：图数据库 Blueprints：类似于JDBC，但是用于图数据库 SoNIA（Social Network Image Animator）:用于动态或纵向数据的可视化。  JavaScript  D3.js： 数据驱动的文档（Data DrivenDocuments） JSNetworkX：NetworkX图形库在JavaScript上的移植版本。 arbor.js：使用webworkers和jQuery的图形可视化库。  社交网挖掘  社交网挖掘 GitHub 社交网挖掘 (第2版) GitHub  Talend  Talend: 构建GML图形  Twitter  Twitter4j Twitter4j Dev Forum TwitterAPI Python, Twitter and the 2012 French Presidential Election Mentionmapp Twiangulate Iteractive：适用于关注人数在340人以内  LinkedIn  LinkedIn Maps  </content>
    </entry>
    
     <entry>
        <title>[数据分析与统计推断] 数字变量推断</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD_%E6%95%B0%E5%AD%97%E5%8F%98%E9%87%8F%E6%8E%A8%E6%96%AD/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据分析</tag><tag>统计推断</tag><tag>coursera</tag><tag>笔记</tag><tag>数字变量推断</tag>
        </tags>
        <content type="html">  本文为Data Analysis and StatisticalInference学习笔记，课程地址为https://www.coursera.org/course/statistics。
两组数据比较 成对数据 独立数据 bootstrapping bootstrapping术语取自短句“pulling oneself up by one’sbootstraps”,暗指没有外力的情况下完成不可能实现的任务。统计中指使用给定样本中的数据估计总体参数。 1. 获得bootstrap样本 - 使用原样本有放回抽样获得相同大小的随机样本 2. 计算bootstrap统计量 - 通过bootstrap样本获得平均值、中值、概率等统计量 3. 多次重复步骤1和2以创建bootstrap分布 - bootstrap统计量的分布
估计方法： - 百分比方法：通过排除两端的数据，直接取bootstrap分布中间的95% - 标准差方法：先计算bootstrap分布的均值与标准差，再计算95%区间
限制： 没有类似于中心极限定理时的严格条件 如果bootstrap分布特别倾斜或稀疏，则可信度低需要一个可以概化的代表性样本。 若原样本有偏，则bootstrap结果很可能有偏
bootstrap vs. 抽样分布 抽样分布使用总体进行有放回抽样 Bootstrap分布使用样本进行有放回抽样 两者都是样本统计分布
t分布 当n比较小且方差σ未知，使用t分布解决标准差估计的不确定性。 t分布有一个参数：自由度(degrees of freedom, df)，决定分布尾部的厚度。自由度越大，越接近正态分布。 t统计用于推断σ未知且n &amp;lt; 30时的均值
基于小样本估计均值 R函数实例：
qt(0.025, df = 21) pt(2.30, df = 21, lower.tail = FALSE)  基于小样本比较均值 多均值比较 当比较两组均值时可以使用Z或T检验，当比较3&#43;组均值时需要使用方差分析(analysis ofvariance,ANOVA)和F检验。 总平方和(sum of squares total, SST) 估量响应变量的总可变性，跟方差计算相似（除了不除以样本数） 组平方和(sum of squares groups, SSG) 估量组间可变性，可解释的可变性：组均值与总均值之间样本数加权的偏差 误差平方和(sum of squares error, SSE) 估量组内可变性，不可解释的可变性：组变量由于其他原因无法解释 自由度 总自由度：总样本量减一组自由度：组样本量减一误差自由度：前两者相减 平均平方和 F统计 p-value计算 pf(21.735, 3, 791, lower.tail = FALSE) ANOVA条件 (1)独立性：抽样的观察值必须彼此独立，总是很重要，但有时很难检查随机样本/分配每组样本数小于总体的10%仔细思考组是否独立（例如，没有配对） (2)近似正态分布：当样本小时尤为重要每组内的响应变量分布必须近似正态分布 (3)恒定的方差：当每组样本个数变化大时尤为重要组之间的可变性必须一致：homoscedastic groups! 数据分析与统计推断：数字变量推断
多次比值 ANOVA可以判别多组均值是否相等，如果不相等，那如何判别每组均值是否相等？ 对可能的每两组进行t检验 多次检验将导致膨胀的第一类错误率 解决方案：使用修改过的显著水平
Bonferroni correction建议更严厉的显著水平更适合这类检验。 </content>
    </entry>
    
     <entry>
        <title>[Hadoop] 通过MultipleOutputs生成多输出文件</title>
        <url>https://mryqu.github.io/post/hadoop_%E9%80%9A%E8%BF%87multipleoutputs%E7%94%9F%E6%88%90%E5%A4%9A%E8%BE%93%E5%87%BA%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>multipleoutputs</tag><tag>源码分析</tag><tag>示例</tag>
        </tags>
        <content type="html">  即前一博文[Hadoop] 通过MultipleInputs处理多输入文件展示如何处理MapReduce多输入问题，本文将展示一下如何处理MapReduce多输出的方法。
MultipleOutputs示例 MultipleOutputsDemo.java源码 Scores.txt Tomas,100 Edward,81 Henry,59 Gordon,60 James,97 Percy,93 Toby,77 Emily,87 Duke,68 Donald,47 Douglas,35  执行 hadoop jar YquMapreduceDemo.jar MultipleOutputsDemo /user/hadoop/mos_input/scores.txt /user/hadoop/mos_output  测试结果 MultipleOutputs分析 普通Driver |API|Job属性 |&amp;mdash;&amp;ndash; |Job.setOutputFormatClass|mapreduce.job.outputformat.class示例：org.apache.hadoop.mapreduce.lib.output.TextOutputFormat |Job.setOutputKeyClass|mapreduce.job.output.key.class示例：org.apache.hadoop.io.Text |Job.setOutputValueClass|mapreduce.job.output.value.class示例：org.apache.hadoop.io.IntWritable
使用MultipleOutputs的Driver |API|Job属性 |&amp;mdash;&amp;ndash; |MultipleOutputs.addNamedOutput|mapreduce.multipleoutputs
示例：pass fail
mapreduce.multipleoutputs.namedOutput.pass.format
示例：org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
mapreduce.multipleoutputs.namedOutput.pass.key
示例：org.apache.hadoop.io.NullWritable
mapreduce.multipleoutputs.namedOutput.pass.value
示例：org.apache.hadoop.io.Text
mapreduce.multipleoutputs.namedOutput.fail.format
示例：org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
mapreduce.multipleoutputs.namedOutput.fail.key
示例：org.apache.hadoop.io.NullWritable
mapreduce.multipleoutputs.namedOutput.fail.value
示例：org.apache.hadoop.io.Text
通过调用org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write方法，根据相应NamedOutput相应的OutputFormat、OutputKeyClass和OutputValueClass创建NamedOutput自己的RecordWriter，完成相应的输出。
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 通过MultipleInputs处理多输入文件</title>
        <url>https://mryqu.github.io/post/hadoop_%E9%80%9A%E8%BF%87multipleinputs%E5%A4%84%E7%90%86%E5%A4%9A%E8%BE%93%E5%85%A5%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>multipleinputs</tag><tag>源码分析</tag><tag>示例</tag>
        </tags>
        <content type="html">  一般MapReduce程序仅处理一个输入文件，但当我们必须处理多个输入文件时，普通MapReduce方法就无能为力了，这时候可以使用org.apache.hadoop.mapreduce.lib.input.MultipleInputs类搞定这一问题。
MultipleInputs示例 MultipleInputsDemo.java源码 people.txt 1,Tomas,1 2,Edward,2 3,Henry,3 4,Gordon,4 5,James,4 6,Percy,3 7,Toby,2 8,Emily,1 9,Duke,3 10,Donald,3 11,Douglas,3  locations.txt 1,China 2,USA 3,Canada 4,New Zealand  执行 hadoop jar YquMapreduceDemo.jar MultipleInputsDemo /user/hadoop/mijoin/people.txt /user/hadoop/mijoin/locations.txt /user/hadoop/mijoin_output  测试结果 MultipleInputs分析 与普通Driver的区别 普通Driver |API|Job属性 |&amp;mdash; |FileInputFormat.addInputPath|mapreduce.input.fileinputformat.inputdir
示例：/user/hadoop/wordcount/book.txt |Job.setMapperClass|mapreduce.job.map.class
示例：WordCount.TokenizerMapper |Job.setInputFormatClass|mapreduce.job.inputformat.class
示例：org.apache.hadoop.mapreduce.lib.input.TextInputFormat
使用MultipleInputs的Driver APIJob属性MultipleInputs.addInputPathmapreduce.input.multipleinputs.dir.formats
示例：/user/hadoop/mijoin/people.txt:o.a.h.m.l.i.TextInputFormat,
/user/hadoop/mijoin/locations.txt:o.a.h.m.l.i.TextInputFormatmapreduce.input.multipleinputs.dir.mappers
示例：/user/hadoop/mijoin/people.txt:MultipleInputsDemo.PersonMapper,
/user/hadoop/mijoin/locations.txt:MultipleInputsDemo.LocationMappermapreduce.job.inputformat.class
示例：org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormatmapreduce.job.map.class
示例：org.apache.hadoop.mapreduce.lib.input.DelegatingMapper 由上可见，MultipleInputs方法不设置mapreduce.input.fileinputformat.inputdir属性，将mapreduce.job.inputformat.class和mapreduce.job.map.class属性设为多输入的委托类，增加了两个专用的属性mapreduce.input.multipleinputs.dir.formats和mapreduce.input.multipleinputs.dir.mappers已用于映射每一输入文件的格式和mapper类。
调用每个输入文件的FileFormat 调用每个输入文件的Mapper 示例流程 </content>
    </entry>
    
     <entry>
        <title>[数据分析与统计推断] 大数极限定理与假设检验</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD_%E5%A4%A7%E6%95%B0%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86%E4%B8%8E%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据分析</tag><tag>统计推断</tag><tag>大数极限定理</tag><tag>假设检验</tag><tag>coursera</tag>
        </tags>
        <content type="html">  本文为Data Analysis and StatisticalInference学习笔记，课程地址为https://www.coursera.org/course/statistics。
采样可变性 中心极限定理 样本统计分布趋向于正态分布，其均值接近总体均值，标准差为原总体标准差除以样本量的平方根CLT条件： - 独立：样本观察值必须独立 - 随机抽样/分配 - 对于无放回抽样，样本个数n必须小于总体的10%，太大以后样本中的数据很难相互独立 - 样本大小/偏斜度：或者总体分布是正态分布，或者总体分布是偏斜的但样本很大(经验法则：n&amp;gt;30)
置信区间(confidence interval) 总体参数值的可能范围置信区间=点估计±误差幅度(误差边际,margin of error) 总体均值的置信区间
z*: 90% 1.65 95% 1.96 98% 2.33 99% 2.58  置信区间增大，宽度增大，准确度变大，精确度变小。唯有增加样本大小，可以获得高准确度和高精确度。 假设检验  基于置信区间的假设检验：判断现状是否存在于置信区间内 基于p-value的假设检验：p-value = P(observed or more extreme outcome | H0 true)使用测试统计计算p-value，虚假设为真的情况下当前数据集倾向备择假设的概率。如果p-value低（低于显著水平α，通常为5%），如果虚假设为真的情况下很难观测到（样本这样的）数据，因此拒绝H0。如果p-value高（高于α），即使虚假设为真也能观测到（样本这样的）数据，因此不拒绝H0。 双边检验(two-sided test):置信区间两边都纳入考虑单边与双边检验，p-value的定义相同，但计算方式有不同  决策失误：  第一类错误(Type 1 error)是当H0为真时拒绝H0 &amp;ndash; 错杀好人。 第二类错误(Type 2 error)是当HA为真时拒绝H0失败 &amp;ndash; 错放坏人。  P(Type 1 error | H0 true) = α，显著性水平(significance level,α)越高，犯第一类错误的可能性越大。 如果第一类错误代价高，选择小的显著水平(例如0.01)；如果第二类错误代价高，选择高的显著水平(例如0.10) 第二类错误发生概率β计算不太直接，但是它取决于效应值(effect size, δ), 点估计与nullvalue之间的差。
置信区间与假设检验的约定 带有门限值α双侧假设检验等同于置信区间CL = 1 - α 带有门限值α单侧假设检验等同于置信区间CL = 1 - (2 x α) statistical vs. practical significance 点估计和null value之间的真实差异更容易在大样本情况下检测到。 然而，太大的样本会导致很小的效应值（样本均值-null value）获得很大的统计显著性，即使其实际并不显著。 </content>
    </entry>
    
     <entry>
        <title>使用微软的机器学习云Azure ML进行预测分析</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8%E5%BE%AE%E8%BD%AF%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BA%91azure_ml%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>云分析</tag><tag>azure</tag><tag>ml</tag><tag>机器学习</tag>
        </tags>
        <content type="html"> 今天看了一个帖子Predictive Analytics with Microsoft Azure Machine Learning。尽管机器学习已经是一项历史悠久而且应用广泛的技术，微软以云服务形式推出希望获得一些市场。AzureML通过拖拽操作在界面上组织数据清理、训练模型、模型打分和评估，最后可以生成C#、R或Python代码。 对云数据进行数据分析，除了用Hadoop/Spark等技术自己搭积木开发实现外，这种通用分析产品还不够丰富。期望早日看到各厂家纷纷推出成熟的旗舰产品那一天，我司（SAS）加油！：）
</content>
    </entry>
    
     <entry>
        <title>[数据分析与统计推断] 概率分布</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据分析</tag><tag>统计推断</tag><tag>概率分布</tag><tag>coursera</tag><tag>笔记</tag>
        </tags>
        <content type="html">  本文为Data Analysis and StatisticalInference学习笔记，课程地址为https://www.coursera.org/course/statistics。
概率规则  事件A或B发生概率： P(A or B) = P(A) &#43; P(B) - P(A and B) 事件A和B同时发生概率： P(A and B) = P(A|B) x P(B) 条件概率：P(A|B) = P(A and B) / P(B) 不相交事件(disjoint, mutually exclusive)：事件A和B不会同时发生 p(A and B)=0,p(A or B)=p(A)&#43;p(B), p(A|B)=0 互补事件(complementary)：概率和为1的不相交事件 p(A and B)=0, p(A|B)=0, p(A orB)=1, p(A)&#43;p(B)=1 独立事件(independent)：事件A是否发生对事件B没有影响 p(A|B)=p(A), p(A andB)=p(A)p(B)  概率分布 列出样本空间所有输出及其概率
概率树 按照多个变量层层写出条件概率 贝叶斯推断 利用先验信息，例如以前发布的研究成果或物理模型当收集数据时自然集成，并更新先验信息避免反直觉的p-value定义：P(observed or more extreme outcome | H0 istrue)而是基于后验概率做判决：P(hypothesis is true | observed data)好的先验信息有帮助，坏的先验信息有损害。但是先验信息跟能收集到更多数据相比不重要。更高级的贝叶斯技术提供了频率模型无法表示的灵活度 过程： - 设定先验信息 - 收集数据 - 获得后验信息 - 使用后验信息更新先验信息
贝叶斯检验是一个迭代过程，课程中以疾病诊断为例探讨了这一过程。根据经典的概率错觉，对于发病率很低的疾病，即使诊断本身作用在发病人群中正确率很高，在整个人群上作用时也会极差，那么这时候针对第一次诊断阳性的人群再迭代进行贝叶斯概率的计算，由于初始条件改变，正确率就能提高了
正态分布 http://bitly.com/dist_calc R:
&amp;gt; 1-2*pnorm(1, mean = 0, sd = 1, lower.tail = FALSE) [1] 0.6826895 &amp;gt; 1-2*pnorm(2, mean = 0, sd = 1, lower.tail = FALSE) [1] 0.9544997 &amp;gt; 1-2*pnorm(3, mean = 0, sd = 1, lower.tail = FALSE) [1] 0.9973002  标准差计分法 standardized (Z) score：观察值落在平均值之上或之下标准差的个数
Z = (observation - mean)/SD  异常观察值: |Z| &amp;gt; 2 百分位数percentiles：观察值落在低于某个数据点的百分比 正态概率图(normal probability plot): Y轴数据值，X轴正态分布的理论分位数
par(mfrow=c(1,2)) yqu &amp;lt;- rnorm(100, mean=10, sd=10) hist(yqu) qqnorm(yqu); qqline(yqu)  二项分布 伯努利随机变量(Bernoulli random variable)：单个实验仅有两种可能输出 二项分布(binomial distribution)：描述n个独立的成功概率为p的伯努利实验获得k次成功的概率R函数：dbinom二项分布条件： - 实验必须独立 - 实验次数n必须固定 - 每次实验输出必须分类为成功或失败 - 每次实验的成功概率p必须相同</content>
    </entry>
    
     <entry>
        <title>将Ultraedit集成到Windows文件资源管理器</title>
        <url>https://mryqu.github.io/post/%E5%B0%86ultraedit%E9%9B%86%E6%88%90%E5%88%B0windows%E6%96%87%E4%BB%B6%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>ultraedit</tag><tag>文件资源管理器</tag><tag>菜单</tag>
        </tags>
        <content type="html"> 最近自己的公司电脑换新的了，IT同事给装好了Ultraedit，可是安装时没有在Windows文件资源管理器的菜单加入Ultraedit项，使用起来不便。在配置里设一下，搞定！ </content>
    </entry>
    
     <entry>
        <title>从TortoiseGit切换到Git Extensions</title>
        <url>https://mryqu.github.io/post/%E4%BB%8Etortoisegit%E5%88%87%E6%8D%A2%E5%88%B0git_extensions/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>tortoisegit</tag><tag>gitextensions</tag><tag>gui</tag><tag>msysgit</tag>
        </tags>
        <content type="html"> 一开始使用GitHub的服务，除了安装了msysGit&amp;rsquo;s Git for Windows，GUI客户端就是GitHub的Web界面。后来使用bitbucket的服务时，想要安装一个好使又通用的GUI客户端。由于使用TortoiseCVS和TortoiseSVN，自然就选择了TortoiseGit。由于三者界面风格一致，使用无障碍上手。最近开始使用公司的git仓库，公司指定的是Git Extensions。好吧，Bye， TortoiseGit。 好奇了一下，上网搜了两者的对比。貌似相当一部分人使用TortoiseGit的理由跟我一样，不过有一些人更推荐Git Extensions。此外还有一个商业产品SmartGit，但貌似使用的人更少。搜索结果：What is the Best Git GUI (Client) for Windows?TortoiseGit vs Git ExtensionsAsk HN: Best Git GUI For Windows?
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] MapReduce定制Counter实践</title>
        <url>https://mryqu.github.io/post/hadoop_mapreduce%E5%AE%9A%E5%88%B6counter%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>custom</tag><tag>counter</tag><tag>demo</tag>
        </tags>
        <content type="html">  MapReduce除了有内建的Counter，还支持应用程序自身定制的Counter。实践如下：
CustomCounterDemo.java 执行 JobHistory显示 </content>
    </entry>
    
     <entry>
        <title>[数据分析与统计推断] 介绍数据</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD_%E4%BB%8B%E7%BB%8D%E6%95%B0%E6%8D%AE/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据分析</tag><tag>统计推断</tag><tag>coursera</tag><tag>笔记</tag>
        </tags>
        <content type="html">  本文为Data Analysis and StatisticalInference学习笔记，课程地址为https://www.coursera.org/course/statistics。
数据基础 观察值、变量、数据矩阵数据类型: - 数字变量：分为连续的和离散的 - 分类变量：分为有序（ordinal）的和普通无序的变量关系: - 关联Assocaited （Dependent）：分为正的和负的 - 独立Independent
研究方法  观察性研究Observational Study：直接取现实中的数据，探究变量间的相关性  retrospective：依赖以前的数据 prospective：在研究中收集新的数据  实验性研究Experiment: 随机分配对象到实验组；建立因果性连接观察性研究和实验性研究主要区别在于是否人为地施加了干预措施  观察性研究和抽样策略 为什么抽样？ - 某些个体很难定位和测量； - 统计总体很少保持不变抽样偏差源 - 便利样本Convenience sample：更易于被访问到的用户更可能被包含到采样中 - 无响应样本Non-response：仅部分被随机采样的被访者填写了调查问卷，这样的采样无法代表统计总体 - 自发性响应样本Voluntary response：对调查问卷中的问题更感兴趣的人积极主动填写调查问卷抽样方法 - 简单随机抽样Simple random sample (SRS)：不做控制随机地取样 - 分层抽样Stratified sample：对人群做分析，按相似性分成若干的阶层，在各阶层内取样 - 整群抽样cluster sample：随机将统计总体分成若干群，随机取一些群，再在这些群内取样混杂变量 Confounding/lurkvariable：对解释变量和响应变量都有影响的额外变量，使解释变量和响应变量看起来有关系
实验性研究 设计原则  control：比较实验组与对照组 randomize：保证对象在两个组分配的随机性 replicate：足够大的实验量或整个实验可复制 block：消除已知或可疑变量对输出的影响。例子:设计实验研究是否能量胶囊有助于奔跑更快： 实验组treatment：能量胶囊 对照组control:无能量胶囊能量胶囊可能对专业运动员和业余运动员影响不同消除专业状态： 将样本拆分为专业运动员和业余运动员 将专业运动员和业余运动员随机平均分配到实验组和对照组消除变量和解释变量的区别：  解释变量(factors)：施加于实验个体的条件 消除变量：我们需要控制的实验个体自带特征消除与分层很相似，区别在于： 在随机分配过程中消除，用于取得因果性 在随机抽样过程中分层，用于概化generalizability   术语  安慰剂：假处理，经常在药物研究中用作对照组 安慰剂效应：展示使用安慰剂的变化 盲 ：实验个体不知道其所在组 双盲：实验个体和研究者都不知道其所在组在需要从人的主观感受中剥离客观结论的社会性研究（比如药物实验）中，如果被试的主观感受会影响数据结果，就需要给予对照组无实质作用的安慰剂，并且对其隐藏其属于实验组还是对照组的信息，以区分实验组的变化和安慰剂效应，这就叫blindexperiment；更进一步，如果研究者的主观感受也会影响实验结果，则对研究者也隐藏实验与对照的分组信息，这就是通常说的双盲实验  随机抽样和随机分配 数值型数据的可视化 散点图scatterplot 可以从中归纳两个变量的相关性。相关性有几个性质：正/负相关、形状（线性、非线性）、强/弱相关，异常值直方图histogram 可以给出一个数据密度的视图，并且可以观察： - 偏度：描述了数据密度的左右分布，左偏/右偏/对称 - 形态（modality）：正态分布/均匀分布/双峰分布/多峰分布等等 - 区间划分不能过宽或过窄点阵图dotplot 当研究个体值时有用，样本量太大时不太适箱形图boxplot 在强调异常点、中位数、四分位距(Interquartile range，IQR，即Q3-Q1)时有用。强度图intensity map 用于空间分布。中心测量 均值mean、中位数median、众数mode分布性测量  范围range：最大值-最小值 方差variance： 标准差standard deviation： 四分位距Interquartile range：  健壮统计量 用于测评特异值是否作用较小数据转换： 通过函数对数据进行尺度变换，用于换一种数据观察角度；降低偏度；非线性转换为线性。示例为对数转换、平方根转换、倒数转换。1. 对X轴和Y轴都做变换；2. 保持X轴不变，对Y轴做变换。
观察类别变量  单个类别变量  频率表和条形图barplot条形图和直方图区别：条形图用于类别变量，直方图用于数值变量；直方图的X轴表示的是数值，并且数序不可更换。 饼图 pie chart：分类较多时不建议使用。  两个类别变量  相依表contingency table 分段条形图segmented barplot 马赛克图mosaicplot  一个类别变量和一个数值变量并排箱体图side-by-side boxplot  推断介绍 推断框架： - 设置代表现状的虚假设(null hypothesis, H0)。 - 设置代表所研究问题的备择假设(alternative hypothesis, HA)。 - 假定虚假设为真，通过模拟或理论方法进行假设检验。 - 如果结果表明数据没有提供支持备择假设的证据，坚持虚假设；否则，拒绝虚假设，采用备选假设。模拟方法进行假设检验： - 设置虚假设和备择假设 - 假定虚假设为真，进行模拟实验 - 评估观测结果比原数据观察到的异常值一样多或更多的概率(p-value) - 如果概率低，则拒绝虚假设，采用备选假设。
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 继承</title>
        <url>https://mryqu.github.io/post/javascript_%E7%BB%A7%E6%89%BF/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>inheritance</tag><tag>override</tag><tag>class</tag>
        </tags>
        <content type="html"> 示例：
function BaseClass() {}; BaseClass.prototype.method1 = function() { console.log(&amp;quot;BaseClass#method1&amp;quot;) }; BaseClass.prototype.method2 = function() { console.log(&amp;quot;BaseClass#method2&amp;quot;) }; BaseClass.prototype.method3 = function() { return &amp;quot;BaseClass#method3&amp;quot;; }; ChildClass.prototype = new BaseClass(); function ChildClass() { //BaseClass.call(this); }; ChildClass.prototype.method2 = function() { console.log(&amp;quot;ChildClass#method2&amp;quot;) }; ChildClass.prototype.method3 = function() { console.log(BaseClass.prototype.method3.call(this)&#43;&amp;quot; by ChildClass!&amp;quot;); }; ChildClass.prototype.method4 = function() { console.log(&amp;quot;ChildClass#method4&amp;quot;) }; var myobj = new ChildClass(); myobj.method1(); myobj.method2(); myobj.method3(); myobj.method4();  测试： 注解：
Javascript的继承要在原型链上进行，没有super()可以调用父类，覆盖父类函数时只能通过父类原型以call或apply函数的形式调用父类的方法。
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 字符串与JSON数据互转</title>
        <url>https://mryqu.github.io/post/javascript_%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%8Ejson%E6%95%B0%E6%8D%AE%E4%BA%92%E8%BD%AC/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>string</tag><tag>json</tag><tag>parse</tag><tag>stringify</tag>
        </tags>
        <content type="html">  字符串-&amp;gt;JSON 转换方法有3种:
使用浏览器内置window.JSON.parse方法 原生方法，速度最快，首选方案。老版本浏览器不支持。
|浏览器|支持版本 |&amp;mdash; |Chrome|(Yes) |Firefox (Gecko)|3.5 (1.9.1) |Internet Explorer|8.0 |Opera|10.5 |Safari|4.0 |Android|(Yes) |Chrome for Android|(Yes) |Firefox Mobile (Gecko)|1.0 (1.0) |IE Mobile|(Yes) ||Opera Mobile|(Yes) |Safari Mobile|(Yes)
使用Funtion()构造函数 较eval_r()快
使用 eval_r() 函数 功能强大，能解析任何JS代码,但是执行效率和安全性都不好示例代码：
var jsonStr = &#39;{&amp;quot;name&amp;quot;:&amp;quot;kxeg&amp;quot;,&amp;quot;data&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;Alpha&amp;quot;,&amp;quot;color&amp;quot;:&amp;quot;lightblue&amp;quot;},{&amp;quot;key&amp;quot;:&amp;quot;Beta&amp;quot;,&amp;quot;color&amp;quot;:&amp;quot;orange&amp;quot;}]}&#39;; //JSON.parse() if (window &amp;amp;&amp;amp; window.JSON &amp;amp;&amp;amp; window.JSON.parse) jsonObj1 = window.JSON.parse(jsonStr); //Function 创建一个闭包,返回一个json数据对象 jsonObj2 = (new Function(&#39;return&#39;&#43;jsonStr))(); //eval_r() jsonObj3 = eval_r(&#39;(&#39;&#43;jsonStr&#43;&#39;)&#39;);  JSON-&amp;gt;字符串 使用浏览器内置window.JSON.stringify方法
参考 js中字符串数据转为json对象的方法
MDN：JSON
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 函数的prototype对象属性</title>
        <url>https://mryqu.github.io/post/javascript_%E5%87%BD%E6%95%B0%E7%9A%84prototype%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>prototype</tag><tag>call</tag><tag>apply</tag><tag>bind</tag>
        </tags>
        <content type="html">  原型（prototype） JavaScript 不包含传统的类继承模型，而是使用原型模型。继承方面，JavaScript中的每个对象都有一个内部私有的链接指向另一个对象，这个对象就是该对象的原型。这个原型对象也有自己的原型，直到对象的原型为 null为止（也就是没有原型）。这种一级一级的链结构就称为原型链。Function.prototype.toString() toString()方法返回表示函数源代码的字符串。Function.prototype.bind() 对于给定函数，bind()方法创建具有与原始函数相同主体的绑定函数。 在绑定函数中，this对象将解析为传入的对象。绑定函数具有指定的初始参数。
fun.bind(thisArg[, arg1[, arg2[, ...]]])  JavaScript bind 方法具有几种用法。 通常，它用于为在其他上下文中执行的函数保留执行上下文。
Function.prototype.call()和Function.prototype.apply() call()和apply()方法都是调用一个对象的方法，用另一个对象上下文替换当前对象上下文。两者仅在定义参数方式有所区别：call传递的是参数列表，apply传递的是数组或arguments对象。
fun.call(thisArg[, arg1[, arg2[, ...]]])  应用call和apply还有一个技巧，就是call和apply应用另外一个函数以后，当前函数就具备了另外一个函数的方法和属性，这也可以称之为“继承”。通过上例可知，extend调用call方法后就继承到了base的方法和属性。
参考 JavaScript函数的Arguments对象属性
Javascript继承机制的设计思想
深入理解JavaScript系列（5）：强大的原型和原型链
Function.prototype.apply()
Function.prototype.call()
Function.prototype.bind()
Function.prototype.toString()
Functional JavaScript, Part 3: .apply(), .call(), and the arguments object
Bind, Call and Apply in JavaScript
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 函数的Arguments对象属性</title>
        <url>https://mryqu.github.io/post/javascript_%E5%87%BD%E6%95%B0%E7%9A%84arguments%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>argruments</tag><tag>function</tag><tag>callee</tag><tag>caller</tag>
        </tags>
        <content type="html">  arguments对象 每个函数表达式在其作用域内都可以访问一个特殊的本地变量：arguments，它是跟数组很类似的对象，同样可以通过下标访问，例如arguments[0]和arguments[1]&amp;hellip;。arguments.length 传递给函数的参数个数。
arguments.callee 返回当前正在调用的函数。callee属性是arguments对象的一个成员，它表示对函数对象自身的引用，有利于匿名函数的递归或者保证函数的封装性，上例中的sumV2仅调用局部变量arguments的callee属性，较sumV1需要调用全局变量sumV1，封装性更好。值得注意的是，callee也拥有一个length属性。通过上例可知arguments.length反映的是函数的实参长度，arguments.callee.length反映的是函数的形参长度。
arguments.caller (已废弃) arguments.caller并属于标准，且已被废弃。可以使用同样不属于标准但被大多数主流浏览器支持的Function.caller获得调用当前函数的函数。参考 Arguments object
arguments.callee
arguments.caller
Why was the arguments.callee.caller property deprecated in JavaScript?
Function caller
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] retrieve data table</title>
        <url>https://mryqu.github.io/post/javascript_retrieve_data_table/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>datatable</tag><tag>ajax</tag><tag>rest</tag><tag>spring</tag>
        </tags>
        <content type="html"> 想学学怎么提交一个Form中的Table，放狗出去，结果不够给力。 曾经有很多年Table标签被用作格式对齐的工具，这使搜出来的页面很少讲的是数据表格。 看了看DataTables这个JQuery插件，可以加载和更新数据，但是没有找到存储所有表格数据的功能。 看了看ajaxsubmit，必须有formcontent，此外可以有可选的data。由于表格里有很多行，没想好path的设置问题。 最后还是用JS提取所有表格数据，生成JS数组，通过AJAX post函数发送给服务器侧。 JS侧的代码示例：http://jsfiddle.net/mryqu/d7rubzut/ 服务器侧的用于REST的Spring控制器代码如下：
@RequestMapping(params=&amp;quot;action=test&amp;quot;, method = RequestMethod.POST) public @ResponseBody TestResultVO test(HttpServletRequest request, @ModelAttribute(&amp;quot;tqs&amp;quot;)ArrayList tqs) throws Exception { ...... }  运行结果正常
</content>
    </entry>
    
     <entry>
        <title>50个必用的Bootstrap扩展插件</title>
        <url>https://mryqu.github.io/post/50%E4%B8%AA%E5%BF%85%E7%94%A8%E7%9A%84bootstrap%E6%89%A9%E5%B1%95%E6%8F%92%E4%BB%B6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>bootstrap</tag><tag>html5</tag>
        </tags>
        <content type="html"> 学习了一下50 Must-have plugins for extending Twitter Bootstrap，对Bootstrap Form Wizard感兴趣。之前看过Twitter Bootstrap Wizard，个人感觉没这个漂亮。
</content>
    </entry>
    
     <entry>
        <title>消息队列技术选型资料</title>
        <url>https://mryqu.github.io/post/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%E8%B5%84%E6%96%99/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>消息队列</tag><tag>mq</tag><tag>技术</tag><tag>选型</tag><tag>对比</tag>
        </tags>
        <content type="html"> Message Queue Evaluation Notes（10/01/12）
消息队列软件产品大比拼（11/05/16）
RabbitMQ / ActiveMQ or Redis for over 250,000 msg/s（11/09/27）
消息中间件的技术选型心得－RabbitMQ、ActiveMQ和ZeroMQ（13/01/07）
消息队列中间件技术选型分析（13/05/24）
谁才是最快的消息队列:ActiveMQ, RabbitMQ, HornetQ, QPID&amp;hellip;（13/12/22）
activemq-or-rabbitmq-or-zeromq-or（14/02/27）
Alibaba： 消息队列中间件调研文档（14/07/01）
Alibaba：RocketMQ 性能测试报告（14/07/01）
消息中间件选型（15/01/20）
RabbitMq、ActiveMq、ZeroMq、kafka之间的比较,资料汇总（15/08/19）
开源 VS 商业，消息中间件你不知道的那些事（15/11/25）
MQ产品比较-ActiveMQ-RocketMQ（16/03/07）
</content>
    </entry>
    
     <entry>
        <title>磁盘分区管理工具</title>
        <url>https://mryqu.github.io/post/%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>硬盘分区</tag><tag>pqmagic</tag><tag>adds</tag><tag>aomei</tag><tag>win7</tag>
        </tags>
        <content type="html"> 一直以来都用PQMagic硬盘分区大师8.0，忽然有一天意识到它在Win7上不好使了，然后就想升级，结果发现PQMagic这款软件早就over了。根据维基百科的说法，Symantec收购由PowerQuest公司推出的PQMagic于2009年12月8日退役。 搜了一下资料： - 目前比较好的商业分区工具是Acronis Disk Director - 目前比较好的免费分区工具是AOMEI Partition Assistant Standard - 简单情况下，可以Win7自带小工具调整分区大小
</content>
    </entry>
    
     <entry>
        <title>用Python清理XMind生成的html文件</title>
        <url>https://mryqu.github.io/post/%E7%94%A8python%E6%B8%85%E7%90%86xmind%E7%94%9F%E6%88%90%E7%9A%84html%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>beautifulsoup</tag><tag>xmind</tag><tag>html</tag><tag>清理</tag>
        </tags>
        <content type="html"> XMind思维导图可以导出成html文件，但是每个Topic都被a标签包着，然后外面再被h1、h2&amp;hellip;h5标题标签包着，看着就难受。此外h1、h2&amp;hellip;h5和p标签都加了class属性，没什么用。 写一段小程序，将XMind生成的html文件进行格式清理。
from bs4 import BeautifulSoup as BeautifulSoup soup = BeautifulSoup(open(&amp;quot;c:/qutemp/123.html&amp;quot;)) for a in soup(&#39;a&#39;): a.parent.string = a.string a.clear() for tag in [&#39;h1&#39;,&#39;h2&#39;,&#39;h3&#39;,&#39;h4&#39;,&#39;h5&#39;,&#39;p&#39;]: for tag in soup(tag): del tag[&#39;class&#39;] print soup  </content>
    </entry>
    
     <entry>
        <title>MySQL Workbench的安全更新模式</title>
        <url>https://mryqu.github.io/post/mysql_workbench%E7%9A%84%E5%AE%89%E5%85%A8%E6%9B%B4%E6%96%B0%E6%A8%A1%E5%BC%8F/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mysql</tag><tag>safe-update</tag><tag>error.code.1175</tag><tag>workbench</tag><tag>安全更新模式</tag>
        </tags>
        <content type="html"> 最近在MySQLWorkbench上使用&amp;rdquo;DELETE FROM TABLE_E;&amp;ldquo;清空一个表时返回错误： Error Code: 1175. You are using safe update mode and you triedto update a table without a WHERE that uses a KEY column To disablesafe mode, toggle the option in Preferences -&amp;gt; SQL Queries andreconnect.
结果在MySQL命令行上却一切正常。
这是由于MySQL Workbench打开了安全更新模式，其有助于初学者使用DELETE FROM tbl_name 语句但是忘记了WHERE子句造成不必要的麻烦。解决方法就是关闭安全更新模式。
解决方法1：
SET SQL_SAFE_UPDATES = 0; DELETE FROM TABLE_E; SET SQL_SAFE_UPDATES = 1;  解决方法2：
参考： mysql delete under safe mode
mysql Tips
</content>
    </entry>
    
     <entry>
        <title>了解CAS（集中式认证服务）</title>
        <url>https://mryqu.github.io/post/%E4%BA%86%E8%A7%A3cas%E9%9B%86%E4%B8%AD%E5%BC%8F%E8%AE%A4%E8%AF%81%E6%9C%8D%E5%8A%A1/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>cas</tag><tag>集中式认证服务</tag><tag>sso</tag><tag>security</tag>
        </tags>
        <content type="html">  集中式认证服务（Central AuthenticationService，缩写CAS）是一种针对万维网的单点登录协议。它的目的是允许一个用户访问多个应用程序，而只需提供一次凭证（如用户名和密码）。它还允许web应用程序在没有获得用户的安全凭据（如密码）的情况下对用户进行身份验证。“CAS”也指实现了该协议的软件包。 CAS协议涉及到至少三个方面：客户端Web浏览器、Web应用请求的身份验证和CAS服务器。它还可能涉及一个后台服务（例如数据库服务器），它并没有自己的HTTP接口，但与一个Web应用程序进行通信。 当客户端访问访问应用程序，请求身份验证时，应用程序重定向到CAS。CAS验证客户端是否被授权，通常通过在数据库对用户名和密码进行检查（例如Kerberos、LDAP或ActiveDirectory）。 如果身份验证成功，CAS令客户端返回到应用程序，并传递身份验证票（Securityticket）。然后，应用程序通过安全连接连接CAS，并提供自己的服务标识和验证票。之后CAS给出了关于特定用户是否已成功通过身份验证的应用程序授信信息。 CAS允许通过代理服务器进行多层身份验证。后端服务（如数据库或邮件服务器）可以组成CAS，通过从Web应用程序接收到的信息验证用户是否被授权。因此，网页邮件客户端和邮件服务器都可以实现CAS。 历史 CAS是由耶鲁大学的Shawn Bayern创始的，后来由耶鲁大学的Drew Mazurek维护。CAS1.0实现了单点登录。CAS2.0引入了多级代理认证（Multi-tier proxyauthentication）。CAS其他几个版本已经有了新的功能。 2004年12月，CAS成为Jasig（Java in Administration Special InterestGroup）的一个项目，2008年该组织负责CAS的维护和发展。CAS原名“Yale CAS”，现在则被称为“JasigCAS”。
参考 Wiki：Central Authentication Service
Jasig CAS主页
GitHub：Jasig/java-cas-client
GitHub：Jasig/cas
CAS协议规范 3.0
</content>
    </entry>
    
     <entry>
        <title>玩一下SQLite3命令</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E4%B8%80%E4%B8%8Bsqlite3%E5%91%BD%E4%BB%A4/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>database</tag><tag>sqlite</tag><tag>命令</tag>
        </tags>
        <content type="html">  SQLite介绍 SQLite是实现自包含、无需服务器、零配置和事务SQL数据库引擎的软件库。它占用资源非常低，既可以用于Windows/Linux/Unix等主流的操作系统，也广泛用于嵌入式产品中。它能够跟很多程序语言相结合，比如Tcl、C#、PHP、Java等，还有ODBC接口。 不像常见的客户-服务器范例，SQLite引擎不是个程序与之通信的独立进程，而是连接到程序中成为它的一个主要部分。所以主要的通信协议是在编程语言内的直接API调用。这在消耗总量、延迟时间和整体简单性上有积极的作用。整个数据库(定义、表、索引和数据本身)都在宿主主机上存储在一个单一的文件中。它的简单的设计是通过在开始一个事务的时候锁定整个数据文件而完成的。
SQLITE常用命令使用 SQLite命令行解释器除了支持SQL语句（大小写不敏感），还支持以.开头、大小写敏感的专有命令。SQLite网站有个帖子Command Line Shell For SQLite介绍了支持的所有命令，这里挑一些常用的玩一下。 - .help命令：给出所有命令的帮助介绍
 sqlite&amp;gt; .help   .prompt：更换提示符  sqlite&amp;gt; .prompt &amp;gt;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&amp;gt;.prompt sqlite&amp;gt; sqlite&amp;gt;  .show：显示当前设置  sqlite&amp;gt;.show echo: off eqp: off explain: off headers: off mode: list nullvalue: &amp;quot;&amp;quot; output: stdout separator: &amp;quot;|&amp;quot; stats: off width:  .database：显示数据库信息  sqlite&amp;gt;.database seq name file --- --------------- ---------------------------------------------------------- 0 main E:\gitws\datasci_course_materials\assignment2\reuters.db 1 temp  .tables：显示表信息  sqlite&amp;gt;.tables Frequency  .schema：显示表的创建语句  sqlite&amp;gt;.schema CREATE TABLE Frequency ( docid VARCHAR(255), term VARCHAR(255), count int, PRIMARY KEY(docid, term));  .mode：设置SQL结果输出格式 ``` sqlite&amp;gt;.mode csv sqlite&amp;gt;select * from Frequency limit 3; 10000_txt_earn,net,1 10000_txt_earn,rogers,4 10000_txt_earn,earnings,2  sqlite&amp;gt;.mode column sqlite&amp;gt;select * from Frequency limit 3; 10000_txt_earn net 1 10000_txt_earn rogers 4 10000_txt_earn earnings 2
sqlite&amp;gt; .mode html sqlite&amp;gt; select * from Frequency limit 3; 10000_txt_earn net 1  10000_txt_earn rogers 4  10000_txt_earn earnings 2 
sqlite&amp;gt;.mode insert sqlite&amp;gt;select * from Frequency limit 3; INSERT INTO table VALUES(&amp;lsquo;10000_txt_earn&amp;rsquo;,&amp;lsquo;net&amp;rsquo;,1); INSERT INTO table VALUES(&amp;lsquo;10000_txt_earn&amp;rsquo;,&amp;lsquo;rogers&amp;rsquo;,4); INSERT INTO table VALUES(&amp;lsquo;10000_txt_earn&amp;rsquo;,&amp;lsquo;earnings&amp;rsquo;,2);
sqlite&amp;gt;.mode line sqlite&amp;gt;select * from Frequency limit 3; docid = 10000_txt_earn term = net count = 1
docid = 10000_txt_earn term = rogers count = 4
docid = 10000_txt_earn term = earnings count = 2
sqlite&amp;gt;.mode list sqlite&amp;gt;select * from Frequency limit 3; 10000_txt_earn,net,1 10000_txt_earn,rogers,4 10000_txt_earn,earnings,2
sqlite&amp;gt;.mode tabs sqlite&amp;gt;select * from Frequency limit 3; 10000_txt_earn net 1 10000_txt_earn rogers 4 10000_txt_earn earnings 2
sqlite&amp;gt;.mode tcl sqlite&amp;gt;select * from Frequency limit 3; &amp;ldquo;10000_txt_earn&amp;rdquo; &amp;rdquo;net&amp;rdquo; &amp;rdquo;1&amp;rdquo; &amp;ldquo;10000_txt_earn&amp;rdquo; &amp;rdquo;rogers&amp;rdquo; &amp;rdquo;4&amp;rdquo; &amp;ldquo;10000_txt_earn&amp;rdquo; &amp;rdquo;earnings&amp;rdquo; &amp;rdquo;2&amp;rdquo; sqlite&amp;gt;
- .open：关闭当前数据库并打开新数据库  sqlite&amp;gt;.open reuters.db
- .backup：备份数据库（默认 main）到文件  sqlite&amp;gt;.backup test.db
- .dump：以SQL文本格式转储数据库  sqlite&amp;gt;.dump Frequency
- .read：执行SQL文件  sqlite&amp;gt;.read test.sql
- .exit或.quit：退出  sqlite&amp;gt;.quit E:\gitws\datasci_course_materials\assignment2&amp;gt; ```
参考 Command Line Shell For SQLiteSQLite Core Functions
</content>
    </entry>
    
     <entry>
        <title>SQL中的（稀疏）矩阵运算</title>
        <url>https://mryqu.github.io/post/sql%E4%B8%AD%E7%9A%84%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>sql</tag><tag>稀疏矩阵</tag><tag>运算</tag>
        </tags>
        <content type="html"> 前两天在一门课上看到SQL中（稀疏）矩阵的乘法，找到两篇相关帖子，介绍了SQL中的（稀疏）矩阵运算，包含矩阵相等判别、加法、乘法、转置。
Matrix Math in SQLMAD skills: new analysis practices for big data
</content>
    </entry>
    
     <entry>
        <title>通过Java反射操作注解</title>
        <url>https://mryqu.github.io/post/%E9%80%9A%E8%BF%87java%E5%8F%8D%E5%B0%84%E6%93%8D%E4%BD%9C%E6%B3%A8%E8%A7%A3/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>annotation</tag><tag>reflection</tag>
        </tags>
        <content type="html">  注解是Java5加入的特性，它是可以插入Java代码的注释或元数据，可被预编译工具在编译时进行处理，或在运行态通过Java反射进行操作。开发者可以通过元编程（Metaprogramming）等技术提高生产率，注解在其中扮演了核心角色。其思想是通过注解够告诉工具如何生成新代码、转换代码或者决定运行期的行为。
MyAnnotation.java package com.yqu.reflection.annotation; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; @Retention(RetentionPolicy.RUNTIME) //@Target(ElementType.TYPE) public @interface MyAnnotation { public String name() default &amp;quot;[unknown name]&amp;quot;; public String value() default &amp;quot;[unassigned value]&amp;quot;; }  定义注解类有点类似于定义Java接口类interface，但和一般的接口类比起来，interface前面多了一个@，这样就声明了注解是一个Annotation类。另外，Stringname()和Stringvalue()这个写法是@interface中一个比较独特的地方。它实际上定义的不并是注解类的方法，而是注解类的属性。 @Target指定此注解的作用域： - TYPE：用于类、接口、注解类和枚举 - CONSTRUCTOR：用于构造方法 - LOCAL_VARIABLE：用于本地变量 - FIELD：用于类的属性(包括枚举常量) - METHOD：用于方法 - PACKAGE：用于包 - PARAMETER：用于方法的参数 - ANNOTATION_TYPE：用于注解类 - TYPE_PARAMETER：使用类型参数，表示注解可以用在Type的声明式前 - TYPE_USE： 使用类型注解。表示注解所有使用Type的地方（如泛型、类型转换等）@Retention指定此注解的生命周期： - SOURCE：代表此注解仅在代码编译前存活。比如@Deprecated，仅在编译前提供一些提示信息。在编译时，这些注解并不会编译到class文件中。 - CLASS：与SOURCE不同，这类标记会编译到class文件中，但不会成为程序的一部分，也不可以通过代码在运行时调用到。 - RUNTIME： 这类标记将成为代码的一部分，并会在实际运行时起到作用。
TheClass.java package com.yqu.reflection.annotation; import java.lang.annotation.Annotation; import java.lang.reflect.Field; import java.lang.reflect.Method; @MyAnnotation(name = &amp;quot;classAnnotation&amp;quot;, value = &amp;quot;Hello Class&amp;quot;) // I18NOK:CLS public class TheClass { @MyAnnotation(name = &amp;quot;fieldAnnotation&amp;quot;, value = &amp;quot;Hello Field&amp;quot;) public String theField = null; public TheClass() { } @MyAnnotation(name = &amp;quot;methodAnnotation&amp;quot;, value = &amp;quot;Hello Method&amp;quot;) public void doSomething() { System.out.println(&amp;quot;doSomething&amp;quot;); } public static void doSomethingElse( @MyAnnotation(name = &amp;quot;paramAnnotation&amp;quot;, value = &amp;quot;Hello Parameter&amp;quot;) String param) { System.out.println(&amp;quot;doSomethingElse:&amp;quot; &#43; param); } public static void testClassAnnotation() { Class aClass = TheClass.class; Annotation[] annotations = aClass.getAnnotations(); for (Annotation annotation : annotations) { if (annotation instanceof MyAnnotation) { MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&amp;quot;name: &amp;quot; &#43; myAnnotation.name()); System.out.println(&amp;quot;value: &amp;quot; &#43; myAnnotation.value()); } } Annotation annotation = aClass.getAnnotation(MyAnnotation.class); if (annotation instanceof MyAnnotation) { MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&amp;quot;name: &amp;quot; &#43; myAnnotation.name()); System.out.println(&amp;quot;value: &amp;quot; &#43; myAnnotation.value()); } } public static void testMethodAnnotation() throws NoSuchMethodException, SecurityException { Class aClass = TheClass.class; Method method = aClass.getMethod(&amp;quot;doSomething&amp;quot;, null); Annotation[] annotations = method.getDeclaredAnnotations(); for (Annotation annotation : annotations) { if (annotation instanceof MyAnnotation) { MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&amp;quot;name: &amp;quot; &#43; myAnnotation.name()); System.out.println(&amp;quot;value: &amp;quot; &#43; myAnnotation.value()); } } Annotation annotation = method.getAnnotation(MyAnnotation.class); if (annotation instanceof MyAnnotation) { MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&amp;quot;name: &amp;quot; &#43; myAnnotation.name()); System.out.println(&amp;quot;value: &amp;quot; &#43; myAnnotation.value()); } } public static void testParameterAnnotation() throws NoSuchMethodException, SecurityException { Class aClass = TheClass.class; Method method = aClass.getMethod(&amp;quot;doSomethingElse&amp;quot;, new Class[] { String.class }); Annotation[][] parameterAnnotations = method.getParameterAnnotations(); Class[] parameterTypes = method.getParameterTypes(); int i = 0; for (Annotation[] annotations : parameterAnnotations) { Class parameterType = parameterTypes[i&#43;&#43;]; for (Annotation annotation : annotations) { if (annotation instanceof MyAnnotation) { MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&amp;quot;param: &amp;quot; &#43; parameterType.getName()); System.out.println(&amp;quot;name : &amp;quot; &#43; myAnnotation.name()); System.out.println(&amp;quot;value: &amp;quot; &#43; myAnnotation.value()); } } } } public static void testFieldAnnotation() throws NoSuchFieldException, SecurityException { Class aClass = TheClass.class; Field field = aClass.getField(&amp;quot;theField&amp;quot;); Annotation[] annotations = field.getDeclaredAnnotations(); for (Annotation annotation : annotations) { if (annotation instanceof MyAnnotation) { MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&amp;quot;name: &amp;quot; &#43; myAnnotation.name()); System.out.println(&amp;quot;value: &amp;quot; &#43; myAnnotation.value()); } } Annotation annotation = field.getAnnotation(MyAnnotation.class); if (annotation instanceof MyAnnotation) { MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&amp;quot;name: &amp;quot; &#43; myAnnotation.name()); System.out.println(&amp;quot;value: &amp;quot; &#43; myAnnotation.value()); } } public static void main(String[] args) throws Exception { testClassAnnotation(); System.out.println(&amp;quot;------------------&amp;gt;&amp;quot;); testMethodAnnotation(); System.out.println(&amp;quot;------------------&amp;gt;&amp;quot;); testParameterAnnotation(); System.out.println(&amp;quot;------------------&amp;gt;&amp;quot;); testFieldAnnotation(); } }  测试结果 name: classAnnotation value: Hello Class name: classAnnotation value: Hello Class ------------------&amp;gt; name: methodAnnotation value: Hello Method name: methodAnnotation value: Hello Method ------------------&amp;gt; param: java.lang.String name : paramAnnotation value: Hello Parameter ------------------&amp;gt; name: fieldAnnotation value: Hello Field name: fieldAnnotation value: Hello Field  </content>
    </entry>
    
     <entry>
        <title>lxml.html.soupparser引入BeautifulSoup 4的work-around</title>
        <url>https://mryqu.github.io/post/lxml.md.soupparser%E5%BC%95%E5%85%A5beautifulsoup_4%E7%9A%84work-around/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>lxml</tag><tag>beautifulsoup</tag><tag>html</tag><tag>xml</tag>
        </tags>
        <content type="html"> 想用一下python的xpath功能分析一个html文件，lxml是比较不错的xml/html解析库，lxml功能强大，性能也不错，此外也包含了ElementTree，html5lib ，beautfulsoup 等库。 可惜我的html文件格式不是很严谨，lxml的ElementTree处理不了，就转而想用lxml的beautfulsoup来处理。 结果lxml找不到BeautifulSoup库。
查了一下Anaconda装的库里面明明有Beautiful Soup 4.3.1，感觉很奇怪！! lxml.html.soupparser引入BeautifulSoup 4的work-around 原来Beautiful Soup 3目前已经停止开发，Beautiful Soup 4移植到了BS4。
下面的语句就可以引入Beautiful Soup 4了，可是lxml还是无法引入beautfulsoup。
from bs4 import BeautifulSoup  stackoverflow有一个帖子import error due to bs4 vs BeautifulSoup讲了一个work-around，可以欺骗lxml从而引入beautfulsoup。测试一下，果然工作正常了。
import sys, bs4 sys.modules[&#39;BeautifulSoup&#39;] = bs4 import lxml.html.soupparser as soupparser  </content>
    </entry>
    
     <entry>
        <title>Python(x,y)功能实现思维导图</title>
        <url>https://mryqu.github.io/post/pythonxy%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>思维导图</tag><tag>项目策划</tag>
        </tags>
        <content type="html"> 今天浏览Python(x,y)时，发现一个介绍其功能实现的思维导图。对Python(x,y)所实现的功能一目了然，感觉很棒。思维导图在整理自己思绪、知识图谱时用了一些，还没用到项目策划上，以后会督促自己多用多思考。</content>
    </entry>
    
     <entry>
        <title>选择Python科学计算发行版</title>
        <url>https://mryqu.github.io/post/%E9%80%89%E6%8B%A9python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%8F%91%E8%A1%8C%E7%89%88/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>pythonxy</tag><tag>winpython</tag><tag>canopy</tag><tag>anaconda</tag><tag>python</tag>
        </tags>
        <content type="html">  最近重装Python，看了一下Python科学计算发行版，打算多玩一下数据处理和数值计算。
Python用于科学计算的一些常用工具和库  IPython-增强的交互环境：支持变量自动补全，自动缩进，支持 bash shell命令，内置了许多很有用的功能和函数 Spyder、Wing IDE或Eclipse/Pydev：集成开发环境 NumPy-数学计算基础库：N维数组、线性代数计算、傅立叶变换、随机数等。 SciPy-数值计算库：线性代数、拟合与优化、插值、数值积分、稀疏矩阵、图像处理、统计等。 SymPy-符号运算 Pandas-数据分析库：数据导入、整理、处理、分析等。 matplotlib-会图库：绘制二维图形和图表 Chaco-交互式图表 OpenCV-计算机视觉库 TVTK-数据的三维可视化 Cython-Python转C的编译器：编写高效运算扩展库的首选工具 BioPython-生物科学  Python科学计算发行版  Python(x,y)当前最新版本:2.7.6.1 (05/30/2014)，支持Windows和Python2.7.6。其库索引列出了所支持的170&#43;Python27库。 WinPython当前最新版本:2.7.6.4和3.3.5.0 (04/2014)，支持Windows和Python2.7.6、3.3.5。其库索引列出了所支持的60&#43;Python27库。其库索引列出了所支持的60&#43;Python33库。 Enthought Canopy（Enthought Python Distribution）当前最新版本:1.4.1 (06/11/2014)，支持Linux, Windows,Mac平台和Python2.7.6。其库索引列出了所支持的150&#43;测试过的Python库。 Anaconda当前最新版本:2.0.1 (06/12/2014)，支持Linux, Windows, Mac平台和Python2.6、2.7、3.3、3.4。其库索引列出了所支持的195&#43;流行Python库。  Sage不是Python发行版，而是一个由Python和Cython实现的开源数学软件系统，将很多已有的（C、C&#43;&#43;、Fortran和Python编写的）数学软件包集成到一个通用接口（记事本文档接口和IPython命令行界面），用户只需了解Python，就可以通过接口或包装器(wrapper)使用NumPy、SciPy、matplotlib、Sympy、Maxima、GAP、FLINT、R和其他已有软件包（具体信息见组件列表），完成代数、组合数学、计算数学和微积分等计算。其最初的目标是创造一个“Magma、Maple、Mathematica和MATLAB的开源替代品”。当前最新版本:6.3(08/10/2014)，支持Linux, Windows, Mac平台和Python2.x。
我的选择和推荐 Python(x,y)和WinPython都是开源项目，其项目负责人都是PierreRaybaut。按Pierre自己的说法是“WinPython不是试图取替Python(x,y)，而是出于不同动机和理念：更灵活、易于维护、可移动、对操作系统侵略性更小，但是用户友好性更差、包更少、没有同Windows资源管理器集成。”。参考1里面说Python(x,y)不是很稳定，此外看它目前的更新不是很频繁，确实有可能Pierre后来的工作重心放在WinPython上了。 Canopy和Anaconda是公司推的，带免费版和商业版/插件。这两款发行版也牵扯到一个人，那就是Travis Oliphant。Travis是SciPy的原始作者，同时也是NumPy的贡献者。Travis在2008年以副总裁身份加入Enthought，2012年以总裁的身份离开，创立了一个新公司continuum.io，并推出了Python的科学计算平台Anaconda。Anaconda相对Canopy支持Python的版本更多，对Python新版本支持跟的很紧（Sage不支持Python3.x的理由是因为其依赖的SciPy还不支持Python3，而Anaconda却实现了支持Python3.3和3.4，这就说明问题了），此外其在Linux平台下（通过conda管理）安装更方便。 不言而喻，我最后选择了安装科学计算发行版Anaconda:)
参考 目前比较流行的Python科学计算发行版
《Python科学计算》 清华大学出版社
Re-packaged Python
Scientific computing with Python
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 使用MRUnit进行MapReduce单元测试</title>
        <url>https://mryqu.github.io/post/hadoop_%E4%BD%BF%E7%94%A8mrunit%E8%BF%9B%E8%A1%8Cmapreduce%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mrunit</tag><tag>mapreduce</tag><tag>unittest</tag><tag>junit</tag>
        </tags>
        <content type="html">  MRUnit介绍 MRUnit是一个用于帮助开发者进行HadoopMapReduce作业单元测试的Java库。它是JUnit架构扩展，无需将代码运行在集群上即可在开发环境测试Mapper和Reducer类的功能。MRUnit由Cloudera开发，并在2012年成为Apache基金会顶级项目。 MRUnit使用LocalJobRunner使用样本数据集模拟一次Mapper/Reducer执行过程。通过定义一或多个输入记录，使用LocalJobRunner运行测试代码，判定是否与期望输出相符。如相符，则安静退出；否则，默认抛出异常。
测试代码 本测试代码基于MRUnit指南中示例代码修改而成，使用junit:junit:4.11和org.apache.mrunit:mrunit:1.1.0:hadoop2两个Java库进行编译和测试。
SMSCDR.java SMSCDRMapperReducerTest 执行测试 成功测试演示 失败测试演示 为了演示测试失败情况，我将testReducer方法中期望值改为错误值123。 参考 Apache MRUnit
MRUnit Tutorial
</content>
    </entry>
    
     <entry>
        <title>数据科学：学习路径和图书</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AD%A6%E4%B9%A0%E8%B7%AF%E5%BE%84%E5%92%8C%E5%9B%BE%E4%B9%A6/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>数据科学</tag>
        </tags>
        <content type="html"> 转发网上的两个图片，时时对照学习。
</content>
    </entry>
    
     <entry>
        <title>slidify</title>
        <url>https://mryqu.github.io/post/slidify/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>slidify</tag><tag>html5幻灯片</tag><tag>可重复性研究</tag><tag>发布</tag>
        </tags>
        <content type="html"> Slidify是使用RMarkdown创建、定制和发布（用于可重复性研究的）HTML幻灯片的工具包。 Slidify支持多种生成框架和主题：
|幻灯片生成框架|主题 |&amp;mdash;&amp;ndash; |io2012|&amp;nbsp; |html5slides|default, uulm |html5rocks|&amp;nbsp; |deck.js|web2.0, swiss, neon |dzslides|&amp;nbsp; |landslide|default, tango, clean |shower|ribbon |slidy|&amp;nbsp; |slideous|&amp;nbsp; |beamer|&amp;nbsp; |showoff|&amp;nbsp;
生成的幻灯片可以发布到Github、Dropbox和Rpubs上。命令集成了一些底层操作，所以很简单。见http://slidify.org/publish.html 前一段发布一个幻灯片碰到些麻烦，只好采用git命令行这种老方式。操作步骤参考如下链接： https://github.com/ramnathv/slidify/wiki/Publishing
</content>
    </entry>
    
     <entry>
        <title>CRAN任务视图：机器学习与统计学习（2014-05-31版）</title>
        <url>https://mryqu.github.io/post/cran%E4%BB%BB%E5%8A%A1%E8%A7%86%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A02014-05-31%E7%89%88/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>机器学习</tag><tag>统计学习</tag><tag>任务视图</tag><tag>r语言包</tag><tag>cran</tag>
        </tags>
        <content type="html">  我在CRAN任务视图使用一帖中学习CRAN中当前所有的任务视图，本帖学习机器学习与统计学习的任务视图，其原链接如下：http://cran.r-project.org/web/views/MachineLearning.html 机器学习是计算机科学和统计学的边缘交叉领域，R关于机器学习的扩展包大概包括以下几个方面： - 神经网络(Neural Networks) : 单隐含层神经网络在nnet 包(与R基础包一同发布)中实现。RSNNS 包提供斯图加特神经网络仿真软件(SNNS)的使用接口。 - 递归拆分（Recursive Partitioning） : 根据CART书中算法完成的用于回归、分类和生存分析的树形结构模型在rpart 包(与R基础包一同发布)和tree包中实现。rpart包推荐用于计算与分类回归树相类似的树结构。 Weka 拥有很多拆分算法的工具包，RWeka提供了Weka的C4.5（J4.8变种）和M5实现的使用接口。Cubist包通过在叶端使用线性回归模型、基于实例修正、boosting拟合基于规则的模型（与树类似）。C50包可以拟合C5.0分类树和基于规则的模型及其boosting版本。party包有两个带有无偏变量选择和统计停止准则的递归拆分算法实现。ctree()函数基于非参数条件推理过程，用于测试响应变量和每个输入变量的无关性；而mob()函数能用于拆分参数模型。party包也提供了用于可视化二叉树和响应变量节点分布的可扩展工具。用于多元响应变量的rpart改进版本在mvpart包提供。对于二元输入变量问题，LogicReg包实现了逻辑回归。maptree包提供了树的可视化工具。用于通过随机效应对纵向数据进行建模的树由REEMtree包提供。RPMM提供了对混合模型(Beta和高斯混合)的拆分。用于表达树的计算基础架构、预测和可视化的统一方法在partykit包内实现。oblique.tree包提供了用于分类数据的斜树。 - 随机森林（Random Forests） : randomForest包提供了用于回归和分类的随机森林算法参考实现。ipred包提供了用于回归、分类和生存分析的bagging（bootstrapaggregating的缩写）算法及通过集成学习对多个模型进行组合的bundling算法。此外，基于条件推断决策树（conditional inferencetree）的随机森林变体（其响应变量可以以任意比例估量）在party包中实现。randomSurvivalForest包提供用于删失数据的随机生存森林算法。分位数回归森林 quantregForest通过随机森林方式对探索变量的数值响应变量分位数进行回归。varSelRF和Boruta包专注于通过随机森林进行变量选择。对于大数据集，bigrf包以并行方式计算随机森林并使用大的内存对象存储数据。 - 正规化和收缩方法（Regularized and ShrinkageMethods） : 参数评估受限的回归模型可以使用lasso2和lars包进行拟合。grplasso包提供了群体LASSO（LeastAbsolute Shrinkage and Selection Operator）惩罚模型; grpreg包实现了一些其他群体惩罚模型，例如群体MCP（MinimaxConcave Penalty）和群体SCAD（Smoothly Clipped Absolute DeviationPenalty）。用于广义线性模型和Cox模型的L1正规化路径可以通过glmpath包里的函数获得，用于线性回归、逻辑回归和多项式回归模型的整个lasso或elastic-net正规化路径（也存在于elasticnet)可以通过glmnet包获得。penalized包提供了lasso(L1)和ridge (L2)惩罚回归模型(支持广义线性模型和Cox模型)的另一个替代实现。RXshrink包能用于标识和显示特定收缩路径的跟踪信息、判断合适的收缩程度。 使用lasso惩罚的半参数加法风险模型由ahaz包提供。一种用于线性回归的广义lasso收缩技术，relaxedlasso，由relaxo包提供。penalizedLDA包实现使用（可选的）LASSO惩罚的费舍尔线性判别分析，用于特征值p远大于观测值n的高维数据。pamr包实现了用于基因表达分析的缩小重心分类法和工具。earth包提供了一个多元自适应样条回归实现。penalizedSVM包提供了使用惩罚功能(SCAD或L1惩罚)的变量选择支持向量机。 各种惩罚判别分析在hda、rda、sda和SDDA包内实现。LiblineaR包提供了LIBLINEAR库的接口。ncvreg包使用坐标下降算法进行SCAD和MCP逻辑惩罚来拟合线性和逻辑回归模型。 bigRR包关注于高吞吐量岭回归(例如，对很多预测变量进行惩罚)和异方差效应模型。 bmrm包提供了一个用于正规化风险最小化的束方法实现。 - Boosting : 各种形式的梯度boosting在gbm包 (基于树的功能性梯度下降boosting)内实现。由boosting优化的Hinge-loss实现在bst包内. GAMBoost包可用于通过boosting算法拟合广义加法模型。mboost包提供一个用于广义线性、加法和非参数模型的可扩展boosting框架。 基于似然估计的boosting实现，用于Cox模型的在CoxBoost包内提供，用于混合模型的在GMMBoost包内提供。GAMLSS模型可以使用gamboostLSS提供的boosting进行拟合。 - 支持向量机（Support Vector Machines and KernelMethods） : e1071包中的svm()函数提供LIBSVM库的接口，包kernlab 为核学习（包括SVM、RVM和其他核学习算法）提供了一个灵活的框架。 klaR 包提供了用于SVMlight实现（仅one-against-all多类分类）的接口。 核特征空间的关联维可以使用rdetools包进行估计，rdetools也提供了用于模型选择和预测的程序。 - 贝叶斯方法（BayesianMethods） : tgp包提供了贝叶斯非稳半参数非线性回归和基于树的高斯过程（包括贝叶斯线性模型、分类和回归树、基于树的线性模型）设计. - 基于遗传算法的优化（Optimization using GeneticAlgorithms） : rgp和rgenoud包提供基于遗传算法的最优化程序。Rmalschains包实现了基于局部搜索链的模因算法（memeticalgorithm），该算法为一种特俗的演化算法类型，是稳态基因算法和用于实数参数优化的局部搜索的结合体。模因算法，又译为文化基因算法，全局搜索策略可以采用遗传算法、进化策略、进化规划等;局部搜索策略可以采用爬山搜索、模拟退火、贪婪算法、禁忌搜索等。 - 关联规则（Association Rules） arules包提供了有效处理稀疏二元数据的数据结构，以及为用于挖掘频繁项集、最大频繁项集、闭频繁项集和关联规则的Apriori和Eclat算法实现提供接口。 - _基于规则的_模糊系统（Fuzzy Rule-based Systems） :frbs包实现了许多用于回归和分类、从数据中学习模糊规则系统的标准方法。RoughSets包提供了粗糙集理论（RST）和模糊粗糙集理论（FRST）的完整实现。 - 模型选择和验证（Model selection andvalidation） :e1071包中用于超参数调优的tune()函数和ipred包中的errorest() 函数可被用于错误率估计。svmpath 包里的函数可用来选取支持向量机的代价参数C。ROCR 包提供了函数用于ROC分析和其他用于对比候选分类器的可视化技术。caret 包供了各种建立预测模型的函数，包括参数调优和变量重要性量度。这些包可被用于各种并行实现（例如MPI、NWS等）。 - 统计学习基础（Elements of StatisticalLearning） :《The Elements of Statistical Learning: Data Mining, Inference, and Prediction 》一书中的数据集、函数、例子都被打包放在ElemStatLearn包中。 - rattle 是R中用于数据挖掘的图形用户界面。CORElearn 包实现了很多机器学习算法，例如最近邻域算法、树、随机森林和一些特征选择方法。与之类似，rminer 包可以使用其他包的许多学习算法并计算多种性能度量。
CRAN上的R包:  ahaz
 arules
 bigrf
 bigRR
 bmrm
 Boruta
 bst
 C50
 caret
 CORElearn
 CoxBoost
 Cubist
 e1071 (core)
 earth
 elasticnet
 ElemStatLearn
 frbs
 GAMBoost
 gamboostLSS
 gbm (core)
 glmnet
 glmpath
 GMMBoost
 grplasso
 grpreg
 hda
 ipred
 kernlab (core)
 klaR
 lars
 lasso2
 LiblineaR
 LogicReg
 maptree
 mboost (core)
 mvpart
 ncvreg
 nnet (core)
 oblique.tree
 pamr
 party
 partykit
 penalized
 penalizedLDA
 penalizedSVM
 quantregForest
 randomForest (core)
 randomSurvivalForest
 rattle
 rda
 rdetools
 REEMtree
 relaxo
 rgenoud
 rgp
 Rmalschains
 rminer
 ROCR
 RoughSets
 rpart (core)
 RPMM
 RSNNS
 RWeka
 RXshrink
 sda
 SDDA
 svmpath
 tgp
 tree
 varSelRF
  相关链接:  MLOSS: 机器学习开源软件
 Boosting 研究网站（已停止维护）
  参考 转帖R的应用领域包介绍 By R-Fox
R包的分类介绍.pdf
R语言中的机器学习
BAYESIAN TREED GAUSSIAN PROCESS MODELS
rpart包和party包的简单比较
基于R软件rpart包的分类与回归树应用
统计学习方法——CART, Bagging, Random Forest, Boosting
bootstrap, boosting, bagging 几种方法的联系
Open-Source Machine Learning: R Meets Weka
Penalized methods for bi-level variable selection
</content>
    </entry>
    
     <entry>
        <title>shiny练习</title>
        <url>https://mryqu.github.io/post/shiny%E7%BB%83%E4%B9%A0/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>shiny</tag><tag>rstudio</tag><tag>rpart</tag><tag>随机森林</tag><tag>作业</tag>
        </tags>
        <content type="html">  Shiny是RStudio公司开发的新包，有了它，无需web开发就可以用R语言轻松开发交互式web应用。
我参加数据科学专业课学习，使用shiny完成一个作业，虽然初学乍练，也能感觉到开发起来很快速。 参考 shiny tutorialshiny examplesLinear Regressions and Linear Models using the Iris Data
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] JQuery AJAX在HTTP响应200OK时却调用了errorcallback</title>
        <url>https://mryqu.github.io/post/javascript_jquery_ajax%E5%9C%A8http%E5%93%8D%E5%BA%94200ok%E6%97%B6%E5%8D%B4%E8%B0%83%E7%94%A8%E4%BA%86errorcallback/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>jquery</tag><tag>ajax</tag><tag>200ok</tag><tag>errorcallback</tag>
        </tags>
        <content type="html"> 运行如下代码时，从结果看操作成功，但是总是调用错误处理回调。
myTest = function(server, lib, table, reqParam, reqInfo, successCallback, errorCallback) { var url = &amp;quot;http://localhost/mytest/&amp;quot; &#43; encodeURIComponent(server) &#43; &amp;quot;/libs/&amp;quot; &#43; encodeURIComponent(lib) &#43; &amp;quot;/tables/&amp;quot; &#43; encodeURIComponent(table); if (reqParam!=undefined &amp;amp;&amp;amp; reqParam) { url &#43;= &amp;quot;?reqParam=&amp;quot; &#43; encodeURIComponent(reqParam); } $.ajax({ cache: false, url: url, type: &amp;quot;PUT&amp;quot;, data: JSON.stringify(reqInfo), contentType: &amp;quot;application/json&amp;quot;, success: function (data) { if (successCallback!==undefined &amp;amp;&amp;amp; successCallback) { successCallback(data); } else { console.log(&amp;quot;success:&amp;quot;&#43;JSON.stringify(data)); } }, error: function (xhr, status, error) { if (errorCallback!==undefined &amp;amp;&amp;amp; errorCallback) { errorCallback(xhr, status, error); } else { console.log(&amp;quot;error:&amp;quot;&#43;JSON.stringify(xhr)); } } }); };  响应码为200OK，表明操作成功，此外也没见到&amp;quot;timeout&amp;quot;、&amp;quot;error&amp;quot;、&amp;quot;abort&amp;quot;和&amp;quot;parsererror&amp;quot; 这些错误。但是消息体内仅为一个字符串，而不是json数据。尝试设置dataType:&amp;ldquo;text&amp;rdquo;，结果调用了successCallback，成功！
myTest = function(server, lib, table, reqParam, reqInfo, successCallback, errorCallback) { var url = &amp;quot;http://localhost/mytest/&amp;quot; &#43; encodeURIComponent(server) &#43; &amp;quot;/libs/&amp;quot; &#43; encodeURIComponent(lib) &#43; &amp;quot;/tables/&amp;quot; &#43; encodeURIComponent(table); if (reqParam!=undefined &amp;amp;&amp;amp; reqParam) { url &#43;= &amp;quot;?reqParam=&amp;quot; &#43; encodeURIComponent(reqParam); } sas.ajax({ cache: false, url: url, type: &amp;quot;PUT&amp;quot;, data: JSON.stringify(reqInfo), dataType: &amp;quot;text&amp;quot;, contentType: &amp;quot;application/application/json&amp;quot;, success: function (data) { if (successCallback!==undefined &amp;amp;&amp;amp; successCallback) { successCallback(data); } else { console.log(&amp;quot;success:&amp;quot;&#43;JSON.stringify(data)); } }, error: function (xhr, status, error) { if (errorCallback!==undefined &amp;amp;&amp;amp; errorCallback) { errorCallback(xhr, status, error); } else { console.log(&amp;quot;error:&amp;quot;&#43;JSON.stringify(xhr)); } } }); };  </content>
    </entry>
    
     <entry>
        <title>QuickEdit mode of command prompt</title>
        <url>https://mryqu.github.io/post/quickedit_mode_of_command_prompt/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>quick</tag><tag>edit</tag><tag>command</tag><tag>prompt</tag>
        </tags>
        <content type="html"> 从Windows命令行复制内容每次都需要点击Edit菜单中的Mark子菜单项，颇为不便。
原来这个行为可以通过属性对话框中的QuickEdit mode改变： </content>
    </entry>
    
     <entry>
        <title>[Hadoop] 在MapReduce中使用HBase数据</title>
        <url>https://mryqu.github.io/post/hadoop_%E5%9C%A8mapreduce%E4%B8%AD%E4%BD%BF%E7%94%A8hbase%E6%95%B0%E6%8D%AE/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>MapReduce</tag><tag>hbase</tag><tag>TableMapper</tag><tag>TableReducer</tag><tag>TableMapReduceUtil</tag>
        </tags>
        <content type="html">  对于MapReduce程序来说，除了可以用HDFS文件系统作为输入源和输出目标，同样可以使用HBase作为输入源和输出目标。下面做一个小练习进行学习。
MapReduceOnHBaseDemo.java rebuild.sh #!/bin/bash CLASSPATH=.:$(hbase classpath):$(hadoop classpath) javac -d classes -cp $CLASSPATH *.java jar -cvf YquMapreduceDemo.jar -C classes/ .  测试 执行下列命令运行MapReduce作业:
HADOOP_CLASSPATH=$(hbase mapredcp):${HBASE_HOME}/conf hadoop jar YquMapreduceDemo.jar MapReduceOnHBaseDemo -libjars $(hbase mapredcp | tr &#39;:&#39; &#39;,&#39;)  HBase结果如下: 与普通MapReduce程序的差异  本例中ScoreMapper类继承自抽象类TableMapper。TableMapper是Mapper抽象类的子类，指定输入键类型为ImmutableBytesWritable，输入值类型为Result。因此ScoreMapper类定义仅指定输出键和值类型，而其mapper方法前两个参数为ImmutableBytesWritable和Result类型。 本例中ScoreReducer类继承自抽象类TableReducer。TableReducer是Reduccer抽象类的子类，指定输出值类型为Mutation。因此ScoreReducer定义仅指定输入键和值、输出键的类型。有下图可知，TableReducer输出值类型支持Append、Delete、Increment和Put。 本例中Driver部分通过TableMapReduceUtil类的initTableMapperJob和initTableReducerJob方法合并Hadoop和HBase配置，配置job属性。  参考 HBase and MapReduce
</content>
    </entry>
    
     <entry>
        <title>JDBC连接池的testQuery/validationQuery设置</title>
        <url>https://mryqu.github.io/post/jdbc%E8%BF%9E%E6%8E%A5%E6%B1%A0%E7%9A%84testquery%E5%92%8Cvalidationquery%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>jdbc</tag><tag>连接池</tag><tag>testquery</tag><tag>validationquery</tag><tag>数据库</tag>
        </tags>
        <content type="html"> 在《Tomcat中使用Connector/J连接MySQL的超时问题》帖子中想要增加对连接池中连接的测试/验证，防止数据库认为连接已死而Web应用服务器认为连接还有效的问题，Mysql文档中提到Tomcat文档中的例子中用的是validationQuery，但是网上还有很多帖子写的是testQuery，到底用哪一个呢？ 原来这跟连接池的实现有关：
|连接池实现|该功能属性名 |&amp;mdash;&amp;ndash; |The Tomcat JDBC Connection Pool|validationQuery |The Apache Commons DBCP Connection Pool|validationQuery |c3p0 - JDBC3 Connection and Statement Pooling|preferredTestQuery |Atomikos：Tomcat Spring ActiveMQ MySQL JMX Integration
分析Atomikos数据连接池源码，弄清testQuery|testQuery
此外，测试/验证连接池连接的SQL语句也因数据库而异： Efficient SQL test query or validation query that will work across all (or most) databases DBCP - validationQuery for different Databases
综合上述两个帖子，汇总结果如下：
|数据库|测试/验证查询 |&amp;mdash;&amp;ndash; |MySQL|SELECT 1 |PostgreSQL|SELECT 1 |Microsoft SQL Server|SELECT 1 |SQLite|SELECT 1 |H2|SELECT 1 |Ingres|SELECT 1 |Oracle|select 1 from dual |DB2|select 1 from sysibm.sysdummy1 或SELECT current date FROM sysibm.sysdummy1 |Apache Derby|VALUES 1 FROM SYSIBM.SYSDUMMY1 或SELECT 1 FROM SYSIBM.SYSDUMMY1 |HSQLDB|SELECT 1 FROM any_existing_table WHERE 1=0 或SELECT 1 FROM INFORMATION_SCHEMA.SYSTEM_USERS |Informix|select count(*) from systables
</content>
    </entry>
    
     <entry>
        <title>Tomcat中使用Connector/J连接MySQL的超时问题</title>
        <url>https://mryqu.github.io/post/tomcat%E4%B8%AD%E4%BD%BF%E7%94%A8connectorj%E8%BF%9E%E6%8E%A5mysql%E7%9A%84%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Web Application Server</category>
        </categories>
        <tags>
          <tag>tomcat</tag><tag>mysql</tag><tag>connector/j</tag><tag>wait_timeout</tag><tag>autoreconnect</tag>
        </tags>
        <content type="html">  最近玩的一个Web项目，上一个晚上做一些操作，第二天超时需要再登陆，却总是报密码不正确。需要重启tomcat才能解决。异常如下：
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 442,814,107 milliseconds ago. The last packet sent successfully to the server was 442,814,107 milliseconds ago. is longer than the server configured value of &#39;wait_timeout&#39;. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Co nnector/J connection property &#39;autoReconnect=true&#39; to avoid this problem. at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at com.mysql.jdbc.Util.handleNewInstance(Util.java:408) at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1137) at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3983) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2596) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2776) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2840) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2082) at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:2212) at org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.extract(ResultSetReturnImpl.java:80) at org.hibernate.loader.Loader.getResultSet(Loader.java:2065) at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1862) at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1838) at org.hibernate.loader.Loader.doQuery(Loader.java:909) at org.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:354) at org.hibernate.loader.Loader.doList(Loader.java:2553) at org.hibernate.loader.Loader.doList(Loader.java:2539) at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2369) at org.hibernate.loader.Loader.list(Loader.java:2364) at org.hibernate.loader.hql.QueryLoader.list(QueryLoader.java:496) at org.hibernate.hql.internal.ast.QueryTranslatorImpl.list(QueryTranslatorImpl.java:387) at org.hibernate.engine.query.spi.HQLQueryPlan.performList(HQLQueryPlan.java:231) at org.hibernate.internal.SessionImpl.list(SessionImpl.java:1264) at org.hibernate.internal.QueryImpl.list(QueryImpl.java:103) at xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.springframework.web.bind.annotation.support.HandlerMethodInvoker.invokeHandlerMethod(HandlerMethodInvoker.java:175) at org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter.invokeHandlerMethod(AnnotationMethodHandlerAdapter.java:446) at org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter.handle(AnnotationMethodHandlerAdapter.java:434) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961) at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863) at javax.servlet.http.HttpServlet.service(HttpServlet.java:646) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837) at javax.servlet.http.HttpServlet.service(HttpServlet.java:727) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1040) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:607) at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316) ... 4 more Caused by: java.net.SocketException: Software caused connection abort: socket write error at java.net.SocketOutputStream.socketWrite0(Native Method) at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113) at java.net.SocketOutputStream.write(SocketOutputStream.java:159) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3964) ... 58 more  学习了如下帖子： MySQL Connector/J Developer Guide :: 10 Using Connector/J with Tomcat
java调用mysql获取不到连接的问题
MySql autoReconnect=true 后连接超时
Mysql 中的autoReconnect=true参数
最后决定采用如下方案：
hibernate增加如下设置: 数据库连接池增加如下设置: </content>
    </entry>
    
     <entry>
        <title>网络资源(主力书籍)</title>
        <url>https://mryqu.github.io/post/%E7%BD%91%E7%BB%9C%E8%B5%84%E6%BA%90%E4%B8%BB%E5%8A%9B%E4%B9%A6%E7%B1%8D/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>书籍</tag><tag>下载</tag>
        </tags>
        <content type="html">  外文书籍  Library Genesis
 BookZZ
 爱挖盘
 manybooks
 14个值得收藏可免费搜索/下载PDF电子图书（文档）的搜索引擎
  图书馆  全球免费开放的电子图书馆
  古籍  书格
  专利  史上最完整的专利信息数据库网址大全
  杂志  http://pdfmagazines.org/
 http://www.gqzzw.com/
 http://www.pdfzj.com/
 http://pdf-giant.com/
 http://www.magazine6.com/
  </content>
    </entry>
    
     <entry>
        <title>[Gradle] 在build.gradle中添加本地包依赖</title>
        <url>https://mryqu.github.io/post/gradle_%E5%9C%A8build.gradle%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%9C%AC%E5%9C%B0%E5%8C%85%E4%BE%9D%E8%B5%96/</url>
        <categories>
          <category>Tool</category><category>Gradle</category>
        </categories>
        <tags>
          <tag>gradle</tag><tag>local</tag><tag>package</tag><tag>dependencies</tag>
        </tags>
        <content type="html"> 一直在Gradle中用的依赖包都是来自仓库，头一次添加本地包依赖。
buildscript { repositories { mavenCentral() } } apply plugin: &#39;java&#39; apply plugin: &#39;eclipse&#39; apply plugin: &#39;idea&#39; jar { baseName = &#39;HelloAlgs&#39; version = &#39;0.1.0&#39; } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { runtime files(&#39;libs/algs4.jar&#39;) } task wrapper(type: Wrapper) { gradleVersion = &#39;2.3&#39; }  </content>
    </entry>
    
     <entry>
        <title>[Hadoop] Failed to exec (compile-ms-winutils) on project hadoop-common</title>
        <url>https://mryqu.github.io/post/hadoop_failed_to_exec_compile-ms-winutils_on_project_hadoop-common/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>vs2013</tag><tag>winutils</tag><tag>compile</tag><tag>windows</tag>
        </tags>
        <content type="html"> 在Windows平台编译Hadoop时遇到了compile-ms-winutils执行失败的问题。Hadoop默认需要用VS2010编译，但是我的环境只有VS2013。
问题及解决方案如下： - Cannot run program &amp;ldquo;msbuild&amp;rdquo; 将C:\Windows\Microsoft.NET\Framework\v4.0.30319放入PATH环境变量 - Command execution failed. Process exited with an error: 1(Exitvalue: 1) -&amp;gt; [Help 1] 打开hadoop-common\hadoop-hdfs-project\hadoop-hdfs\pom.xml，将&amp;rsquo;VisualStudio 10 Win64&amp;rsquo;改成&amp;rsquo;Visual Studio 12 Win64&amp;rsquo;。 将如下两个Visual Studio项目用VS2013打开，手动编译。 - hadoop-common\hadoop-common-project\hadoop-common\src\main\winutils\winutils.sln - hadoop-common\hadoop-common-project\hadoop-common\src\main\native\winutils.sln
重新执行：
C:\Program Files (x86)\Microsoft Visual Studio 12.0\Common7\Tools\VsDevCmd.bat mvn install -DskipTests  上述问题不再出现。
</content>
    </entry>
    
     <entry>
        <title>在R作图系统中自定义坐标轴</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8r%E4%BD%9C%E5%9B%BE%E7%B3%BB%E7%BB%9F%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9D%90%E6%A0%87%E8%BD%B4/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>r</tag><tag>base</tag><tag>lattice</tag><tag>ggplot2</tag><tag>axis</tag>
        </tags>
        <content type="html">  在R作图系统中自定义坐标轴 R作图系统中坐标中的标签默认是等间隔，下列尝试是为了将少量数据的坐标显示在坐标轴上。
加载库并定义数据 library(lattice) library(ggplot2) library(grid) library(gridExtra) x &amp;lt;- c(1, 2, 3, 7, 8, 9) y &amp;lt;- c(1, 23, 12, 77, 14, 2) data &amp;lt;- data.frame(x = x, y = y) data  ## x y ## 1 1 1 ## 2 2 23 ## 3 3 12 ## 4 7 77 ## 5 8 14 ## 6 9 2  Base作图系统 opar &amp;lt;- par(mfrow = c(1, 2), mar = c(4, 6, 4, 2)) with(data, plot(x, y, type = &amp;quot;b&amp;quot;, main = &amp;quot;Default Axis&amp;quot;)) par(las = 1) with(data, plot(x, y, type = &amp;quot;b&amp;quot;, main = &amp;quot;customised Axis&amp;quot;, xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;)) axis(1, data$x) axis(2, data$y)  par(opar)  Lattice作图系统 plot1 &amp;lt;- xyplot(y ~ x, data, type = &amp;quot;b&amp;quot;, main = &amp;quot;Default Axis&amp;quot;) plot2 &amp;lt;- xyplot(y ~ x, data, type = &amp;quot;b&amp;quot;, main = &amp;quot;customised Axis&amp;quot;, scales = list(x = list(at = unlist(data$x)), y = list(at = unlist(data$y))), las = 1) grid.arrange(plot1, plot2, ncol = 2)  ggplot2作图系统 vplayout &amp;lt;- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y) grid.newpage() pushViewport(viewport(layout = grid.layout(1, 2))) g1 &amp;lt;- ggplot(data, aes(x, y)) &#43; geom_point() &#43; labs(title = &amp;quot;Default Axis&amp;quot;) g2 &amp;lt;- ggplot(data, aes(x, y)) &#43; geom_point() &#43; labs(title = &amp;quot;customised Axis&amp;quot;) &#43; scale_x_continuous(breaks = x) print(g1, vp = vplayout(1, 1)) print(g2, vp = vplayout(1, 2))  引用 Controlling Axes of R Plots
Getting rid of axis values in R Plot
rotate ylab
Custom lattice axis scales
lattice坐标系和坐标轴
ggplot2: customised X-axis ticks
Combined plot of ggplot2
Quick-R Graph
</content>
    </entry>
    
     <entry>
        <title>谷歌拼音输入法-笔划/组件输入</title>
        <url>https://mryqu.github.io/post/%E8%B0%B7%E6%AD%8C%E6%8B%BC%E9%9F%B3%E8%BE%93%E5%85%A5%E6%B3%95-%E7%AC%94%E5%88%92%E7%BB%84%E4%BB%B6%E8%BE%93%E5%85%A5/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>谷歌拼音</tag><tag>生僻字</tag><tag>输入</tag>
        </tags>
        <content type="html"> 有些工具用法总不使就记不住了，到处搜还不如搜自己的博客。 对于某些生僻字，您有可能并不清楚它的拼音。这时，您可以尝试使用谷歌拼音提供的笔划/组件输入功能。例如：先按&amp;rdquo;u&amp;rdquo;进入笔划输入模式，然后用使用代码h(横),s(竖),p(撇),n(捺),z(折),d(点)依次输入该字的各个笔划。其中，n和d是相同的。例如，您希望输入&amp;rdquo;谷&amp;rdquo;，则可以依次按下&amp;rdquo;upnpnszh&amp;rdquo;。 除了笔划以外，你也可以用拼音直接输入组成该汉字的可读部件，例如，输入&amp;rdquo;ucaowei&amp;rdquo;可以输入&amp;rdquo;莅&amp;rdquo;，输入&amp;rdquo;ujinxing&amp;rdquo;可以输入&amp;rdquo;鍟&amp;rdquo;。输入部件时，直接使用该部件的读音，或近似汉字的读音即可，例如，提手旁可以用&amp;rdquo;shou&amp;rdquo;来输入。
</content>
    </entry>
    
     <entry>
        <title>Outlook2013中日历的天气设置</title>
        <url>https://mryqu.github.io/post/outlook2013%E4%B8%AD%E6%97%A5%E5%8E%86%E7%9A%84%E5%A4%A9%E6%B0%94%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>outlook</tag><tag>calendar</tag><tag>天气</tag><tag>设置</tag>
        </tags>
        <content type="html"> </content>
    </entry>
    
     <entry>
        <title>MongoDB副本集实践</title>
        <url>https://mryqu.github.io/post/mongodb%E5%89%AF%E6%9C%AC%E9%9B%86%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mongodb</tag><tag>nosql</tag><tag>replica_set</tag><tag>high_availablity</tag><tag>副本集</tag>
        </tags>
        <content type="html">  副本集（ReplicaSet）就是有自动故障恢复功能的主从集群。副本集和主从集群最为明显的区别就是副本集没有固定的“主节点”：整个集群会选取出一个“主节点”，当其不能工作时则变更到其他节点。然而，两者看上去非常相似：副本集总会有一个活跃节点（primary）和一个或多个备份节点（secondary）。副本集中可以有以下类型的节点： - 常规节点（standard）：存储一份完整的数据副本，参与选举投票，有可能成为活跃节点。 - 被动节点（passive）：存储完整的数据副本，参与投票，不能成为活跃节点。 - 仲裁节点（arbiter）：仅参与投票，不接受复制的数据，也不能成为活跃节点。
副本集对数据库管理员和开发者都是非常友好的。副本集的管理是自动化的，无需数据库管理员频繁监控和干预，可以自动提升备份节点成为活跃节点，以确保运转正常。对于开发者而言，仅需为副本集指定一下服务器，驱动程序就会自动找到副本集的节点集群。 每个参与节点（非仲裁节点）都有一个权重，权重值取值范围0-1000，默认值为1。权重为0的节点不能成为活跃节点。活跃节点选举根据高优先级优先、优先级相同时数据较新优先的原则进行比较。应用仅能向活跃节点进行写操作。默认情况下，应用会向活跃节点进行读操作也获取最新数据。如果无需获得最新数据时，可以为了提高读操作吞吐量或减少应用延迟而向备份节点进行读操作。读操作偏好模式如下： - primary - primaryPreferred - secondary - secondaryPreferred - nearestMongoDB命令行提供了下列关于副本集的指令：
&amp;gt; rs.help() rs.status() { replSetGetStatus : 1 } checks repl set status rs.initiate() { replSetInitiate : null } initiates set with default settings rs.initiate(cfg) { replSetInitiate : cfg } initiates set with configuration cfg rs.conf() get the current configuration object from local.system.replset rs.reconfig(cfg) updates the configuration of a running replica set with cfg (disconnects) rs.add(hostportstr) add a new member to the set with default attributes (disconnects) rs.add(membercfgobj) add a new member to the set with extra attributes (disconnects) rs.addArb(hostportstr) add a new member which is arbiterOnly:true (disconnects) rs.stepDown([secs]) step down as primary (momentarily) (disconnects) rs.syncFrom(hostportstr) make a secondary to sync from the given member rs.freeze(secs) make a node ineligible to become primary for the time specified rs.remove(hostportstr) remove a host from the replica set (disconnects) rs.slaveOk() shorthand for db.getMongo().setSlaveOk() rs.printReplicationInfo() check oplog size and time range rs.printSlaveReplicationInfo() check replica set members and replication lag db.isMaster() check who is primary reconfiguration helpers disconnect from the database so the shell will display an error, even if the command succeeds. see also http://[mongod_host]:28017/_replSet for additional diagnostic info  实验环境 机器： - 10.120.8.231 - 10.120.5.131 - 10.120.8.165
三个节点中，MongoDB都安装在c:\mongodb目录下且MongoDB配置文件（c:\mongodb\etc\mongo.conf）均为：
systemLog: destination: file path: C:/mongodb/log/mongo.log logAppend: true storage: dbPath: C:/mongodb/data/db journal: enabled: false net: http: enabled: true RESTInterfaceEnabled: true  启动MongoDB 分别在上面三个节点启动如下命令：
c:\mongodb\bin\mongod.exe --config c:\mongodb\etc\mongo.conf --replSet rs0  在任一节点进去MongoDB命令行初始化副本集：
rs.initiate({ &amp;quot;_id&amp;quot; : &amp;quot;rs0&amp;quot;, &amp;quot;members&amp;quot; : [ {&amp;quot;_id&amp;quot; : 1, &amp;quot;host&amp;quot; : &amp;quot;10.120.8.231:27017&amp;quot;, &amp;quot;priority&amp;quot;: 8}, {&amp;quot;_id&amp;quot; : 2, &amp;quot;host&amp;quot; : &amp;quot;10.120.5.131:27017&amp;quot;, &amp;quot;priority&amp;quot;: 6}, {&amp;quot;_id&amp;quot; : 3, &amp;quot;host&amp;quot; : &amp;quot;10.120.8.165:27017&amp;quot;, &amp;quot;arbiterOnly&amp;quot;: true}, ] });  下列结果显示副本集部署成功：故障切换测试 客户端使用Python MongoDB驱动pymongo，下载链接为https://pypi.python.org/pypi/pymongo/2.7。测试代码如下：
&amp;gt;&amp;gt;&amp;gt; import pymongo &amp;gt;&amp;gt;&amp;gt; client = pymongo.MongoClient(&#39;mongodb://10.120.8.231,10.120.5.131,10.120.8.165/?replicaSet=rs0&#39;) &amp;gt;&amp;gt;&amp;gt; print client MongoClient([u&#39;10.120.8.231:27017&#39;, u&#39;10.120.5.131:27017&#39;]) &amp;gt;&amp;gt;&amp;gt; db = client.test &amp;gt;&amp;gt;&amp;gt; db.name u&#39;test&#39; &amp;gt;&amp;gt;&amp;gt; db.mycoll.save({&amp;quot;Name&amp;quot;:&amp;quot;Thanks&amp;quot;, &amp;quot;Count&amp;quot;:123}) ObjectId(&#39;533f7ae2f6adc81bb87f418a&#39;) &amp;gt;&amp;gt;&amp;gt; db.mycoll.find_one() {u&#39;Count&#39;: 123, u&#39;_id&#39;: ObjectId(&#39;533f7ae2f6adc81bb87f418a&#39;), u&#39;Name&#39;: u&#39;Thanks&#39;} &amp;gt;&amp;gt;&amp;gt; db.connection.host &#39;10.120.8.231&#39; &amp;gt;&amp;gt;&amp;gt; db.connection.port 27017 &amp;gt;&amp;gt;&amp;gt; print &#39;shutdown 10.120.8.231 now&#39; shutdown 10.120.8.231 now &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; db.mycoll.find_one() Traceback (most recent call last): File &amp;quot;&amp;quot;, line 1, in db.mycoll.find_one() File &amp;quot;C:\Python27\lib\site-packages\pymongo\collection.py&amp;quot;, line 713, in find_one for result in cursor.limit(-1): File &amp;quot;C:\Python27\lib\site-packages\pymongo\cursor.py&amp;quot;, line 1038, in next if len(self.__data) or self._refresh(): File &amp;quot;C:\Python27\lib\site-packages\pymongo\cursor.py&amp;quot;, line 982, in _refresh self.__uuid_subtype)) File &amp;quot;C:\Python27\lib\site-packages\pymongo\cursor.py&amp;quot;, line 906, in __send_message res = client._send_message_with_response(message, **kwargs) File &amp;quot;C:\Python27\lib\site-packages\pymongo\mongo_client.py&amp;quot;, line 1186, in _send_message_with_response sock_info = self.__socket(member) File &amp;quot;C:\Python27\lib\site-packages\pymongo\mongo_client.py&amp;quot;, line 913, in __socket &amp;quot;%s %s&amp;quot; % (host_details, str(why))) AutoReconnect: could not connect to 10.120.8.231:27017: [Errno 10061] No connection could be made because the target machine actively refused it &amp;gt;&amp;gt;&amp;gt; db.mycoll.find_one() {u&#39;Count&#39;: 123, u&#39;_id&#39;: ObjectId(&#39;533f7ae2f6adc81bb87f418a&#39;), u&#39;Name&#39;: u&#39;Thanks&#39;} &amp;gt;&amp;gt;&amp;gt; db.connection.host u&#39;10.120.5.131&#39; &amp;gt;&amp;gt;&amp;gt; db.connection.port 27017  参考 MongoDB Manual: Replication
MongoDB Doc: High Availability and PyMongo
</content>
    </entry>
    
     <entry>
        <title>MongoDB主从复制实践</title>
        <url>https://mryqu.github.io/post/mongodb%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mongodb</tag><tag>nosql</tag><tag>master</tag><tag>slave</tag><tag>主从复制</tag>
        </tags>
        <content type="html">  MongoDB主从复制（Master-slave replication）跟副本集（ReplicaSet）相比可以对主MongoDB节点设置多个从节点，并且可以将复制操作限制到特定数据库。但是MongoDB主从复制提供的冗余度更低而且无法自动故障切换，因此一般新的部署都倾向使用副本集而不是主从复制。
实验环境 两个节点中，MongoDB都安装在c:\mongodb目录下且MongoDB配置文件（c:\mongodb\etc\mongo.conf）均为：
systemLog: destination: file path: C:/mongodb/log/mongo.log logAppend: true storage: dbPath: C:/mongodb/data/db journal: enabled: false net: http: enabled: true RESTInterfaceEnabled: true  启动MongoDB 主节点启动命令：
c:\mongodb\bin\mongod.exe --config c:\mongodb\etc\mongo.conf --master --oplogSize=2048  从节点启动命令：
c:\mongodb\bin\mongod.exe --config c:\mongodb\etc\mongo.conf --slave --source 10.120.8.231 --slavedelay 10 --autoresync  主从复制测试 检查主节点 &amp;gt; show dbs; admin (empty) local 8.074GB test 0.078GB &amp;gt; db.isMaster() { &amp;quot;ismaster&amp;quot; : true, &amp;quot;maxBsonObjectSize&amp;quot; : 16777216, &amp;quot;maxMessageSizeBytes&amp;quot; : 48000000, &amp;quot;maxWriteBatchSize&amp;quot; : 1000, &amp;quot;localTime&amp;quot; : ISODate(&amp;quot;2014-04-03T01:47:49.780Z&amp;quot;), &amp;quot;maxWireVersion&amp;quot; : 2, &amp;quot;minWireVersion&amp;quot; : 0, &amp;quot;ok&amp;quot; : 1 }  检查从节点 &amp;gt; show dbs; admin (empty) local 0.078GB test 0.078GB &amp;gt; db.isMaster(); { &amp;quot;ismaster&amp;quot; : false, &amp;quot;maxBsonObjectSize&amp;quot; : 16777216, &amp;quot;maxMessageSizeBytes&amp;quot; : 48000000, &amp;quot;maxWriteBatchSize&amp;quot; : 1000, &amp;quot;localTime&amp;quot; : ISODate(&amp;quot;2014-04-03T01:48:26.416Z&amp;quot;), &amp;quot;maxWireVersion&amp;quot; : 2, &amp;quot;minWireVersion&amp;quot; : 0, &amp;quot;ok&amp;quot; : 1 }  主节点插入数据并检查 &amp;gt; use hellomsrep; switched to db hellomsrep &amp;gt; db.msr.insert({&amp;quot;name&amp;quot;:&amp;quot;MongoDB&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;Master-slave replication&amp;quot;}) WriteResult({ &amp;quot;nInserted&amp;quot; : 1 }) &amp;gt; db.msr.find() { &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;533cfbce455c73a31f941981&amp;quot;), &amp;quot;name&amp;quot; : &amp;quot;MongoDB&amp;quot;, &amp;quot;type&amp;quot; : &amp;quot;Master-slave replication&amp;quot; }  检查从节点数据 &amp;gt; show dbs; admin (empty) hellomsrep 0.078GB local 0.078GB test 0.078GB &amp;gt; use hellomsrep; switched to db hellomsrep &amp;gt; db.msr.find(); { &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;533cfbce455c73a31f941981&amp;quot;), &amp;quot;name&amp;quot; : &amp;quot;MongoDB&amp;quot;, &amp;quot;type&amp;quot; : &amp;quot;Master-slave replication&amp;quot; } &amp;gt; db.printReplicationInfo() this is a slave, printing slave replication info. source: 10.120.8.231 syncedTo: Thu Apr 03 2014 02:16:46 GMT-0400 (Eastern Daylight Time) 15 secs (0 hrs) behind the primary  以上操作证明了主节点插入的数据已被复制到从节点。
参考 MongoDB Manual: Master Slave Replication
MongoDB Manual: Configuration File Options
</content>
    </entry>
    
     <entry>
        <title>差一点搞混了Transactional注解</title>
        <url>https://mryqu.github.io/post/%E5%B7%AE%E4%B8%80%E7%82%B9%E6%90%9E%E6%B7%B7%E4%BA%86transactional%E6%B3%A8%E8%A7%A3/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>transactional</tag><tag>anotation</tag><tag>javaee</tag><tag>spring</tag>
        </tags>
        <content type="html"> 今天给我的Srping业务层加如下Service和Transactional注解：
@Service @Scope(BeanDefinition.SCOPE_SINGLETON) @Transactional(propagation=Propagation.REQUIRED, timeout=600, rollbackFor=Exception.class)  结果总是不认propagation、timeout和rollbackFor，后来才发现我引入类定义错了，本来应该用Spring的org.springframework.transaction.annotation.Transactional，可是引入了JavaEE用于CDI(Contextsand Dependency Injection for the Java EEplatform，上下文和依赖注入)bean的javax.transaction.Transactional,不注意还真容易混淆。
</content>
    </entry>
    
     <entry>
        <title>使用YCSB测试MongoDB</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8ycsb%E6%B5%8B%E8%AF%95mongodb/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>ycsb</tag><tag>benchmark</tag><tag>nosql</tag><tag>mongodb</tag><tag>性能测试</tag>
        </tags>
        <content type="html">  YCSB介绍 YCSB（Yahoo! Cloud Serving Benchmark）是雅虎开源的一款用于测试各类云服务/NoSQL/键值对存储的性能基准测试工具。覆盖的测试对象有： - PNUTS - BigTable - HBase - Hypertable - Azure - Cassandra - CouchDB - Voldemort - MongoDB - OrientDB - Infinispan - Dynomite - Redis - GemFire - GigaSpaces XAP - DynamoDB
YCSB项目的目标是提供一个标准的工具用来衡量不同键值对存储或云服务存储的性能。YCSB做了很多优化来提高客户端性能，例如在数据类型上用了最原始的比特数组以减少数据对象本身创建转换所需的时间等。YCSB的几大特性： - 支持常见的数据存储读写操作，如插入，修改，删除及读取 - 多线程支持。YCSB用Java实现，有很好的多线程支持。 - 灵活定义场景文件。可以通过参数灵活的指定测试场景，如100%插入， 50%读50%写等等 - 数据请求分布方式：支持随机，zipfian(只有小部分的数据得到大部分的访问请求）以及最新数据几种请求分布方式 - 可扩展性：可以通过扩展Workload的方式来修改或者扩展YCSB的功能
使用YCSB测试MongoDB 初试YCSB 下载https://github.com/downloads/brianfrankcooper/YCSB/ycsb-0.1.4.tar.gz 并解压缩。执行如下命令查看帮助：接下来开始使用YCSB测试MongoDB2.6，不过这其中遇到一些问题。期望YCSB的下一版本能够解决这些问题。
解决java.lang.ClassNotFoundException:com.yahoo.ycsb.Client问题 执行测试失败： C:\ycsb-0.1.4&amp;gt;python bin/ycsb load mongodb -s -P workloads/workloada java -cp C:\ycsb-0.1.4\core\lib\core-0.1.4.jar:C:\ycsb-0.1.4\gemfire-binding\conf:C:\ycsb-0.1.4\hbase-binding\conf:C:\ycsb-0.1.4\infinispan-binding\conf:C:\ycsb-0.1.4\jdbc-binding\conf:C:\ycsb-0.1.4\mongodb-binding\lib\mongodb-binding-0.1.4.jar:C:\ycsb-0.1.4\nosqldb-binding\conf:C:\ycsb-0.1.4\voldemort-binding\conf com.yahoo.ycsb.Client -db com.yahoo.ycsb.db.MongoDbClient -s -P workloads/workloada -load Exception in thread &amp;ldquo;main&amp;rdquo; java.lang.NoClassDefFoundError: com/yahoo/ycsb/Client Caused by: java.lang.ClassNotFoundException: com.yahoo.ycsb.Client at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) Could not find the main class: com.yahoo.ycsb.Client. Program will exit.
问题出在C:\ycsb-0.1.4\bin\ycsb，对于Windows操作系统将classpath参数连接符由&amp;rdquo;:&amp;ldquo;改成&amp;rdquo;;&amp;ldquo;即可
ycsb_command = [&amp;quot;java&amp;quot;, &amp;quot;-cp&amp;quot;, &amp;quot;:&amp;quot;.join(find_jars(ycsb_home, database)), \ COMMANDS[sys.argv[1]][&amp;quot;main&amp;quot;], &amp;quot;-db&amp;quot;, db_classname] &#43; options  解决Could not initialize MongoDB connection pool for Loader:java.lang.NullPointerException问题 重新执行测试失败： C:\ycsb-0.1.4&amp;gt;python bin/ycsb load mongodb -s -P workloads/workloada java -cp C:\ycsb-0.1.4\core\lib\core-0.1.4.jar;C:\ycsb-0.1.4\gemfire-binding\conf;C:\ycsb-0.1.4\hbase-binding\conf;C:\ycsb-0.1.4\infinispan-binding\conf;C:\ycsb-0.1.4\jdbc-binding\conf;C:\ycsb-0.1.4\mongodb-binding\lib\mongodb-binding-0.1.4.jar;C:\ycsb-0.1.4\nosqldb-binding\conf;C:\ycsb-0.1.4\voldemort-binding\conf com.yahoo.ycsb.Client -db com.yahoo.ycsb.db.MongoDbClient -s -P workloads/workloada -load YCSB Client 0.1 Command line: -db com.yahoo.ycsb.db.MongoDbClient -s -P workloads/workloada -load Loading workload&amp;hellip; Starting test. Could not initialize MongoDB connection pool for Loader: java.lang.NullPointerException java.lang.NullPointerException at com.yahoo.ycsb.db.MongoDbClient.init(MongoDbClient.java:78) at com.yahoo.ycsb.DBWrapper.init(DBWrapper.java:63) at com.yahoo.ycsb.ClientThread.run(Client.java:189) java.lang.NullPointerException 0 sec: 0 operations; [INSERT AverageLatency(us)=1320] 0 sec: 0 operations; [OVERALL], RunTime(ms), 7.0 [OVERALL], Throughput(ops/sec), 0.0
看源代码也没什么问题。网上有帖子说可以自己编译源代码，然后就会一切正常了。下载源代码并编译，还是有问题，不过遇山开山、遇水架桥，好歹Core YCSB和Mongo DBBinding都编译出来了。 C:\gitws\YCSB&amp;gt;set MAVEN_OPTS=-Xmx512m -Xms128m -Xss2m C:\gitws\YCSB&amp;gt;mvn clean package -fae &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; [INFO] Reactor Summary: [INFO] [INFO] YCSB Root &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; SUCCESS [ 0.905 s] [INFO] Core YCSB &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; SUCCESS [ 8.986 s] [INFO] Cassandra DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. SUCCESS [ 8.315 s] [INFO] HBase DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. SUCCESS [ 24.290 s] [INFO] Hypertable DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; SUCCESS [ 6.412 s] [INFO] Accumulo DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. SUCCESS [ 27.691 s] [INFO] DynamoDB DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. SUCCESS [ 7.644 s] [INFO] ElasticSearch Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; SUCCESS [ 14.290 s] [INFO] Infinispan DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; SUCCESS [ 28.955 s] [INFO] JDBC DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; SUCCESS [ 5.538 s] [INFO] Mapkeeper DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. FAILURE [ 0.015 s] [INFO] Mongo DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. SUCCESS [ 3.588 s] [INFO] OrientDB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. SUCCESS [ 4.384 s] [INFO] Redis DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. SUCCESS [ 3.604 s] [INFO] Voldemort DB Binding &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. SUCCESS [ 4.493 s] [INFO] YCSB Release Distribution Builder &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; SUCCESS [ 17.488 s] [INFO] &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; [INFO] BUILD FAILURE [INFO] &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; [INFO] Total time: 02:46 min [INFO] Finished at: 2014-03-30T08:29:03-04:00 [INFO] Final Memory: 61M/182M [INFO] &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;
测试 C:\ycsb-0.1.4python bin/ycsb load mongodb -s -P workloads/workloada java -cp C:\ycsb-0.1.4\core\lib\core-0.1.4.jar;C:\ycsb-0.1.4\gemfire-binding\conf;C:\ycsb-0.1.4\hbase-binding\conf;C:\ycsb-0.1.4\infinispan-binding\conf;C:\ycsb -0.1.4\jdbc-binding\conf;C:\ycsb-0.1.4\mongodb-binding\lib\mongodb-binding-0.1.4.jar;C:\ycsb-0.1.4\nosqldb-binding\conf;C:\ycsb-0.1.4\voldemort-binding\conf com.yahoo.ycsb.Clie nt -db com.yahoo.ycsb.db.MongoDbClient -s -P workloads/workloada -load YCSB Client 0.1 Command line: -db com.yahoo.ycsb.db.MongoDbClient -s -P workloads/workloada -load Loading workload... Starting test. new database url = localhost:27017/ycsb 2014-03-31 15:37:36:410 0 sec: 0 operations; mongo connection created with localhost:27017/ycsb 2014-03-31 15:37:37:147 0 sec: 1000 operations; 1356.85 current ops/sec; [INSERT AverageLatency(us)=666.46] [CLEANUP AverageLatency(us)=549] [OVERALL], RunTime(ms), 769.0 [OVERALL], Throughput(ops/sec), 1300.3901170351105 [INSERT], Operations, 1000 [INSERT], AverageLatency(us), 666.457 [INSERT], MinLatency(us), 141 [INSERT], MaxLatency(us), 466548 [INSERT], 95thPercentileLatency(ms), 0 [INSERT], 99thPercentileLatency(ms), 0 [INSERT], Return=0, 1000 ............... [CLEANUP], Operations, 1 [CLEANUP], AverageLatency(us), 549.0 [CLEANUP], MinLatency(us), 549 [CLEANUP], MaxLatency(us), 549 [CLEANUP], 95thPercentileLatency(ms), 0 [CLEANUP], 99thPercentileLatency(ms), 0 ............... C:\ycsb-0.1.4python bin/ycsb run mongodb -s -P workloads/workloada java -cp C:\ycsb-0.1.4\core\lib\core-0.1.4.jar;C:\ycsb-0.1.4\gemfire-binding\conf;C:\ycsb-0.1.4\hbase-binding\conf;C:\ycsb-0.1.4\infinispan-binding\conf;C:\ycsb -0.1.4\jdbc-binding\conf;C:\ycsb-0.1.4\mongodb-binding\lib\mongodb-binding-0.1.4.jar;C:\ycsb-0.1.4\nosqldb-binding\conf;C:\ycsb-0.1.4\voldemort-binding\conf com.yahoo.ycsb.Clie nt -db com.yahoo.ycsb.db.MongoDbClient -s -P workloads/workloada -t YCSB Client 0.1 Command line: -db com.yahoo.ycsb.db.MongoDbClient -s -P workloads/workloada -t Loading workload... Starting test. new database url = localhost:27017/ycsb 2014-03-31 15:42:01:511 0 sec: 0 operations; mongo connection created with localhost:27017/ycsb 2014-03-31 15:42:01:776 0 sec: 1000 operations; 3773.58 current ops/sec; [UPDATE AverageLatency(us)=216.92] [READ AverageLatency(us)=173.41] [CLEANUP AverageLatency(us)=547] [OVERALL], RunTime(ms), 265.0 [OVERALL], Throughput(ops/sec), 3773.5849056603774 [UPDATE], Operations, 491 [UPDATE], AverageLatency(us), 216.9205702647658 [UPDATE], MinLatency(us), 138 [UPDATE], MaxLatency(us), 20078 [UPDATE], 95thPercentileLatency(ms), 0 [UPDATE], 99thPercentileLatency(ms), 0 [UPDATE], Return=0, 491 ............... [READ], Operations, 509 [READ], AverageLatency(us), 173.4106090373281 [READ], MinLatency(us), 100 [READ], MaxLatency(us), 19125 [READ], 95thPercentileLatency(ms), 0 [READ], 99thPercentileLatency(ms), 0 [READ], Return=0, 509 ............... [CLEANUP], Operations, 1 [CLEANUP], AverageLatency(us), 547.0 [CLEANUP], MinLatency(us), 547 [CLEANUP], MaxLatency(us), 547 [CLEANUP], 95thPercentileLatency(ms), 0 [CLEANUP], 99thPercentileLatency(ms), 0 ............... C:\ycsb-0.1.4 YCSB现在大概可用了，下一步就是编写所需的workload了。
参考  YCSB（GitHub）
 YCSB Wiki
  </content>
    </entry>
    
     <entry>
        <title>Hello MongoDB Java Driver</title>
        <url>https://mryqu.github.io/post/hello_mongodb_java_driver/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mongodb</tag><tag>java</tag><tag>driver</tag><tag>nosql</tag>
        </tags>
        <content type="html">  首先需要下载MongoDB JavaDriver，我一开始以为mongo-java-driver-2.9.3.jar是最新版，后来才发现MongoDB JavaDriver下载页面版本排序是按照文件名而不是按照日期排序，其实mongo-java-driver-2.12.0.jar是最新版。因为从版本2.10.0开始提供新的类MongoClient而不是Mongo来连接数据库，所以需要注意所使用的MongoDB Java Driver版本。 Java代码如下：
package com.yqu.mongodb; import java.net.UnknownHostException; import java.util.List; import java.util.Set; import com.mongodb.BasicDBObject; import com.mongodb.DB; import com.mongodb.DBCollection; import com.mongodb.DBCursor; import com.mongodb.DBObject; import com.mongodb.MongoClient; import com.mongodb.MongoClientURI; import com.mongodb.MongoException; public class HelloMongoDB { public static void main(String[] args) { MongoClient mongoClient = null; try { // Since 2.10.0, uses MongoClient // mongoClient = new MongoClient(&amp;quot;localhost&amp;quot;, 27017); String host = &amp;quot;mongodb://localhost:27017&amp;quot;; mongoClient = new MongoClient(new MongoClientURI(host)); showdbs(mongoClient); // MongoDB will create it if database doesn&#39;t exists DB db = mongoClient.getDB(&amp;quot;hellodb&amp;quot;); System.out.println(&amp;quot;\nget database &#39;hellodb&#39;.&amp;quot;); showdbs(mongoClient); showcollections(db); DBCollection coll = null; // MongoDB will create it if collection doesn&#39;t exists // There is a difference: // showcollections display the new collection name when // createCollection is used, // but not before any document is inserted when // getCollection is used. // // coll = db.createCollection(&amp;quot;helloCollection&amp;quot;, // new BasicDBObject(&amp;quot;capped&amp;quot;, false)); coll = db.getCollection(&amp;quot;helloCollection&amp;quot;); System.out.println(&amp;quot;\nget collection &#39;helloCollection&#39;.&amp;quot;); showcollections(db); BasicDBObject doc, query; doc = new BasicDBObject(&amp;quot;name&amp;quot;, &amp;quot;MongoDB&amp;quot;) .append(&amp;quot;type&amp;quot;, &amp;quot;database&amp;quot;) .append(&amp;quot;count&amp;quot;, 1) .append(&amp;quot;info&amp;quot;, new BasicDBObject(&amp;quot;x&amp;quot;, 123) .append(&amp;quot;y&amp;quot;, 321)); coll.insert(doc); doc = new BasicDBObject(&amp;quot;name&amp;quot;, &amp;quot;Java&amp;quot;) .append(&amp;quot;type&amp;quot;, &amp;quot;language&amp;quot;) .append(&amp;quot;count&amp;quot;, 3) .append(&amp;quot;info&amp;quot;, new BasicDBObject(&amp;quot;x&amp;quot;, 76) .append(&amp;quot;y&amp;quot;, 265)); coll.insert(doc); doc = new BasicDBObject(&amp;quot;name&amp;quot;, &amp;quot;Python&amp;quot;) .append(&amp;quot;type&amp;quot;, &amp;quot;language&amp;quot;) .append(&amp;quot;count&amp;quot;, 5) .append(&amp;quot;info&amp;quot;, new BasicDBObject(&amp;quot;x&amp;quot;, 2) .append(&amp;quot;y&amp;quot;, 2014)); coll.insert(doc); coll.createIndex(new BasicDBObject(&amp;quot;name&amp;quot;, &amp;quot;text&amp;quot;)); coll.createIndex(new BasicDBObject(&amp;quot;count&amp;quot;, 1)); System.out.println(&amp;quot;\nSome document are inserted.&amp;quot;); showdocuments(coll); searchdocuments(coll, new BasicDBObject(&amp;quot;name&amp;quot;, &amp;quot;MongoDB&amp;quot;)); searchdocuments(coll, new BasicDBObject(&amp;quot;count&amp;quot;, new BasicDBObject(&amp;quot;$lt&amp;quot;, 4))); // The embedded document query example searchdocuments(coll, new BasicDBObject(&amp;quot;info.x&amp;quot;, new BasicDBObject(&amp;quot;$lt&amp;quot;, 100))); // update document which name is &#39;Java&#39; query = new BasicDBObject(&amp;quot;name&amp;quot;, &amp;quot;Java&amp;quot;); BasicDBObject newDoc = new BasicDBObject(&amp;quot;name&amp;quot;, &amp;quot;R&amp;quot;); BasicDBObject updateObj = new BasicDBObject(&amp;quot;$set&amp;quot;, newDoc); coll.update(query, updateObj); System.out.println(&amp;quot;\nUpdate document which name is &#39;Java&#39;&amp;quot;); showdocuments(coll); showindexs(coll); showcollections(db); mongoClient.dropDatabase(&amp;quot;hellodb&amp;quot;); System.out.println(&amp;quot;drop database &#39;hellodb&#39;.&amp;quot;); showdbs(mongoClient); } catch (UnknownHostException e) { e.printStackTrace(); } catch (MongoException e) { e.printStackTrace(); } finally { if (mongoClient != null) { mongoClient.close(); } } } private static void showdbs(MongoClient mongoClient) { System.out.println(&amp;quot;\nCurrent databases:&amp;quot;); List dbs = mongoClient.getDatabaseNames(); if (dbs.isEmpty()) System.out.println(); else { for (String s : dbs) { System.out.println(&amp;quot; &amp;quot; &#43; s); } } } private static void showcollections(DB db) { System.out.println(&amp;quot;\nCurrent collections under &amp;quot; &#43; db.getName() &#43; &amp;quot;:&amp;quot;); Set collections = db.getCollectionNames(); if (collections.isEmpty()) System.out.println(); else { for (String s : collections) { System.out.println(&amp;quot; &amp;quot; &#43; s); } } } private static void showdocuments(DBCollection coll) { System.out.println(&amp;quot;\nCurrent documents under &amp;quot; &#43; coll.getName() &#43; &amp;quot;:&amp;quot;); DBCursor cur = coll.find(); while (cur.hasNext()) { System.out.println(&amp;quot; &amp;quot; &#43; cur.next()); } } private static void showindexs(DBCollection coll) { System.out.println(&amp;quot;\nCurrent index info under &amp;quot; &#43; coll.getName() &#43; &amp;quot;:&amp;quot;); List list = coll.getIndexInfo(); for (DBObject o : list) { System.out.println(o.get(&amp;quot;key&amp;quot;)); } } private static void searchdocuments( DBCollection coll, BasicDBObject query) { System.out.println(&amp;quot;\nQuery &amp;quot; &#43; query &#43; &amp;quot; for &amp;quot; &#43; coll.getName() &#43; &amp;quot;:&amp;quot;); DBCursor cur = coll.find(query); while (cur.hasNext()) { System.out.println(&amp;quot; &amp;quot; &#43; cur.next()); } } }  输出：  Current databases: local admin test
get database &amp;lsquo;hellodb&amp;rsquo;.
Current databases: local admin test
Current collections under hellodb:
get collection &amp;lsquo;helloCollection&amp;rsquo;.
Current collections under hellodb:
Some document are inserted.
Current documents under helloCollection: { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff8&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;MongoDB&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;database&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 1 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 123 , &amp;ldquo;y&amp;rdquo; : 321}} { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff9&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;Java&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;language&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 3 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 76 , &amp;ldquo;y&amp;rdquo; : 265}} { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ffa&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;Python&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;language&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 5 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 2 , &amp;ldquo;y&amp;rdquo; : 2014}}
Query { &amp;ldquo;name&amp;rdquo; : &amp;ldquo;MongoDB&amp;rdquo;} for helloCollection: { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff8&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;MongoDB&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;database&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 1 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 123 , &amp;ldquo;y&amp;rdquo; : 321}}
Query { &amp;ldquo;count&amp;rdquo; : { &amp;ldquo;$lt&amp;rdquo; : 4}} for helloCollection: { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff8&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;MongoDB&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;database&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 1 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 123 , &amp;ldquo;y&amp;rdquo; : 321}} { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff9&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;Java&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;language&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 3 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 76 , &amp;ldquo;y&amp;rdquo; : 265}}
Query { &amp;ldquo;info.x&amp;rdquo; : { &amp;ldquo;$lt&amp;rdquo; : 100}} for helloCollection: { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff9&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;Java&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;language&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 3 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 76 , &amp;ldquo;y&amp;rdquo; : 265}} { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ffa&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;Python&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;language&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 5 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 2 , &amp;ldquo;y&amp;rdquo; : 2014}}
update document which name is &amp;lsquo;Java&amp;rsquo;
Current documents under helloCollection: { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff8&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;MongoDB&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;database&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 1 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 123 , &amp;ldquo;y&amp;rdquo; : 321}} { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ff9&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;R&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;language&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 3 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 76 , &amp;ldquo;y&amp;rdquo; : 265}} { &amp;ldquo;_id&amp;rdquo; : { &amp;ldquo;$oid&amp;rdquo; : &amp;ldquo;5336644eb767266396077ffa&amp;rdquo;} , &amp;ldquo;name&amp;rdquo; : &amp;ldquo;Python&amp;rdquo; , &amp;ldquo;type&amp;rdquo; : &amp;ldquo;language&amp;rdquo; , &amp;ldquo;count&amp;rdquo; : 5 , &amp;ldquo;info&amp;rdquo; : { &amp;ldquo;x&amp;rdquo; : 2 , &amp;ldquo;y&amp;rdquo; : 2014}}
Current index info under helloCollection: { &amp;ldquo;_id&amp;rdquo; : 1} { &amp;ldquo;_fts&amp;rdquo; : &amp;ldquo;text&amp;rdquo; , &amp;ldquo;_ftsx&amp;rdquo; : 1} { &amp;ldquo;count&amp;rdquo; : 1}
Current collections under hellodb: helloCollection system.indexes drop database &amp;lsquo;hellodb&amp;rsquo;.
Current databases: local admin test
参考 Getting Started with Java Driver
MongoDB Drivers
MongoDB Driver Syntax Table
MongoDB Java Driver API Document
</content>
    </entry>
    
     <entry>
        <title>Windows下MongoDB安装与配置</title>
        <url>https://mryqu.github.io/post/windows%E4%B8%8Bmongodb%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mongodb</tag><tag>安装</tag><tag>配置</tag><tag>nosql</tag><tag>windows</tag>
        </tags>
        <content type="html">  到官网http://www.mongodb.org/downloads下载MongoDB并安装在c:mongodb目录下，安装后需要进行配置才能启动MongoDB。首先创建如下目录： - md c:\mongodb\data - md c:\mongodb\db - md c:\mongodb\log - md c:\mongodb\etc 接着创建MongoDB配置文件：c:\mongodb\etc\mongo.conf
systemLog: destination: file path: C:/mongodb/log/mongo.log logAppend: true storage: dbPath: C:/mongodb/data/db journal: enabled: true net: http: enabled: true RESTInterfaceEnabled: true  通过如下命令启动MongoDB：
C:\mongodb\bin\mongod.exe --config C:\mongodb\etc\mongo.conf  可以通过以下三种方式验证MongoDB是否安装成功： - 访问MongoDB监听端口 http://localhost:27017/ ，确认出现类似信息。- 通过C:\mongodb\bin\mongo.exe登录Javascript shell并使用 - 访问HTTP接口 http://localhost:28017/ 并查看MongoDB服务器信息
在启动MongoDB的窗口通过Ctrl&#43; C停止MongoDB的运行。 最后通过如下命令将MongoDB配置成Windows服务：
C:\mongodb\bin\mongod.exe --config C:\mongodb\etc\mongo.conf--install  当然也可以通过如下命令手工配置Windows服务：
sc create MongoDB binPath= &amp;quot;C:\mongodb\bin\mongod.exe--config=C:\mongodb\etc\mongo.conf--service&amp;quot; start= demand DisplayName= &amp;quot;MongoDB&amp;quot;  参考 Install MongoDB on Windows
MongoDB Configuration Options
</content>
    </entry>
    
     <entry>
        <title>重温MVC:一个很好的MVC图</title>
        <url>https://mryqu.github.io/post/%E9%87%8D%E6%B8%A9mvc%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A5%BD%E7%9A%84mvc%E5%9B%BE/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>mvc</tag><tag>model/view/controlle</tag><tag>pattern</tag><tag>uml</tag>
        </tags>
        <content type="html"> 今天看了一个帖子A terrific Model View Controller (MVC) diagram，感觉文中说的很对：一个技术如果能简洁地表达出来，才容易被人记住并记的牢。该文中的Model/View/ControllerUML图共有两个，第一个图非常简单，仅展示作者用于控制器、视图和模型的符号。第二个图展示了MVC模式允许的操作和禁止的操作。- 用户与视图对象交互。 - 视图对象和控制器对象可以相互访问调用。 - 不同的控制器对象之间可以相互访问调用。 - 控制器对象可以访问调用模型对象。 - 除了上面这四种访问方式，禁止对象之间的其他通信方法。
</content>
    </entry>
    
     <entry>
        <title>玩一下LineNumberReader</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E4%B8%80%E4%B8%8Blinenumberreader/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>linenumberreader</tag><tag>java</tag><tag>文件</tag><tag>行数</tag>
        </tags>
        <content type="html"> 找资料的副产品就是发现了LineNumberReader这个类，跟它的父类BufferedReader相比多了计算文件行数的功能。记不得以前是否用过了，这里记录一下备用。
import java.io.BufferedReader; import java.io.FileReader; import java.io.IOException; import java.io.LineNumberReader; public class TestFileLineNum { public static int countLinesV1(String flName) throws IOException { BufferedReader reader = new BufferedReader(new FileReader(flName)); int cnt = 0; while (reader.readLine() != null) { cnt&#43;&#43; ; } reader.close(); return cnt; } public static int countLinesV2(String flName) throws IOException { LineNumberReader reader = new LineNumberReader(new FileReader(flName)); while (reader.readLine() != null) { } int cnt = reader.getLineNumber(); reader.close(); return cnt; } public static void main(String[] args) throws IOException { for(int i=0;i&amp;lt;4;i ) { String flName = &amp;quot;test&amp;quot; &#43; i &#43; &amp;quot;.txt&amp;quot;; System.out.println(&amp;quot;File &amp;quot;&#43;flName&#43;&amp;quot; has &amp;quot;&#43;countLinesV2(flName)&#43;&amp;quot; line(s).&amp;quot;); } } }  </content>
    </entry>
    
     <entry>
        <title>[算法] 学习无向图</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_%E5%AD%A6%E4%B9%A0%E6%97%A0%E5%90%91%E5%9B%BE/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>algorithm</tag><tag>graph</tag><tag>cycle</tag><tag>bipartite</tag><tag>eulerian</tag>
        </tags>
        <content type="html">  本文是学习http://algs4.cs.princeton.edu/41graph/的吐槽和体会。
一点吐槽 一开始对GraphClient.java中numberOfSelfLoops函数进行count/2不解，翻回Graph.java，看看addEdge函数才明白，合着对自循环连接，adj里面加两次。这么干有什么好处么？！！
再后来看Cycle.java，对hasParallelEdges函数又不解，仔细想想Graph.java不但接受addEdge(1,23)和addEdge(23, 1)，对执行多次addEdge(1,23)也不拒绝。一个无向图这么弄，对么？算不算检查不严格呀？！！
理解Cycle.java 示例代码：
public static void main(String[] args) { Graph G = new Graph(4); G.addEdge(0, 1); G.addEdge(1, 2); G.addEdge(1, 3); G.addEdge(2, 3); Cycle finder = new Cycle(G); if (finder.hasCycle()) { for (int v : finder.cycle()) { StdOut.print(v &#43; &amp;quot; &amp;quot;); } StdOut.println(); } else { StdOut.println(&amp;quot;Graph is acyclic&amp;quot;); } }  dfs搜索：
|上一节点u|当前节点v|下一节点w|注解 |&amp;mdash;&amp;ndash; |-1|0|1|节点1没有标注，嵌套dfs |0|1|3|节点3没有标注，嵌套dfs |1|3|2|节点2没有标注，嵌套dfs |3|2|1|节点1已标注，发现cycle，创建栈，依次： - (在for循环中)push 2 - (在for循环中)push 3 - push 1 - push 2说白了就是当发现下一节点w已经标注，那就返回去找w到v的路径，然后凑上w和v，就是一个环路。
理解Bipartite.java 二分图又称作二部图、两偶图，是图论中的一种特殊模型。设G=(V,E）是一个无向图，如果顶点V可分割为两个互不相交的子集(A,B），并且图中的每条边（i，j）所关联的两个顶点i和j分别属于这两个不同的顶点集（iin A,j in B），则称图G为一个二分图。
简而言之，就是顶点集V可分割为两个互不相交的子集，并且图中每条边依附的两个顶点都分属于这两个互不相交的子集。
Bipartite.java的控制流与CC.java差不多。设定图中节点为红黑两种颜色，为了使相邻节点不同颜色，dfs时下一节点颜色为当前节点反色。当遇到环路时，如果环路节点数为奇数个（下一节点已标注且颜色与当前节点相同），则该图非二分图。
理解Bridge.java 桥（割边cut-edge）是一个边，当其被删除时连通组件个数会增多。同样道理，一个边仅没有被任何环路包含时才是一个桥。
示例代码：
public static void main(String[] args) { Graph G = new Graph(5); G.addEdge(0, 1); G.addEdge(1, 3); G.addEdge(1, 2); G.addEdge(2, 3); G.addEdge(3, 4); StdOut.println(G); Bridge bridge = new Bridge(G); StdOut.println(&amp;quot;Edge connected components = &amp;quot; &#43; bridge.components()); }  dfs搜索： dfs过程中，发现环路后整个环路上所有节点low值降成环路中第一个处理节点的pre值。下一个节点pre值与最终low值一致的话，当前节点和下一节点之间的边为桥。
注：对于连接组件的第一个处理节点，Cycle.java中dfs函数的u参数为-1，而Bridge.java中的dfs函数的u参数为当前节点v。
输出日志：
5 vertices, 5 edges 0: 1 1: 2 3 0 2: 3 1 3: 4 2 1 4: 3 3-4 is a bridge 0-1 is a bridge Edge connected components = 3  理解Biconnected.java 关节点（articulation vertex，又称割点cutvertex）是一个节点，当其被删除时连通组件个数会增多。当没有关节点时，图为双连通图。
示例代码：
public static void main(String[] args) { Graph G = new Graph(5); G.addEdge(0, 1); G.addEdge(1, 3); G.addEdge(1, 2); G.addEdge(2, 3); G.addEdge(3, 4); StdOut.println(G); Biconnected bic = new Biconnected(G); // print out articulation points StdOut.println(); StdOut.println(&amp;quot;Articulation points&amp;quot;); StdOut.println(&amp;quot;-------------------&amp;quot;); for (int v = 0; v &amp;lt; G.V(); v&#43;&#43;) if (bic.isArticulation(v)) StdOut.println(v); }  dfs搜索： dfs过程中，发现环路后整个环路上所有节点low值降成环路中第一个处理节点的pre值。当前节点pre值小于等于下一节点最终low值，且当前节点非连接组件第一个处理节点时，当前节点即为割点。
注：对于连接组件的第一个处理节点，Cycle.java中dfs函数的u参数为-1，而Biconnected.java中的dfs函数的u参数为当前节点v。
输出日志：
5 vertices, 5 edges 0: 1 1: 2 3 0 2: 3 1 3: 4 2 1 4: 3 Articulation points ------------------- 1 3  理解EulerianCycle.java 通过图（无向图或有向图）中所有边且每边仅通过一次通路称为欧拉通路（Eulerian trail，Eulerianpath），相应的回路称为欧拉回路（ Eulerian circuit，Euleriancycle）。具有欧拉回路的图称为欧拉图（EulerGraph），具有欧拉通路而无欧拉回路的图称为半欧拉图（Semi-Eulerian）。
|图论起源于18世纪，1736年瑞士数学家欧拉（Euler）发表了图论的第一篇论文“哥尼斯堡七桥问题”。在当时的哥尼斯堡城有一条横贯全市的普雷格尔河，河中的两个岛与两岸用七座桥连结起来。当时那里的居民热衷于一个难题：有游人怎样不重复地走遍七桥，最后回到出发点。为了解决这个问题，欧拉用A,B,C,D4个字母代替陆地，作为4个顶点，将联结两块陆地的桥用相应的线段表示，于是哥尼斯堡七桥问题就变成了图中，是否存在经过每条边一次且仅一次，经过所有的顶点的回路问题了。欧拉在论文中指出，这样的回路是不存在的。|无向连通图G含有欧拉通路，当且仅当G有零个或两个奇数度的结点；无向连通图G是欧拉图，当且仅当G不含奇数度结点(G的所有结点度数为偶数)。
EulerianCycle.java判断当某个节点度为奇数，则不是欧拉图。否则任选一点度大于零的点做起点，记录路径是否走过，一点接一点直到当前点对相邻点没有未走路径为止。最后判断所走路径上节点数是否为图所有节点个数&#43;1，如不等则非欧拉图。（感觉即使是欧拉图靠贪婪查找也不确保能获得正确路径。）
平面图 一个图是片面的，指其画在平面时边没有交叉。Hopcroft-Tarjan算法基于dfs能在线性时间内判断图是否为平面的。
汉密尔顿图 通过图（无向图或有向图）中所有节点且每节点仅通过一次的通路称为哈密顿通路（Hamiltonian path，Traceablepath），相应的回路称作汉密尔顿回路（Hamiltonian cycle）。
美国图论数学家奥勒在1960年给出了一个图是哈密尔顿图的充分条件：对于顶点个数大于2的图，如果图中任意两点度的和大于或等于顶点总数，那这个图一定是哈密尔顿图。寻找哈密顿路径是一个典型的NP-完全问题。
</content>
    </entry>
    
     <entry>
        <title>GitHub fork操作</title>
        <url>https://mryqu.github.io/post/github_fork%E6%93%8D%E4%BD%9C/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>github</tag><tag>fork</tag><tag>push_request</tag><tag>upstream</tag><tag>origin</tag>
        </tags>
        <content type="html">  Fork（分支）操作不是Git实现的一部分，仅是GitHub独有的一个在服务器端克隆代码库的操作。假定我作为GitHub用户usr2，对用户usr1的github.com/usr1/demo代码库有兴趣。我可以通过clone命令将该代码库克隆到本机，并且可以通过pull命令获得该代码库的更新。但是除非用户usr1将我（用户usr2）设为该代码库的贡献者，否则我无法将我提交的修改通过push命令推送到该代码库。但是通过fork操作，我就可以将github.com/usr1/demo代码库完整复制到我的GitHub帐号下（包括代码库中的文件、提交历史、问题等等），例如下图的github.com/usr2/demo。我可通过clone命令将自己的github.com/usr2/demo代码库克隆到本机，我完全有权限将本机提交的修改通过push命令推送到上述自己的代码库。如果我希望我的修改被usr1采纳，我可以发送一个pullrequest通知usr1。至于usr1是否接收我的修改，决定权在usr1。克隆代码库的时候，所使用的远程代码库地址自动被Git命名为origin。其效果类似于：
git remote add origin git://github.com/usr2/demo.git  github.com/usr2/demo代码库在fork操作之后就不再获得github.com/usr1/demo的后继更新了。可以手工添加github.com/usr1/demo为上游代码库地址：
git remote add upstream git://github.com/usr1/demo.git  我可通过fetch命令获取上游代码库的更新，在本机合并后，通过push命令推送到自己的远程代码库。
参考 GitHub help: Fork A Repo
GitHub help: Syncing a fork
GitHub help: Adding collaborators to a personal repository
Simple guide to forks in GitHub and Git
stackOverflow: Git fork is git clone?
stackOverflow: What is the difference between origin and upstream in github
</content>
    </entry>
    
     <entry>
        <title>转战Octave</title>
        <url>https://mryqu.github.io/post/%E8%BD%AC%E6%88%98octave/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>octave</tag><tag>matlab</tag><tag>机器学习</tag>
        </tags>
        <content type="html"> 可赶上机器学习的课了，原来学过《集体编程智慧》里面机器学习算法的Python实现，这次想着有个巩固提高。可是老师偏偏使用Octave，杂就杂吧，学学matlab风格的编程也不错。 Matlab是一款数值计算和分析的优秀软件，可是价格昂贵。其开源替代品就是Scilab和Octave。
Scilab是由INRIA（法国国立计算机及自动化研究院）和ENPC（法国国立桥梁学院）开发的开源科学计算自由软件。与Matlab类似，Scilab数据类型丰富，可以很方便地实现各种矩阵运算与图形显示，能应用于科学计算、数学建模、信号处理、决策优化、线性/非线性控制等各个方面。它还提供可以满足不同工程与科学需要的工具箱，例如Scicos，信号处理工具箱，图与网络工具箱等。可以说，就基本的功能如科学计算、矩阵处理及图形显示而言，Matlab能完成的工作Scilab都可以实现。由于Scilab的语法与Matlab非常接近，熟悉Matlab编程的人很快就会掌握Scilab的使用。有意思的是，Scilab提供的语言转换函数可以自动将用Matlab语言编写的程序翻译为Scilab语言。
GNU Octave是自由软件基金会支持的遵循GPL协议的一个自由再发布的软件，作者是以John W.Eaton为首的一些志愿者。它提供了一个环境，该环境支持叫做GNUOctave的高级语言，这种语言与Matlab兼容，主要用于数值计算。它提供了一个方便的命令行方式，可以数值求解线性和非线性问题，以及做一些数值模拟。 Octave也提供了一些工具包，可以解决一般的线性代数问题，非线性方程求根，常规函数积分，处理多项式，处理常微分方程和微分代数方程。它也很容易的使用Octave自带的接口方式扩展和定制功能。
Octave相对于Scilab，对Matlab的语法兼容性更好，几乎没有差别。比如，Octave也使用M文件的形式来扩展功能和定义函数。因此熟悉Matlab的用户更容易接受Octave环境。 它可编程的性能更好，Octave语言功能更为强大，几乎提供所有系统函数的支持，Octave在语法上也更接近C的语法，比如提供&#43;&#43;和&amp;ndash;这样的预算符。这样，我们可以在Octave环境里面增加一些更为强大和易用的扩展。不象在Matlab和Scilab环境中限制比较多，有时无法充分的利用系统资源。它的计算库都是用C写，而Scilab则基本是Fortran的编写的。这也是一份有用的资源。和GNU下面的其他软件也可以较多协作。劣势就是Octave的功能比起Scilab要简单一些，这使得Octave对一些用户来说意义不如Scilab大。而且Octave目前没有图形界面，只能以命令行方式进行交互。
第一次编程作业的效果图：
将原来的一些编程思路转换成矩阵和矢量是一个挑战，对于复杂一些编程，还是逃不开纠结和调试。
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 压缩MapReduce的Mapper输出</title>
        <url>https://mryqu.github.io/post/hadoop_%E5%8E%8B%E7%BC%A9mapreduce%E7%9A%84mapper%E8%BE%93%E5%87%BA/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>mapper</tag><tag>output</tag><tag>compression</tag>
        </tags>
        <content type="html">  介绍 压缩map输出是以压缩和解压缩的CPU消耗为代价来减少磁盘和网络IO开销。Hadoop中，mapper的输出数据是写到Mapper所在的本地磁盘的，并以网络传输的方式传送到Reducer所在的节点上。mapreduce.map.output.compress默认为false，即不对map输出进行压缩。如果中间数据不是二进制的，通常建议使用map输出压缩。
Hadoop支持的的内建压缩编码如下： 如果设置mapreduce.map.output.compress为true且没有设置mapreduce.map.output.compression.codec的话，默认使用org.apache.hadoop.io.compress.DefaultCodec。
|压缩格式|算法|工具|默认扩展名|说明 |&amp;mdash;&amp;ndash; |DEFLATE|DEFLATE|N/A|.deflate|DEFLATE是一种压缩算法，标准实现是zlib，尚没有命令行工具支持。文件扩展名.deflate是一个Hadoop的约定。所有的压缩算法都存在空间与时间的权衡：更快的压缩速率和解压速率是以牺牲压缩率为代价的。org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionLevel中定义了0~9压缩级别，0为无压缩，9为最佳压缩。支持Java实现和原生库实现。 |GZip|DEFLATE|gzip|.gz|GZip与DEFLATE使用同样的压缩算法，不过相对于DEFLATE压缩格式增加了额外的头部和尾部。GZip是一种常规的压缩工具，空间与时间得到很好的权衡。支持Java实现和原生库实现。 |BZip2|BZip2|BZip2|.bz2|BZip2压缩率高于GZip，但压缩速度较慢；解析速度优于它的压缩速度，但还是较其它压缩算法偏慢。由上图可知，BZip2是Hadoop内嵌压缩算法中唯一可以被分割的，这样一个输入文件可分成多个InputSplit，便于本地数据加载并被Mapper处理。相关技术可见处理跨块边界的InputSplit一文。支持Java实现和原生库实现。 |LZ4|LZ4|N/A|.lz4|LZ4和Snappy相对于GZip压缩速度得到很大提升，但没有GZip的压缩率高。仅支持原生库实现。org.apache.hadoop.io.compress.Lz4Codec.isNativeCodeLoaded()用于检查是否加载原生库，其调用者在没有加载原生库时会抛异常。 |Snappy|Snappy|N/A|.snappy|LZ4和Snappy相对于GZip压缩速度得到很大提升，但没有GZip的压缩率高。仅支持原生库实现。org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded()在没有加载原生库时会抛异常。
如果使用原生库压缩编码，需配置LD_LIBRARY_PATH。默认情况下，Hadoop自动在本地库路径（java.library.path）下查询并加载合适的本地库实现。通过设置属性io.native.lib.available为false禁用原生库，此时内建的Java实现将被使用。
Hadoop源码分析 在org.apache.hadoop.mapred.MapTask.MapOutputBuffer.init(Context)和org.apache.hadoop.mapred.ReduceTask.initCodec()方法中检查mapreduce.map.output.compress属性，如果为true，则加载mapreduce.map.output.compression.codec属性所设的压缩编解码器。 MapTask当将数据spill到硬盘时使用压缩编码器进行数据压缩。 ReduceTask在使用Shuffle结果是使用压缩解码器进行数据解压缩。
使用map输出压缩的应用示例 </content>
    </entry>
    
     <entry>
        <title>手工修改Chrome配置文件</title>
        <url>https://mryqu.github.io/post/%E6%89%8B%E5%B7%A5%E4%BF%AE%E6%94%B9chrome%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>chrome</tag><tag>配置</tag>
        </tags>
        <content type="html"> 当初装Chrome，g.cn上死活下载不了，随便装了一个澳洲版的。结果访问taobao、weibo被认成了澳洲用户，都被推到了海外入口。 一开始折腾Chrome的配置菜单，没找到。后来直接修改C:\Users\AppData\Local\Google\Chrome\UserData\Default\Preferences搞定了。 原始值：
 &amp;quot;last_known_google_url&amp;quot;: &amp;quot;https://www.google.com.au/&amp;quot;, &amp;quot;last_prompted_google_url&amp;quot;: &amp;quot;https://www.google.com.au/&amp;quot;,  修改后：
 &amp;quot;last_known_google_url&amp;quot;: &amp;quot;https://www.google.com.hk/&amp;quot;, &amp;quot;last_prompted_google_url&amp;quot;: &amp;quot;https://www.google.com.hk/&amp;quot;,  </content>
    </entry>
    
     <entry>
        <title>[算法] Mergesort练习</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_mergesort%E7%BB%83%E4%B9%A0/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>algorithm</tag><tag>mergesort</tag><tag>exercise</tag>
        </tags>
        <content type="html"> 本作业帖用于练习http://algs4.cs.princeton.edu/22mergesort/里面的作业。
 Merge with atmost log N compares per item. Design a mergingalgorithm such that each item is compared at most a logarithmicnumber of times. (In the standard merging algorithm, an item can becompared N/2 times when merging two subarrays of size N/2.) reference  Lower boundfor sorting a Youngtableaux. A _Youngtableaux_ is an N-by-N matrix such that theentries are sorted both column wise and row wise. Prove thatTheta(N^2 log N) compares are necessary to sort the N^2 entries(where you can access the data only through the pairwisecomparisons). _Solution sketch._ If entry (i, j) iswithin 1&amp;frasl;2 of i &#43; j, then all of the 2N-1 grid diagonals areindependent of one another. Sorting the diagonals takes N^2 log Ncompares. Given anarray a of size 2N with N items insorted order in positions 0 through N-1, and anarray b of size N with N items inascending order, merge the array b into a sothat a contains all of the items inascending order. Use O(1) extra memory. _Hint:_ merge from right to left.  k-near-sorting. Suppose you have anarray a[] of N distinct items whichis nearly sorted: each item at most k positions away from itsposition in the sorted order. Design an algorithm to sort the arrayin time proportional to N log k. Hint: First, sort the subarray from 0 to 2k; the smallestk items will be in their correct position. Next, sort the subarrayfrom k to 3k; the smallest 2k items will now be in their correctposition.  Find a family ofinputs for which mergesort makes strictly fewer than 1&amp;frasl;2 N lg Ncompares to sort an array of N distinct keys. _Solution:_ a reverse-sorted array of N =2^k &#43; 1 keys uses approximately 1&amp;frasl;2 N lg N - (k/2 - 1)compares. Write aprogram SecureShuffle.java toread in a sequence of string from standard input and securelyshuffle them. Use the following algorithm: associate each card witha random real number between 0 and 1. Sort the values based ontheir associated real numbers. Use java.security.SecureRandom to generate therandom real numbers. Use Merge.indexSort() toget the random permutation. Merging twoarrays of different lengths. Given two sortedarrays a[] and b[] of sizes M and N where M ≥ N, devisean algorithm to merge them into a new sortedarray c[] using ~ N lg M compares. Hint: use binary search. Note: there is a lower bound of Omega(N log (1 &#43; M/N)) compares. Thisfollows because there are M&#43;N choose N possible merged outcomes. Adecision tree argument shows that this requires at least lg (M&#43;Nchoose N) compares. We note that n choose r &amp;gt;= (n/r)^r. 测试结果：  1 2 3 4 5 7 merge=4 ,mergeBinary=4 ,2*lg4=4.0 1 2 3 4 5 6 7 8 9 10 11 12 13 15 merge=12 ,mergeBinary=16 ,6*lg8=17.999999999999996 1 2 3 4 5 6 7 8 9 merge=8 ,mergeBinary=4 ,1*lg8=3.0  使用二分法查找的合并比较次数有可能反而更多，但是它能保证最差情况下比较次数~ N lgM。例如最后一个案例merge跟mergeBinary相比比较次数差距悬殊。 Merging threearrays. Given three sortedarrays a[], b[], and c[], each of size N,design an algorithm to merge them into a new sortedarray d[] using at most ~ 6 Ncompares in the worst case (or, even better, ~ 5 N compares). 测试结果：  N=4:merge=21 ,mergeBy2Steps=18 ,binaryMergeBy2Steps=17 N=4:merge=12 ,mergeBy2Steps=12 ,binaryMergeBy2Steps=7 N=4:merge=12 ,mergeBy2Steps=8 ,binaryMergeBy2Steps=20 N=8:merge=45 ,mergeBy2Steps=38 ,binaryMergeBy2Steps=44  从测试结果来看，三种方式都能满足最欢情况下不超过6N， 其中mergeBy2Steps没有出现高出另两种方式的比较次数。 Merging threearrays. Given three sortedarrays a[], b[], and c[], each of size N,prove that no compare-based algorithm can merge them into a newsorted array d[] using fewer than ~4.754887503 N compares in the worst case. Arrays withN^(3&amp;frasl;2) inversions. Prove that anycompare-based algorithms that can sort arrays with N^(3&amp;frasl;2) or fewerinversions must make ~ 1&amp;frasl;2 N lg N compares in the worst case. Proof sketch: divide up the array into sqrt(N) consecutivesubarrays of sqrt(N) items each, such that the there are noinversion between items in different subarrays but the order ofitems within each subarray is arbitrary. Such an array has at mostN^(3&amp;frasl;2) inversions—at most ~N/2 inversions in each of the sqrt(N)subarrays. By the sorting lower bound, it takes ~ sqrt(N) lgsqrt(N) compares to sort each subarray, for a total of ~ 1&amp;frasl;2 N lgN.  </content>
    </entry>
    
     <entry>
        <title>[算法] 求数组中倒置个数</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_%E6%B1%82%E6%95%B0%E7%BB%84%E4%B8%AD%E5%80%92%E7%BD%AE%E4%B8%AA%E6%95%B0/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>algorithm</tag><tag>inversion</tag><tag>mergesort</tag><tag>nlogn</tag>
        </tags>
        <content type="html"> http://algs4.cs.princeton.edu/22mergesort/中有一道题是求数组中倒置个数的，原题如下：
Inversions. Develop and implement alinearithmic algorithm Inversions.java forcomputing the number of inversions in a given array (the number ofexchanges that would be performed by insertion sort for thatarray). This quantity is related tothe Kendall tau distance; 解决思路： Inversions.java在做MergeSort的过程中顺便求出了数组中倒置个数。假设上图中已经获得左右两个子部分中的倒置个数并进行了MergeSort。现在学习一下做整个数组的Merge时如何顺便计算这一层的倒置个数。 - 当aux[0]和aux[5]进行比较时，A小于E，则可知A比左半部分都小（少比较了4次），倒置&#43;5 - 当aux[0]和aux[6]进行比较时，C小于E，则可知C比左半部分都小（少比较了4次），倒置&#43;5 - 当aux[2]和aux[7]进行比较时，E小于G，则可知E比左半部分中G及其之后的数都小（少比较了2次），倒置=&#43;(4-2&#43;1)=&#43;3
整个数组的倒置数为左边部分内部倒置数&#43;右边部分内部倒置数&#43;13。算法时间复杂度为NlogN。
</content>
    </entry>
    
     <entry>
        <title>Markdown介绍</title>
        <url>https://mryqu.github.io/post/markdown%E4%BB%8B%E7%BB%8D/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>markdown</tag><tag>rmarkdown</tag><tag>knitr</tag><tag>文艺编程</tag><tag>可重复性研究</tag>
        </tags>
        <content type="html">  Markdown是一种用于普通文本的便于读写的轻量级标记语言，可以转换成HTML、LaTeX或Docbook文档。很多网站都支持Markdown，如GitHub、Wikipedia和博客平台WordPress。RMarkdown将R代码和markdown进行有效集成以用于文学编程（LiterateProgramming），R代码会在RMarkdown转化markdown处理过程中被执行，并将R代码结果一同插入markdown文档。下面的示例是基于knitr工具完成RMarkDown转换的。
Markdown 文字样式：斜体和粗体、删除线 *斜体表述1* 和 _斜体表述2_ 示例。 **粗体表述1** 和 __粗体表述2__ 示例。 ___斜粗体表述1___ 和 ***斜粗体表述2*** 示例。 ~~删除线表述~~ 示例。  斜体表述1 和 斜体表述2 示例。粗体表述1 和 粗体表述2 示例。斜粗体表述1 和 斜粗体表述2示例。删除线表述 示例。
段落和换行 一个空行（两个回车）会被转换成分段，在行末加两个或多个空格会被转换成换行。
标题 在行首用一到六个井号 (#) 开始该行为标题行 # H1示例 ## H2示例 ### H3示例 #### H4示例 ##### H5示例 ###### H6示例  H1示例 H2示例 H3示例 H4示例 H5示例 H6示例 另一种方式用下一行的一到多个等号表示一级标题，一到多个短划线表示二级标题。
H1示例 ========= H2示例 ---------  H1示例 H2示例 水平分割线 通过单行输入3个或3个以上中短划线、星号或下划线进行输入水平分隔线, 短划线、星号或下划线之间可以包含空格：
--- *** ___ - - - * * * _ _ _  列表 HTML 列表分无序列表 (unordered list, ul) 和有序列表 (ordered list, ol) 两种。在Markdown 中用星号、加号、减号开始一行表示无序列表，用数字开始一行表示有序列表。例如：
无序列表示例 - 无序列表1 &#43; 无序列表2 * 无序列表3   无序列表1 无序列表2 无序列表3  嵌套列表示例，下一级列表需较当前列表级增加4个空格缩进。
- 无序列表级1 - 无序列表级2 &#43; 无序列表级1 &#43; 无序列表级2 * 无序列表级1 * 无序列表级2   无序列表级1  无序列表级2  无序列表级1  无序列表级2  无序列表级1  无序列表级2   有序列表示例 1. 有序列表 2. 有序列表 3. 有序列表   有序列表 有序列表 有序列表  表格 | 左对齐列 | 居中对齐列 | 右对齐列 | | :------------- |:----------------:| ---------:| | col 7 is | thanks, bye bye | $2012 | | col 123 is | centered | $14 | | zebra stripes | are neat | $1 |  | 左对齐列 | 居中对齐列 | 右对齐列 |
| :&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- |:&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-:| &amp;mdash;&amp;mdash;&amp;mdash;:|
| col 7 is | thanks, bye bye | $2012 |
| col 123 is | centered | $14 |
| zebra stripes | are neat | $1 |
代码及代码块 使用`printf()`函数，转换后的printf()会在HTML的code标签体内. 如果一到多行代码段每行前使用4个空格对齐，标准MD就会识别为代码段。
引用块 用右尖括号 (&amp;gt;) 表示 blockquote，你一定见过邮件中这样表示引用别人的内容。可以嵌套，可以包含其它的Markdown 元素，例如：
&amp;gt; ## This is a header. &amp;gt; &amp;gt; 1. This is the first list item. &amp;gt; 2. This is the second list item. &amp;gt; &amp;gt; Here&#39;s some example code: &amp;gt; &amp;gt; return shell_exec(&amp;quot;echo $input | $markdown_script&amp;quot;);   This is a header.  This is the first list item. This is the second list item.  Here&amp;rsquo;s some example code:
return shell_exec(&amp;quot;echo $input | $markdown_script&amp;quot;);   链接 链接 [Download R] (http://www.r-project.org/ ) [RStudio] (http://www.rstudio.com/ )  Download R
RStudio
高级链接 两个网址 [R bloggers][1] 和 [Simply Statistics][2] 值得关注！ [1]: http://www.r-bloggers.com/ &amp;quot;R bloggers&amp;quot; [2]: http://simplystatistics.org/ &amp;quot;Simply Statistics&amp;quot;  两个网址 R bloggers 和 Simply Statistics 值得关注！
自动URL连接 当MD中存在URL，转换后的HTML会包含其链接。 http://example.com  http://example.com
反斜杠 Markdown可以利用反斜杠来消除Markdown语法中的标记。例如：如果期望最终HTML的字符串两侧外包星号，可以在星号的前面加上反斜杠消除粗体转换：
\*literal asterisks\* *literal asterisks*  *literal asterisks* literal asterisks
RMarkdown 下列是RMarkdown特有的R代码段。 ```{r echo=TRUE} plot(sin, -pi, 2 * pi) ```
plot(sin, -pi, 2 * pi) knitr RStudio对knitr有很好的集成，可以通过工具栏完成对RMarkdown文件进行编译生成md和html文件，还可以发布到RPubs网站上。 但有时候烦懒只想用RGui，可以通过下列命令使用knitr：
library(knitr) setwd(working directory) knit2html(&amp;quot;test.Rmd&amp;quot;) browseURI(&amp;quot;test.html&amp;quot;)  引用 Markdown语法
GitHub: Markdown基础
GitHub Flavored Markdown
Knitr with R Markdown
文艺编程 Literate Programming
</content>
    </entry>
    
     <entry>
        <title>[算法] Elementary Sorts练习</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_elementary_sorts%E7%BB%83%E4%B9%A0/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>algorithm</tag><tag>sort</tag><tag>selection</tag><tag>insertion</tag>
        </tags>
        <content type="html"> 本作业帖用于练习http://algs4.cs.princeton.edu/21elementary/里面的作业。
 Stoogesort. Analyze the running time andcorrectness of the following recursive sorting algorithm: if theleftmost item is larger than the rightmost item, swap them. Ifthere are 2 or more items in the current subarray, (i) sort theinitial two-thirds of the array recursively, (ii) sort the finaltwo-thirds of the array, (iii) sort the initial two-thirds of thearray again.StoogeSort时间复杂度为O(_n_log3 / log 1.5 )= O(_n_2.7095&amp;hellip;)，比合并排序慢，甚至比冒泡排序慢，仅用于低效简单排序示范。
 Guess-sort. Pick two indices i and j atrandom; if a[i] &amp;gt; a[j], then swap them. Repeat until the inputis sorted. Analyze the expected running time of thisalgorithm. Hint: after each swap, thenumber of inversions strictly decreases. If there are m bad pairs,then the expected time to find a bad pair is Theta(n^2/m). Summingup from m =1 to n^2 yields O(N^2 log N) overall, ala couponcollector. This bound is tight: consider input 1 0 3 2 5 4 7 6&amp;hellip; Bogosort. Bogosort is a randomizedalgorithm that works by throwing the N cards up in the air,collecting them, and checking whether they wound up in increasingorder. If they didn&amp;rsquo;t, repeat until they do. Implement bogosortusing the shuffling algorithm from Section 1.4. Estimate therunning time as a function of N. Slowsort. Consider the following sortingalgorithm: choose two integer i and j at random. If i &amp;lt; j, buta[i] &amp;gt; a[j], swap them. Repeat until the array is in ascendingorder. Argue that the algorithm will eventually finish (withprobability 1). How long will it takes as a function ofN? Hint: How many swaps will it make inthe worst case?
 Minimum numberof moves to sort an array. Given a list of Nkeys, a _moveoperation_ consists of removing any one keyfrom the list and appending it to the end of the list. No otheroperations are permitted. Design an algorithm that sorts a givenlist using the minimum number of moves.
 Guess-Sort. Consider the followingexchanged-based sorting algorithm: pick two random indices; if a[i]and a[j] are an inversion, swap them; repeat. Show that theexpected time to sort an array of size N is at most N^2 log N.See this paperfor an analysis and related sorting algorithm known asFun-Sort.
 Swapping aninversion. Given an array of N keys, let a[i]and a[j] be an inversion (i &amp;lt; j but a[i] &amp;gt; a[j]). Prove ordisprove: swapping a[i] and a[j] strictly decreases the number ofinversions.
 Binaryinsertion sort. Develop animplementation BinaryInsertion.java ofinsertion sort that uses binary search to find the insertion pointj for entry a[i] and then shifts all of the entries a[j] to a[i-1]over one position to the right. The number of compares to sort anarray of length N should be ~ N lg N in the worst case. Note thatthe number of array accesses will still be quadratic in the worstcase. Use SortCompare.java toevaluate the effectiveness of doing so.BinaryInsertionSort与二分法查找有一处不同在于：搜索中hi=mid，而二分法查找则hi=mid-1。
  a[mid]大于v时：如果另hi=mid-1，则此时的a[hi]有可能小于v，而lo肯定小于hi，则插入会出现错误。
</content>
    </entry>
    
     <entry>
        <title>[算法] UNION-FIND练习</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_union-find%E7%BB%83%E4%B9%A0/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>algorithm</tag><tag>union-find</tag><tag>quick-find</tag><tag>quick-union</tag>
        </tags>
        <content type="html"> 本作业帖用于练习http://algs4.cs.princeton.edu/15uf/里面的作业。
 True or false. Inthe quick union implementation, suppose weset id[p] to id[root(q)] insteadof setting id[root(p)] Would the resultingalgorithm be correct? _答案：_ 否。使用id[root(p)]可以将p所在连接全部合并到q所在连接，而使用id[p]仅会将p及其子连接合并到q所在连接。
 Which of thefollowing arrays could not possibly occur during the execution ofweighted quick union with path compression:
 0 1 2 3 4 5 67 8 9 7 3 8 3 4 5 68 8 1 6 3 8 0 4 5 69 8 1 0 0 0 0 0 0 00 0 0 9 6 2 6 1 4 58 8 9 9 8 7 6 5 4 32 1 0 _答案：_ B,、C、E和F。
 B中连接{9 1 3}不可能存在。节点1的节点数为2，明显比连接前的节点3多。
 C中连接{9 1 3 0 6}不可能存在。这个没有做weight才可能出现。
 E中{1 3 6 5 4}不可能存在。节点6的节点数为3，明显比连接前的节点4和5都多。
 F中连接{0 9}、{1 8}、{2 7}、{3 6}、{4 5}不可能存在。父子无法互指。
  Recursive pathcompression. Implement path compression usingrecursion. 答案：
public int find(int p) { if (p != id[p]) id[p] = find(id[p]); return id[p];  Pathhalving. WeightedQuickUnionPathHalvingUF.java implementsa simpler strategy known as path halving,that makes every other node on the find path link to itsgrandparent. Remark: the amortized costper operation for this algorithm is known to be bounded by afunction known as the inverse Ackermannfunction. 答案：
public int find(int p) { while (p != parent[p]) { parent[p] = parent[parent[p]]; p = parent[p]; } return p; }  Pathsplitting. Implement an alternate strategyknown as path splitting, that makes everynode on the find path link to itsgrandparent. Remark: the amortized costper operation for this algorithm is known to be bounded by afunction known as the inverse Ackermannfunction. 答案：
public int find(int p) { while (p != parent[p]) { int newp = parent[p]; parent[p] = parent[newp]; p = newp; } return p; }  Random quickunion. Implement the following version ofquick union: Assign the integers 0 through N-1 uniformly at randomto the N elements. When linking two roots, always link the rootwith the smaller label into the root with the larger label. Add inpath compression. Remark: the expectedcost per operation for the version without path compression islogarithmic; the expected amortized cost per operation for theversion with path compression is bounded by a function known asthe inverse Ackermann function. _答案：_将WeightedQuickUnionPathCompressionUF类中的sz替换成label，在构造时做一次shuffle即可。
 3D sitepercolation. Repeat for 3D lattice. Thresholdaround 0.3117.
 Bondpercolation. Same as site percolation, butchoose edges at random instead of sites. True threshold is exactly0.5.
 Given a set of Nelements, create a sequence of N union operations so that weightedquick union has height Theta(log N). Repeat for weighted quickunion with path compression.
 Hex. The game of Hex is played on atrapezoidal grid of hexagons&amp;hellip;. Describe how to detect when whiteor black has won the game. Use the union-find data structure.
 Hex. Prove that the game cannot end in atie. Hint: consider the set of cellsreachable from the left side of the board.
 Hex. Prove that the first player canguarantee a win with perfect play. Hint:if the second player had a winning strategy, you could choose arandom cell initially, and then just copy the second player&amp;rsquo;swinning strategy. This is called strategystealing.
 Labelingclusters on a grid. Physicists refer to it asthe Hoshen-Kopelman algorithm although it is union-find on a gridgraph with raster scan order. Applications include modelingpercolation and electrical conductance. Plot site occupancyprobability vs. number of clusters (say 100-by-100, with p between0 and 1, number of clusters between 0 and 1500) or distribution ofclusters. (seems like DFS would suffice here) Matlab has afunction bwlabel in the image processingtoolbox that performs cluster labeling.
  </content>
    </entry>
    
     <entry>
        <title>[Git] 裸代码仓库和镜像代码仓库</title>
        <url>https://mryqu.github.io/post/git_%E8%A3%B8%E4%BB%A3%E7%A0%81%E4%BB%93%E5%BA%93%E5%92%8C%E9%95%9C%E5%83%8F%E4%BB%A3%E7%A0%81%E4%BB%93%E5%BA%93/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>clone</tag><tag>init</tag><tag>bare</tag><tag>mirror</tag>
        </tags>
        <content type="html">  注：本文中操作都没有设置$GIT_DIR环境变量。
Git init和clone命令对bare和mirror参数的支持 ||&amp;ndash;bare参数|&amp;ndash;mirror参数 |&amp;mdash;&amp;ndash; |git init命令|支持|/ |git clone命令|支持|支持
裸代码仓库与普通代码仓库的区别 普通代码仓库裸代码仓库git init命令git init命令会创建一个空的Git代码仓库，一个在当前目录下包含hooks、info、objects和refs子目录和config、description和HEAD文件的.git目录。当前目录下可以创建工作树（工作文件和目录）。
config文件内容如下：
 [core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true hideDotFiles = dotGitOnlygit init --bare命令会创建一个空的裸Git代码仓库，当前目录下直接创建hooks、info、objects和refs子目录和config、description和HEAD文件。裸Git代码仓库只包含版本控制信息而不包含工作树。
config文件内容如下：
 [core] repositoryformatversion = 0 filemode = false bare = true symlinks = false ignorecase = true hideDotFiles = dotGitOnly git clone命令git clone命令会创建的一个包含.git子目录的目录，其中.git目录包含branches、hooks、info、logs、objects和refs子目录和config、description、HEAD、index和packed-refs文件。git clone命令所创建的目录中包含克隆的工作树（工作文件和目录）。
config文件内容如下：
 [core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true hideDotFiles = dotGitOnly [remote &#34;origin&#34;] url = https://github.com/usr1/demo.git fetch = &#43;refs/heads/*:refs/remotes/origin/* [branch &#34;master&#34;] remote = origin merge = refs/heads/master git clone --bare命令会创建一个后缀为&#34;.git&#34;的目录，直接包含hooks、info、objects和refs子目录和config、description和HEAD文件，不包含远程Git代码仓库的工作树。
config文件内容如下：
 [core] repositoryformatversion = 0 filemode = false bare = true symlinks = false ignorecase = true hideDotFiles = dotGitOnly [remote &#34;origin&#34;] url = https://github.com/usr1/demo.git  从技术的角度上讲，理论上无论Git代码仓库是否为裸代码仓库都可以接受push。可Git的策略是仅向裸代码仓库发送push。在Mercurial中，任何普通代码仓库都可以用于远程代码仓库，接受push。这是因为push所含的变化仅影响Mercurial代码仓库的版本控制，而不会影响其工作树。在Git中，如果向普通代码仓库push的话，Git会将推送的内容与工作文件进行比较，它会认为工作文件发生改变，从而影响工作树。而裸代码仓库由于没有工作树，所以push所含的变化仅影响裸代码仓库的版本控制。Git FAQ提到：A quick rule ofthumb is to never push into a repository that has a work treeattached to it, until you know what you are doing.
镜像代码仓库 git clone&amp;ndash;mirror命令会创建一个后缀为&amp;rdquo;.git&amp;rdquo;的目录，直接包含hooks、info、objects和refs子目录和config、description和HEAD文件，不包含远程Git代码仓库的工作树。config文件内容如下：
[core] repositoryformatversion = 0 filemode = false bare = true symlinks = false ignorecase = true hideDotFiles = dotGitOnly [remote &amp;quot;origin&amp;quot;] url = https://github.com/usr1/demo.git fetch = &#43;refs/*:refs/* mirror = true  镜像代码仓库也是裸代码仓库，它与裸代码仓库的区别在于：它不仅将源代码仓库的本地分支映射到目标代码仓库的本地分支，而且将所有引用（包括远程跟踪分支、备注等）都进行映射并建立refspec配置以使目标代码仓库的所有引用可被gitremote update命令覆盖。裸代码仓库在克隆命令结束后，所有源代码仓库的本地分支映射到目标代码仓库的本地分支，但是不包含远程分支。它就被完全独立地建立，不再期望后继fetch操作，所有远程分支及其他引用会被忽略掉。镜像代码仓库类似源代码仓库被完整复制，当执行git remote update命令时类似源代码仓库再次被完整复制。
参考 Git docs: init
Git docs: clone
Git docs: Git Internals - The Refspec
Git docs: Git on the Server - Getting Git on a Server
Git bare vs. non-bare repositories
What&amp;rsquo;s the difference between git clone &amp;ndash;mirror and git clone &amp;ndash;bare
How do I view a git repo&amp;rsquo;s receive history?
</content>
    </entry>
    
     <entry>
        <title>VirtualBox镜像资源</title>
        <url>https://mryqu.github.io/post/virtualbox%E9%95%9C%E5%83%8F%E8%B5%84%E6%BA%90/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>virtualbox</tag><tag>image</tag>
        </tags>
        <content type="html"> VirtualBox镜像资源: http://virtualboxes.org/images/
</content>
    </entry>
    
     <entry>
        <title>MinGW安装和使用</title>
        <url>https://mryqu.github.io/post/mingw%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>mingw</tag><tag>cygwin</tag><tag>gcc</tag><tag>windows</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html">  MinGW简介 MinGW全称Minimalist GNU For Windows，，是将GCC编译器和GNUBinutils移植到Win32平台下的产物，包括一系列头文件（Win32API）、库和可执行文件。MinGW是从Cygwin1.3基础上发展而来，相比Cygwin而言体积更小、使用更方便。MinGW分两个分支，MinGW（即MinGW32）和MinGW-w64。 MinGW包括： - GNU编译器套件，包括C/C&#43;&#43;、ADA语言和Fortran语言编译器 - 用于生成Windows二进制文件的GNU工具（编译器、链接器和档案管理器） - 用于Windows平台安装和部署MinGW和MSYS的命令行安装器（mingw-get） - 用于命令行安装器的GUI打包器（mingw-get-inst）
MinGW安装和使用 我只想使用C/C&#43;&#43;，所以仅安装mingw32-base、mingw32-gcc-g&#43;&#43;。msys-base其实也是可以不用安装的，因为我可以使用已有的GitBash。 我选择了默认的安装路径c:\MinGW，可以将c:\MinGW\bin加入环境变量以便使用。 参考 MinGW 官方网站
SourceForge.net：MinGW - Minimalist GNU for Windows
SourceForge.net：MinGW-w64 - for 32 and 64 bit Windows
</content>
    </entry>
    
     <entry>
        <title>处理注解@RequestParam的&#34;Required String parameter is not present&#34;</title>
        <url>https://mryqu.github.io/post/%E5%A4%84%E7%90%86%E6%B3%A8%E8%A7%A3requestparam%E7%9A%84required_string_parameter_is_not_present/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>springmvc</tag><tag>requestparam</tag><tag>required属性</tag>
        </tags>
        <content type="html"> 最近玩一下SpringMVC，代码如下：
@Controller @RequestMapping(&amp;quot;/test.do&amp;quot;) public class TestController { @RequestMapping(method = RequestMethod.GET) public ModelAndView sync(Model m, @RequestParam(&amp;quot;fid&amp;quot;) String fid, @RequestParam(&amp;quot;sid&amp;quot;) String sid) throws Exception { if(fid==null || sid==null) { m.addAttribute(&amp;quot;file&amp;quot;, new FileMetaDAO()); return new ModelAndView(ViewProvider.UPLOAD); } else { m.addAttribute(&amp;quot;sync&amp;quot;, new SyncDAO()); return new ModelAndView(ViewProvider.HR_SYNC); } } }  访问http://localhost:8080/hellorest/test.do 时发生如下问题：查了一下Annotation Type RequestParam的javadoc，加上可选参数required解决战斗。
@Controller @RequestMapping(&amp;quot;/test.do&amp;quot;) public class TestController { @RequestMapping(method = RequestMethod.GET) public ModelAndView sync(Model m, @RequestParam(value=&amp;quot;fid&amp;quot;, required = false) String fid, @RequestParam(value=&amp;quot;sid&amp;quot;, required = false) String sid) throws Exception { if(fid==null || sid==null) { m.addAttribute(&amp;quot;file&amp;quot;, new FileMetaDAO()); return new ModelAndView(ViewProvider.UPLOAD); } else { m.addAttribute(&amp;quot;sync&amp;quot;, new SyncDAO()); return new ModelAndView(ViewProvider.HR_SYNC); } } }  </content>
    </entry>
    
     <entry>
        <title>在chrome控制台中加载Javascript文件</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8chrome%E6%8E%A7%E5%88%B6%E5%8F%B0%E4%B8%AD%E5%8A%A0%E8%BD%BDjavascript%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>jquery</tag><tag>getscript</tag><tag>console</tag><tag>import</tag><tag>javascript</tag>
        </tags>
        <content type="html"> Include javascript file in chrome console里面介绍了几种在Chrome控制台加载Javascript文件的方法，对我来说使用JQuery是最方便的做法。
$.getScript(&#39;script.js&#39;);  </content>
    </entry>
    
     <entry>
        <title>免费私有代码托管平台</title>
        <url>https://mryqu.github.io/post/%E5%85%8D%E8%B4%B9%E7%A7%81%E6%9C%89%E4%BB%A3%E7%A0%81%E6%89%98%E7%AE%A1%E5%B9%B3%E5%8F%B0/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>代码托管</tag><tag>bitbucket</tag><tag>gitlab</tag><tag>私有</tag><tag>免费</tag>
        </tags>
        <content type="html">  SourceForge、GitHub免费版、Gitorious免费版和GoogleCode等源代码仓库都要求项目必须开源。这里做一下免费私有代码托管平台资料汇总。 - GitLab:支持Git库。首页说可以创建无限个私有或公开的库。使用时提示可以创建100000个级别为私有、GitLab内访问或公开的项目。 - Atlassian公司的Bitbucket:支持Git库。公有和私有仓库都可以无限制创建，支持在线协作。免费版限制为5个用户。 - Assembla:支持SVN/Git/Perforce库，支持在线协作。免费版限制为2个用户，一个私有库和500M存储。 - beanstalk：支持SVN/Git库，支持在线协作。免费版限制为1个用户，一个私有库和100M存储。 - unfuddle：有帖子说有免费帐户，但是没有找到。 - 开源中国的Git@OSC：支持Git库。最多可以创建 1000个项目，不限私有或者公开。 - CSDN的CODE：支持Git库，支持在线协作。支持私有库和公开库。 - 京东的京东代码库：支持Git库。支持私有库和公开库。
参考 免费的Git私有代码托管服务
如何在“Google Code 代码托管”上建立私有代码版本库
Tortoisegit&#43;BitBucket创造私有代码托管仓库
</content>
    </entry>
    
     <entry>
        <title>[算法] 算法课笔记-排序</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_%E7%AE%97%E6%B3%95%E8%AF%BE%E7%AC%94%E8%AE%B0-%E6%8E%92%E5%BA%8F/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>算法</tag><tag>排序</tag><tag>coursera</tag><tag>robert sedgewick</tag>
        </tags>
        <content type="html">  排序算法分类  就地排序（inplace）：排序算法所需辅助空间不依赖于元素个数N 稳定排序（stable）：同键值的元素在排序后原相对顺序不变  排序算法对比 |排序算法|就地
排序|稳定
排序|最差时间
复杂度|平均时间
复杂度|最佳时间
复杂度|备注 |&amp;mdash;&amp;ndash; |选择排序（selection）|是||C=N²/2|C=N²/2
M=N|C=N²/2|C比较 M移动 |冒泡排序（Bubble）|是|是|C=N²/2|C=N²/2|C=N|当N较小或部分已排序时使用 |插入排序（insertion）|是|是|C=N²/2|C=N²/4
M=N²/4|C=N|当N较小或部分已排序时使用（部分已排序时，插入排序比选择排序要快） |希尔排序（shell）|是||?|?|C=N|严谨代码，次二次时间 |归并排序（merge）||是|C=NlgN|C=NlgN|C=NlgN|NlgN保证，稳定
Java中对对象排序
Perl, C&#43;&#43; stable sort, Python stable sort, Firefox JavaScript,&amp;hellip; |快速排序（quick）|是||C=N²/2|C=2NlnN|C=NlgN|NlgN概率保证，实践中最快
Java中对原始数据类型排序
C qsort, Unix, Visual C&#43;&#43;, Python, Matlab, Chrome JavaScript,&amp;hellip; |三路基数快速排序
（3-way quick）|是||C=N²/2|C=2NlnN|C=N|当存在重复键值时改善快速排序 |堆排序（heap）|是||C=2NlgN|C=2NlgN|C=NlgN|NlgN保证，就地
选择排序 插入排序 希尔排序 Knuth Shuffle 合并排序 快速排序 快选 三路基数快速排序 堆排序 </content>
    </entry>
    
     <entry>
        <title>Hello Android!</title>
        <url>https://mryqu.github.io/post/hello_android/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>android</tag><tag>coursera</tag><tag>homework</tag><tag>adt</tag><tag>emulator</tag>
        </tags>
        <content type="html">  第一次再简陋总比不做强！ ADT安装 不经常装ADT （Android development tool），第二次装又犯晕了。 ADT要跟JDK平台一致： - 对应32位JDK的adt-bundle-windows-x86-20131030.zip - 对应64位JDK的adt-bundle-windows-x86_64-20131030.zip
JDK装好久了，不知道是32位还是64位的了，可以跑一下下面的代码：
public class JdkPlatformTest { public static voidmain(String[] args) { String arch =System.getProperty(&amp;quot;sun.arch.data.model&amp;quot;); System.out.println(arch&#43;&amp;quot;-bit&amp;quot;); } }  此外Windows平台可以通过下列命令获取：wmic os get osarchitecture
Android虚拟设备仿真器操作命令 http://developer.android.com/tools/devices/emulator.html
</content>
    </entry>
    
     <entry>
        <title>[Chrome插件] 一键保存当前打开的所有标签页</title>
        <url>https://mryqu.github.io/post/chrome%E6%8F%92%E4%BB%B6_%E4%B8%80%E9%94%AE%E4%BF%9D%E5%AD%98%E5%BD%93%E5%89%8D%E6%89%93%E5%BC%80%E7%9A%84%E6%89%80%E6%9C%89%E6%A0%87%E7%AD%BE%E9%A1%B5/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>chrome</tag><tag>plugin</tag><tag>保存</tag><tag>标签页</tag><tag>网页</tag>
        </tags>
        <content type="html"> 经常是用Chrome打开一堆网页，但是由于某种原因必须重启机器，不想将这些页面存入书签，但是又想保存下来开机后继续浏览。
在网上搜了一下，下列Chrome插件可以满足这个需求： - OneTab - Session Buddy - PanicButton - Stash - Read Later - VIEW LATER - Save For Later
其中OneTab和Session Buddy都是五星插件，并且用户众多，最终我选择了OneTab。
</content>
    </entry>
    
     <entry>
        <title>使用Spring MVC下载Excel文件</title>
        <url>https://mryqu.github.io/post/%E4%BD%BF%E7%94%A8spring_mvc%E4%B8%8B%E8%BD%BDexcel%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Spring</category>
        </categories>
        <tags>
          <tag>spring</tag><tag>mvc</tag><tag>excel</tag><tag>download</tag><tag>rest</tag>
        </tags>
        <content type="html">  想使用Spring MVC下载Excel文件，照着下面的样例，很容易就实现了。 Spring MVC with Excel View Example (Apache POI and JExcelApi)
Spring MVC and Excel file via AbstractExcelView
问题一：数据仅能生成xls，不能生成xlsx 通过org.springframework.web.servlet.view.document.AbstractExcelView源代码可知，Spring的AbstractExcelView仅支持HSSFWorkbook，不支持XSSFWorkbook。这一问题可以通过Github上的hmkcode/Spring-Framework来解决。 com.hmkcode.view.abstractview.AbstractExcelView
com.hmkcode.view.ExcelView
问题二：下载的文件是我配置的视图路径export.do，而不是Excel后缀 通过在Rest Controller里添加如下代码解决：
SimpleDateFormat myFmt=new SimpleDateFormat(&amp;quot;yyyyMMdd_HHmmss&amp;quot;); response.setHeader(&amp;quot;Pragma&amp;quot;, &amp;quot;public&amp;quot;); response.setHeader(&amp;quot;Cache-Control&amp;quot;, &amp;quot;max-age=0&amp;quot;); if(excelVersion.equals(&amp;quot;xlsx&amp;quot;)){ response.setContentType(&amp;quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&amp;quot;); response.setHeader(&amp;quot;Content-Disposition&amp;quot;, &amp;quot;attachment; filename=test&amp;quot;&#43;myFmt.format(new Date())&#43;&amp;quot;.xlsx&amp;quot;); }else{ response.setContentType(&amp;quot;application/vnd.ms-excel&amp;quot;); response.setHeader(&amp;quot;Content-Disposition&amp;quot;, &amp;quot;attachment; filename=\&amp;quot;test&amp;quot;&#43;myFmt.format(new Date())&#43;&amp;quot;.xls\&amp;quot;&amp;quot;); }  </content>
    </entry>
    
     <entry>
        <title>遭遇Python*重复运算符陷阱</title>
        <url>https://mryqu.github.io/post/%E9%81%AD%E9%81%87python%E9%87%8D%E5%A4%8D%E8%BF%90%E7%AE%97%E7%AC%A6%E9%99%B7%E9%98%B1/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>重复运算符</tag><tag>陷阱</tag>
        </tags>
        <content type="html"> 在python中有个特殊的符号“*”，可以用做数值运算的乘法算子，也是用作对象的重复算子，但在作为重复算子使用时一定要注意* 重复出来的对象有可能是指向在内存中同一块地址的同一对象。
测试代码：
grid_width=2 grid_height=2 def modify_grid(cells, row, col, val): cells[row][col]=val print cells #testing 1 print &#39;\ntesting 1&#39; cells=[ [88 for col in range(grid_width)] for row in range(grid_height)] print cells modify_grid(cells,0,1,66) #testing 2: In the trap print &#39;\ntesting 2&#39; cells=[[88]*grid_width]*grid_height print cells modify_grid(cells,0,1,66) print &#39;\n&#39; cells=[[&amp;quot;88&amp;quot;]*grid_width]*grid_height print cells modify_grid(cells,0,1,&amp;quot;66&amp;quot;) #testing 3 print &#39;\ntesting 3&#39; cells=[] for idx in range(grid_height): cells.append([88]*grid_width) print cells modify_grid(cells,0,1,66) #testing 4 print &#39;\ntesting 4&#39; cells=[123]*(grid_height*grid_width) print cells cells[1]=321 print cells  结果
testing 1 [[88, 88], [88, 88]] [[88, **66**], [88, 88]] testing 2 [[88, 88], [88, 88]] [[88, 66], [88, 66]] [[&#39;88&#39;, &#39;88&#39;], [&#39;88&#39;, &#39;88&#39;]] [[&#39;88&#39;, &#39;66&#39;], [&#39;88&#39;, &#39;66&#39;]] testing 3 [[88, 88], [88, 88]] [[88, 66], [88, 88]] testing 4 [123, 123, 123, 123] [123, 321, 123, 123]  </content>
    </entry>
    
     <entry>
        <title>安装和卸载JRE或JDK时出现Error 1723的解决办法</title>
        <url>https://mryqu.github.io/post/%E5%AE%89%E8%A3%85%E5%92%8C%E5%8D%B8%E8%BD%BDjre%E6%88%96jdk%E6%97%B6%E5%87%BA%E7%8E%B0error_1723%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>error1723</tag><tag>jdk</tag><tag>jre</tag>
        </tags>
        <content type="html"> 卸载JDK和JRE时失败，报错“Error 1723:java dll missing”。 网上的很多攻略都不管用，最后还是从微软网站找到fix程序。 Fix problems that programs cannot be installed or uninstalled
</content>
    </entry>
    
     <entry>
        <title>用JS处理粘贴而来的HTML表单</title>
        <url>https://mryqu.github.io/post/%E7%94%A8js%E5%A4%84%E7%90%86%E7%B2%98%E8%B4%B4%E8%80%8C%E6%9D%A5%E7%9A%84html%E8%A1%A8%E5%8D%95/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>regex</tag><tag>removeattribute</tag><tag>getelementsbytagname</tag>
        </tags>
        <content type="html"> 今天用Javascript处理粘贴而来的HTML表单，代码如下：某日又写了一小段，代码如下：</content>
    </entry>
    
     <entry>
        <title>[算法] O(0)的exch函数</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_o0%E7%9A%84exch%E5%87%BD%E6%95%B0/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>算法</tag><tag>exch</tag><tag>swap</tag><tag>o(0)</tag>
        </tags>
        <content type="html"> 常用的exch函数：
public static void exch(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; }  不使用辅助空间的exch函数：
public static void exch(int[] nums, int i, int j) { nums[i] ^= nums[j]; nums[j] ^= nums[i]; nums[i] ^= nums[j]; }  控制流及nums[i]和nums[j]状态如下：
|nums[i]|nums[j] |&amp;mdash; |= nums[i] ^ nums[j]|/ |/|= nums[j] ^ (nums[i] ^ nums[j])
= nums[i] |= (nums[i] ^ nums[j]) ^ nums[i]
= nums[j]|/
</content>
    </entry>
    
     <entry>
        <title>ClusterShell实践</title>
        <url>https://mryqu.github.io/post/clustershell%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>clustershell</tag><tag>linux</tag><tag>clush</tag><tag>clubak</tag><tag>nodeset</tag>
        </tags>
        <content type="html">  ClusterShell介绍 ClusterShell提供了一个轻量级、统一和健壮的命令执行Python框架，非常适于减轻Linux集群日常管理任务负担。ClusterShell的好处如下： - 提供高效、并行和高可扩展的Python命令执行引擎。 - 提供统一节点组语法和对外部组的访问 - 当使用clush和nodeset等工具可有效提升集群创建和日常管理任务的效率
ClusterShell实践 安装 首先在集群内节点配置无密钥ssh访问。然后在主/工作节点上安装ClusterShell。
apt-get install clustershell  配置和实践 ClusterShell工具 ClusterShell包含如下工具： - clush帮助文档 - clubak帮助文档 - nodesetcluset帮助文档
ClusterShell在线文档为http://clustershell.readthedocs.io/。
</content>
    </entry>
    
     <entry>
        <title>CRAN任务视图使用</title>
        <url>https://mryqu.github.io/post/cran%E4%BB%BB%E5%8A%A1%E8%A7%86%E5%9B%BE%E4%BD%BF%E7%94%A8/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>r语言</tag><tag>cran</tag><tag>任务视图</tag><tag>taskviews</tag><tag>install.views</tag>
        </tags>
        <content type="html">  一般都是从CRAN（Comprehensive R ArchiveNetwork）安装R的各种包（package）来满足我们的编程需求。CRAN将包按照应用范畴归类，并公布在如下链接：http://cran.r-project.org/web/views/我们可以通过安装视图的命令把所有同一应用范畴的包都安装上。
为了自动安装视图，需要安装ctv包。
install.packages(&amp;quot;ctv&amp;quot;) library(&amp;quot;ctv&amp;quot;)  通过如下命令安装或更新视图。
install.views(&amp;quot;Econometrics&amp;quot;) update.views(&amp;quot;MachineLearning&amp;quot;)  CRAN任务视图 |视图|说明 |&amp;mdash; |Bayesian|贝叶斯推断 |ChemPhys|化学计量学和计算物理学 |ClinicalTrials|临床试验设计、监控和分析 |Cluster|聚类分析和有限混合模型 |DifferentialEquations|微分方程 |Distributions|概率分布 |Econometrics|计量经济学 |Environmetrics|生态与环境数据分析 |ExperimentalDesign|实验设计和数据分析 |Finance|实证金融 |Genetics|统计遗传学 |Graphics|图形显示 &amp;amp; 动态图 &amp;amp; 图形设备 &amp;amp; 可视化 |HighPerformanceComputing|高性能计算和并行计算 |MachineLearning|机器学习与统计学习 |MedicalImaging|医学图像分析 |MetaAnalysis|元分析 |Multivariate|多元统计 |NaturalLanguageProcessing|自然语言处理 |NumericalMathematics|数值数学 |OfficialStatistics|官方统计和调查方法 |Optimization|优化和数学规划 |Pharmacokinetics|药物动力学数据分析 |Phylogenetics|系统发育、进化和遗传学分析 |Psychometrics|心理测量模型和方法 |ReproducibleResearch|可重复性研究 |Robust|稳健统计方法 |SocialSciences|社会科学统计 |Spatial|空间数据分析 |SpatioTemporal|时空数据处理和分析 |Survival|存活分析 |TimeSeries|时间序列分析 |WebC&#43;&#43;nologies|Web技术与服务 |gR|图模型
</content>
    </entry>
    
     <entry>
        <title>[HBase] HBase Shell中的put操作解析</title>
        <url>https://mryqu.github.io/post/hbase_hbase_shell%E4%B8%AD%E7%9A%84put%E6%93%8D%E4%BD%9C%E8%A7%A3%E6%9E%90/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>shell</tag><tag>ruby</tag><tag>put</tag><tag>datatype</tag>
        </tags>
        <content type="html"> 阅读了HBase Shell datatype conversion一贴，感觉下列两个操作结果中的单元格数据值都像是文本类型的：
put &#39;mytable&#39;, &#39;2342&#39;, &#39;cf:c1&#39;, &#39;67&#39; put &#39;mytable&#39;, &#39;2341&#39;, &#39;cf:c1&#39;, 23  预知真相，看来只好看HBase Shell代码了。HBase Shell是Ruby代码，首先找到这些代码的位置：
cd $HBASE_HOME find . -name &#39;*.rb&#39; -print  找到了$HBASE_HOME/lib/ruby/shell/commands/put.rb，其GitHub代码库位置为https://github.com/apache/hbase/commits/master/hbase-shell/src/main/ruby/shell/commands/put.rb：
def command(table, row, column, value, timestamp=nil, args = {}) put table(table), row, column, value, timestamp, args end def put(table, row, column, value, timestamp = nil, args = {}) format_simple_command do table._put_internal(row, column, value, timestamp, args) end end  继而找到了$HBASE_HOME/lib/ruby/hbase/table.rb，其GitHub代码库位置为https://github.com/apache/hbase/blob/master/hbase-shell/src/main/ruby/hbase/table.rb：
def _put_internal(row, column, value, timestamp = nil, args = {}) p = org.apache.hadoop.hbase.client.Put.new(row.to_s.to_ family, qualifier = parse_column_name(column) if args.any? attributes = args[ATTRIBUTES] set_attributes(p, attributes) if attributes visibility = args[VISIBILITY] set_cell_visibility(p, visibility) if visibility end #Case where attributes are specified without timestamp if timestamp.kind_of?(Hash) timestamp.each do |k, v| if v.kind_of?(Hash) set_attributes(p, v) if v end if v.kind_of?(String) set_cell_visibility(p, v) if v end end timestamp = nil end if timestamp p.add(family, qualifier, timestamp, value.to_s.to_j-a-v-a_bytes) else p.add(family, qualifier, value.to_s.to_j-a-v-a_bytes) end @table.put(p) end  看到value.to_s后可知数值型value经过to_s函数后也变成文本型了。下面做一下测试： &amp;ldquo;123&amp;rdquo;.to_s和123.to_s结果是相同的。而Bytes.toBytes方法经过to_s函数后变成对象引用的文本表现，对底层存储来说跟输入数据没有任何关联。
</content>
    </entry>
    
     <entry>
        <title>[HBase] Java客户端程序构建脚本</title>
        <url>https://mryqu.github.io/post/hbase_java%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%A8%8B%E5%BA%8F%E6%9E%84%E5%BB%BA%E8%84%9A%E6%9C%AC/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>java</tag><tag>client</tag><tag>build</tag><tag>script</tag>
        </tags>
        <content type="html"> 上一博文[HBase] 原始数据类型存储中所用到的构建脚本build.sh如下：
#!/bin/bash HADOOP_HOME=/usr/local/hadoop HBASE_HOME=/usr/local/hbase CLASSPATH=.:$HBASE_HOME/conf:$(hbase classpath) javac -cp $CLASSPATH HBasePrimitiveDataTypeTest.java java -cp $CLASSPATH HBasePrimitiveDataTypeTest  </content>
    </entry>
    
     <entry>
        <title>[HBase] 原始数据类型存储</title>
        <url>https://mryqu.github.io/post/hbase_%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%AD%98%E5%82%A8/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>primitive</tag><tag>datatype</tag><tag>bytes</tag><tag>storage</tag>
        </tags>
        <content type="html">  对原始数据类型如何在HBase中存储，如何在HBaseShell中如何显示尚不了解，做一下小实验满足一下好奇心。使用下列代码存放和读取原始数据类型：
byte[] cf = Bytes.toBytes(CF_DEFAULT); Put put = new Put(Bytes.toBytes(&amp;quot;test&amp;quot;)); byte[] val = Bytes.toBytes(&amp;quot;123&amp;quot;); System.out.println(&amp;quot;Bytes for str: &amp;quot;&#43; bytesToHex(val)&#43;&amp;quot;,len=&amp;quot;&#43;val.length); put.addColumn(cf, Bytes.toBytes(&amp;quot;str&amp;quot;), val); short shortVal = 123; val = Bytes.toBytes(shortVal); System.out.println(&amp;quot;Bytes for short:&amp;quot;&#43; bytesToHex(val)&#43;&amp;quot;,len=&amp;quot;&#43;val.length); put.addColumn(cf, Bytes.toBytes(&amp;quot;short&amp;quot;), val); int intVal = 123; val = Bytes.toBytes(intVal); System.out.println(&amp;quot;Bytes for int:&amp;quot;&#43; bytesToHex(val)&#43;&amp;quot;,len=&amp;quot;&#43;val.length); put.addColumn(cf, Bytes.toBytes(&amp;quot;int&amp;quot;), val); long longVal = 123L; val = Bytes.toBytes(longVal); System.out.println(&amp;quot;Bytes for long:&amp;quot;&#43; bytesToHex(val)&#43;&amp;quot;,len=&amp;quot;&#43;val.length); put.addColumn(cf, Bytes.toBytes(&amp;quot;long&amp;quot;), val); float floatVal = 123; val = Bytes.toBytes(floatVal); System.out.println(&amp;quot;Bytes for float:&amp;quot;&#43; bytesToHex(val)&#43;&amp;quot;,len=&amp;quot;&#43;val.length); put.addColumn(cf, Bytes.toBytes(&amp;quot;float&amp;quot;), val); double doubleVal = 123; val = Bytes.toBytes(doubleVal); System.out.println(&amp;quot;Bytes for double:&amp;quot;&#43; bytesToHex(val)&#43;&amp;quot;,len=&amp;quot;&#43;val.length); put.addColumn(cf, Bytes.toBytes(&amp;quot;double&amp;quot;), val); boolean boolVal = true; val = Bytes.toBytes(boolVal); System.out.println(&amp;quot;Bytes for bool:&amp;quot;&#43; bytesToHex(val)&#43;&amp;quot;,len=&amp;quot;&#43;val.length); put.addColumn(cf, Bytes.toBytes(&amp;quot;bool&amp;quot;), val); table.put(put); Get get = new Get(Bytes.toBytes(&amp;quot;test&amp;quot;)); get.addFamily(Bytes.toBytes(cf)); Result res = table.get(get); System.out.println(res);  日志如下：
Bytes for str: \x31\x32\x33,len=3 Bytes for short:\x00\x7B,len=2 Bytes for int:\x00\x00\x00\x7B,len=4 Bytes for long:\x00\x00\x00\x00\x00\x00\x00\x7B,len=8 Bytes for float:\x42\xF6\x00\x00,len=4 Bytes for double:\x40\x5E\xC0\x00\x00\x00\x00\x00,len=8 Bytes for bool:\xFF,len=1 keyvalues={test/JavaGenValues:bool/1388673053337/Put/vlen=1/seqid=0, test/JavaGenValues:double/1388673053337/Put/vlen=8/seqid=0, test/JavaGenValues:float/1388673053337/Put/vlen=4/seqid=0, test/JavaGenValues:int/1388673053337/Put/vlen=4/seqid=0, test/JavaGenValues:long/1388673053337/Put/vlen=8/seqid=0, test/JavaGenValues:short/1388673053337/Put/vlen=4/seqid=0, test/JavaGenValues:str/1388673053337/Put/vlen=3/seqid=0}  HBase Shell扫描表返回如下结果： 总结： - 对于byte、short、int、long、float和double数据类型，其Bytes存储的vlen固定为1、2、4、8、4、和8 - 对于字符串，其Bytes存储其UTF8编码的字节码 - 对于布尔类型，值true存储为\xFF，值false存储为\x00 - HBaseShell显示Bytes存储时，可打印字节显示字符，否则显示十六进制内容。具体实现可参见org.apache.hadoop.hbase.util.Bytes#toStringBinary(byte[],int, int)方法。
参考 Apache HBase APIs Example
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 处理跨块边界的InputSplit</title>
        <url>https://mryqu.github.io/post/hadoop_%E5%A4%84%E7%90%86%E8%B7%A8%E5%9D%97%E8%BE%B9%E7%95%8C%E7%9A%84inputsplit/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>inputformat</tag><tag>inputsplit</tag><tag>recordreader</tag>
        </tags>
        <content type="html">  Mapper从HDFS中读取文件进行数据处理的。凭借InputFormat、InputSplit、RecordReader、LineReader等类，Mapper用户代码可以处理输入键值对进行数据处理。下面学习一下MapReduce是如何分割无压缩文本文件输入的。
涉及的类有： - InputFormat及其子类InputFormat类执行下列操作： - 检验作业的输入文件和目录是否存在。 - 将输入文件分割策划功能InputSlit，基于文件的InputFormat根据文件大小将文件分割为逻辑上的Split。 - 实例化用于对每个文件分割块解析记录的RecordReaderInputFormat类包括下列两个主要的子类： - TextInputFormat：用于解析文本文件。将文件按行生成记录；键为LongWritable，文件偏移量；值为Text，行的内容。 - SequenceFileInputFormat：用于解析Hadoop压缩二进制文件。SequenceFile可为无压缩、记录压缩或块压缩。与TextInputFormat不同，SequenceFileInputFormat的键值对是泛型的。 - InputSplit及其子类InputSplit是单个Mapper所要处理的数据子集逻辑表现形式。每个InputSplit很可能不会包含完整记录数，即在输入分割中首尾记录有可能是不完整的，处理全部记录由RecordReader负责。InputSplit的子类包括： - FileSplit代表输入文件的GetLength()大小的一个片段。FileSplit由InputFormat.getSplits(JobContext)调用返回，并传给InputFormat类用于实例化RecordReader。 - CombineFileSplit将多个文件并入一个分割内（默认每个文件小于分割大小） - RecordReader及其子类RecordReader将输入分割片内的数据分析成Mapper所要处理的键值对。记录跟分割边界/块边界不一定匹配，RecordReader判断记录位置并处理日志边界。RecordReader包括下列子类： - LineRecordReader：处理文本文件。 - SequenceFileRecordReader：处理Sequence文件。 - LineReader：用于对文件进行读取操作、分析行并获得键值对。
处理的具体流程如下： - FileInputFormat.getSplits(JobContext)方法主要完成计算InputSplit的工作。 - 首先判断输入文件是否可被分割的。如果文件流没有被压缩或者使用bzip2这种可分割的压缩算法，则文件可被分割；否则整个文件作为一个InputSplit。 - 如果文件可被分割的话，分割尺寸为max( max( 1,mapreduce.input.fileinputformat.split.minsize), min(mapreduce.input.fileinputformat.split.maxsize, blockSize))。如果没有对分割最小/大值进行设置的话，则分割尺寸即等于块大小，而块大小默认为64MB。 - 文件按照上述分割尺寸分割记录文件路径、每一分割的起始偏移量、分割块实际尺寸、输入文件所在机器。只要文件剩余数据量在1.1倍分割尺寸范围内，就会放到一个InputSplit中。- LineRecordReader主要完成从InputSplit获取键值对的工作。 - LineRecordReader构造方法获知行分隔符是否为定制分割符； - initialize(InputSplit,TaskAttemptContext)方法获知InputSplit的start和end(=start&#43;splitLength)，如果start不为0的话，跳过第一行（不用管第一行是否完整）。即处理上一InputSplit的RecordReader处理本InputSplit的第一行，处理本InputSplit的RecordReader处理下一个InputSplit的第一行。 - nextKeyValue()方法处理第一个InputSplit，需要跳过可能存在的UTF BOM。- LineReader主要完成从从文件输入流获取数据、如没有定制换行符则需判别CR/LF/CRLF换行符，并获得键值对。以上类都不涉及对HDFS文件和块的实际读操作，本地和远程读取可学习org.apache.hadoop.hdfs.client.HdfsDataInputStream、org.apache.hadoop.hdfs.DFSInputStream等类的代码。
参考 How does Hadoop process records split across block boundaries?
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] MapReduce输出SequenceFile实践</title>
        <url>https://mryqu.github.io/post/hadoop_mapreduce%E8%BE%93%E5%87%BAsequencefile%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>output</tag><tag>format</tag><tag>sequencefile</tag>
        </tags>
        <content type="html">  Hadoop的Mapper输出默认格式为SequenceFile，而Reducer默认输出则为TextFile。在一个MapReduce工作流中，经常有多个MapReduce作业级联完成应用功能。如果中间MapReduce是输入输出都为SequenceFile，则性能很可能获得很大提升。 SequenceFile文件是Hadoop用来存储二进制形式的键值对而设计的一种平面文件(FlatFile)。SequenceFile可压缩可切分,非常适合Hadoop文件存储特性，SequenceFile的写入由org.apache.hadoop.io.SequenceFile.Writer来实现，根据压缩类型Writer又派生出两个子类BlockCompressWriter和RecordCompressWriter，压缩方式由SequenceFile类的内部枚举类CompressionType来表示： - NONE: 对记录不进行压缩; - RECORD: 仅压缩每一个记录中的值; - BLOCK: 将一个块中的所有记录压缩在一起;
输入SequenceFile示例 job.setInputFormatClass(SequenceFileInputFormat.class);  输出SequenceFile示例 </content>
    </entry>
    
     <entry>
        <title>*nux中的Here documents和Here strings</title>
        <url>https://mryqu.github.io/post/linux%E4%B8%AD%E7%9A%84here_documents%E5%92%8Chere_strings/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>here-document</tag><tag>heredoc</tag><tag>here-string</tag><tag>here-text</tag><tag>here-script</tag>
        </tags>
        <content type="html">  介绍 here document(又称之为here-document、here-text、heredoc、hereis、here-string或here-script)，是shell中的一种特殊重定向方式，用来将输入重定向到一个交互式的shell脚本或程序。格式如下：
command &amp;lt;&amp;lt; [-]delimiter here-document delimiter  here documents始于Unixshell的最通用语法，在&amp;lt;&amp;lt;紧跟一个分割标识符（通常为EOF或END），跟随一堆多行字符，最后一行用分割标识符收尾。
注意： - 结尾的分割标识符一定要顶格写，前后不能有任何字符，包括空格和tab缩进。 - 开始的分割标识符前后的空格会被省略掉。 - 开始的分割标识符前如果使用-的话，内容部分每行前面的 tab (制表符)将会被删除掉。这种用法是为了编写HereDocument的时候可以将内容部分进行缩进，方便代码阅读。
here strings语法跟here documents类似。格式如下：
command &amp;lt;&amp;lt;&amp;lt; word  实践及测试 Here documents简单测试 Here documents中变量替换和执行命令测试 通常，Heredocuments中内容会进行变量替换，反勾号中的命令也会执行。可以通过在开始的分割标识符上加单引号禁掉这种行为。 Here string简单测试 在shell文件中使用Here documents 参考 Here document
Bash Reference Manual - Here Documents
Bash Reference Manual - Here Strings
Java 的多行字符串 Here Document 的实现
[](http://stackoverflow.com/questions/2500436/how-does-cat-eof-work-in-bash)
</content>
    </entry>
    
     <entry>
        <title>数据源/Hibernate配置明文密码加密思考</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E6%BA%90hibernate%E9%85%8D%E7%BD%AE%E6%98%8E%E6%96%87%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86%E6%80%9D%E8%80%83/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Hibernate</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>spring</tag><tag>数据源</tag><tag>明文密码</tag><tag>加密</tag>
        </tags>
        <content type="html"> 无论是Web应用服务器数据源配置还是Hibernate配置，一般数据库用户和密码都是明文的，感觉很不安全。上网搜了一圈，博客帖子还不少，不过都跟Web应用服务器官方文档差不太多。 Tomcat坚持明文，理由是最终需要用原始用户名和密码去连接数据库，而Tomcat是开源的，攻击者很容易找到加密/解密方法，所以也得不到真正的保护。 另一方就是用AES/DES/3DES等密钥算法对明文密码进行加密，然后在程序某处进行解密，例如使用Tomcat连接池时用org.apache.tomcat.jdbc.pool.DataSourceFactory继承子类实现自己的数据源工厂时进行解密，使用Srping时用LocalSessionFactoryBean继承子类读取配置进行解密然后将其写回运行态的配置。这种方式说白了，如果程序不是很大，使用JAD等工具对程序进行反编译，找到如何加解密的算法还是不难的。 我个人认为，真正的Web应用实施肯定是要设置服务器访问权限及服务器内目录的访问权限的，一般人不应该能访问到Web服务器程序及配置，这样即使使用明文密码也能保证相同的安全等级。当然，如果开发一个不严肃的小项目，并且部署在一个公共访问机器上，做做障眼法瞒瞒那些不是码农的人也是可以的。
Web应用服务器文档： - Tomcat Wiki：FAQ/Password - JBoss：Encrypting Data Source Passwords - JBoss EAP：Encrypting Data Source Passwords - TomEE：DataSource Password Encryption博客： - Encrypting passwords in Tomcat - Hibernate的配置文件中用户和密码的加密 - hibernate配置文件中数据库密码加密,该如何解决 - Hibernate的验证，而不存储在纯文本密码 - 如何给工程中的配置文件加密 解密 - 通过spring对hibernate/ibatis的配置文件加密 - jndi 数据源配置密码加密 - spring 属性文件加密码及解密 - 怎么实现数据库连接的密码加密 - Jboss数据源密码加密 - Tomcat数据源连接池加密 - 使用 Jasypt 保护数据库配置 - spring datasource 密码加密后运行时解密的解决办法
</content>
    </entry>
    
     <entry>
        <title>在Ubuntu Linux上安装netstat</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8ubuntu_linux%E4%B8%8A%E5%AE%89%E8%A3%85netstat/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>apt-get</tag><tag>ubuntu</tag><tag>linux</tag><tag>netstat</tag><tag>install</tag>
        </tags>
        <content type="html"> 在UbuntuLinux上安装netstat，apt-get其实是找不到netstat包的，需要用apt-get安装net-tools。net-tools是Linux平台NET-3网络分发包，包括arp、hostname、ifconfig、netstat、rarp、route、plipconfig、slattach、mii-tool、iptunnel和ipmaddr工具。
apt-get install net-tools  </content>
    </entry>
    
     <entry>
        <title>swirl介绍</title>
        <url>https://mryqu.github.io/post/swirl%E4%BB%8B%E7%BB%8D/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>swirl</tag><tag>R语言</tag><tag>学习</tag><tag>交互式</tag>
        </tags>
        <content type="html"> swirl是在R命令行使用、用于R统计编程语言交互式教学的软件包。 swirl需要R3.0.2或更新版本。如果使用老版本R，需要更新R之后才能使用swirl。如果不清楚当前R版本，可以在R命令行敲入R.version.string获得当前的版本信息。 swirl可以通过如下命令安装：
install.packages(&amp;quot;swirl&amp;quot;)  每次使用前，通过如下命令加载包并执行：
library(swirl) swirl()  </content>
    </entry>
    
     <entry>
        <title>Windows常用命令汇总</title>
        <url>https://mryqu.github.io/post/windows%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>windows</tag><tag>命令</tag>
        </tags>
        <content type="html"> windows命令现在也就sysdm.cpl和mstsc用的比较多，很多还真不熟悉，搜了一篇备查。
 appwiz.cpl：程序和功能  calc：启动计算器  certmgr.msc：证书管理实用程序  charmap：启动字符映射表  chkdsk.exe：Chkdsk磁盘检查（管理员身份运行命令提示符）  cleanmgr: 打开磁盘清理工具  cliconfg：SQL SERVER 客户端网络实用工具  cmstp：连接管理器配置文件安装程序  cmd.exe：CMD命令提示符  自动关机命令 Shutdown -s -t600：表示600秒后自动关机 shutdown -a：可取消定时关机 Shutdown -r -t600：表示600秒后自动重启 rundll32user32.dll,LockWorkStation：表示锁定计算机  colorcpl：颜色管理，配置显示器和打印机等中的色彩  CompMgmtLauncher：计算机管理  compmgmt.msc：计算机管理  credwiz：备份或还原储存的用户名和密码  comexp.msc：打开系统组件服务  control：控制面版  dcomcnfg：打开系统组件服务  Dccw：显示颜色校准  devmgmt.msc：设备管理器  desk.cpl：屏幕分辨率  dfrgui：优化驱动器 Windows7→dfrg.msc：磁盘碎片整理程序  dialer：电话拨号程序  diskmgmt.msc：磁盘管理  dvdplay：DVD播放器  dxdiag：检查DirectX信息  eudcedit：造字程序  eventvwr：事件查看器  explorer：打开资源管理器  Firewall.cpl：Windows防火墙  FXSCOVER：传真封面编辑器  fsmgmt.msc：共享文件夹管理器  gpedit.msc：组策略  hdwwiz.cpl：设备管理器  inetcpl.cpl：Internet属性  intl.cpl：区域  iexpress：创建自解压/安装文件，系统自带  joy.cpl：游戏控制器  logoff：注销命令  lusrmgr.msc：本地用户和组  lpksetup：语言包安装/删除向导，安装向导会提示下载语言包  lusrmgr.msc：本机用户和组  main.cpl：鼠标属性  mmsys.cpl：声音  magnify：放大镜实用程序  MdSched:Windows内存诊断程序  mmc：打开控制台  mobsync：同步命令  mplayer2：简易widnows media player  Msconfig.exe：系统配置实用程序  msdt：微软支持诊断工具  msinfo32：系统信息  mspaint：画图  Msra：Windows远程协助  mstsc：远程桌面连接  NAPCLCFG.MSC：客户端配置  ncpa.cpl：网络连接  narrator：屏幕“讲述人”  Netplwiz：高级用户帐户控制面板，设置登陆安全相关的选项  netstat : an(TC)命令检查接口  notepad：打开记事本  Nslookup：IP地址侦测器  odbcad32：ODBC数据源管理器  OptionalFeatures：打开“打开或关闭Windows功能”对话框  osk：打开屏幕键盘  perfmon.msc：计算机性能监测器  perfmon：计算机性能监测器  PowerShell：提供强大远程处理能力  printmanagement.msc：打印管理  powercfg.cpl：电源选项  psr：问题步骤记录器  Rasphone：网络连接  Recdisc：创建系统修复光盘  Resmon：资源监视器  Rstrui：系统还原  regedit.exe：注册表  regedt32：注册表编辑器  rsop.msc：组策略结果集  sdclt：备份状态与配置，就是查看系统是否已备份  secpol.msc：本地安全策略  services.msc：本地服务设置  sfc /scannow：扫描错误并复原/windows文件保护  sfc.exe：系统文件检查器  shrpubw：创建共享文件夹  sigverif：文件签名验证程序  slui：Windows激活，查看系统激活信息  软件许可管理工具 slmgr.vbs -dlv：显示详细的许可证信息 slmgr.vbs -dli ：显示许可证信息 slmgr.vbs -xpr：当前许可证截止日期 slmgr.vbs -dti ：显示安装ID以进行脱机激 slmgr.vbs -ipk ：(ProductKey)安装产品密钥 slmgr.vbs -ato：激活Windows slmgr.vbs -cpky：从注册表中清除产品密钥（防止泄露引起的攻击） slmgr.vbs -ilc ：(Licensefile)安装许可证 slmgr.vbs -upk ：卸载产品密钥 slmgr.vbs -skms：(name[ort] )批量授权  snippingtool：截图工具，支持无规则截图  soundrecorder：录音机，没有录音时间的限制  StikyNot：便笺  sysdm.cpl：系统属性  sysedit：系统配置编辑器  syskey：系统加密，一旦加密就不能解开，保护系统的双重密码  taskmgr：任务管理器（旧版）  TM任务管理器（新版）  taskschd.msc：任务计划程序  timedate.cpl：日期和时间  UserAccountControlSettings用户账户控制设置  utilman：辅助工具管理器  wf.msc：高级安全Windows防火墙  WFS：Windows传真和扫描  wiaacmgr：扫描仪和照相机向导  winver：关于Windows  wmimgmt.msc：打开windows管理体系结构(WMI)  write：写字板  wscui.cpl：操作中心  wscript：windows脚本宿主设置  wuapp：Windows更新  有次想知道我自己的LDAP信息，IT同事教了一条命令:gpresult /R
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 调试及console.log</title>
        <url>https://mryqu.github.io/post/javascript_%E8%B0%83%E8%AF%95%E5%8F%8Aconsole.log/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>console</tag><tag>log</tag><tag>调试</tag>
        </tags>
        <content type="html">  最近玩一下javascipt，在回调里碰到一个问题，需要调试。加入了console.log函数打印日志，在我的chrome浏览器按Ctrl&#43;Shift&#43;J快捷键调出DevTool并显示控制台来查找问题。结合Wireshark，最后才发现对Json数据解析错误。下面介绍一下console.log的使用。javascript的代码示例如下：
$(function () { $(&#39;#fileupload&#39;).fileupload({ url: url, dataType: &#39;json&#39;, done: function (e, data) { $.each(data.result.files, function (index, file) { $(&#39;&#39;).text(file.name).appendTo(&#39;#files&#39;); }); }, progressall: function (e, data) { var progress = parseInt(data.loaded / data.total * 100, 10); console.log(&amp;quot;complete:&amp;quot;&#43;progress); $(&#39;#progress .progress-bar&#39;).css( &#39;width&#39;, progress &#43; &#39;%&#39; ); } }).prop(&#39;disabled&#39;, !$.support.fileInput) .parent().addClass($.support.fileInput ? undefined : &#39;disabled&#39;); });  浏览器控制台使用 Firefox http://getfirebug.com/ (可以使用Firefox内建的开发工具Ctrl&#43;Shift&#43;J (Tools &amp;gt; Web Developer &amp;gt;Error Console)，但是Firebug更出色；建议使用Firebug)
Safari和Chrome 使用方法基本相同。 https://developer.chrome.com/devtools/index https://developer.apple.com/technologies/safari/developer-tools.html
Internet Explorer 不要忘了在IE9或IE10中调试IE7和IE8时使用兼容模式。 http://msdn.microsoft.com/en-us/library/ie/gg589507(v=vs.85).aspx http://msdn.microsoft.com/en-us/library/dd565628(v=vs.85).aspx 如果必须访问IE6或IE7的控制台时使用Firebug Lite功能性书签 http://getfirebug.com/firebuglite/ 查找稳定版功能性书签 http://en.wikipedia.org/wiki/Bookmarklet
Opera http://www.opera.com/dragonfly/
iOS 对所有iPhones、iPod touch和iPads适用。 http://developer.apple.com/library/ios/ipad/#DOCUMENTATION/AppleApplications/Reference/SafariWebContent/DebuggingSafarioniPhoneContent/DebuggingSafarioniPhoneContent.html 使用iOS 6，如果将设备插入OS X时，可以通过OSX的Safari浏览器看到控制台。或者可以通过仿真器，打开Safari浏览器窗口并点击&amp;rdquo;Develop&amp;rdquo;标签，之后设置选项获得与设备通信的Safari的诊断工具。
Windows Phone、Android 这两类设备都没有内建控制台，也没有功能性书签。可以使用http://jsconsole.com/ type:listen，它会提供植入你HTML代码的脚本标签，之后就可以在jsconsole网站看到你的控制台了。 iOS和Android 可以通过http://html.adobe.com/edge/inspect/访问Web诊断工具及其在设备上安装的浏览器插件的控制台。
老浏览器问题 如果在代码中使用console.log但没有打开开发者工具时，一些老的浏览器(如微软的老版本IE)会崩溃。可以通过下面的代码避免这一问题:
 if(!window.console){ window.console = {log: function(){} }; }  参考 http://stackoverflow.com/questions/4539253/what-is-console-log
#javascript: console lesser known features.
</content>
    </entry>
    
     <entry>
        <title>jQuery资料帖</title>
        <url>https://mryqu.github.io/post/jquery%E8%B5%84%E6%96%99%E5%B8%96/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>jquery</tag><tag>资料</tag>
        </tags>
        <content type="html">  学习网站 http://jqfundamentals.com/
http://try.jquery.com/
http://www./tracks/jquery
http://www.w3school.com.cn/jquery/
图书 jQuery基础教程：官方培训教材 jQuery实战：书不错，就是内容有点 老锋利的jQuery
代码组织 http://learn.jquery.com/code-organization/
其他 jQuery设计思想：http://www.ruanyifeng.com/blog/2011/07/jquery_fundamentals.html
jQuery最佳实践：http://www.ruanyifeng.com/blog/2011/08/jquery_best_practices.html
jQuery源码分析：http://www.cnblogs.com/chyingp/archive/2013/06/03/jquery-souce-code-study.html
</content>
    </entry>
    
     <entry>
        <title>Chrome浏览器离线安装包下载地址</title>
        <url>https://mryqu.github.io/post/chrome%E6%B5%8F%E8%A7%88%E5%99%A8%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>chrome</tag><tag>离线</tag><tag>安装</tag>
        </tags>
        <content type="html"> http://www.google.com/chrome/eula.html?standalone=1&amp;amp;hl=zh-CN
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 逻辑操作符的特殊行为</title>
        <url>https://mryqu.github.io/post/javascript_%E9%80%BB%E8%BE%91%E6%93%8D%E4%BD%9C%E7%AC%A6%E7%9A%84%E7%89%B9%E6%AE%8A%E8%A1%8C%E4%B8%BA/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>logical</tag><tag>operator</tag><tag>behavior</tag><tag>return_value</tag>
        </tags>
        <content type="html">  Javascript中并不要求逻辑运算的两个操作数为布尔类型，并且返回值也不一定为布尔类型。&amp;amp;&amp;amp;操作符，如果第一个操作表达式能被转换成false，返回第一个操作表达式；否则返回第二个操作表达式。当用于两个布尔类型值时，两个值都为true时返回ture，否则返回false。||操作符，如果第一个操作表达式能被转换成true，返回第一个操作表达式；否则返回第二个操作表达式。当用于两个布尔类型值时，任一个值为true时返回ture，否则返回false。示例：参考 MDN：Logical operators
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] 原始数据类型</title>
        <url>https://mryqu.github.io/post/javascript_%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>primitive</tag><tag>typeof</tag><tag>operator</tag>
        </tags>
        <content type="html">  原始数据类型 JavaScript共有5种原始数据类型：
|原始数据类型|包装对象|介绍 |&amp;mdash;&amp;ndash; |string|String|字符串遇到加号之外的计算操作符，会转换成数值。内容为不为数值的字符串转换成NaN。当用比较操作符比较两个字符串时，比较的是第一个字母的unicode。 |number|Number|十进制数：123八进制数：0123十六进制数：0x123指数：1e1、1E&#43;1、2E-3无穷：Infinity、-Infinity非数字：NaN |Boolean|Boolean| |null||与undefined的区别在于，已定义但没有值 |undefined||
typeof操作符 typeof的返回值有六种可能：number、string、boolean、object、function、undefined。条件判断或3元条件运算符(?:)判断 |值|Boolean结果 |&amp;mdash;&amp;ndash; |undefined|false |null|false |number|0和NaN为false，其他为true |string|空字符串&amp;rdquo;&amp;ldquo;为false，其他为true |对象|不为null的对象始终为true
参考 MDN：Primitive data type
MDN：typeof operator
</content>
    </entry>
    
     <entry>
        <title>[Git] Git代理配置</title>
        <url>https://mryqu.github.io/post/git_git%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>proxy</tag><tag>configuration</tag><tag>http</tag><tag>https</tag>
        </tags>
        <content type="html">  设置Git的http和https代理 git config --global http.proxy http://proxyUser:proxyPwd@proxyServer:proxyPort git config --global https.proxy https://proxyUser:proxyPwd@proxyServer:proxyPort  查询Git的http和https代理 git config --global --get http.proxy git config --global --get https.proxy  移除Git的http和https代理 git config --global --unset http.proxy git config --global --unset https.proxy  </content>
    </entry>
    
     <entry>
        <title>Wget笔记</title>
        <url>https://mryqu.github.io/post/wget%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>NetWork</category>
        </categories>
        <tags>
          <tag>wget</tag><tag>下载</tag><tag>命令行</tag><tag>参数</tag><tag>速查</tag>
        </tags>
        <content type="html">  介绍 GNU Wget是一个命令行的下载工具，支持HTTP、HTTPS、FTP协议，支持断点续传。在宽带状态不佳的情况下，Wget能表现出很强的稳定性。
常用命令速查表 |操作|命令 |&amp;mdash;&amp;ndash; |下载文件|wget URL |恢复下载|wget -c URL |递归下载|以最大深度10级递归下载某链接：wget -r -l 10 URL |下载整个网站|wget -m URL-m等同于-r -N -l inf &amp;ndash;no-remove-listing |下载某个页面特定类型文件|仅下载pdf和mp3文件：wget -A &amp;ldquo;.pdf,.mp3&amp;rdquo; -r -l 1 -nd –np URL |下载某个页面特定类型之外的其他文件|下载pdf以外的文件：wget -R.pdf -r -l 1 -nd –np URL |忽略 robots.txt|wget -e robots=off URL |限制下载速率|wget –-limit-rate=20 URL |模拟firefox下载|wget -U Mozilla URL |不下载仅检查链接是否存在|wget &amp;ndash;spider &amp;ndash;force-html -i bookmarks.html |将输出写入文件|wget -O filepath URL |下载FTP文件|wget &amp;ndash;ftp-­use­r=yqu&amp;ndash;ftp-­pas­swo­rd=yqu URL |下载多个连接|wget URL1 URL2 URL3 |配置接收数据的限额大小|对单文件下载无效，对递归下载或从输入文件获取连接下载有效wget -Q2m -i URL当下载量超过限额后下载中断
启动选项： -V, --version 显示 Wget 的版本并且退出。 -h, --help 打印此帮助。 -b, -background 启动后进入后台操作。 -e, -execute=COMMAND 运行‘.wgetrc’形式的命令。
日志记录及输入文件选项： -o, --output-file=文件 将日志消息写入到指定文件中。 -a, --append-output=文件 将日志消息追加到指定文件的末端。 -d, --debug 打印调试输出。 -q, --quiet 安静模式(不输出信息)。 -v, --verbose 详细输出模式(默认)。 -nv, --non-verbose 关闭详细输出模式，但不进入安静模式。 -i, --input-file=文件 下载从指定文件中找到的 URL。 -F, --force-html 以HTML方式处理输入文件。 -B, --base=URL 使用 -F -i 文件选项时，在相对链接前添加指定的 URL 。
下载选项： -t, --tries=次数 配置重试次数（0 表示无限）。 --retry-connrefused 即使拒绝连接也重试。 -O --output-document=文件 将数据写入此文件中。 -nc, --no-clobber 不更改已经存在的文件，也不使用在文件名后添加 .#（# 为数字）的方法写入新的文件。 -c, --continue 继续接收已下载了一部分的文件。 --progress=方式 选择下载进度的表示方式。 -N, --timestamping 除非远程文件较新，否则不再取回。 -S, --server-response 显示服务器回应消息。 --spider 不下载任何数据。 -T, --timeout=秒数 配置读取数据的超时时间 (秒数)。 -w, --wait=秒数 接收不同文件之间等待的秒数。 --waitretry=秒数 在每次重试之间稍等一段时间 (由 1 秒至指定的 秒数 不等)。 --random-wait 接收不同文件之间稍等一段时间(由 0 秒至 2*WAIT 秒不等)。 -Y, --proxy=on/off 打开或关闭代理服务器。 -Q, --quota=大小 配置接收数据的限额大小。 --bind-address=地址 使用本机的指定地址 (主机名称或 IP) 进行连接。 --limit-rate=速率 限制下载的速率。 --dns-cache=off 禁止查找存于高速缓存中的 DNS。 --restrict-file-names=OS 限制文件名中的字符为指定的 OS (操作系统) 所允许 的字符。
目录选项： -nd --no-directories 不创建目录。 -x, --force-directories 强制创建目录。 -nH, --no-host-directories 不创建含有远程主机名称的目录。 -P, --directory-prefix=名称 保存文件前先创建指定名称的目录。 --cut-dirs=数目 忽略远程目录中指定数目的目录层。
HTTP 选项： --http-user=用户 配置 http 用户名。 --http-passwd=密码 配置 http 用户密码。 -C, --cache=on/off (不)使用服务器中的高速缓存中的数据 (默认是使用的)。 -E, --html-extension 将所有 MIME 类型为 text/html 的文件都加上 .html 扩 展文件名。 --ignore-length 忽略“Content-Length”文件头字段。 --header=字符串 在文件头中添加指定字符串。 --proxy-user=用户 配置代理服务器用户名。 --proxy-passwd=密码 配置代理服务器用户密码。 --referer=URL 在 HTTP 请求中包含“Referer：URL”头。 -s, --save-headers 将 HTTP 头存入文件。 -U, --user-agent=AGENT 标志为 AGENT 而不是 Wget/VERSION。 --no-http-keep-alive 禁用 HTTP keep-alive（持久性连接）。 --cookies=off 禁用 cookie。 --load-cookies=文件 会话开始前由指定文件载入 cookie。 --save-cookies=文件 会话结束后将 cookie 保存至指定文件。 --post-data=字符串 使用 POST 方法，发送指定字符串。 --post-file=文件 使用 POST 方法，发送指定文件中的内容。
HTTPS (SSL) 选项： --sslcertfile=文件 可选的客户段端证书。 --sslcertkey=密钥文件 对此证书可选的“密钥文件”。 --egd-file=文件 EGD socket 文件名。 --sslcadir=目录 CA 散列表所在的目录。 --sslcafile=文件 包含 CA 的文件。 --sslcerttype=0/1 Client-Cert 类型 0=PEM (默认) / 1=ASN1 (DER) --sslcheckcert=0/1 根据提供的 CA 检查服务器的证书 --sslprotocol=0-3 选择 SSL 协议；0=自动选择，1=SSLv2， 2=SSLv3， 3=TLSv1
FTP 选项： -nr, --dont-remove-listing 不删除“.listing”文件。 -g, --glob=on/off 设置是否展开有通配符的文件名。 --passive-ftp 使用“被动”传输模式。 --retr-symlinks 在递归模式中，下载链接所指示的文件(连至目录则例外）。
递归下载选项： -r, --recursive 递归下载。 -l, --level=数字 最大递归深度(inf 或 0 表示无限)。 --delete-after 删除下载后的文件。 -k, --convert-links 将绝对链接转换为相对链接。 -K, --backup-converted 转换文件 X 前先将其备份为 X.orig。 -m, --mirror 等效于 -r -N -l inf -nr 的选项。 -p, --page-requisites 下载所有显示完整网页所需的文件，例如图像。 --strict-comments 打开对 HTML 备注的严格(SGML)处理选项。
递归下载时有关接受/拒绝的选项： -A, --accept=列表 接受的文件样式列表，以逗号分隔。 -R, --reject=列表 排除的文件样式列表，以逗号分隔。 -D, --domains=列表 接受的域列表，以逗号分隔。 --exclude-domains=列表 排除的域列表，以逗号分隔。 --follow-ftp 跟随 HTML 文件中的 FTP 链接。 --follow-tags=列表 要跟随的 HTML 标记，以逗号分隔。 -G, --ignore-tags=列表 要忽略的 HTML 标记，以逗号分隔。 -H, --span-hosts 递归时可进入其它主机。 -L, --relative 只跟随相对链接。 -I, --include-directories=列表 要下载的目录列表。 -X, --exclude-directories=列表 要排除的目录列表。 -np, --no-parent 不搜索上层目录。
参考 Wget Manual
</content>
    </entry>
    
     <entry>
        <title>Joda-Time笔记</title>
        <url>https://mryqu.github.io/post/joda-time%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>joda</tag><tag>time</tag><tag>java</tag><tag>date</tag><tag>calendar</tag>
        </tags>
        <content type="html">  Joda简介 Joda项目致力于为Java平台提供替代API的一些质量保证的基础库。包括如下子项目： - Joda-Time -日期和时间库 - Joda-Money -货币库 - Joda-Beans -下一代JavaBeans - Joda-Convert -字符串与对象转换库 - Joda-Collect - 提供JDK或Google Guava没有的集合数据类型 - Joda-Primitives -提供原始数据类型集合
Joda-Time简介 其中Joda-Time由于JDK自身时间日期API的不给力而被广泛使用，已经成为事实上的标准时间日期库。Joda-Time在JavaSE8将融入JDK API内，使用者可以使用java.time (JSR-310)内的API了。Joda-Time在时区、时间差和时间解析等方面支持多种历法系统，但仍然提供很简单的API。默认的历法是ISO8601标准，此外也支持Gregorian(现行公历、格里历)、Julian(儒略历)、Buddhist(佛历)、Coptic(科普特历)、Ethiopic(埃塞俄比亚历)和Islamic(伊斯兰历)历法系统。 为什么要使用Joda-Time（以下简称Joda）？考虑创建一个用时间表示的某个随意的时刻，例如2000年1月1日0时0分。如何创建一个用时间表示这个瞬间的JDK对象？使用java.util.Date？事实上这是行不通的，因为自JDK1.1 之后的每个 Java 版本的 Javadoc 都声明应当使用java.util.Calendar。Date中Date(intyear, int month, int date) 、Date(int year, int month, int date, inthrs, int min)、Date(int year, int month, int date, int hrs, int min,int sec)已经废弃、不建议使用，严重限制了您创建此类对象的途径。然而，Date确实有一个构造函数Date(long date)，您可以用来创建用时间表示某个瞬间的对象（除“当前时间”以外）。该方法使用距离1970年1月1日子时格林威治标准时间（也称为_epoch_）以来的毫秒数作为一个参数，对时区进行校正。 那么Calendar又如何呢？可以使用下面的方式创建必需的实例：
Calendar calendar = Calendar.getInstance(); calendar.set(2000, Calendar.JANUARY, 1, 0, 0, 0);  使用Joda，代码应该类似如下所示：
DateTime dateTime = new DateTime(2000, 1, 1, 0, 0, 0, 0);  这一行简单代码没有太大的区别。但是如果使问题稍微复杂化，假设希望在这个日期上加上90天并输出结果。使用JDK，需要如下代码：
Calendar calendar = Calendar.getInstance(); calendar.set(2000, Calendar.JANUARY, 1, 0, 0, 0); SimpleDateFormat sdf = new SimpleDateFormat(&amp;quot;E MM/dd/yyyy HH:mm:ss.SSS&amp;quot;); calendar.add(Calendar.DAY_OF_MONTH, 90); System.out.println(sdf.format(calendar.getTime()));  使用Joda，代码如下所示：
DateTime dateTime = new DateTime(2000, 1, 1, 0, 0, 0, 0); System.out.println(dateTime.plusDays(90).toString(&amp;quot;E MM/dd/yyyy HH:mm:ss.SSS&amp;quot;);  两者之间的差距拉大了（Joda用了两行代码，JDK则是 5 行代码）。 现在假设希望输出这样一个日期：距离Y2K45天之后的某天在下一个月的当前周的最后一天的日期。坦白地说，使用Calendar处理这个问题实在太痛苦了，即使是简单的日期计算，比如上面这个计算。使用Joda，代码如下所示：
DateTime dateTime = new DateTime(2000, 1, 1, 0, 0, 0, 0); System.out.println(dateTime.plusDays(45).plusMonths(1).dayOfWeek() .withMaximumValue().toString(&amp;quot;E MM/dd/yyyy HH:mm:ss.SSS&amp;quot;);  输出为：
Sun 03/19/2000 00:00:00.000  如果您正在寻找一种易于使用的方式替代JDK日期处理，那么您真的应该考虑Joda，具体原因如下： - 易于使用 - 易于扩展 - 丰富特性 - 最新的时区计算 - 历法支持 - 易于互操作 - 性能更佳 - 良好的测试覆盖率 - 完整的文档 - 成熟 - 开源
Joda-Time关键概念 Joda 和 JDK 互操作性 Calendar类缺乏可用性，这一点很快就能体会到，而Joda弥补了这一不足。Joda的设计者还做出了一个决定，这可以说是它取得成功的关键一点：JDK互操作性。Joda的类能够（正如您将看到的一样，有时会采用一种比较迂回的方式）生成java.util.Date（和Calendar）的实例。这使您能够保留现有的依赖JDK的代码，但是又能够使用Joda 处理复杂的日期/时间计算。 例如，完成上一示例计算后。只需要稍作更改就可以将结果返回到JDK中：
Calendar calendar = Calendar.getInstance(); DateTime dateTime = new DateTime(2000, 1, 1, 0, 0, 0, 0); dateTime = dateTime.plusDays(45).plusMonths(1).dayOfWeek().withMaximumValue(); System.out.println(dateTime.toString(&amp;quot;E MM/dd/yyyy HH:mm:ss.SSS&amp;quot;); calendar.setTime(dateTime.toDate());  就是这么简单，完成了计算，但是可以继续在JDK对象中处理结果。这是Joda的一个非常棒的特性。 Joda也接受使用JDK对象用来构造Joda对象。 Joda构造函数将指定从epoch到某个时刻所经过的毫秒数来进行构造。它根据JDK Date对象的毫秒值创建一个DateTime对象。epoch与Joda定义是相同的，Joda的时间精度也用毫秒表示：
java.util.Date jdkDate = obtainDateSomehow(); long timeInMillis = jdkDate.getTime(); DateTime dateTime = new DateTime(timeInMillis);  下面这个例子与前例类似，唯一不同之处是我在这里将Date或Calendar对象直接传递给构造函数：
// Use a Date java.util.Date jdkDate = obtainDateSomehow(); dateTime = new DateTime(jdkDate); // Use a Calendar java.util.Calendar calendar = obtainCalendarSomehow(); dateTime = new DateTime(calendar);  参考 Joda-Time项目主页
IBM developerWorks：Joda-Time
开源时间开发工具Joda-time介绍
Joda项目主页
</content>
    </entry>
    
     <entry>
        <title>MySQL:清空具有外键约束的表</title>
        <url>https://mryqu.github.io/post/mysql%E6%B8%85%E7%A9%BA%E5%85%B7%E6%9C%89%E5%A4%96%E9%94%AE%E7%BA%A6%E6%9D%9F%E7%9A%84%E8%A1%A8/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mysql</tag><tag>清空</tag><tag>truncate</tag><tag>外键约束</tag><tag></tag>
        </tags>
        <content type="html"> 最近在MySQL Workbench上使用&amp;rdquo;TRUNCATE TABLE TABLE_E;&amp;ldquo;清空一个表时返回错误：Error Code: 1701. Cannot truncate a table referenced in a foreignkey constraint (yqutesting.table_f, CONSTRAINT table_f_ibfk_4FOREIGN KEY (old_id) REFERENCES yqutesting.table_e(ID))解决方法1: - 删除约束 - 清空表 - 手工删除引用该表的记录 - 创建约束解决方法2:
 SET FOREIGN_KEY_CHECKS = 0; TRUNCATE TABLE TABLE_E; SET FOREIGN_KEY_CHECKS = 1;  参考:truncate foreign key constrained table
</content>
    </entry>
    
     <entry>
        <title>以管理员权限执行命令行</title>
        <url>https://mryqu.github.io/post/%E4%BB%A5%E7%AE%A1%E7%90%86%E5%91%98%E6%9D%83%E9%99%90%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%E8%A1%8C/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          
        </tags>
        <content type="html"> 以管理员权限执行命令行，只需点击出Command Prompt的右键菜单，选择&amp;rdquo;Run asadministrator&amp;rdquo;即可。 </content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;]不错的C&#43;&#43;引用参数分析</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E4%B8%8D%E9%94%99%E7%9A%84c&#43;&#43;%E5%BC%95%E7%94%A8%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>引用参数</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html">  阅读了一篇不错的C&#43;&#43; 引用参数 深入分析，摘要如下：
把参数声明成引用，实际上改变了缺省的按值传递参数的传递机制，在按值传递时，函数操纵的是实参的本地拷贝。
一、引用参数的三种常见用法：  需要改变实参的值，比如swap()。参数是引用时，函数接收的是实参的左值而不是值的拷贝。这意味着函数知道实参在内存中的位置，因而能够改变它的值或取它的地址。 向主调函数返回额外的结果。 向函数传递大型的类对象。  二、如果引用参数不希望在被调用的函数内部被修改，那么把参数声明为 const 型的引用是个不错的办法。 三、 我们可以声明任意内置数据类型的引用参数 四、引用参数还是指针参数 这两种参数都能够改变实参的值，也可以有效的传递大型类对象，怎么样决定参数该声明成哪种呢？ 根本区别是：引用必须被初始化为指向一个对象，一旦初始化了，它就不能再指向其他对象；指针可以指向一系列不同的对象也可以什么都不指向。因为指针可能指向一个对象或没有任何对象，所以函数在确定指针实际指向一个有效的对象之前不能安全地解引用（dereference）一个指针。另一方面，对于引用参数，函数不需要保证它指向一个对象。引用必须指向一个对象，甚至在我们不希望这样时也是如此。 如果一个参数可能在函数中指向不同的对象，或者这个参数可能不指向任何对象，则必须使用指针参数 。 引用参数的一个重要用法是：它允许我们在有效地实现重载操作符的同时，还能保证用法的直观性。
</content>
    </entry>
    
     <entry>
        <title>[Git] 查看某文件历史记录</title>
        <url>https://mryqu.github.io/post/git_%E6%9F%A5%E7%9C%8B%E6%9F%90%E6%96%87%E4%BB%B6%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>log</tag><tag>gitk</tag><tag>文件</tag><tag>历史记录</tag>
        </tags>
        <content type="html"> 在Git中查看某个文件历史记录，方式如下： - git log [filename]：显示对该文件的提交记录 - git log -p [filename]：显示对该文件的提交记录及每次提交增量内容 - gitk [filename]：图形显示对该文件的提交记录及每次提交增量内容
</content>
    </entry>
    
     <entry>
        <title>Tomcat的会话持久化</title>
        <url>https://mryqu.github.io/post/tomcat%E7%9A%84%E4%BC%9A%E8%AF%9D%E6%8C%81%E4%B9%85%E5%8C%96/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Web Application Server</category>
        </categories>
        <tags>
          <tag>tomcat</tag><tag>session</tag><tag>persistence</tag><tag>会话持久化</tag><tag>session.ser</tag>
        </tags>
        <content type="html">  《玩玩HTTP servlet和session资源监控器》提到我hello的一个web项目，刚开始我只是用监视器清除了Tomcat容器外部的文件资源，以为会话失效后Tomcat会自己清除会话属性，但后来发现想的太简单了。不但会话失效，并且重启Tomcat服务器，服务器端原来的会话及其属性都在,如果客户端浏览器的会话没有丢的话，刷新页面仍然可以获得以前的信息。原来Tomcat（起码5.x之后的版本）在默认的情况下提供了会话持久化这项功能，见$TOMCAT_HOME$/conf/context.xml：Tomcat的默认会话管理器是标准会话管理器(StandardManager)，用于非集群环境中对单个处于运行状态的Tomcat实例会话进行管理。当Tomcat关闭时，这些会话相关的数据会被写入磁盘上的一个名叫SESSION.ser的文件，并在Tomcat下次启动时读取此文件。 - 默认只有在Tomcat正常关闭时才会保存完整的用户会话信息 - 默认保存于$CATALINA_HOME$/work/Catalina/[host]/[webapp]/下的SESSIONS.ser文件中 - 若是自定义的虚拟主机则保存在$CATALINAHOME/work/Catalina/[host]//下的SESSIONS.ser文件中
如果不想再获得失效会话属性的话，解决办法为： - 关闭Tomcat会话持久化功能。去掉context.xml中那句注释即可。 - HttpSessionListener实现类的sessionDestroyed方法删除该会话所有属性。
参考 Apache Tomcat 5.5 Configuration ReferenceTomcat会话管理
</content>
    </entry>
    
     <entry>
        <title>[JavaScript] === 与 == 操作符的区别</title>
        <url>https://mryqu.github.io/post/javascript_%E4%B8%8E%E6%93%8D%E4%BD%9C%E7%AC%A6%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>strict</tag><tag>lenient</tag><tag>equality</tag><tag>operator</tag>
        </tags>
        <content type="html">  JavaScript中有两个等值比较操作符：严格相等===和宽松相等==。很多JavaScript指南都建议避免使用宽松相等，而是使用严格相等。 - ===：只有在两个操作数的数据类型和值都相等的情况下才为true - ==：用于比较两个操作数是否相等，这两个操作数的数据类型不一定要相等，只要进行数据类型转换后相等即为true
严格相等=== （严格不相等!==） 规则如下： - 如果类型不同，就[不相等] - 如果两个都是数值原始类型，并且是同一个值，那么[相等]；(！例外)的是，如果其中至少一个是NaN，那么[不相等]。（判断一个值是否是NaN，只能用isNaN()来判断） - 如果两个都是字符串原始类型，每个位置的字符都一样，那么[相等]；否则[不相等]。 - 如果两个都是布尔原始类型，两个值值都是true，或者都是false，那么[相等]。 - 如果两个原始类型值都是null，或者都是undefined，那么[相等]。 - 如果两个值都引用同一个对象（含数组和函数），那么[相等]；否则[不相等]。示例：宽松相等== （宽松不相等!=） 规则如下： - 如果两个值类型相同，进行 === 比较。 - 如果两个值类型不同，他们可能相等。根据下面规则进行类型转换再比较： - null与undefined是[相等]的。 - 如果字符串原始类型和数值原始类型进行比较，把字符串转换成数值再进行比较。 - 如果Boolean对象与其他类型进行比较，Boolean对象会转换成数值(true:1,false:0)再进行比较。 - 如果一个是对象，另一个是数值或字符串原始类型，把对象转换成原始类型的值再比较。对象利用它的toString或者valueOf方法转换成原始类型。JavaScript内置核心对象(例如Array、Boolean、Function、Math、Number、RegExp和String)，会尝试valueOf先于toString；例外的是Date，Date利用的是toString转换。如果类型转换失败，则会产生一个runtime错误。 - 对象和原始类型比较，对象才会转换成原始类型。两个对象比较，如果两个值都引用同一个对象（含数组和函数），那么[相等]；否则[不相等]。示例：参考 MDN：Comparison Operators
</content>
    </entry>
    
     <entry>
        <title>玩玩HTTP servlet和session资源监控器</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E7%8E%A9http_servlet%E5%92%8Csession%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E5%99%A8/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>weblistener</tag><tag>servletcontextlisten</tag><tag>httpsessionlistener</tag><tag>web</tag><tag>javaee</tag>
        </tags>
        <content type="html">  WebListener是Servlet的监视器，它可以监听Http请求、会话生命周期事件等。其中包含如下接口： - ServletContextListener:监视servlet上下文的初始化(启动Web应用的初始化过程)、销毁（关闭Web应用）。 - ServletContextAttributeListener:监视servlet上下文属性的添加、删除和替换。 - ServletRequestListener:监视Http请求的初始化（进入第一个servlet或过滤器）、Web应用对请求处理结束（离开最后一个servlet或过滤器）。 - ServletRequestAttributeListener:监视Http请求属性的添加、删除和替换。 - HttpSessionListener:监视Http会话创建和失效操作。 - HttpSessionAttributeListener:监视Http会话属性的添加、删除和替换。最近hello一个Web项目，其中有个向导上传文件进行处理，文件被保存在tomcat的临时目录，处理后再删除。过段时间发现临时目录里有一些未删除的临时文件，应该是上传文件后，在处理结束前有一段时间未进行操作，会话失效，临时文件没有被删除。后来添加了一个MyResourceMonitorListener完成这些未处理临时文件的删除操作，解决了这个问题。代码如下：
MyResourceMonitorListener package com.yqu.http.session; import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; import javax.servlet.http.HttpSession; import javax.servlet.http.HttpSessionEvent; import javax.servlet.http.HttpSessionListener; public class MyResourceMonitorListener implements ServletContextListener, HttpSessionListener { @Override public void sessionCreated(HttpSessionEvent event) { trace(&amp;quot;sessionCreated!&amp;quot;); } @Override public void sessionDestroyed(HttpSessionEvent event) { HttpSession session = event.getSession(); if (session != null) { String sessionId = session.getId(); trace(&amp;quot;sessionDestroyed with sessionId=&amp;quot; &#43; sessionId &#43; &amp;quot;!&amp;quot;); MyUtil.cleanSessionResources(session); } } @Override public void contextDestroyed(ServletContextEvent event) { MyUtil.cleanServletResources(); } @Override public void contextInitialized(ServletContextEvent event) { MyUtil.cleanServletResources(); } private void trace(String msg) { System.out.println(msg); } }  MyUtil package com.yqu.http.session; import java.io.File; import java.io.FileFilter; import java.io.IOException; import javax.servlet.http.HttpSession; public class MyUtil { static final String PREFIX = &amp;quot;yqu-&amp;quot;; static final String SUFFIX = &amp;quot;.tmp&amp;quot;; public static final String FILEMETA = &amp;quot;yqu_resfile&amp;quot;; public static File createTempFile() throws IOException { File tempFile = File.createTempFile(PREFIX, SUFFIX); return tempFile; } public static void cleanSessionResources(HttpSession session) { if (session != null) { String flName = (String) session.getAttribute(FILEMETA); if (flName != null) { File fl = new File(flName); if (fl.exists()) { try { fl.delete(); trace(flName &#43; &amp;quot; is deleted during cleanSessionResources().&amp;quot;); } catch (Throwable t) { trace(t); fl.deleteOnExit(); } } } } } public static void cleanServletResources() { try { File tempFile = File.createTempFile(PREFIX, SUFFIX); File tempFolder = tempFile.getParentFile(); File[] files = tempFolder.listFiles(new TempFileFilter(PREFIX, SUFFIX)); for (File fl : files) { try { fl.delete(); trace(fl.getName() &#43; &amp;quot; is deleted during cleanServletResources().&amp;quot;); } catch (Exception e2) { trace(e2); fl.deleteOnExit(); } } } catch (Exception e1) { trace(e1); } } private static void trace(String msg) { System.out.println(msg); } private static void trace(Throwable t) { t.printStackTrace(); } private static class TempFileFilter implements FileFilter { private String prefix; private String suffix; public TempFileFilter(String prefix, String suffix) { this.prefix = prefix; this.suffix = suffix; } public boolean accept(File pathname) { if (pathname.exists() &amp;amp;&amp;amp; pathname.isFile()) { String name = pathname.getName(); return name.startsWith(prefix) &amp;amp;&amp;amp; name.endsWith(suffix); } else { return false; } } } }  web.xml </content>
    </entry>
    
     <entry>
        <title>用python分析FM代码和日志</title>
        <url>https://mryqu.github.io/post/%E7%94%A8python%E5%88%86%E6%9E%90fm%E4%BB%A3%E7%A0%81%E5%92%8C%E6%97%A5%E5%BF%97/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>fm</tag><tag>分析</tag><tag>日志</tag>
        </tags>
        <content type="html">  学python有一段时间了，学完不用，结果就是很快遗忘。这周分析一些FM代码和日志，终于弃Java转python，自己乐一下！
filterDV t6Found=False with open(&#39;SASFinancialManagement5.4f.log&#39;, &#39;w&#39;) as fdst: with open(&#39;SASFinancialManagement5.4.log&#39;, &#39;r&#39;) as fsrc: for line in fsrc: if t6Found: if &amp;quot;DataValidation&amp;quot; in line: fdst.write(line) else: if &amp;quot;formSetId=436,formId=,tableCode=NewTable6&amp;quot; in line: t6Found=True if not fsrc.closed: fsrc.close() if not fdst.closed: fdst.close()  calcDVDuration import re totalTime=0 with open(&#39;SASFinancialManagement5.4f.log&#39;, &#39;r&#39;) as fsrc: for line in fsrc: if &amp;quot;begin data validation at formSubScope for&amp;quot; in line: m = re.search(&#39;formSetId=.* ]&#39;, line) target = m.group(0) m = re.search(&#39;( &amp;lt;=when )\d&#43;&#39;, line) startTime = m.group(0) elif &amp;quot;DataValidationMapManagerBean addResopnse()&amp;quot; in line: m = re.search(&#39;( &amp;lt;=when )\d&#43;&#39;, line) endTime = m.group(0) print(target,&amp;quot; &amp;quot;,int(endTime)-int(startTime)) totalTime&#43;=int(endTime)-int(startTime) print(&#39;total time:&#39;,totalTime) if not fsrc.closed: fsrc.close()  findArrayListConv import os def checkArraylistConv(fl,listConvInfo): with open(fl, &#39;r&#39;) as fsrc: idx = 0 lastLine=&amp;quot;&amp;quot; for line in fsrc: idx &#43;= 1 if &amp;quot;super.find(&amp;quot; in line and (&amp;quot;ArrayList&amp;lt;&amp;quot; in line or &amp;quot;ArrayList&amp;lt;&amp;quot; in lastLine): listConvInfo.write(fl &#43; &#39; &#39; &#43; str(idx) &#43; &#39;:&#39; &#43; line &#43; &#39;\n&#39;) lastLine = line if not fsrc.closed: fsrc.close() def walkDir(dirpath,listConvInfo,topdown=True): for root, dirs, files in os.walk(dirpath, topdown): for fl in files: sufix = os.path.splitext(fl)[1][1:] if sufix.lower() == &amp;quot;java&amp;quot;: checkArraylistConv(os.path.join(root, fl),listConvInfo) dirpath = input(&#39;please input the FM source directory:&#39;) listConvInfo = open(&#39;listConvInfo.txt&#39;,&#39;w&#39;) walkDir(dirpath,listConvInfo) listConvInfo.close()  </content>
    </entry>
    
     <entry>
        <title>Linux/Unix下显示二进制目标文件的符号表</title>
        <url>https://mryqu.github.io/post/linux%E4%B8%8B%E6%98%BE%E7%A4%BA%E4%BA%8C%E8%BF%9B%E5%88%B6%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E7%9A%84%E7%AC%A6%E5%8F%B7%E8%A1%A8/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>symbol</tag><tag>elf</tag><tag>linux</tag><tag>nm</tag><tag>readelf</tag>
        </tags>
        <content type="html">  nm 查看二进制目标文件符号表的标准工具是nm，可以执行下列命令查看二进制目标文件（.o）/静态库（.a）/动态库（.so）的符号表：
nm -g yourObj.o nm -g yourLib.a nm -g yourLib.so  C/C&#43;&#43;语言在C&#43;&#43;编译器编译以后，函数的名字会被编译器修改，改成编译器内部的名字，这个名字会在链接的时候用到。例如std::string::size()经过修饰后是_ZNKSs4sizeEv。通过添加&amp;rdquo;-C&amp;rdquo;选项，可以对底层符号表译成用户级名称（demangle），具有更好的可读性。
以test.cpp为例：将其编译后，通过nm查看符号表，带&amp;rdquo;-C&amp;rdquo;选项与否的结果如下：readelf 如果你的二进制目标文件（.o）/静态库（.a）/动态库（.so）是ELF（Executableand linkingformat）格式，则可以使用readelf命令提取符号表信息。
readelf -Ws usr/lib/yourLib.so  如果仅想输出函数名，可以通过awk命令进行解析：
readelf -Ws test.o | awk &#39;$4==&amp;quot;FUNC&amp;quot; {print $8}&#39;;  以上面的test.o为例：
显示test.o的elf文件头信息：显示test.o的符号表：参考 How do I list the symbols in a .so file
nm - Linux man page
readelf - Linux man page
[](http://man.linuxde.net/nm)
[](http://man.linuxde.net/readelf)
</content>
    </entry>
    
     <entry>
        <title>事务管理器同步机制应用</title>
        <url>https://mryqu.github.io/post/%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%E5%99%A8%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6%E5%BA%94%E7%94%A8/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>事务同步</tag><tag>websphere</tag><tag>weblogic</tag><tag>jboss</tag><tag>tomcat</tag>
        </tags>
        <content type="html">  在我目前的项目中，缓存管理器需要根据事务提交状态对应用缓存进行管理。这里对其机制应用做一个小总结。
事务管理器 javax.transaction.TransactionManager接口定义一些方法以便应用服务器管理事务边界，比如获取与当前线程绑定的事务、对事务执行resume和suspend方法。
事务 javax.transaction.Transaction接口允许对目标事务对象执行相关的事务操作。在每个全局事务创建时，Transaction对象也对应的创建出来。 事务对象可以被用来做资源获取(resource enlistment)、同步器注册(synchronizationregistration)、事务结束(transaction completion)和状态查询(statusquery)操作。
事务同步机制 事务管理器通过javax.transaction.Synchronization接口支持一种同步机制。通过对当前事务的目标Transaction对象调用registerSynchronization方法，应用服务器为该事务注册一个Synchronization对象。 事务管理器在启动2PC事务提交过程时,回调Synchronization对象的beforeCompletion方法通知有关部分事务将要被提交；事务管理器在事务已经提交或回滚后,回调Synchronization对象的afterCompletion方法通知有关部分事务的结束状态。 应用一般很少使用beforeCompletion方法，afterCompletion方法使用比较多。在我目前的项目中，就是使用afterCompletion方法来通知缓存管理器对缓存进行更新操作。
实例代码 package com.yqu.transaction; import java.lang.reflect.Method; import javax.naming.InitialContext; import javax.transaction.RollbackException; import javax.transaction.Synchronization; import javax.transaction.SystemException; import javax.transaction.Transaction; import javax.transaction.TransactionManager; public class SynchronizationSample implements Synchronization { private static int wepsphereVer; private static final String WEBSPHERE_TM_FACTORY51 = &amp;quot;com.ibm.ws.Transaction.TransactionManagerFactory&amp;quot;; private static final String WEBSPHERE_TM_FACTORY50 = &amp;quot;com.ibm.ejs.jts.jta.TransactionManagerFactory&amp;quot;; private static final String WEBSPHERE_TM_FACTORY4 = &amp;quot;com.ibm.ejs.jts.jta.JTSXA&amp;quot;; private static final String WEBLOGIC_TM_OBJNAME = &amp;quot;javax.transaction.TransactionManager&amp;quot;; private static final String JBOSS_TM_OBJNAME = &amp;quot;java:/TransactionManager&amp;quot;; private static final String TOMCAT_TM_OBJNAME = &amp;quot;java:comp/env/TransactionManager&amp;quot;; private static final String WEBSPHERE_RECOG_CLASS = &amp;quot;com.ibm.websphere.rsadapter.WSCallHelper&amp;quot;; private static final String WEBLOGIC_RECOG_CLASS = &amp;quot;weblogic.transaction.Transaction&amp;quot;; private static final String JBOSS_RECOG_CLASS = &amp;quot;org.jboss.Main&amp;quot;; public enum ServerType { WEBSPHERE, WEBLOGIC, JBOSS, TOMCAT } public static ServerType getServerType() { Class clazz; try { clazz = Class.forName(WEBSPHERE_RECOG_CLASS); return ServerType.WEBSPHERE; } catch (Throwable x1) { try { clazz = Class.forName(WEBLOGIC_RECOG_CLASS); return ServerType.WEBLOGIC; } catch (Throwable x2) { try { clazz = Class.forName(JBOSS_RECOG_CLASS); return ServerType.JBOSS; } catch (Throwable x3) {} } } return ServerType.TOMCAT; } public static TransactionManager getTransactionManager() { ServerType type = getServerType(); try { if (type == ServerType.WEBSPHERE) { Class clazz; try { clazz = Class.forName(WEBSPHERE_TM_FACTORY51); wepsphereVer = 5; // 5.1 } catch (Exception e) { try { clazz = Class.forName(WEBSPHERE_TM_FACTORY50); wepsphereVer = 5; // 5.0 } catch (Exception e2) { clazz = Class.forName(WEBSPHERE_TM_FACTORY4); wepsphereVer = 4; } } Method method = clazz.getMethod(&amp;quot;getTransactionManager&amp;quot;, (Class[]) null); return (TransactionManager) method .invoke(null, (Object[]) null); } else { String name = null; if (type == ServerType.WEBLOGIC) name = WEBLOGIC_TM_OBJNAME; else if (type == ServerType.JBOSS) name = JBOSS_TM_OBJNAME; else name = TOMCAT_TM_OBJNAME; InitialContext ctx = new InitialContext(); return (TransactionManager) ctx.lookup(name); } } catch (Throwable x) { x.printStackTrace(); } return null; } public static Transaction getTransaction() throws IllegalStateException { TransactionManager mgr = getTransactionManager(); try { if (mgr == null) { return null; } return mgr.getTransaction(); } catch (SystemException e) { String msg = &amp;quot;can&#39;t get current transaction&amp;quot;; IllegalStateException x = new IllegalStateException(msg); x.initCause(e); throw x; } } public void register() { register(getTransaction()); } public void register(Transaction transaction) { if (transaction == null) { return; } try { transaction.registerSynchronization(this); } catch (SystemException x) { throw new RuntimeException(x); } catch (RollbackException x) { throw new RuntimeException(x); } catch (IllegalStateException x) { throw new RuntimeException(x); } } @Override public void beforeCompletion() { } @Override public void afterCompletion(int status) { // notify cache manager to update cache } public static void main(String[] args) { // (new SynchronizationSample()).register(); } }  </content>
    </entry>
    
     <entry>
        <title>[Eclipse] 确认Eclipse是32/64bit的方法</title>
        <url>https://mryqu.github.io/post/eclipse_%E7%A1%AE%E8%AE%A4eclipse%E6%98%AF32%E6%88%9664bit%E7%9A%84%E6%96%B9%E6%B3%95/</url>
        <categories>
          <category>Tool</category><category>Eclipse</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>platform</tag><tag>32bit</tag><tag>64bit</tag>
        </tags>
        <content type="html"> 两种方法：
 打开Eclipse界面：Help -&amp;gt; About Eclipse -&amp;gt; Installation Details -&amp;gt; Configuration，若看到x86字样说明Eclipse是32bit的，若看到x86_64或x64字样则说明Eclipse是64bit的。 查看Eclipse安装目录下的eclipse.ini文件，如果launcher.library设置的值中写的是X86就说明Eclipse是32bit的，如果写的是x86_64或x64则说明Eclipse是64bit的。  </content>
    </entry>
    
     <entry>
        <title>[JPA] CascadeType.REMOVE与orphanRemoval的区别</title>
        <url>https://mryqu.github.io/post/jpa_cascadetype.remove%E4%B8%8Eorphanremoval%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>jpa</tag><tag>hibernate</tag><tag>remove</tag><tag>cascadetype</tag><tag>orphanremoval</tag>
        </tags>
        <content type="html">  Cascading Remove 将引用字段标注为CascadeType.REMOVE（或包含REMOVE的CascadeType.ALL）表明删除操作将应该自动级联到由该字段引用的实体对象（多个实体对象可以由集合字段引用）:
@Entity class Employee { : @OneToOne(cascade=CascadeType.REMOVE) private Address address; : }  Orphan Removal JPA2额外支持一种更积极的删除级联模式，可以通过@OneToOne和@OneToMany注释的orphanRemoval元素设置:
@Entity class Employee { : @OneToOne(orphanRemoval=true) private Address address; : }  区别 两个设置的区别在于关系断开的响应。 例如，将地址字段设置为null或另一个Address对象时，不同设置的结果是不同。 - 如果指定了 orphanRemoval = true，则断开关系的的Address实例将被自动删除。这对于清除没有所有者对象（例如Employee）引用的、不该存在的依赖对象（例如Address）很有用。 - 如果仅指定 cascade = CascadeType.REMOVE，则不会执行上述自动删除操作，因为断开关系不是删除操作。
参考 Deleting JPA Entity Objects
</content>
    </entry>
    
     <entry>
        <title>nohup命令笔记</title>
        <url>https://mryqu.github.io/post/nohup%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>linux</tag><tag>nohug</tag>
        </tags>
        <content type="html"> Unix/Linux下一般比如想让某个程序在后台运行，很多都是使用 &amp;amp;在程序结尾来让程序自动运行。比如我们要运行mysql在后台：
/usr/local/mysql/bin/mysqld_safe --user=mysql &amp;amp;  但是很多程序并不象mysqld一样做成守护进程，一般普通程序使用 &amp;amp;结尾在后台运行，如果终端关闭了，普通程序还是会被关闭。如果想要在退出帐户/关闭终端之后继续运行相应的普通进程。我们就可以使用nohup这个命令，nohup就是不挂起的意思(nohang up)。
nohup COMMAND [ARG]...  nohup 命令运行由 Command参数和任何相关的 Arg参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加&amp;amp;到命令的尾部。
如果不将 nohup 命令的输出重定向，输出将附加到当前目录的 nohup.out 文件中。如果当前目录的 nohup.out文件不可写，输出重定向到 $HOME/nohup.out 文件中。如果没有文件能创建或打开以用于追加，那么 Command参数指定的命令不可调用。如果标准错误是一个终端，那么把指定的命令写给标准错误的所有输出作为标准输出重定向到相同的文件描述符。
nohup command &amp;gt; myout.file 2&amp;gt;&amp;amp;1  该命令返回下列出口值：
 126：可以查找但不能调用 Command 参数指定的命令。 127：nohup 命令发生错误或不能查找由 Command 参数指定的命令。 其他：Command 参数指定命令的退出状态。 使用jobs查看任务。使用fg %n关闭。  </content>
    </entry>
    
     <entry>
        <title>切记：Java中long字面量以L结尾</title>
        <url>https://mryqu.github.io/post/%E5%88%87%E8%AE%B0java%E4%B8%ADlong%E5%AD%97%E9%9D%A2%E9%87%8F%E4%BB%A5l%E7%BB%93%E5%B0%BE/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>negative</tag><tag>int</tag><tag>long</tag><tag>cast</tag><tag>literal</tag>
        </tags>
        <content type="html"> 最近碰到一个问题，输入是int，要求按无符号整数使用。为了实现要求，升级成long然后与上0xffffffff，结果不正常。问了几个基友，都有点茫然。 问题出在0xffffffff上了，现在就操练一下。
将0xffffffff赋给一个long变量，它的值是多少？
long n = 0xffffffff; System.out.println(Long.toHexString(n));  结果是0xffffffffffffffff呦！
ffffffffffffffff  将0x000000008fffffff赋给一个long变量，它的值是多少？
long = 0x000000008fffffff; System.out.println(Long.toHexString(n));  结果是0xffffffff8fffffff呦！！！
ffffffff8fffffff  int转换成long的规则是，一个负的int数，会自动扩展符号位，升级成负的long数的。 正确的用法是0xffffffffL，字面量表明是long类型，所以头32位才会全是0。
long = 0xffffffffL; System.out.println(Long.toHexString(n));  输出结果：
ffffffff  </content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 重温函数隐藏和重写</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E9%87%8D%E6%B8%A9%E5%87%BD%E6%95%B0%E9%9A%90%E8%97%8F%E5%92%8C%E9%87%8D%E5%86%99/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>overload</tag><tag>hide</tag><tag>override</tag><tag>virtual</tag>
        </tags>
        <content type="html">  首先回顾一下C&#43;&#43;的重载、隐藏和重写概念：
 在相同作用域中，同名不同参的函数称为重载，这是c&#43;&#43;多态的一种表现。对相同名字的成员函数，编译器可以根据传递的参数类型调用相应的成员函数。同名不同参的全局函数和类成员函数由于作用域不同，不是重载。不能通过函数返回值进行重载。像int和float这样不同的参数类型，可能会由于隐式转换隐患而无法通过编译。 当派生类中的成员函数/变量和基类中的成员函数/变量同名时，会隐藏基类的成员函数/变量，也就是指在派生类调用这个同名的成员函数/变量，调用的是派生类的成员函数/变量，而不是基类的那个成员函数/变量。可以通过类名::成员函数/变量去访问基类中同名的成员函数/变量。 派生类中的成员函数与基类的成员函数同名同参，就称为重写。当直接访问成员函数调用的是在派生类中重写的函数而不是从基类继承下来的成员函数，如果要访问从基类继承下来的成员函数也是通过类名::成员函数这种方式去调用基类的成员函数。  下面的小示例testOverride.cpp用于测试添加virtual与否对重写的影响：
class BaseClass { public: BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } BaseClass(const BaseClass&amp;amp;) { cout &amp;lt;&amp;lt; &amp;quot;BaseClass(BaseClass) on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } virtual void vfun1() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:vfun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } virtual void vfun2() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:vfun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun1() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:fun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun2() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:fun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } virtual ~BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;~BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } }; class DerivedClass : public BaseClass { public: DerivedClass():name(new string(&amp;quot;NULL&amp;quot;)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } DerivedClass(const string&amp;amp; n):name(new string(n)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass(string) on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void vfun1() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:vfun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun1() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:fun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } ~DerivedClass() { delete name; cout &amp;lt;&amp;lt; &amp;quot;~DerivedClass(): name has been deleted on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } private: void vfun2() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:vfun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun2() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:fun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } private: string* name; }; int main() { cout &amp;lt;&amp;lt; &amp;quot;=== test bo1 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass* bo1 = new BaseClass(); bo1-&amp;gt;vfun1(); bo1-&amp;gt;vfun2(); bo1-&amp;gt;fun1(); bo1-&amp;gt;fun2(); delete bo1; cout &amp;lt;&amp;lt; &amp;quot;=== test do1 ===&amp;quot; &amp;lt;&amp;lt; endl; DerivedClass* do1 = new DerivedClass(); do1-&amp;gt;vfun1(); // error: &#39;virtual void DerivedClass::vfun2()&#39; is private // within this context // do1-&amp;gt;vfun2(); do1-&amp;gt;fun1(); // error: &#39;void DerivedClass::fun2()&#39; is private // within this context // do1-&amp;gt;fun2(); delete do1; cout &amp;lt;&amp;lt; &amp;quot;=== test bo2 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass* bo2 = new DerivedClass(&amp;quot;123&amp;quot;); bo2-&amp;gt;vfun1(); bo2-&amp;gt;vfun2(); bo2-&amp;gt;fun1(); bo2-&amp;gt;fun2(); delete bo2; return 0; }  vfun2和fun2在BaseClass类中是public访问权限，而在DerivedClass类中是private访问权限。 - 对DerivedClass指针，vfun2和fun2无法访问，这个满足期望。 - 对于指向DerivedClass对象的BaseClass指针，vfun2和fun2仍然可以访问。我只能在心里留一个&amp;rdquo;?&amp;ldquo;了！！！
日志输出如下：
=== test bo1 === BaseClass() on 0x800e05058 BaseClass:vfun1() on 0x800e05058 BaseClass:vfun2() on 0x800e05058 BaseClass:fun1() on 0x800e05058 BaseClass:fun2() on 0x800e05058 ~BaseClass() on 0x800e05058 === test do1 === BaseClass() on 0x800e05040 DerivedClass() on 0x800e05040 DerivedClass:vfun1() on 0x800e05040 DerivedClass:fun1() on 0x800e05040 ~DerivedClass(): name has been deleted on 0x800e05040 ~BaseClass() on 0x800e05040 === test bo2 === BaseClass() on 0x800e06040 DerivedClass(string) on 0x800e06040 DerivedClass:vfun1() on 0x800e06040 DerivedClass:vfun2() on 0x800e06040 BaseClass:fun1() on 0x800e06040 BaseClass:fun2() on 0x800e06040 ~DerivedClass(): name has been deleted on 0x800e06040 ~BaseClass() on 0x800e06040  对于指向DerivedClass对象的BaseClass指针，fun2仍然可以访问，且指向的是BaseClass的fun2实现，这个满足期望。而vfun2由于带virtual关键字，则指向的是DerivedClass的vfun2实现，但是在DerivedClass类中可是private访问权限呀，无法理解，在心里继续保留&amp;rdquo;??&amp;ldquo;！！！
下面对testOverride.cpp稍加改造，让DerivedClass类保护或私有继承BaseClass类。
class BaseClass { public: BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } BaseClass(const BaseClass&amp;amp;) { cout &amp;lt;&amp;lt; &amp;quot;BaseClass(BaseClass) on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } virtual void vfun1() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:vfun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } virtual void vfun2() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:vfun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun1() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:fun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun2() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass:fun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } virtual ~BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;~BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } }; class DerivedClass : BaseClass { public: DerivedClass():name(new string(&amp;quot;NULL&amp;quot;)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } DerivedClass(const string&amp;amp; n):name(new string(n)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass(string) on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void vfun1() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:vfun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun1() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:fun1() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } ~DerivedClass() { delete name; cout &amp;lt;&amp;lt; &amp;quot;~DerivedClass(): name has been deleted on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } private: void vfun2() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:vfun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } void fun2() { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass:fun2() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } private: string* name; }; int main() { cout &amp;lt;&amp;lt; &amp;quot;=== test bo1 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass* bo1 = new BaseClass(); bo1-&amp;gt;vfun1(); bo1-&amp;gt;vfun2(); bo1-&amp;gt;fun1(); bo1-&amp;gt;fun2(); delete bo1; cout &amp;lt;&amp;lt; &amp;quot;=== test do1 ===&amp;quot; &amp;lt;&amp;lt; endl; DerivedClass* do1 = new DerivedClass(); do1-&amp;gt;vfun1(); // error: &#39;virtual void DerivedClass::vfun2()&#39; is private // within this context // do1-&amp;gt;vfun2(); do1-&amp;gt;fun1(); // error: &#39;void DerivedClass::fun2()&#39; is private // within this context // do1-&amp;gt;fun2(); delete do1; // cout &amp;lt;&amp;lt; &amp;quot;=== test bo2 ===&amp;quot; &amp;lt;&amp;lt; endl; // error: &#39;BaseClass&#39; is an inaccessible // base of &#39;DerivedClass&#39; // BaseClass* bo2 = new DerivedClass(&amp;quot;123&amp;quot;); // bo2-&amp;gt;vfun1(); // bo2-&amp;gt;vfun2(); // bo2-&amp;gt;fun1(); // bo2-&amp;gt;fun2(); // delete bo2; return 0; }  修改后，编译器始终对指向DerivedClass对象的BaseClass指针返回error: &amp;lsquo;BaseClass&amp;rsquo; is aninaccessible base of &amp;lsquo;DerivedClass&amp;rsquo;，所以注释掉对bo2的测试。
新日志输出如下：
=== test bo1 === BaseClass() on 0x800e05058 BaseClass:vfun1() on 0x800e05058 BaseClass:vfun2() on 0x800e05058 BaseClass:fun1() on 0x800e05058 BaseClass:fun2() on 0x800e05058 ~BaseClass() on 0x800e05058 === test do1 === BaseClass() on 0x800e05040 DerivedClass() on 0x800e05040 DerivedClass:vfun1() on 0x800e05040 DerivedClass:fun1() on 0x800e05040 ~DerivedClass(): name has been deleted on 0x800e05040 ~BaseClass() on 0x800e05040  在不使用公开继承的方式下，示例结果都符合期望。
参考 access specifiers
Derived classes
virtual function specifier
</content>
    </entry>
    
     <entry>
        <title>[Eclipse] Find the override method implementation in subclasses</title>
        <url>https://mryqu.github.io/post/eclipse_find_the_override_method_implementation_in_subclasses/</url>
        <categories>
          <category>Tool</category><category>Eclipse</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>override</tag><tag>method</tag><tag>implementation</tag><tag>subclass</tag>
        </tags>
        <content type="html"> 已知当前对象为org.apache.hadoop.hdfs.client.HdfsDataInputStream实例，调用其祖宗抽象类java.io.InputStream的publicint read(byte b[], int off, int len) throwsIOException方法，究竟最后使用的是那个类的实现呢？ 由如上类继承图可知，应该从对象类往上依次查找方法实现：HdfsDataInputStream、FSDataInputStream、&amp;hellip;&amp;hellip;。工作很繁琐，挑战眼力！
一个快捷的方式就是进入java.io.InputStream类，选中read(byte b[], int off, intlen) 方法，按住CTRL键后点击鼠标左键，这样会出现一个菜单： 选择OpenImplementation菜单，输入HdfsDataInputStream、FSDataInputStream、&amp;hellip;&amp;hellip;，可以更轻松的找到重写该方法的子类。 最终可知使用的是java.io.DataInputStream类中的实现：
public final int read(byte b[], int off, int len) throws IOException { return in.read(b, off, len); }  </content>
    </entry>
    
     <entry>
        <title>学习十二要素应用宣言</title>
        <url>https://mryqu.github.io/post/%E5%AD%A6%E4%B9%A0%E5%8D%81%E4%BA%8C%E8%A6%81%E7%B4%A0%E5%BA%94%E7%94%A8%E5%AE%A3%E8%A8%80/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>12-factor</tag><tag>十二要素</tag><tag>saas</tag><tag>方法论</tag>
        </tags>
        <content type="html">  Heroku是业内知名的云应用平台，从对外提供服务以来，他们已经有上百万应用的托管和运营经验。大概在去年，创始人Adam Wiggins根据这些经验，发布了一个“十二要素应用宣言（The Twelve-Factor App）”。 简介 如今，软件通常会作为一种服务来交付，它们被称为网络应用程序，或软件即服务（SaaS）。12-Factor为构建如下的SaaS应用提供了方法论： - 使用标准化流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。 - 和操作系统之间尽可能的划清界限，在各个系统中提供最大的可移植性。 - 适合部署在现代的云计算平台，从而在服务器和系统管理方面节省资源。 - 将开发环境和生产环境的差异降至最低，并使用持续交付实施敏捷开发。 - 可以在工具、架构和开发流程不发生明显变化的前提下实现扩展。
这套理论适用于任意语言和后端服务（数据库、消息队列、缓存等）开发的应用程序。
背景 本文的贡献者者参与过数以百计的应用程序的开发和部署，并通过Heroku平台间接见证了数十万应用程序的开发，运作以及扩展的过程。 本文综合了我们关于 SaaS应用几乎所有的经验和智慧，是开发此类应用的理想实践标准，并特别关注于应用程序如何保持良性成长，开发者之间如何进行有效的代码协作，以及如何避免软件污染。 我们的初衷是分享在现代软件开发过程中发现的一些系统性问题，并加深对这些问题的认识。我们提供了讨论这些问题时所需的共享词汇，同时使用相关术语给出一套针对这些问题的广义解决方案。本文格式的灵感来自于Martin Fowler 的书籍：_Patterns of Enterprise Application Architecture，Refactoring_。
读者应该是哪些人？ 任何SaaS应用的开发人员。部署和管理此类应用的运维工程师。
12-Factors I. 基准代码 一份基准代码，多份部署 12-Factor应用通常会使用版本控制系统加以管理，如Git、Mercurial、Subversion。一份用来跟踪代码所有修订版本的数据库被称作_代码库_（coderepository, code repo, repo）。 在类似 SVN这样的集中式版本控制系统中，_基准代码_就是指控制系统中的这一份代码库；而在Git那样的分布式版本控制系统中，_基准代码_则是指最上游的那份代码库。 基准代码和应用之间总是保持一一对应的关系： - 一旦有多个基准代码，就不能称为一个应用，而是一个分布式系统。分布式系统中的每一个组件都是一个应用，每一个应用可以分别使用12-Factor进行开发。 - 多个应用共享一份基准代码是有悖于12-Factor原则的。解决方案是将共享的代码拆分为独立的类库，然后使用依赖管理策略去加载它们。
尽管每个应用只对应一份基准代码，但可以同时存在多份部署。每份_部署_相当于运行了一个应用的实例。通常会有一个生产环境，一个或多个预发布环境。此外，每个开发人员都会在自己本地环境运行一个应用实例，这些都相当于一份部署。 所有部署的基准代码相同，但每份部署可以使用其不同的版本。比如，开发人员可能有一些提交还没有同步至预发布环境；预发布环境也有一些提交没有同步至生产环境。但它们都共享一份基准代码，我们就认为它们只是相同应用的不同部署而已。
II. 依赖 显式声明依赖关系 大多数编程语言都会提供一个打包系统，用来为各个类库提供打包服务，就像Perl的CPAN或是Ruby的Rubygems。通过打包系统安装的类库可以是系统级的（称之为 &amp;ldquo;sitepackages&amp;rdquo;），或仅供某个应用程序使用，部署在相应的目录中（称之为 &amp;ldquo;vendoring&amp;rdquo; 或 &amp;ldquo;bunding&amp;rdquo;）。 12-Factor规则下的应用程序不会隐式依赖系统级的类库。它一定通过_依赖清单_，确切地声明所有依赖项。此外，在运行过程中通过_依赖隔离_工具来确保程序不会调用系统中存在但清单中未声明的依赖项。这一做法会统一应用到生产和开发环境。 例如，Ruby的Gem Bundler使用Gemfile 作为依赖项声明清单，使用bundle exec 来进行依赖隔离。Python中则可分别使用两种工具 &amp;ndash; Pip用作依赖声明，Virtualenv用作依赖隔离。甚至C语言也有类似工具，Autoconf用作依赖声明，静态链接库用作依赖隔离。无论用什么工具，依赖声明和依赖隔离必须一起使用，否则无法满足12-Factor规范。 显式声明依赖的优点之一是为新进开发者简化了环境配置流程。新进开发者可以检出应用程序的基准代码，安装编程语言环境和它对应的依赖管理工具，只需通过一个_构建命令_来安装所有的依赖项，即可开始工作。例如，Ruby/Bundler下使用bundle install ，而Clojure/Leiningen则是lein deps。 12-Factor应用同样不会隐式依赖某些系统工具，如ImageMagick或是curl 。即使这些工具存在于几乎所有系统，但终究无法保证所有未来的系统都能支持应用顺利运行，或是能够和应用兼容。如果应用必须使用到某些系统工具，那么这些工具应该被包含在应用之中。
III. 配置 在环境中存储配置 通常，应用的_配置_在不同部署(预发布、生产环境、开发环境等等)间会有很大差异。这其中包括： - 数据库，Memcached，以及其他后端服务的配置 - 第三方服务的证书，如 Amazon S3、Twitter等 - 每份部署特有的配置，如域名等
有些应用在代码中使用常量保存配置，这与12-Factor所要求的代码和配置严格分离显然大相径庭。配置文件在各部署间存在大幅差异，代码却完全一致。 判断一个应用是否正确地将配置排除在代码之外，一个简单的方法是看该应用的基准代码是否可以立刻开源，而不用担心会暴露任何敏感的信息。 需要指出的是，这里定义的&amp;rdquo;配置&amp;rdquo;并不包括应用的内部配置，比如Rails的config/routes.rb ，或是使用Spring时代码模块间的依赖注入关系。这类配置在不同部署间不存在差异，所以应该写入代码。 另外一个解决方法是使用配置文件，但不把它们纳入版本控制系统，就像Rails的config/database.yml 。这相对于在代码中使用常量已经是长足进步，但仍然有缺点：总是会不小心将配置文件签入了代码库；配置文件的可能会分散在不同的目录，并有着不同的格式，这让找出一个地方来统一管理所有配置变的不太现实。更糟的是，这些格式通常是语言或框架特定的。 12-Factor推荐将应用的配置存储于_环境变量_中（envvars,_env_）。环境变量可以非常方便地在不同的部署间做修改，却不动一行代码；与配置文件不同，不小心把它们签入代码库的概率微乎其微；与一些传统的解决配置问题的机制（比如Java的属性配置文件）相比，环境变量与语言和系统无关。 配置管理的另一个方面是分组。有时应用会将配置按照特定部署进行分组（或叫做“环境”），例如Rails中的development 、test 和production 环境。这种方法无法轻易扩展：更多部署意味着更多新的环境，例如staging 或qa 。随着项目的不断深入，开发人员可能还会添加他们自己的环境，比如joes-staging ，这将导致各种配置组合的激增，从而给管理部署增加了很多不确定因素。 12-Factor应用中，环境变量的粒度要足够小，且相对独立。它们永远也不会组合成一个所谓的“环境”，而是独立存在于每个部署之中。当应用程序不断扩展，需要更多种类的部署时，这种配置管理方式能够做到平滑过渡。
IV. 后端服务 把后端服务当作附加资源 _后端服务_是指程序运行所需要的通过网络调用的各种服务，如数据库（MySQL、CouchDB），消息/队列系统（RabbitMQ、Beanstalkd），SMTP邮件发送服务（Postfix），以及缓存系统（Memcached）。 类似数据库的后端服务，通常由部署应用程序的系统管理员一起管理。除了本地服务之外，应用程序有可能使用了第三方发布和管理的服务。示例包括SMTP（例如Postmark），数据收集服务（例如New Relic或Loggly），数据存储服务（如Amazon S3），以及使用API访问的服务（例如Twitter、Google Maps、Last.fm）。 12-Factor应用不会区别对待本地或第三方服务。对应用程序而言，两种都是附加资源，通过一个 url 或是其他存储在配置中的服务定位/服务证书来获取数据。12-Factor应用的任意部署，都应该可以在不进行任何代码改动的情况下，将本地MySQL数据库换成第三方服务（例如Amazon RDS）。类似的，本地 SMTP 服务应该也可以和第三方 SMTP 服务（例如 Postmark ）互换。上述 2个例子中，仅需修改配置中的资源地址。 每个不同的后端服务是一份_资源_。例如，一个MySQL数据库是一个资源，两个MySQL数据库（用来数据分区）就被当作是2个不同的资源。12-Factor应用将这些数据库都视作_附加资源_，这些资源和它们附属的部署保持松耦合。 部署可以按需加载或卸载资源。例如，如果应用的数据库服务由于硬件问题出现异常，管理员可以从最近的备份中恢复一个数据库，卸载当前的数据库，然后加载新的数据库&amp;ndash; 整个过程都不需要修改代码。
V. 构建，发布，运行 严格分离构建和运行 基准代码转化为一份部署(非开发环境)需要以下三个阶段： - 构建阶段是指将代码仓库转化为可执行包的过程。构建时会使用指定版本的代码，获取和打包依赖项，编译成二进制文件和资源文件。 - 发布阶段会将构建的结果和当前部署所需配置相结合，并能够立刻在运行环境中投入使用。 - 运行阶段（或者说“运行时”）是指针对选定的发布版本，在执行环境中启动一系列应用程序进程。
12-facfor应用严格区分构建，发布，运行这三个步骤。举例来说，直接修改处于运行状态的代码是非常不可取的做法，因为这些修改很难再同步回构建步骤。 部署工具通常都提供了发布管理工具，最引人注目的功能是退回至较旧的发布版本。比如，Capistrano将所有发布版本都存储在一个叫releases 的子目录中，当前的在线版本只需映射至对应的目录即可。该工具的rollback 命令可以很容易地实现回退版本的功能。 每一个发布版本必须对应一个唯一的发布ID，例如可以使用发布时的时间戳（2011-04-06-20:32:17），亦或是一个增长的数字（v100）。发布的版本就像一本只能追加的账本，一旦发布就不可修改，任何的变动都应该产生一个新的发布版本。 新的代码在部署之前，需要开发人员触发构建操作。但是，运行阶段不一定需要人为触发，而是可以自动进行。如服务器重启，或是进程管理器重启了一个崩溃的进程。因此，运行阶段应该保持尽可能少的模块，这样假设半夜发生系统故障而开发人员又捉襟见肘也不会引起太大问题。构建阶段是可以相对复杂一些的，因为错误信息能够立刻展示在开发人员面前，从而得到妥善处理。
VI. 进程 以一个或多个无状态进程运行应用 运行环境中，应用程序通常是以一个和多个_进程_运行的。 最简单的场景中，代码是一个独立的脚本，运行环境是开发人员自己的笔记本电脑，进程由一条命令行（例如python my_script.py）。另外一个极端情况是，复杂的应用可能会使用很多进程类型，也就是零个或多个进程实例。 12-Factor 应用的进程必须无状态且无共享。任何需要持久化的数据都要存储在后端服务内，比如数据库。 内存区域或磁盘空间可以作为进程在做某种事务型操作时的缓存，例如下载一个很大的文件，对其操作并将结果写入数据库的过程。12-Factor应用根本不用考虑这些缓存的内容是不是可以保留给之后的请求来使用，这是因为应用启动了多种类型的进程，将来的请求多半会由其他进程来服务。即使在只有一个进程的情形下，先前保存的数据（内存或文件系统中）也会因为重启（如代码部署、配置更改、或运行环境将进程调度至另一个物理区域执行）而丢失。 源文件打包工具（Jammit,django-compressor）使用文件系统来缓存编译过的源文件。12-Factor应用更倾向于在构建步骤做此动作——正如Rails资源管道，而不是在运行阶段。 一些互联网系统依赖于“粘性 session”，这是指将用户session中的数据缓存至某进程的内存中，并将同一用户的后续请求路由到同一个进程。粘性session是12-Factor极力反对的。Session中的数据应该保存在诸如Memcached或Redis这样的带有过期时间的缓存中。
VII. 端口绑定 通过端口绑定提供服务 互联网应用有时会运行于服务器的容器之中。例如PHP经常作为Apache HTTPD的一个模块来运行，正如Java运行于Tomcat 。 12-Factor应用完全自我加载而不依赖于任何网络服务器就可以创建一个面向网络的服务。互联网应用通过端口绑定来提供服务，并监听发送至该端口的请求。 本地环境中，开发人员通过类似http://localhost:5000/ 的地址来访问服务。在线上环境中，请求统一发送至公共域名而后路由至绑定了端口的网络进程。 通常的实现思路是，将网络服务器类库通过依赖声明载入应用。例如，Python的Tornado,Ruby的Thin,Java以及其他基于JVM语言的Jetty。完全由_用户端_，确切的说应该是应用的代码，发起请求。和运行环境约定好绑定的端口即可处理这些请求。 HTTP并不是唯一一个可以由端口绑定提供的服务。其实几乎所有服务器软件都可以通过进程绑定端口来等待请求。例如，使用XMPP的ejabberd，以及使用Redis协议的Redis。 还要指出的是，端口绑定这种方式也意味着一个应用可以成为另外一个应用的后端服务，调用方将服务方提供的相应URL当作资源存入配置以备将来调用。
VIII. 并发 通过进程模型进行扩展 任何计算机程序，一旦启动，就会生成一个或多个进程。互联网应用采用多种进程运行方式。例如，PHP进程作为Apache的子进程存在，随请求按需启动。Java进程则采取了相反的方式，在程序启动之初JVM就提供了一个超级进程储备了大量的系统资源(CPU 和内存)，并通过多线程实现内部的并发管理。上述 2个例子中，进程是开发人员可以操作的最小单位。 在 12-actor 应用中，进程是一等公民。12-Factor应用的进程主要借鉴于unix守护进程模型。开发人员可以运用这个模型去设计应用架构，将不同的工作分配给不同的_进程类型_。例如，HTTP请求可以交给web进程来处理，而常驻的后台工作则交由worker进程负责。 这并不包括个别较为特殊的进程，例如通过虚拟机的线程处理并发的内部运算，或是使用诸如EventMachine、Twisted、Node.js的异步/事件触发模型。但一台独立的虚拟机的扩展有瓶颈（垂直扩展），所以应用程序必须可以在多台物理机器间跨进程工作。 上述进程模型会在系统急需扩展时大放异彩。12-Factor 应用的进程所具备的无共享、水平分区的特性意味着添加并发会变得简单而稳妥。这些进程的类型以及每个类型中进程的数量就被称作_进程构成_。 12-Factor 应用的进程不需要守护进程 或是写入PID文件。相反的，应该借助操作系统的进程管理器(例如Upstart，分布式的进程管理云平台，或是类似Foreman的工具)，来管理输出流，响应崩溃的进程，以及处理用户触发的重启和关闭超级进程的请求。
IX. 易处理 快速启动和优雅终止可最大化健壮性 12-Factor 应用的进程是_易处理（disposable）_的，意思是说它们可以瞬间开启或停止。这有利于快速、弹性的伸缩应用，迅速部署变化的代码或配置，稳健的部署应用。 进程应当追求最小启动时间。理想状态下，进程从敲下命令到真正启动并等待请求的时间应该只需很短的时间。更少的启动时间提供了更敏捷的发布以及扩展过程，此外还增加了健壮性，因为进程管理器可以在授权情形下容易的将进程搬到新的物理机器上。 进程 一旦接收终止信号（SIGTERM ）就会优雅的终止。就网络进程而言，优雅终止是指停止监听服务的端口，即拒绝所有新的请求，并继续执行当前已接收的请求，然后退出。此类型的进程所隐含的要求是HTTP请求大多都很短(不会超过几秒钟)，而在长时间轮询中，客户端在丢失连接后应该马上尝试重连。 对于 worker 进程来说，优雅终止是指将当前任务退回队列。例如，RabbitMQ中，worker可以发送一个NACK 信号。Beanstalkd中，任务终止并退回队列会在worker断开时自动触发。有锁机制的系统诸如Delayed Job则需要确定释放了系统资源。此类型的进程所隐含的要求是，任务都应该可重复执行，这主要由将结果包装进事务或是使重复操作幂等来实现。 进程还应当在面对突然死亡时保持健壮，例如底层硬件故障。虽然这种情况比起优雅终止来说少之又少，但终究有可能发生。一种推荐的方式是使用一个健壮的后端队列，例如Beanstalkd，它可以在客户端断开或超时后自动退回任务。无论如何，12-Factor应用都应该可以设计能够应对意外的、不优雅的终结。Crash-only design将这种概念转化为合乎逻辑的理论。
X. 开发环境与线上环境等价 尽可能的保持开发，预发布，线上环境相同 从以往经验来看，开发环境（即开发人员的本地部署）和线上环境（外部用户访问的真实部署）之间存在着很多差异。这些差异表现在以下三个方面： - 时间差异：开发人员正在编写的代码可能需要几天，几周，甚至几个月才会上线。 - 人员差异：开发人员编写代码，运维人员部署代码。 - 工具差异：开发人员或许使用 Nginx，SQLite，OS X，而线上环境使用 Apache，MySQL 以及Linux。
12-Factor 应用想要做到 持续部署 就必须缩小本地与线上差异。 再回头看上面所描述的三个差异: - 缩小时间差异：开发人员可以几小时，甚至几分钟就部署代码。 - 缩小人员差异：开发人员不只要编写代码，更应该密切参与部署过程以及代码在线上的表现。 - 缩小工具差异：尽量保证开发环境以及线上环境的一致性。
将上述总结变为一个表格如下：
||传统应用|12-Factor 应用 |&amp;mdash;&amp;ndash; |每次部署间隔|数周|几小时 |开发人员 vs 运维人员|不同的人|相同的人 |开发环境 vs 线上环境|不同|尽量接近
后端服务是保持开发与线上等价的重要部分，例如数据库、队列系统以及缓存。许多语言都提供了简化获取后端服务的类库，例如不同类型服务的_适配器_。下列表格提供了一些例子。
|类型|语言|类库|适配器 |&amp;mdash;&amp;ndash; |数据库|Ruby/Rails|ActiveRecord|MySQL, PostgreSQL, SQLite |队列|Python/Django|Celery|RabbitMQ, Beanstalkd, Redis |缓存|Ruby/Rails|ActiveSupport::Cache|Memory, filesystem, Memcached
开发人员有时会觉得在本地环境中使用轻量的后端服务具有很强的吸引力，而那些更重量级的健壮的后端服务应该使用在生产环境。例如，本地使用SQLite 线上使用 PostgreSQL；又如本地缓存在进程内存中而线上存入 Memcached。 12-Factor应用的开发人员应该反对在不同环境间使用不同的后端服务，即使适配器已经可以几乎消除使用上的差异。这是因为，不同的后端服务意味着会突然出现的不兼容，从而导致测试、预发布都正常的代码在线上出现问题。这些错误会给持续部署带来阻力。从应用程序的生命周期来看，消除这种阻力需要花费很大的代价。 与此同时，轻量的本地服务也不像以前那样引人注目。借助于Homebrew、apt-get等现代的打包系统，诸如Memcached、PostgreSQL、RabbitMQ等后端服务的安装与运行也并不复杂。此外，使用类似Chef和Puppet的声明式配置工具，结合像Vagrant这样轻量的虚拟环境就可以使得开发人员的本地环境与线上环境无限接近。与同步环境和持续部署所带来的益处相比，安装这些系统显然是值得的。 不同后端服务的适配器仍然是有用的，因为它们可以使移植后端服务变得简单。但应用的所有部署，这其中包括开发、预发布以及线上环境，都应该使用同一个后端服务的相同版本。
XI. 日志 把日志当作事件流 _日志_使得应用程序运行的动作变得透明。在基于服务器的环境中，日志通常被写在硬盘的一个文件里，但这只是一种输出格式。 日志应该是事件流的汇总，将所有运行中进程和后端服务的输出流按照时间顺序收集起来。尽管在回溯问题时可能需要看很多行，日志最原始的格式确实是一个事件一行。日志没有确定开始和结束，但随着应用在运行会持续的增加。 12-factor应用本身从不考虑存储自己的输出流。不应该试图去写或者管理日志文件。相反，每一个运行的进程都会直接的标准输出（stdout ）事件流。开发环境中，开发人员可以通过这些数据流，实时在终端看到应用的活动。 在预发布或线上部署中，每个进程的输出流由运行环境截获，并将其他输出流整理在一起，然后一并发送给一个或多个最终的处理程序，用于查看或是长期存档。这些存档路径对于应用来说不可见也不可配置，而是完全交给程序的运行环境管理。类似Logplex和Fluent的开源工具可以达到这个目的。 这些事件流可以输出至文件，或者在终端实时观察。最重要的，输出流可以发送到Splunk这样的日志索引及分析系统，或Hadoop/Hive这样的通用数据存储系统。这些系统为查看应用的历史活动提供了强大而灵活的功能，包括： - 找出过去一段时间特殊的事件。 - 图形化一个大规模的趋势，比如每分钟的请求量。 - 根据用户定义的条件实时触发警报，比如每分钟的报错超过某个警戒线。
XII. 管理进程 后台管理任务当作一次性进程运行 进程构成（processformation）是指用来处理应用的常规业务（比如处理 web请求）的一组进程。与此不同，开发人员经常希望执行一些管理或维护应用的一次性任务，例如： - 运行数据移植（Django中的manage.py migrate ,Rails中的rake db:migrate）。 - 运行一个控制台（也被称为REPL shell），来执行一些代码或是针对线上数据库做一些检查。大多数语言都通过解释器提供了一个REPL工具（python或perl），或是其他命令（Ruby使用irb,Rails使用rails console）。 - 运行一些提交到代码仓库的一次性脚本。 一次性管理进程应该和正常的常驻进程使用同样的环境。这些管理进程和任何其他的进程一样使用相同的代码和配置，基于某个发布版本运行。后台管理代码应该随其他应用程序代码一起发布，从而避免同步问题。
所有进程类型应该使用同样的依赖隔离技术。例如，如果Ruby的web进程使用了命令bundle exec thin start，那么数据库移植应使用bundle exec rake db:migrate。同样的，如果一个Python程序使用了 Virtualenv，则需要在运行 TornadoWeb服务器和任何manage.py管理进程时引入bin/python。 12-factor尤其青睐那些提供了REPLshell的语言，因为那会让运行一次性脚本变得简单。在本地部署中，开发人员直接在命令行使用 shell命令调用一次性管理进程。在线上部署中，开发人员依旧可以使用ssh或是运行环境提供的其他机制来运行这样的进程。
参考 The Twelve-Factor App
GitHub：heroku/12factor
talk :Millions of Apps Deployed: What We&amp;rsquo;ve Learned
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 重温析构函数</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E9%87%8D%E6%B8%A9%E6%9E%90%E6%9E%84%E5%87%BD%E6%95%B0/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>c</tag><tag>constructor</tag><tag>destructor</tag><tag>virtual</tag><tag>copy</tag>
        </tags>
        <content type="html"> 创建一个C&#43;&#43;对象时，一般先调用父类构造函数，再调用自己的构造函数；而在销毁一个C&#43;&#43;对象时，一般先调用自己的析构函数，再调用父类的析构函数。我用如下testDestructor.cpp进行测试。
class BaseClass { public: BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } ~BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;~BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } }; class DerivedClass : public BaseClass { public: DerivedClass():name(new string(&amp;quot;NULL&amp;quot;)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } DerivedClass(const string&amp;amp; n):name(new string(n)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass(string) on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } ~DerivedClass() { delete name; cout &amp;lt;&amp;lt; &amp;quot;~DerivedClass(): name has been deleted on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } private: string* name; }; int main() { cout &amp;lt;&amp;lt; &amp;quot;=== test bo1 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass* bo1 = new BaseClass(); delete bo1; cout &amp;lt;&amp;lt; &amp;quot;=== test do1 ===&amp;quot; &amp;lt;&amp;lt; endl; DerivedClass* do1 = new DerivedClass(); delete do1; cout &amp;lt;&amp;lt; &amp;quot;=== test bo2 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass* bo2 = new DerivedClass(&amp;quot;123&amp;quot;); delete bo2; cout &amp;lt;&amp;lt; &amp;quot;=== test bo3 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass bo3 = DerivedClass(&amp;quot;321&amp;quot;); return 0; }  输出结果如下：
=== test bo1 === BaseClass() on 0x800e05104 ~BaseClass() on 0x800e05104 === test do1 === BaseClass() on 0x800e05058 DerivedClass() on 0x800e05058 ~DerivedClass(): name has been deleted on 0x800e05058 ~BaseClass() on 0x800e05058 === test bo2 === BaseClass() on 0x800e06058 DerivedClass(string) on 0x800e06058 ~BaseClass() on 0x800e06058 === test bo3 === BaseClass() on 0x7fffffffe840 DerivedClass(string) on 0x7fffffffe840 ~DerivedClass(): name has been deleted on 0x7fffffffe840 ~BaseClass() on 0x7fffffffe840 ~BaseClass() on 0x7fffffffe82f   bo1：创建时调用BaseClass构造函数，销毁时调用BaseClass析构函数； do1：创建时先调用BaseClass构造函数，再调用DerivedClass构造函数；销毁时先调用DerivedClass析造函数，再调用BaseClass析造函数； bo2：为什么没有调用DerivedClass析造函数? bo3：怎么调用了两次BaseClass析造函数? 0x7fffffffe82f这个对象是什么，怎么没见到构造函数? 分析：  对于bo2对象来说，它是BaseClass指针，指向的是DerivedClass对象。由于BaseClass类中的析构函数是非虚析构函数，当删除父类指针指向的派生类对象时就不会触发动态绑定，因而只会调用父类的析构函数，而不会调用派生类的析构函数。为了防止这种情况的发生，C&#43;&#43;中父类的析构函数应采用virtual虚析构函数。
对于bo3来说，上段代码实现实际是创建了一个DerivedClass类临时对象，然后通过BaseClass类的默认拷贝构造函数创建的bo3。因而第一个析构函数调用对象是DerivedClass类临时对象，第二个析构函数调用对象才是bo3。
下面对testDestructor.cpp进行改造：为BaseClass类的析构函数添加virtual关键字，添加BaseClass类的拷贝构造函数：
class BaseClass { public: BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } BaseClass(const BaseClass&amp;amp;) { cout &amp;lt;&amp;lt; &amp;quot;BaseClass(BaseClass) on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; std::endl; } virtual ~BaseClass() { cout &amp;lt;&amp;lt; &amp;quot;~BaseClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } }; class DerivedClass : public BaseClass { public: DerivedClass():name(new string(&amp;quot;NULL&amp;quot;)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass() on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } DerivedClass(const string&amp;amp; n):name(new string(n)) { cout &amp;lt;&amp;lt; &amp;quot;DerivedClass(string) on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } ~DerivedClass() { delete name; cout &amp;lt;&amp;lt; &amp;quot;~DerivedClass(): name has been deleted on &amp;quot; &amp;lt;&amp;lt; this &amp;lt;&amp;lt; endl; } private: string* name; }; int main() { cout &amp;lt;&amp;lt; &amp;quot;=== test bo1 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass* bo1 = new BaseClass(); delete bo1; cout &amp;lt;&amp;lt; &amp;quot;=== test do1 ===&amp;quot; &amp;lt;&amp;lt; endl; DerivedClass* do1 = new DerivedClass(); delete do1; cout &amp;lt;&amp;lt; &amp;quot;=== test bo2 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass* bo2 = new DerivedClass(&amp;quot;123&amp;quot;); delete bo2; cout &amp;lt;&amp;lt; &amp;quot;=== test bo3 ===&amp;quot; &amp;lt;&amp;lt; endl; BaseClass bo3 = DerivedClass(&amp;quot;321&amp;quot;); return 0; }  新的测试日志显示bo2正确调用DerivedClass类的析构函数，而创建bo3的拷贝构造函数也确实被调用。
=== test bo1 === BaseClass() on 0x800e05058 ~BaseClass() on 0x800e05058 === test do1 === BaseClass() on 0x800e05040 DerivedClass() on 0x800e05040 ~DerivedClass(): name has been deleted on 0x800e05040 ~BaseClass() on 0x800e05040 === test bo2 === BaseClass() on 0x800e06040 DerivedClass(string) on 0x800e06040 ~DerivedClass(): name has been deleted on 0x800e06040 ~BaseClass() on 0x800e06040 === test bo3 === BaseClass() on 0x7fffffffe840 DerivedClass(string) on 0x7fffffffe840 BaseClass(BaseClass) on 0x7fffffffe820 ~DerivedClass(): name has been deleted on 0x7fffffffe840 ~BaseClass() on 0x7fffffffe840 ~BaseClass() on 0x7fffffffe820  </content>
    </entry>
    
     <entry>
        <title>[HBase] HBase Shell交互实践</title>
        <url>https://mryqu.github.io/post/hbase_hbase_shell%E4%BA%A4%E4%BA%92%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>shell</tag><tag>interacting</tag><tag>practice</tag>
        </tags>
        <content type="html">  HBase Shell是对HBase的脚本接口，是一个JRuby REPL(Read-Eval-PrintLoop，“读取-求值-输出”循环)，可以通过脚本访问所有HBase客户端API。
单列族练习 创建表friends $ hbase shell hbase(main):001:0&amp;gt; list TABLE customer 1 row(s) in 0.2050 seconds =&amp;gt; [&amp;quot;customer&amp;quot;, &amp;quot;student&amp;quot;] hbase(main):002:0&amp;gt; create &#39;friends&#39;, &#39;d&#39; 0 row(s) in 1.3350 seconds =&amp;gt; Hbase::Table - friends hbase(main):003:0&amp;gt; list TABLE customer friends 2 row(s) in 0.0060 seconds =&amp;gt; [&amp;quot;customer&amp;quot;, &amp;quot;friends&amp;quot;, &amp;quot;student&amp;quot;]  获得表friends的描述说明 hbase(main):004:0&amp;gt; describe &#39;friends&#39; Table friends is ENABLED friends COLUMN FAMILIES DESCRIPTION {NAME =&amp;gt; &#39;d&#39;, DATA_BLOCK_ENCODING =&amp;gt; &#39;NONE&#39;, BLOOMFILTER =&amp;gt; &#39;ROW&#39;, REPLICATION_SCO PE =&amp;gt; &#39;0&#39;, VERSIONS =&amp;gt; &#39;1&#39;, COMPRESSION =&amp;gt; &#39;NONE&#39;, MIN_VERSIONS =&amp;gt; &#39;0&#39;, TTL =&amp;gt; &#39;FO REVER&#39;, KEEP_DELETED_CELLS =&amp;gt; &#39;FALSE&#39;, BLOCKSIZE =&amp;gt; &#39;65536&#39;, IN_MEMORY =&amp;gt; &#39;false&#39;, BLOCKCACHE =&amp;gt; &#39;true&#39;} 1 row(s) in 0.1000 seconds  插入一条数据并查询一行记录 hbase(main):005:0&amp;gt; put &#39;friends&#39;, &#39;ross&#39;, &#39;d:portrayedBy&#39;, &#39;David&#39; 0 row(s) in 0.0990 seconds hbase(main):006:0&amp;gt; get &#39;friends&#39;, &#39;ross&#39; COLUMNCELL d:portrayedBy timestamp=1381748454624, value=David 1 row(s) in 0.0670 seconds  插入多条数据并查询整张表 hbase(main):007:0&amp;gt; put &#39;friends&#39;, &#39;chandler&#39;, &#39;d:portrayedBy&#39;, &#39;Matthew&#39; 0 row(s) in 0.0080 seconds hbase(main):008:0&amp;gt; put &#39;friends&#39;, &#39;joey&#39;, &#39;d:portrayedBy&#39;, &#39;Matt&#39; 0 row(s) in 0.0050 seconds hbase(main):009:0&amp;gt; put &#39;friends&#39;, &#39;rachel&#39;, &#39;d:gender&#39;, &#39;female&#39; 0 row(s) in 0.0050 seconds hbase(main):010:0&amp;gt; put &#39;friends&#39;, &#39;monica&#39;, &#39;d:gender&#39;, &#39;female&#39; 0 row(s) in 0.0060 seconds hbase(main):011:0&amp;gt; put &#39;friends&#39;, &#39;phoebe&#39;, &#39;d:gender&#39;, &#39;female&#39; 0 row(s) in 0.0070 seconds hbase(main):012:0&amp;gt; scan &#39;friends&#39; ROW COLUMN&#43;CELL chandler column=d:portrayedBy, timestamp=1381748473491, value=Matthew joey column=d:portrayedBy, timestamp=1381748473532, value=Matt monica column=d:gender, timestamp=1381748473574, value=female phoebe column=d:gender, timestamp=1381748474832, value=female rachel column=d:gender, timestamp=1381748473552, value=female ross column=d:portrayedBy, timestamp=1381748454624, value=David 6 row(s) in 0.0390 seconds  查询一行记录 其中行键joey对应有记录，而janice对应无记录：
hbase(main):013:0&amp;gt; get &#39;friends&#39;, &#39;joey&#39; COLUMNCELL d:portrayedBy timestamp=1381748473532, value=Matt 1 row(s) in 0.0080 seconds hbase(main):014:0&amp;gt; get &#39;friends&#39;, &#39;janice&#39; COLUMNCELL 0 row(s) in 0.0080 seconds  查询整张表中行键大于等于janice的记录 行键chandler比janice小，因此没有显示：
hbase(main):016:0&amp;gt; scan &#39;friends&#39;, {STARTROW =&amp;gt; &#39;janice&#39;} ROW COLUMN&#43;CELL joey column=d:portrayedBy, timestamp=1381748473532, value=Matt monica column=d:gender, timestamp=1381748473574, value=female phoebe column=d:gender, timestamp=1381748474832, value=female rachel column=d:gender, timestamp=1381748473552, value=female ross column=d:portrayedBy, timestamp=1381748454624, value=David 5 row(s) in 0.0180 seconds  插入多条数据并重新查询整张表 hbase(main):017:0&amp;gt; put &#39;friends&#39;, &#39;ross&#39;, &#39;d:occupation&#39;, &#39;paleontologist&#39; 0 row(s) in 0.0080 seconds hbase(main):018:0&amp;gt; put &#39;friends&#39;, &#39;joey&#39;, &#39;d:occupation&#39;, &#39;actor&#39; 0 row(s) in 0.0050 seconds hbase(main):019:0&amp;gt; put &#39;friends&#39;, &#39;chandler&#39;, &#39;d:occupation&#39;, &#39;statistical analysis and data reconfiguration&#39; 0 row(s) in 0.0060 seconds hbase(main):020:0&amp;gt; put &#39;friends&#39;, &#39;monica&#39;, &#39;d:occupation&#39;, &#39;chef&#39; 0 row(s) in 0.0080 seconds hbase(main):021:0&amp;gt; scan &#39;friends&#39; ROW COLUMN&#43;CELL chandler column=d:occupation, timestamp=1381757551434, value=statistical analysis and data reconfiguration chandler column=d:portrayedBy, timestamp=1381748473491, value=Matthew joey column=d:occupation, timestamp=1381757551378, value=actor joey column=d:portrayedBy, timestamp=1381748473532, value=Matt monica column=d:gender, timestamp=1381748473574, value=female monica column=d:occupation, timestamp=1381757552772, value=chef phoebe column=d:gender, timestamp=1381748474832, value=female rachel column=d:gender, timestamp=1381748473552, value=female ross column=d:occupation, timestamp=1381757551357, value=paleontologist ross column=d:portrayedBy, timestamp=1381748454624, value=David 6 row(s) in 0.0310 seconds  删除joey行的d:portrayedBy列并更新d:occupation列 hbase(main):022:0&amp;gt; delete &#39;friends&#39;, &#39;joey&#39;, &#39;d:portrayedBy&#39; 0 row(s) in 0.0310 seconds hbase(main):023:0&amp;gt; put &#39;friends&#39;, &#39;joey&#39;, &#39;d:occupation&#39;, &#39;Waiter&#39; 0 row(s) in 0.0080 seconds hbase(main):024:0&amp;gt; scan &#39;friends&#39; ROW COLUMN&#43;CELL chandler column=d:occupation, timestamp=1381757551434, value=statistical analysis and data reconfiguration chandler column=d:portrayedBy, timestamp=1381748473491, value=Matthew joey column=d:occupation, timestamp=1381748815753, value=Waiter monica column=d:gender, timestamp=1381748473574, value=female monica column=d:occupation, timestamp=1381757552772, value=chef phoebe column=d:gender, timestamp=1381748474832, value=female rachel column=d:gender, timestamp=1381748473552, value=female ross column=d:occupation, timestamp=1381757551357, value=paleontologist ross column=d:portrayedBy, timestamp=1381748454624, value=David 6 row(s) in 0.0280 seconds  删除ross行 hbase(main):025:0&amp;gt; deleteall &#39;friends&#39;, &#39;ross&#39; 0 row(s) in 0.0060 seconds hbase(main):026:0&amp;gt; scan &#39;friends&#39; ROW COLUMN&#43;CELL chandler column=d:occupation, timestamp=1381757551434, value=statistical analysis and data reconfiguration chandler column=d:portrayedBy, timestamp=1381748473491, value=Matthew joey column=d:occupation, timestamp=1381748815753, value=Waiter monica column=d:gender, timestamp=1381748473574, value=female monica column=d:occupation, timestamp=1381757552772, value=chef phoebe column=d:gender, timestamp=1381748474832, value=female rachel column=d:gender, timestamp=1381748473552, value=female 5 row(s) in 0.0160 seconds  禁止并丢弃friends表 hbase(main):027:0&amp;gt; list TABLE customer friends 2 row(s) in 0.0090 seconds =&amp;gt; [&amp;quot;customer&amp;quot;, &amp;quot;friends&amp;quot;, &amp;quot;student&amp;quot;] hbase(main):028:0&amp;gt; disable &#39;friends&#39; 0 row(s) in 2.2720 seconds hbase(main):029:0&amp;gt; drop &#39;friends&#39; 0 row(s) in 1.2460 seconds hbase(main):030:0&amp;gt; list TABLE customer 1 row(s) in 0.0040 seconds =&amp;gt; [&amp;quot;customer&amp;quot;, &amp;quot;student&amp;quot;] hbase(main):031:0&amp;gt; exit $  Hbase Shell执行脚本练习 $ cat friends.rb if !list.include?(&amp;quot;friends&amp;quot;) create &#39;friends&#39;, &#39;d&#39; end put &#39;friends&#39;, &#39;ross&#39;, &#39;d:portrayedBy&#39;, &#39;David&#39; put &#39;friends&#39;, &#39;chandler&#39;, &#39;d:portrayedBy&#39;, &#39;Matthew&#39; put &#39;friends&#39;, &#39;joey&#39;, &#39;d:portrayedBy&#39;, &#39;Matt&#39; put &#39;friends&#39;, &#39;rachel&#39;, &#39;d:gender&#39;, &#39;female&#39; put &#39;friends&#39;, &#39;monica&#39;, &#39;d:gender&#39;, &#39;female&#39; put &#39;friends&#39;, &#39;phoebe&#39;, &#39;d:gender&#39;, &#39;female&#39; put &#39;friends&#39;, &#39;ross&#39;, &#39;d:occupation&#39;, &#39;paleontologist&#39; put &#39;friends&#39;, &#39;joey&#39;, &#39;d:occupation&#39;, &#39;actor&#39; put &#39;friends&#39;, &#39;chandler&#39;, &#39;d:occupation&#39;, &#39;statistical analysis and data reconfiguration&#39; put &#39;friends&#39;, &#39;monica&#39;, &#39;d:occupation&#39;, &#39;chef&#39; exit $ hbase shell -n friends.rb TABLE customer 1 row(s) in 0.2340 seconds 0 row(s) in 1.2530 seconds 0 row(s) in 0.1790 seconds 0 row(s) in 0.0060 seconds 0 row(s) in 0.0040 seconds 0 row(s) in 0.0070 seconds 0 row(s) in 0.0040 seconds 0 row(s) in 0.0040 seconds 0 row(s) in 0.0030 seconds 0 row(s) in 0.0040 seconds 0 row(s) in 0.0030 seconds 0 row(s) in 0.0040 seconds $ hbase shell hbase(main):001:0&amp;gt; list TABLE customer friends 2 row(s) in 0.2010 seconds =&amp;gt; [&amp;quot;customer&amp;quot;, &amp;quot;friends&amp;quot;, &amp;quot;student&amp;quot;] hbase(main):002:0&amp;gt; scan &#39;friends&#39; ROW COLUMN&#43;CELL chandler column=d:occupation, timestamp=1381748474147, value=statistical analysis and data reconfiguration chandler column=d:portrayedBy, timestamp=1381748474109, value=Matthew joey column=d:occupation, timestamp=1381748474143, value=actor joey column=d:portrayedBy, timestamp=1381748474116, value=Matt monica column=d:gender, timestamp=1381748474129, value=female monica column=d:occupation, timestamp=1381748474151, value=chef phoebe column=d:gender, timestamp=1381748474133, value=female rachel column=d:gender, timestamp=1381748474124, value=female ross column=d:occupation, timestamp=1381748474138, value=paleontologist ross column=d:portrayedBy, timestamp=1381748474094, value=David 6 row(s) in 0.1620 seconds  多列族练习 创建有两个列族的表并插入多条数据 $ hbase shell hbase(main):001:0&amp;gt; create &#39;student&#39;, {NAME=&amp;gt;&#39;addr&#39;}, {NAME=&amp;gt;&#39;score&#39;} 0 row(s) in 1.4160 seconds =&amp;gt; Hbase::Table - student hbase(main):002:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;addr:city&#39;, &#39;xian&#39; 0 row(s) in 0.1420 seconds hbase(main):003:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;addr:state&#39;, &#39;ShanXi&#39; 0 row(s) in 0.0050 seconds hbase(main):004:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;score:numb&#39;, &#39;1234&#39; 0 row(s) in 0.0050 seconds hbase(main):005:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;score:date&#39;, &#39;2013-10-01&#39; 0 row(s) in 0.0050 seconds hbase(main):006:0&amp;gt; put &#39;student&#39;, &#39;nancy&#39;, &#39;addr:city&#39;, &#39;beijing&#39; 0 row(s) in 0.0060 seconds hbase(main):007:0&amp;gt; put &#39;student&#39;, &#39;nancy&#39;, &#39;addr:state&#39;, &#39;BeiJing&#39; 0 row(s) in 0.0040 seconds hbase(main):008:0&amp;gt; put &#39;student&#39;, &#39;nancy&#39;, &#39;score:numb&#39;, &#39;6666&#39; 0 row(s) in 0.0040 seconds hbase(main):009:0&amp;gt; put &#39;student&#39;, &#39;tina&#39;, &#39;addr:city&#39;, &#39;tianjin&#39; 0 row(s) in 0.0060 seconds hbase(main):010:0&amp;gt; put &#39;student&#39;, &#39;tina&#39;, &#39;addr:state&#39;, &#39;TianJin&#39; 0 row(s) in 0.0050 seconds hbase(main):011:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;addr:city&#39;, &#39;chengdu&#39; 0 row(s) in 0.0050 seconds hbase(main):012:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;addr:state&#39;, &#39;SiChuan&#39; 0 row(s) in 0.0050 seconds hbase(main):013:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;score:numb&#39;, &#39;5555&#39; hbase(main):014:0&amp;gt; put &#39;student&#39;, &#39;nancy&#39;, &#39;addr:state&#39;, &#39;ShanXi&#39; 0 row(s) in 0.0040 seconds hbase(main):015:0&amp;gt; put &#39;student&#39;, &#39;alex&#39;, &#39;addr:state&#39;, &#39;HeNan&#39; 0 row(s) in 0.0080 seconds  查询行键为jason的所有addr列族数据 hbase(main):016:0&amp;gt; get &#39;student&#39;, &#39;jason&#39;, {COLUMNS=&amp;gt;[&#39;addr&#39;]} COLUMN CELL addr:city timestamp=1381757474426, value=chengdu addr:state timestamp=1381757474446, value=SiChuan 2 row(s) in 0.0360 seconds  查询行键为jason的所有score:numb列数据 hbase(main):017:0&amp;gt; get &#39;student&#39;, &#39;jason&#39;, {COLUMNS=&amp;gt;[&#39;score:numb&#39;]} COLUMN CELL score:numb timestamp=1381757474588, value=5555 1 row(s) in 0.0080 seconds  让score列族存储更多版本的数据 hbase(main):018:0&amp;gt; describe &#39;student&#39; Table student is ENABLED student COLUMN FAMILIES DESCRIPTION {NAME =&amp;gt; &#39;addr&#39;, DATA_BLOCK_ENCODING =&amp;gt; &#39;NONE&#39;, BLOOMFILTER =&amp;gt; &#39;ROW&#39;, REPLICATION_ SCOPE =&amp;gt; &#39;0&#39;, VERSIONS =&amp;gt; &#39;1&#39;, COMPRESSION =&amp;gt; &#39;NONE&#39;, MIN_VERSIONS =&amp;gt; &#39;0&#39;, TTL =&amp;gt; &#39;FOREVER&#39;, KEEP_DELETED_CELLS =&amp;gt; &#39;FALSE&#39;, BLOCKSIZE =&amp;gt; &#39;65536&#39;, IN_MEMORY =&amp;gt; &#39;false&#39;, BLOCKCACHE =&amp;gt; &#39;true&#39;} {NAME =&amp;gt; &#39;score&#39;, DATA_BLOCK_ENCODING =&amp;gt; &#39;NONE&#39;, BLOOMFILTER =&amp;gt; &#39;ROW&#39;, REPLICATION _SCOPE =&amp;gt; &#39;0&#39;, VERSIONS =&amp;gt; &#39;1&#39;, COMPRESSION =&amp;gt; &#39;NONE&#39;, MIN_VERSIONS =&amp;gt; &#39;0&#39;, TTL =&amp;gt; &#39;FOREVER&#39;, KEEP_DELETED_CELLS =&amp;gt; &#39;FALSE&#39;, BLOCKSIZE =&amp;gt; &#39;65536&#39;, IN_MEMORY =&amp;gt; &#39;false&#39;, BLOCKCACHE =&amp;gt; &#39;true&#39;} 2 row(s) in 0.0300 seconds hbase(main):019:0&amp;gt; alter &#39;student&#39; , NAME =&amp;gt; &#39;score&#39;, VERSIONS =&amp;gt; 3 Updating all regions with the new schema... 1/1 regions updated. Done. 0 row(s) in 1.9230 seconds hbase(main):020:0&amp;gt; describe &#39;student&#39; Table student is ENABLED student COLUMN FAMILIES DESCRIPTION {NAME =&amp;gt; &#39;addr&#39;, DATA_BLOCK_ENCODING =&amp;gt; &#39;NONE&#39;, BLOOMFILTER =&amp;gt; &#39;ROW&#39;, REPLICATION_SCOPE =&amp;gt; &#39;0&#39;, VERSIONS =&amp;gt; &#39;1&#39;, COMPRESSION =&amp;gt; &#39;NONE&#39;, MIN_VERSIONS =&amp;gt; &#39;0&#39;, TTL =&amp;gt; &#39;FOREVER&#39;, KEEP_DELETED_CELLS =&amp;gt; &#39;FALSE&#39;, BLOCKSIZE =&amp;gt; &#39;65536&#39;, IN_MEMORY =&amp;gt; &#39;false&#39;, BLOCKCACHE =&amp;gt; &#39;true&#39;} {NAME =&amp;gt; &#39;score&#39;, DATA_BLOCK_ENCODING =&amp;gt; &#39;NONE&#39;, BLOOMFILTER =&amp;gt; &#39;ROW&#39;, REPLICATION_SCOPE =&amp;gt; &#39;0&#39;, VERSIONS =&amp;gt; &#39;3&#39;, COMPRESSION =&amp;gt; &#39;NONE&#39;, MIN_VERSIONS =&amp;gt; &#39;0&#39;, TTL =&amp;gt; &#39;FOREVER&#39;, KEEP_DELETED_CELLS =&amp;gt; &#39;FALSE&#39;, BLOCKSIZE =&amp;gt; &#39;65536&#39;, IN_MEMORY =&amp;gt; &#39;false&#39;, BLOCKCACHE =&amp;gt; &#39;true&#39;} 2 row(s) in 0.0200 seconds  获取一个单元格更多版本的数据 hbase(main):021:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;score:numb&#39;, &#39;1235&#39; 0 row(s) in 0.0080 seconds hbase(main):022:0&amp;gt; put &#39;student&#39;, &#39;jason&#39;, &#39;score:numb&#39;, &#39;1236&#39; 0 row(s) in 0.0190 seconds hbase(main):023:0&amp;gt; get &#39;student&#39;, &#39;jason&#39;, {COLUMNS=&amp;gt;[&#39;score:numb&#39;]} COLUMN CELL score:numb timestamp=1381748525860, value=1236 1 row(s) in 0.0100 seconds hbase(main):024:0&amp;gt; get &#39;student&#39;, &#39;jason&#39;, {COLUMNS=&amp;gt;[&#39;score:numb&#39;], VERSIONS =&amp;gt; 3} COLUMN CELL score:numb timestamp=1381748525860, value=1236 score:numb timestamp=1381748524871, value=1235 score:numb timestamp=1381757474588, value=5555 3 row(s) in 0.0120 seconds  获取行键大于等于j、小于t的的数据 hbase(main):025:0&amp;gt; scan &#39;student&#39;, { STARTROW =&amp;gt; &#39;j&#39;, STOPROW =&amp;gt; &#39;t&#39;} ROW COLUMN&#43;CELL jason column=addr:city, timestamp=1381757474426, value=chengdu jason column=addr:state, timestamp=1381757474446, value=SiChuan jason column=score:date, timestamp=1381757474194, value=2013-10-01 jason column=score:numb, timestamp=1381748525860, value=1236 nancy column=addr:city, timestamp=1381757474320, value=beijing nancy column=addr:state, timestamp=1381757474607, value=ShanXi nancy column=score:numb, timestamp=1381757474364, value=6666 2 row(s) in 0.0340 seconds hbase(main):026:0&amp;gt; scan &#39;student&#39; ROW COLUMN&#43;CELL alex column=addr:state, timestamp=1381757475608, value=HeNan jason column=addr:city, timestamp=1381757474426, value=chengdu jason column=addr:state, timestamp=1381757474446, value=SiChuan jason column=score:date, timestamp=1381757474194, value=2013-10-01 jason column=score:numb, timestamp=1381748525860, value=1236 nancy column=addr:city, timestamp=1381757474320, value=beijing nancy column=addr:state, timestamp=1381757474607, value=ShanXi nancy column=score:numb, timestamp=1381757474364, value=6666 tina column=addr:city, timestamp=1381757474385, value=tianjin tina column=addr:state, timestamp=1381757474405, value=TianJin 4 row(s) in 0.0330 seconds  删除行键为nancy的addr:city列的数据 MapR发行版还支持删除某一行某一列族的全部数据。
hbase(main):027:0&amp;gt; delete &#39;student&#39;, &#39;nancy&#39;, &#39;addr:city&#39; 0 row(s) in 0.0200 seconds hbase(main):028:0&amp;gt; get &#39;student&#39;, &#39;nancy&#39; COLUMN CELL addr:state timestamp=1381757474607, value=ShanXi score:numb timestamp=1381757474364, value=6666 2 row(s) in 0.0100 seconds  统计表中行数 hbase(main):029:0&amp;gt; count &#39;student&#39; 4 row(s) in 0.0240 seconds =&amp;gt; 4  命名空间练习 hadoop@node50064:~$ hbase shell hbase(main):001:0&amp;gt; list_namespace NAMESPACE default hbase 2 row(s) in 0.2330 seconds hbase(main):002:0&amp;gt; list TABLE customer friends student 3 row(s) in 0.0190 seconds =&amp;gt; [&amp;quot;customer&amp;quot;, &amp;quot;friends&amp;quot;, &amp;quot;student&amp;quot;] hbase(main):003:0&amp;gt; create_namespace &#39;yqu_ns&#39; 0 row(s) in 0.2910 seconds hbase(main):004:0&amp;gt; create &#39;yqu_ns:person&#39;, &#39;d&#39; 0 row(s) in 13.7110 seconds =&amp;gt; Hbase::Table - yqu_ns:person hbase(main):005:0&amp;gt; create &#39;person&#39;, &#39;d&#39; 0 row(s) in 13.6960 seconds =&amp;gt; Hbase::Table - person hbase(main):006:0&amp;gt; list_namespace NAMESPACE default hbase yqu_ns 3 row(s) in 0.0120 seconds hbase(main):007:0&amp;gt; list TABLE customer friends person student yqu_ns:person 5 row(s) in 0.0090 seconds =&amp;gt; [&amp;quot;customer&amp;quot;, &amp;quot;friends&amp;quot;, &amp;quot;person&amp;quot;, &amp;quot;student&amp;quot;, &amp;quot;yqu_ns:person&amp;quot;] hbase(main):008:0&amp;gt; list_namespace_tables &#39;default&#39; TABLE customer friends person student 4 row(s) in 0.0250 seconds hbase(main):009:0&amp;gt; list_namespace_tables &#39;yqu_ns&#39; TABLE person 1 row(s) in 0.0060 seconds hbase(main):010:0&amp;gt; drop_namespace &#39;yqu_ns&#39; ERROR: org.apache.hadoop.hbase.constraint.ConstraintException: Only empty namespaces can be removed. Namespace yqu_ns has 1 tables at org.apache.hadoop.hbase.master.TableNamespaceManager.remove(TableNamespaceManager. at org.apache.hadoop.hbase.master.HMaster.deleteNamespace(HMaster. at org.apache.hadoop.hbase.master.MasterRpcServices.deleteNamespace(MasterRpcServices. at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos. at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer. at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner. at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor. at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor. at Here is some help for this command: Drop the named namespace. The namespace must be empty. hbase(main):011:0&amp;gt; disable &#39;yqu_ns:person&#39; 0 row(s) in 2.3230 seconds hbase(main):012:0&amp;gt; drop &#39;yqu_ns:person&#39; 0 row(s) in 3.5720 seconds hbase(main):013:0&amp;gt; drop_namespace &#39;yqu_ns&#39; 0 row(s) in 0.0200 seconds hbase(main):014:0&amp;gt; list_namespace NAMESPACE default hbase 2 row(s) in 0.0130 seconds hbase(main):015:0&amp;gt;  </content>
    </entry>
    
     <entry>
        <title>[Linux] 判断可执行文件或动态库是否包含符号表</title>
        <url>https://mryqu.github.io/post/linux_%E5%88%A4%E6%96%AD%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%E6%88%96%E5%8A%A8%E6%80%81%E5%BA%93%E6%98%AF%E5%90%A6%E5%8C%85%E5%90%AB%E7%AC%A6%E5%8F%B7%E8%A1%A8/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>linux</tag><tag>debug</tag><tag>symbol</tag><tag>file</tag><tag>objdump</tag>
        </tags>
        <content type="html"> 在Linux下生成一个可执行文件或动态库，可以使用gcc/g&#43;&#43;的&amp;rdquo;-g&amp;rdquo;选项使文件包含调试符号表。 要在Linux下判断一个第三方的可执行文件或动态库是否包含调试符号表，可以通过file命令实现：
srv01&amp;gt; file libcurl.so.6 libcurl.so.6: ELF 64-bit LSB shared object, x86-64, version 1 (FreeBSD), dynamically linked, not stripped srv01&amp;gt; file /usr/bin/X11/curl /usr/bin/X11/curl: ELF 64-bit LSB executable, x86-64, version 1 (FreeBSD), dynamically linked (uses shared libs), for FreeBSD 8.0 (800107), stripped  显示not stripped，表明文件带调试符号表；而显示stripped，表明文件已去除符号表。
如果文件包含调试符号表，可以通过objdump -t命令及选项打印文件的符号表：
srv01&amp;gt; objdump -t libcurl.so.6 libcurl.so.6: file format elf64-x86-64 SYMBOL TABLE: 0000000000000120 l d .hash 0000000000000000 00000000000012b0 l d .dynsym 0000000000000000 0000000000004b08 l d .dynstr 0000000000000000 0000000000006f36 l d .gnu.version 0000000000000000 00000000000073e8 l d .gnu.version_r 0000000000000000 0000000000007418 l d .rela.dyn 0000000000000000 00000000000092a8 l d .rela.plt 0000000000000000 000000000000bfc0 l d .init 0000000000000000 000000000000bfd4 l d .plt 0000000000000000 000000000000de00 l d .text 0000000000000000 0000000000043388 l d .fini 0000000000000000 ........  </content>
    </entry>
    
     <entry>
        <title>[C] GCC对UTF8 BOM的支持</title>
        <url>https://mryqu.github.io/post/c_gcc%E5%AF%B9utf8_bom%E7%9A%84%E6%94%AF%E6%8C%81/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>gcc</tag><tag>utf8</tag><tag>bom</tag><tag>support</tag><tag>compile</tag>
        </tags>
        <content type="html">  最近玩些特俗字符，结果对yqutest.cpp源码文件编译时先碰到error:converting to execution character set: Illegal bytesequence错误。GCC的源码字符集与执行字符集默认是UTF-8编码，为了避免源码文件乱码，最好也是采用UTF-8编码来存储源码文件。将源码编码转成UTF-8，问题得以解决。 但是否需要UTF-8 BOM(byte-order mark)呢？ 我一时兴起添加了BOM，十六进制为EF BB BF，即对应八进制的357 273 277，编译结果如下：
mryqu&amp;gt; g&#43;&#43; yqutest.cpp -o yqutst123 yqutest.cpp:1: error: stray &#39;\357&#39; in program yqutest.cpp:1: error: stray &#39;\273&#39; in program yqutest.cpp:1: error: stray &#39;\277&#39; in program yqutest.cpp:1: error: stray &#39;#&#39; in program yqutest.cpp:1: error: expected constructor, destructor, or type conversion before &#39;&amp;lt;&#39; token mryqu&amp;gt; g&#43;&#43; -v Using built-in specs. Target: amd64-undermydesk-freebsd Configured with: FreeBSD/amd64 system compiler Thread model: posix gcc version 4.2.1 20070719 [FreeBSD]  折腾一下-finput-charset和-fextended-identifiers选项，不管用。
g&#43;&#43; -finput-charset=UTF-8 -fextended-identifiers yqutest.cpp -o yqutst123  后来看到Bug 33415 - Can&amp;rsquo;t compile .cpp file with UTF-8 BOM，才知道是我用的G&#43;&#43;版本太低，起码GCC4.4.0才支持UTF-8 BOM。老老实实去掉BOM就可以编译过了。
参考 [C/C&#43;&#43;] 各种C/C&#43;&#43;编译器对UTF-8源码文件的兼容性测试（VC、GCC、BCB）
Using UTF-8 as the internal representation for strings in C and C&#43;&#43; with Visual Studio
</content>
    </entry>
    
     <entry>
        <title>*nux下导出文件16进制内容的命令xxd</title>
        <url>https://mryqu.github.io/post/linux%E4%B8%8B%E5%AF%BC%E5%87%BA%E6%96%87%E4%BB%B616%E8%BF%9B%E5%88%B6%E5%86%85%E5%AE%B9%E7%9A%84%E5%91%BD%E4%BB%A4xxd/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>linux</tag><tag>hex</tag><tag>bin</tag><tag>dump</tag><tag>xxd</tag>
        </tags>
        <content type="html">  xxd命令可以导出文件的16进制内容，也能将16进制内容转换成2进制，还可以将内容导出成C语言变量，很不错的一个工具！ 参考 xxd(1) - Linux man page
5 Unix Commands I Wish I’d Discovered Years Earlier
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 从静态库获取GCC版本和编译平台</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E4%BB%8E%E9%9D%99%E6%80%81%E5%BA%93%E8%8E%B7%E5%8F%96gcc%E7%89%88%E6%9C%AC%E5%92%8C%E7%BC%96%E8%AF%91%E5%B9%B3%E5%8F%B0/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>cpp</tag><tag>static</tag><tag>library</tag><tag>gcc</tag><tag>platform</tag>
        </tags>
        <content type="html">  获取GCC版本：strings -a {library} | grep &amp;ldquo;GCC: (&amp;rdquo;
mryqu&amp;gt; strings -a libcurl.a | grep &amp;quot;GCC: (&amp;quot; GCC: (GNU) 4.4.5 20110214 (Red Hat 4.4.5-6)  获取编译平台信息：ar -x {library}file *.o
mryqu&amp;gt; ar -x libcurl.a mryqu&amp;gt;file libcurl_la-url.o libcurl_la-url.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped  参考 How to retrieve the GCC version used to compile a given ELF executable?
How to see the compilation platform of a static library file
</content>
    </entry>
    
     <entry>
        <title>用find和grep搜索特定目录下特定文件中的特定关键字</title>
        <url>https://mryqu.github.io/post/%E7%94%A8find%E5%92%8Cgrep%E6%90%9C%E7%B4%A2%E7%89%B9%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%89%B9%E5%AE%9A%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E7%89%B9%E5%AE%9A%E5%85%B3%E9%94%AE%E5%AD%97/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>find</tag><tag>grep</tag><tag>文件</tag><tag>关键字</tag>
        </tags>
        <content type="html"> 下例为递归搜索当前目录下所有以yqu为后缀的文件中的关键字123：
find . -name &amp;quot;*.yqu&amp;quot; | xargs grep -H &amp;quot;123&amp;quot;  </content>
    </entry>
    
     <entry>
        <title>cut命令笔记</title>
        <url>https://mryqu.github.io/post/cut%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>cut</tag><tag>linux</tag><tag>unix</tag>
        </tags>
        <content type="html">  cut命令比较简单，但是也没测试过所有的选项，这里试一下没有细扣过的选项。
命令名 cut -- 对文件每一行的选定部分进行裁剪 概要 cut -b list [-n] [file ...] cut -c list [file ...] cut -f list [-d delim] [-s] [file ...] 描述 cut工具会从每个文件每一行裁剪出选定部分并写入标准输出。如果没有指定file参数，或file参数为单个破折号(&#39;-&#39;)，cut将从标准输入进行读取。list指定可以是列位置或特定字符分隔的字段序号，起始值为1。 list选项参数为逗号或空白字符分隔的数字或数字范围集合。数字范围可由数字&#43;破折号（&#39;-&#39;）&#43;第二个数字组成内包含范围。 数字和数字范围可以重复、重叠，但字段或列如果多次被选中，则仅显示一次。输入中没有选定的字段或列，不会报错。 N- 从第N个开始到所在行结束的所有字节、字符或列 N-M 从第N个开始到第M个之间(包括第M个)的所有字节、字符或列 -M 从第1个开始到第M个之间(包括第M个)的所有字节、字符或列 命令选项如下： -b list list指定字节位置。 -c list list指定字符位置。 -d delim 使用delim而不是制表符作为字段分隔符。 -f list list指定由字段分隔符(见-d选项)对输入分割后的字段。 输出字段由单个字段分隔符分开。 -n 不拆分多字节字符。 仅在多字节字符全部选中的情况下，字符才会被输出。 -s 抑制没有字段分隔符的行。如果没指定该选项的话，没有分隔符的行会原封不动地输出。 环境 环境变量LANG、LC_ALL和LC_CTYPE将影响cut的执行结果。 退出码 cut工具执行成功时返回0，执行出错时返回值大于0。 示例 从系统passwd文件抽取用户登录名和Shell(5)，显示成&#39;&#39;name:shell&#39;&#39;对: cut -d : -f 1,7 /etc/passwd 显示当前登录用户的名称和登录时间: who | cut -c 1-16,26-38  个人体会  cut命令在unix/mac和linux/MinGW上实现并不一样。 cut命令在linux/MinGW上的实现忽略-n选项，此外通过GNU CoreUtils中的cut源码可知-b和-c选项实现是一样的，operating_mode变量都是枚举byte_mode，走的是cut_bytes函数。而unix/mac上的实现通过-n选项可以同时是否输出多字节字符的部分字节码。 cut命令在linux/MinGW上的实现还有&amp;ndash;output-delimiter=STRING选项控制输出字符分隔符。  </content>
    </entry>
    
     <entry>
        <title>grep命令笔记</title>
        <url>https://mryqu.github.io/post/grep%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>grep</tag><tag>linux</tag><tag>unix</tag>
        </tags>
        <content type="html">  使用grep时一直没有使用什么命令选项，这里过一遍grep帮助，试一遍不明白的选项。
grep简介 grep是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。
grep命令帮助 示例: grep -i &#39;hello world&#39; menu.h main.c 正则选择和解释: -E, --extended-regexp PATTERN为扩展正则表达式（ERE） -F, --fixed-strings PATTERN是一套新行分割字符串 -G, --basic-regexp PATTERN为基本正则表达式（BRE） -P, --perl-regexp PATTERN为Perl正则表达式 -e, --regexp=PATTERN 使用PATTERN作为正则表达式 -f, --file=FILE 从文件中获得PATTERN -i, --ignore-case 忽略大小写 -w, --word-regexp 强制PATTERN仅匹配整个单词 -x, --line-regexp 强制PATTERN仅匹配整行 -z, --null-data 数据行以0字节而不是新行截至（打印整个文件了） 杂项: -s, --no-messages 抑制错误消息 -v, --invert-match 选择非匹配行 -V, --version 显示版本信息并退出 --help 显示该帮助并退出 -J, --bz2decompress 在进行搜索前解压缩bzip压缩输入 -Z, --decompress 在进行搜索前解压缩输入(HAVE_LIBZ=1) --mmap 如可能使用内存映射输入 输出控制: -m, --max-count=NUM 在NUM个匹配后停止工作 -b, --byte-offset 在输出行显示字节偏移 -n, --line-number 显示行号 --line-buffered 对每行都清除缓存强制输出 -H, --with-filename 对每个匹配结果显示文件名 -h, --no-filename 抑制输出中前缀的文件名 --label=LABEL 将LABEL作为标准输入的文件名显示（输入为管道有用） -o, --only-matching 仅显示行匹配PATTERN部分 -q, --quiet, --silent 抑制所有正常输出。通常用于脚本条件语句中，判断匹配结果是1还是0 --binary-files=TYPE 将二进制文件假定为类型，分别为&#39;binary&#39;、&#39;text&#39;或 &#39;without-match&#39;。默认为&#39;binary&#39;，对二进制文件进行搜索 而不显示；&#39;text&#39;，一概视为文本文件，搜索并显示； &#39;without-match&#39;，对二进制文件直接忽略，不搜索不显示。 -a, --text 等同于--binary-files=text -I 等同于--binary-files=without-match -d, --directories=ACTION 如何处理目录的操作项，分别为&#39;read&#39;、&#39;recurse&#39;或&#39;skip&#39;。 -D, --devices=ACTION 如何处理设备、FIFO和socket的操作项，分别为&#39;read&#39;或&#39;skip&#39;。 -R, -r, --recursive 等同于--directories=recurse --include=PATTERN 匹配PATTERN的文件将被检查 --exclude=PATTERN 匹配PATTERN的文件将被忽略 --exclude-from=FILE 匹配模式文件中PATTERN的文件将被忽略 -L, --files-without-match 仅显示不包含匹配的文件名 -l, --files-with-matches 仅显示包含匹配的文件名 -c, --count 仅显示每个文件匹配行数 --null 在文件名后显示0字节（跟不带这个选项就少一个冒号？） 上下文控制: -B, --before-context=NUM 输出匹配结果及其前NUM行 -A, --after-context=NUM 输出匹配结果及其后NUM行 -C, --context=NUM 输出匹配结果及其前后各NUM行 -NUM 等同--context=NUM --color[=WHEN], --colour[=WHEN] 使用标记突显匹配字符串，WHEN可为&#39;always&#39;、&#39;never&#39;或&#39;auto&#39; 默认项为&#39;never&#39;，&#39;always&#39;总是使用标记，而&#39;auto&#39;仅输出 在没有被管道到其他命令或重定向到文件时才使用标记。 -U, --binary 在行结尾EOL不除去回车换行CR字符(MSDOS) -u, --unix-byte-offsets 如果没有回车换行则报告字节偏移(MSDOS)  </content>
    </entry>
    
     <entry>
        <title>Xterm DISPLAY变量</title>
        <url>https://mryqu.github.io/post/xterm_display%E5%8F%98%E9%87%8F/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>xterm</tag><tag>display</tag>
        </tags>
        <content type="html"> DISPLAY变量的格式为[host]:&amp;lt;display&amp;gt;[.screen] host 指网络主机名，空缺则指本机。 每个主机可以有多个显示，每个显示可以有多个屏幕。因此，DISPLAY=:0通常指本机内的所有GPU，DISPLAY=:0.0指本机内的第一个配置屏幕/GPU，DISPLAY=:0.1指本机内的第二个配置屏幕/GPU。
</content>
    </entry>
    
     <entry>
        <title>Xming使用笔记</title>
        <url>https://mryqu.github.io/post/xming%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>xming</tag>
        </tags>
        <content type="html"> XWindow系统里有一个统一的Server来负责各个程序与显示器、键盘和鼠标等输入输出设备的交互，每个有GUI的应用程序都通过网络协议与Server进行交互。所以对于任何一个应用程序，本地运行和远程运行的差别仅仅是XServer的地址不同，别的没有差别。所以在Windows运行一个XServer，就可以很方便的远程运行有GUI的Linux应用了。 Xming是一个在Microsoft Windows操作系统上运行XWindow系统的服务器。它非常简单易用，搭配强大。一般搭配putty的下列配置，就可以在Windows平台上远程运行有GUI的Linux应用了：
Connection-SSH-X11-Enable X11 forwarding  Xming操作笔记如下： - 选择单词：双击鼠标左键 - 选择一行：三击鼠标左键 - 选择特定文本：鼠标左键点击文本起点，然后按住鼠标右键选择。 - 复制文本：Alt &#43; Shift &#43; C
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] hadoop job -list已废弃</title>
        <url>https://mryqu.github.io/post/hadoop_hadoop_job_-list%E5%B7%B2%E5%BA%9F%E5%BC%83/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>job</tag><tag>deprecated</tag><tag>mapred</tag><tag>yarn</tag>
        </tags>
        <content type="html"> 执行hadoop job -list，显示该命令已废弃，不过还能执行成功。
~$ hadoop job -list DEPRECATED: Use of this script to execute mapred command is deprecated. Instead use the mapred command for it.  看一下Hadoop 2.2.0的代码hadoop-common-project/hadoop-common/src/main/bin/hadoop：
#hdfs commands namenode|secondarynamenode|datanode|dfs|dfsadmin|fsck|balancer|fetchdt|oiv|dfsgroups|portmap|nfs3) echo &amp;quot;DEPRECATED: Use of this script to execute hdfs command is deprecated.&amp;quot; 1&amp;gt;&amp;amp;2 echo &amp;quot;Instead use the hdfs command for it.&amp;quot; 1&amp;gt;&amp;amp;2 echo &amp;quot;&amp;quot; 1&amp;gt;&amp;amp;2 #try to locate hdfs and if present, delegate to it. shift if [ -f &amp;quot;${HADOOP_HDFS_HOME}&amp;quot;/bin/hdfs ]; then exec &amp;quot;${HADOOP_HDFS_HOME}&amp;quot;/bin/hdfs ${COMMAND/dfsgroups/groups} &amp;quot;$@&amp;quot; elif [ -f &amp;quot;${HADOOP_PREFIX}&amp;quot;/bin/hdfs ]; then exec &amp;quot;${HADOOP_PREFIX}&amp;quot;/bin/hdfs ${COMMAND/dfsgroups/groups} &amp;quot;$@&amp;quot; else echo &amp;quot;HADOOP_HDFS_HOME not found!&amp;quot; exit 1 fi ;; #mapred commands for backwards compatibility pipes|job|queue|mrgroups|mradmin|jobtracker|tasktracker) echo &amp;quot;DEPRECATED: Use of this script to execute mapred command is deprecated.&amp;quot; 1&amp;gt;&amp;amp;2 echo &amp;quot;Instead use the mapred command for it.&amp;quot; 1&amp;gt;&amp;amp;2 echo &amp;quot;&amp;quot; 1&amp;gt;&amp;amp;2 #try to locate mapred and if present, delegate to it. shift if [ -f &amp;quot;${HADOOP_MAPRED_HOME}&amp;quot;/bin/mapred ]; then exec &amp;quot;${HADOOP_MAPRED_HOME}&amp;quot;/bin/mapred ${COMMAND/mrgroups/groups} &amp;quot;$@&amp;quot; elif [ -f &amp;quot;${HADOOP_PREFIX}&amp;quot;/bin/mapred ]; then exec &amp;quot;${HADOOP_PREFIX}&amp;quot;/bin/mapred ${COMMAND/mrgroups/groups} &amp;quot;$@&amp;quot; else echo &amp;quot;HADOOP_MAPRED_HOME not found!&amp;quot; exit 1 fi ;;  hadoop Shell命令下的命令列表已经分割成几块，划给其他Shell命令了： - Hadoop通用：hadoop Shell命令 - HDFS相关：hdfs Shell命令 - MapReduce相关：mapred Shell命令 - YARN相关：yarn Shell命令
</content>
    </entry>
    
     <entry>
        <title>[C] Exec format error</title>
        <url>https://mryqu.github.io/post/c_exec_format_error/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C</tag><tag>gcc</tag><tag>-c</tag><tag>executable</tag><tag>format</tag>
        </tags>
        <content type="html"> 很久没用g&#43;&#43;了，结果编个小程序还出错。
mryqu:~/ctest$ g&#43;&#43; -g -c wvc.cpp -o wvc mryqu:~/ctest$ chmod a&#43;x wvc mryqu:~/ctest$ ./wvc -bash: ./wvc: cannot execute binary file: Exec format error mryqu:~/ctest$ file wvc wvc: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped  查了查gcc的帮助，才发现用了-c选项后其实是只编译不链接的：
-c Compile or assemble the source files, but do not link. The linking stage simply is not done. The ultimate output is in the form of an object file for each source file. By default, the object file name for a source file is made by replacing the suffix .c, .i, .s, etc., with .o. Unrecognized input files, not requiring compilation or assembly, are ignored.  去掉-c选项后一切OK！
mryqu:~/ctest$ g&#43;&#43; -g wvc.cpp -o wvc mryqu:~/ctest$ file wvc wvc: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.24, BuildID[sha1]=XXXX, not stripped mryqu:~/ctest$ chmod a&#43;x wvc  </content>
    </entry>
    
     <entry>
        <title>批处理：搜索Jar包中的类文件</title>
        <url>https://mryqu.github.io/post/%E6%89%B9%E5%A4%84%E7%90%86%E6%90%9C%E7%B4%A2jar%E5%8C%85%E4%B8%AD%E7%9A%84%E7%B1%BB%E6%96%87%E4%BB%B6/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>批处理</tag><tag>jar</tag><tag>class</tag><tag>windows</tag><tag>搜索</tag>
        </tags>
        <content type="html"> 网上转载的很多，原文出处不详。
批处理文件利用7z和findstr两个命令搜索当前目录下所有Jar包，分析是否有所要查找的类文件：
@echo off SETLOCAL set WHICH_CLASS=%1 echo WHICH_CLASS=%WHICH_CLASS% for /F %%i in (&#39;dir /A:-D /S /B *.jar&#39;) do 7z l %%i | findstr %WHICH_CLASS% &amp;amp;&amp;amp; echo %WHICH_CLASS% found in: &amp;quot;%%i&amp;quot; echo &amp;quot;Finished class finding...&amp;quot; echo &amp;quot;======================================&amp;quot; ENDLOCAL  命令运行格式：
findclass com\\yqu\\kxmt\\TestFindClass.class  </content>
    </entry>
    
     <entry>
        <title>Ubuntu下显示本机IP</title>
        <url>https://mryqu.github.io/post/ubuntu%E4%B8%8B%E6%98%BE%E7%A4%BA%E6%9C%AC%E6%9C%BAip/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>ubuntu</tag><tag>linux</tag><tag>show</tag><tag>ipaddress</tag>
        </tags>
        <content type="html"> 命令为：ip addr show 感觉比ifconfig eth0更通用些！
</content>
    </entry>
    
     <entry>
        <title>[C] #和##宏操作符</title>
        <url>https://mryqu.github.io/post/c_%E5%AE%8F%E6%93%8D%E4%BD%9C%E7%AC%A6/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C</tag><tag>macro</tag><tag>token</tag>
        </tags>
        <content type="html">  在看# and ## in macros之前觉得对#和##宏操作符挺明白的，看了之后才感觉需要重新学习一下。
#define f(a,b) a##b #define g(a) #a #define h(a) g(a) int main() { printf(&amp;quot;%s\n&amp;quot;,h(f(1,2))); printf(&amp;quot;%s\n&amp;quot;,g(f(1,2))); return 0; }  如果你能确保自己能写出正确答案的话，那么你可以略过这篇帖子。 C/C&#43;&#43;语言中对宏的处理属于编译器预处理的范畴，属于编译期概念而非运行期概念。其中#操作符用于对指定的宏参数进行字符串化，而##操作符用来将两个符号连接为一个符号。
struct command { char *name; void (*function) (void); };   ``` #define COMMAND(NAME) \ { #NAME, NAME ## _command } struct command commands[] = { COMMAND (quit), COMMAND (help), … }; ``` 等同 ``` struct command commands[] = { { &#34;quit&#34;, quit_command }, { &#34;help&#34;, help_command }, … }; ```  但是这里还有一个参数预扫描问题。即如果参数不被字符串化或者用于符号连接的情况下，才会在宏定义体内被替换之前进行展开。
#define xstr(s) str(s) #define str(s) #s #define foo 4 str (foo) → &amp;quot;foo&amp;quot; //由于str中对s参数进行字符串化，所以foo不会展开 xstr (foo) → xstr (4) //由于xstr中对foo施加#和##操作符，因此foo先展开为4 → str (4) → &amp;quot;4&amp;quot;  通过gcc -E可以查看预处理结果、更好地理解宏是如何被评估（evaluate）的。 最后必须说明的是，在使用C&#43;&#43;开发时应尽可能地避免使用宏。
参考 The C Preprocessor - Macro
Macro Parameter Stringizing
Macro Token Concatenation
Macro Argument Prescan
</content>
    </entry>
    
     <entry>
        <title>在Vi中搜索多字节unicode字符</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8vi%E4%B8%AD%E6%90%9C%E7%B4%A2%E5%A4%9A%E5%AD%97%E8%8A%82unicode%E5%AD%97%E7%AC%A6/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>vi</tag><tag>unicode</tag><tag>hex</tag><tag>decimal</tag><tag>octal</tag>
        </tags>
        <content type="html"> 使用Vi编辑unicode字符文本文件时，可以通过下列方式搜索和替换字符：
\%d 匹配特定十进制字符 (例如 \%d 123) \%x 匹配特定十六进制字符 (例如 \%x2a) \%o 匹配特定八进制字符 (例如 \%o040) \%u 匹配特定多字节字符 (例如 \%u20ac) \%U 匹配特定大的多字节字符(例如 \%U12345678)  为了在文本中查看任何字符的unicode或十六机制格式内容，将光标置于该字符上之后输入ga 命令。这会以十进制、十六机制和八进制显示显示字符值： </content>
    </entry>
    
     <entry>
        <title>host笔记</title>
        <url>https://mryqu.github.io/post/host%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>NetWork</category>
        </categories>
        <tags>
          <tag>dns</tag><tag>host</tag>
        </tags>
        <content type="html">  host命令是常用的分析域名查询工具，可以用来测试域名系统工作是否正常。
host参数 -a等价于使用“-v -t ANY”-c Class当搜索非网际数据时要指定要查找的类。有效类为:
IN网际类CHAOSChaos类HESIODMIT AlthenaHesiod类ANY通配符-C在需要认证的域名服务器上查找SOA记录-d等价于使用“-v”-l列出一个域内所有的主机-i反向查找-N改变点数-r禁用递归处理。-R指定UDP包数-t Type指定要查询的记录类型。常用类型为:
A主机的网际地址CNAME别名的规范名称HINFO主机CPU与操作系统类型KEY安全密钥记录MINFO邮箱或邮件列表信息MX邮件交换器NS指定范围的名称服务器PTR如果查询的是一个网际地址则为主机名；否则，为其他信息的指针SIG签名记录SOA域的&#34;授权开始&#34;信息TXT文本信息UINFO用户信息WKS所支持的众所周知的服务。-T支持TCP/IP模式-v运行时显示详细的处理信息-w永远等待DNS服务器的一个回答。-W Waite指定等待回复的时间-4用于IPv4的查询-6用于IPv6的查询-m设置内存调试标志 host使用 </content>
    </entry>
    
     <entry>
        <title>清除Chrome中特定网站的所有历史记录</title>
        <url>https://mryqu.github.io/post/%E6%B8%85%E9%99%A4chrome%E4%B8%AD%E7%89%B9%E5%AE%9A%E7%BD%91%E7%AB%99%E7%9A%84%E6%89%80%E6%9C%89%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>chrome</tag><tag>site</tag><tag>history</tag><tag>clear</tag>
        </tags>
        <content type="html"> 想要Chrome中特定网站的所有历史记录，可是搜出特定网站的所有历史记录后必须对每一项进行选择，然后才能删除。终于在网上搜出一个妙招来： - 进入Chrome历史记录页面； - 搜索想要删除的URL； - 为了删除所搜结果的所有记录，将第一个待删项的checkbox勾选上，接着滚动页面到最后一个待删项，按住SHIFT键将最后一个待删项的checkbox勾选上，这样所有待删项都会被勾选上； - 点击&amp;rdquo;Remove selected items&amp;rdquo;按钮即可。
</content>
    </entry>
    
     <entry>
        <title>Cscope笔记</title>
        <url>https://mryqu.github.io/post/cscope%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>cscope</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html">  Cscope简介 Cscope是一个类似Ctags的工具，但功能比ctags强大很多。Cscope是一款自带一个基于文本的用户界面的源代码浏览工具，尽管它最初是为C代码的搜索（包括lex、yacc文件）设计的，但是也可以用于对C&#43;&#43;和Java代码的搜索。用Cscope你可以轻易地搜索到你的标识符是在哪里被定义和使用的，它可以轻而易举地解决以下问题： - 这个变量在哪里被使用？ - 这个预处理符号的值是什么？ - 这个函数都在哪些源代码文件中出现过？ - 都有哪些函数调用了这个函数？ - &amp;ldquo;out of space&amp;rdquo;的消息是从哪里来的？ - 这个源文件在在目录结构中的位置？ - 都有哪些源文件包含了这个头文件？
Cscope是由Santa Cruz Operation, Inc发布的，它遵循BSD开源协议。
安装 Cscope项目仅提供源代码，不提供二进制文件。cscope-win32项目提供了使用MinGW、MSYS和Cygwin编译Windows平台Cscope的方法，此外也提供编译好好的csope.exe文件。下载cscope-15.8a-win64rev1-static.zip，将其中的cscope.exe解压缩到系统环境变量path包含的路径即可。 使用 创建符号数据库 Cscope在第一次被使用在指定的源文件时会建立一个符号的数据库。接下来调用时，Cscope仅仅重建那些被改动或者和新文件相关的数据库。那些没有被改动的文件相关的数据库会被直接复制使用。这使得重建数据库要比第一次运行快许多。 Cscope命令的参数如下： - -R: 在生成索引文件时，搜索子目录树中的代码 - -b: 只生成索引文件，不进入cscope的界面 - -q:生成cscope.in.out和cscope.po.out文件，加快cscope的索引速度 - -k: 在生成索引文件时，不搜索/usr/include目录 - -i:如果保存文件列表的文件名不是cscope.files时，需要加此选项告诉cscope到哪儿去找源文件列表。可以使用”-“，表示由标准输入获得文件列表。 - -Idir:在-I选项指出的目录中查找头文件 - -u: 扫描所有文件，重新生成交叉索引文件 - -C: 在搜索时忽略大小写 - -Ppath:在以相对路径表示的文件前加上的path，这样，你不用切换到你数据库文件所在的目录也可以使用它了。 我针对curl项目执行cscope -Rkq ，这样会启动Cscope的文本用户界面，之后我搜索set_binmode函数： 搜索符号 在Cscope的文本界面里可以在命令模式执行:cs find或:cs f命令搜索符号，其参数为： - s: 查找C语言符号，即查找函数名、宏、枚举值等出现的地方 - g: 查找函数、宏、枚举等定义的位置，类似ctags所提供的功能 - d: 查找本函数调用的函数 - c: 查找调用本函数的函数 - t: 查找指定的字符串 - e: 查找egrep模式，相当于egrep功能，但查找速度快多了 - f: 查找并打开文件，类似vim的find功能 - i: 查找包含本文件的文件
下面示例为用
:cs f c set_binmode  命令搜索set_binmode函数的调用者：E567: no cscope connections错误处理 有个帖子vim cannot connect to cscope database讲过解决方案，但是在我的Windows平台cscope文本界面不起作用。通过:cs add {prj path}/cscope.out添加符号数据库可在一定程度避免这一问题。
参考 Cscope网站
SourceForge：Cscope项目
Cscope手册
Cscope-win32下载
</content>
    </entry>
    
     <entry>
        <title>Exuberant Ctags笔记</title>
        <url>https://mryqu.github.io/post/exuberant_ctags%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>exuberant</tag><tag>ctags</tag><tag>C&#43;&#43;</tag>
        </tags>
        <content type="html">  Ctags简介 Ctags（Generate tag files for sourcecode）产生标记(/索引)文件以帮助在源文件中定位对象。Ctags最初支持C语言，现在已经支持C/C&#43;&#43;/Java/JS/Python等41种语言。Vim/Emacs/SublimeText/UltraEdit等编辑器或工具都支持Ctags生成的标记文件。 对于C/C&#43;&#43;语言来说，其生成的标记文件tags中包括这些对象的列表： - 用#define定义的宏 - 枚举型变量的值 - 函数的定义、原型和声明 - 名字空间（namespace） - 类型定义（typedefs） - 变量（包括定义和声明） - 类（class）、结构（struct）、枚举类型（enum）和联合（union） - 类、结构和联合中成员变量或函数
安装 下载ctags58.zip，将其中的ctags.exe解压缩到系统环境变量path包含的路径即可。 使用选项 如果没有指定−−language−force选项，每个源文件的语言基于文件名和语言的映射进行自动选择。该映射可用−−list−maps选项显示，它可能会被−−langmap选项改变。对于操作系统所支持的文件，如果文件名无法映射到某种语言且该文件可被执行，则会对文件第一行检查是为&amp;rdquo;#!&amp;ldquo;公认的语言脚本。默认情况下，所有其他文件名都会被忽略。由于仅文件名可匹配某种语言的文件会被扫描，这使得在单个目录对所有文件(例如&amp;rdquo;ctags*&amp;ldquo;)或对目录树的所有文件(例如&amp;rdquo;ctags −R&amp;rdquo;)执行ctags成为可能。.h扩展名即用于C&#43;&#43;也用于C，所以Ctags将.h映射为C&#43;&#43;，这样做不会有不良后果。
 -R：等同于&amp;ndash;recurse，递归子目录遍历 -L：从文件读取Ctags待处理文件列表并对其执行ctags  find . -name &amp;quot;*.h&amp;quot; -o -name &amp;quot;*.c&amp;quot; -o -name &amp;quot;*.cpp&amp;quot; -o -name &amp;quot;*.bld&amp;quot; -o -name &amp;quot;*.blt&amp;quot; &amp;gt; prj.files ctags -L prj.files  --list−maps：显示文件名和语言的映射  --list−languanges：显示所有支持的语言  --langmap：设置文件名和语言的映射 如果程序中有的.c文件其实是C&#43;&#43;程序，这该怎么办？答案是使用ctags &amp;ndash;langmap=c&#43;&#43;:&#43;.c。 −−language−force：强制使用特定语言，而不是通过文件名和语言的映射进行自动选择 像C&#43;&#43;标准库stl中文件名没有后缀，怎么办？ 使用ctags−−language−force=C&#43;&#43;这样就把所有文件当成C&#43;&#43;来处理了。 −−fields：指定标记文件中条目的可用扩展字段（没有指明的默认关闭） a
类成员的访问属性
f
文件限制范围 [enabled]
i
继承信息
k
单字符形式的标记类型 [enabled]
K
全名称形式的标记类型
l
包含该标记的源文件语言
m
实现信息
n
标记定义行号
s
标记定义范围 [enabled]
S
函数签名（例如原型或参数类表）
z
包含kind字段中包含&amp;rdquo;kind:&amp;ldquo;的关键字
t
变量的类型和名称，或typedef的&amp;rdquo;typeref:&amp;ldquo;字段。 [enabled]
 --{language}-kinds：指定输出中需包含的特定语言标记列表 使用ctags &amp;ndash;list-kinds=c&#43;&#43;可以查看选项： c
classes
d
macro definitions
e
enumerators (values inside an enumeration)
f
function definitions
g
enumeration names
l
local variables [off]
m
class, struct, and union members
n
namespaces
p
function prototypes [off]
s
structure names
t
typedefs
u
union names
v
variable definitions
x
external and forward variable declarations[off]
 --extra：用于增加额外的标记条目 f
为每个源文件包含一个用于基本文件名(除去路径的文件名，例如&amp;rdquo;example.c&amp;rdquo;)的条目，用于定位文件首行。
q
（对C&#43;&#43;、Eiffel和Java这些支持全类名语言）为类成员标记包含一个额外的全类名条目，其形式取决于特定语言。对于C&#43;&#43;，形式为&amp;rdquo;class::member&amp;rdquo;；对于Eiffel和Java，形式为&amp;rdquo;class.member&amp;rdquo;。
  OmniCppComplete 提供的ctags生成语句示例：
ctags -R –c&#43;&#43;-kinds=&#43;px –fields=&#43;iaS –extra=&#43;q .  通过Vim使用标记文件 |命令|介绍 |&amp;mdash; |vi -t tag|启动Vim并定位到标记所定义的文件和代码行上。 |:ta tag|查找一个标记。 |Ctrl-]|跳到光标所在函数或者结构体的定义处。 |Ctrl-T|返回到跳转标记之前的位置。 |:ts|:tselect, 列出一个列表供用户选标记。 |:tp|:tprevious，上一个tag标记文件。 |:tn|:tnext，下一个tag标记文件。 |:tfirst|第一个tag标记文件。 |:tlast|最后一个tag标记文件。
Ctags局限性 Ctags生成的标记文件包含了类、函数和变量等的定义信息，而没有包含使用信息。如果要知道一个函数都在什么地方使用过，需要使用cscope工具。
参考 Exuberant Ctags网站
SourceForge：Exuberant Ctags项目
Ctags手册
ctags 小记
</content>
    </entry>
    
     <entry>
        <title>REST的Richardson成熟度模型</title>
        <url>https://mryqu.github.io/post/rest%E7%9A%84richardson%E6%88%90%E7%86%9F%E5%BA%A6%E6%A8%A1%E5%9E%8B/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>rest</tag><tag>restful</tag><tag>web服务</tag><tag>成熟度</tag><tag>模型</tag>
        </tags>
        <content type="html">  一个web服务有多么的&amp;rdquo;restful&amp;rdquo;，最有名的就是《RESTful Web Services》的合著者Leonard Richardson提出的REST 成熟度模型，简称Richardson成熟度模型。 - 第0级：使用HTTP作为传输方式；一个URI，一个HTTP方法。SOAP、XML-RPM都属于这一级别，仅是来回传送&amp;rdquo;Plain OldXML&amp;rdquo;(POX)。即使没有显式调用RPC接口（SOAP、XML-RPM），通常会调用服务器端的一个处理过程。一个接口会有一个端点，文档的内容会被解析用还判断所要调用的处理过程及其参数。这种做法相当于把HTTP 这个应用层协议降级为传输层协议用。HTTP 头和有效载荷是完全隔离的，HTTP头只用于保证传输，不涉及业务逻辑；有效载荷包含全部业务逻辑，因此 API 可以无视 HTTP 头中的任何信息。 - 第1级：引入了资源的概念，每个资源有对应的标识符和表达；多个URI，一个HTTP方法。这些资源仍是被&amp;rdquo;GETful&amp;rdquo;接口操作而不是HTTP动词，但服务基本上提供和操作资源。例如： - GET http://example.com/app/createUser - GET http://example.com/app/getUser?id=123 - GEThttp://example.com/app/changeUser?id=123&amp;amp;field=value - GET http://example.com/app/deleteUser?id=123 - 第2级：根据语义使用HTTP动词，适当处理HTTP响应状态码；多个URI，多个HTTP方法。 - GET用于查询资源； - HEAD用于查询资源是否存在； - POST创建新资源； - PUT更新已存在的资源； - PATCH部分更新已存在的资源； - DELETE删除已存在的资源。在这一级别，资源名称为基URI的一部分，而不是查询参数。 - 第3级：使用超媒体作为应用状态引擎（HATEOAS）；多个URI，多个HTTP方法。在资源的表达中包含了链接信息。客户端可以根据链接来发现可以执行的动作。链接推荐使用ATOM (RFC4287)中的显式语义。
当然围绕这一模型，争论很多，Martin Fowler、Rest之父Roy Fielding、《RESTful WebServices Cookbook》作者Subbu Allamaraju都有不同的见解。
参考 Leonard Richardson：REST 成熟度模型
Martin Fowler：Richardson Maturity Model
Roy Fielding：REST APIs must be hypertext-driven
Subbu Allamaraju：Measuring REST
如何度量应用的RESTful成熟度？
</content>
    </entry>
    
     <entry>
        <title>RESTful Web Services Cookbook笔记（三）</title>
        <url>https://mryqu.github.io/post/restful_web_services_cookbook%E7%AC%94%E8%AE%B0%E4%B8%89/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>rest</tag><tag>最佳实践手册</tag><tag>webservice</tag><tag>http</tag>
        </tags>
        <content type="html">  杂项 为了复制资源而不泄漏服务器实现细节，可以设计一个用于复制的控制器资源。客户端向控制器发送POST请求复制资源。为了实现条件POST，可以提供一次性URI。控制器创建副本后，返回状态码201 (Created)及Location头带有副本URI。 为了合并两个或多个资源，可以设计一个用于合并的应用程序特定控制器资源。客户端想控制器发送GET请求，其查询参数包含待合并资源的URI或标识符。服务器返回Last-Modified、ETag头和包含待合并资源摘要的表述体。ETag由时戳和随机数连接串构成。为了验证摘要，客户端向同一地址发送带有If-Unmodified-Since和If-Match头的POST请求发起合并。服务器合并后在事务日志中保留If-Match头的值并返回状态码201 (Created)及含有合并后资源URI的Location头。如果客户端再发送相同If-Match头的POST请求，服务器返回状态码412 (Preconditon Failed)。 为了移动资源，服务器会提供负责移动资源控制器的链接或链接模板以使客户端可以发送POST请求，并在完成请求后根据输出返回状态码201 (Created)或303 (See Other)。 WebDAV （RFC 4918）是用于资源分布式创作和版本管理的HTTP扩展，它扩展了一些HTTP方法和头用于管理文件和文档。当Web服务是内容创作应用且服务器支持WebDAV时使用WebDAV特定方法，避免对其他类型应用使用WebDAV。
|方法|介绍 |&amp;mdash;&amp;ndash; |PROPFIND|WebDAV中的文档具有属性，客户端可用此方法获得属性 |PROPPATCH|客户端用此方法设置、添加或修改资源属性 |MKCOL|WebDAV可以将文档放入集合（文件夹），客户端可用此方法创建集合 |COPY|客户端可用此方法复制资源 |MOVE|客户端可用此方法移动资源 |LOCK|客户端可用此方法对给定文档加锁，以支持悲观并发控制 |UNLOCK|客户端可用此方法对给定文档去锁
为了支持跨服务器边界的操作（例如，将用户配置从一个应用移植到另一个应用，将文档从草稿服务器发布到生产服务器），需要服务器之间彼此就数据格式、后台接口、并发控制、数据加载、范式化和存储等方面协作、设计和实现设计跨服务器操作。 wiki的网页都会维护当前和过去的修订历史，以便客户可以获取、比较和评估页面改变。为了支持资源以往历史快照，服务器在收到客户端PUT请求更新资源时，在更新资源之前会默认创建快照（资源副本），并在更新后的资源表述中包含快照链接，快照表述中包含更新后资源链接。当用户发送DELETE请求，删除资源及所有快照。 提供用于撤销操作的控制器资源。当客户端发送POST请求进行撤销操作，在事务日志中记录资源当前状态以用于审计。服务器将资源状态恢复到上一快照并将客户端重定向到资源URI。 当资源很大而改动很小时，发送GET请求获取整个表述、进行小的修改、发送PUT请求将整个表述传回服务器进行更新很费时费带宽。为了支持对资源进行部分更新，可以将可修改的资源部分封装为一个新资源。客户端通过PUT请求更新该新资源，等效于部分更新原来的资源。 HTTP PUT方法用于对资源的整个更新或替换，PATCH方法（RFC 5789）用于支持部分更新。PATCH方法不是安全和幂等的，请求体是一系列对资源进行改变的表述。当收到请求，服务器将整个补丁原子性地施加于资源，并返回响应码 200 (OK)或204 (No Content)。如果服务器无法将整体补丁施加于资源，就不会做任何局部修改。可以通过请求中包含If-Unmodified-Since和/或If-Match头支持条件PATCH请求，如果不匹配则返回状态码412 (Precondition Failed)。建议在OPTIONS响应的Allow头支持PATCH，并在PTACH方法包含Accept-Patch头，其值为支持的媒体类型。 当客户端需要为不同资源提交若干类似请求时，只要对每个资源的操作是相同的且资源是类似的，可以将这些操作组合成一个针对集合资源的单个操作。使用POST请求和集合资源一次性批量创建若干资源。服务器为集合资源分配一个URI，并使用状态码303 (See Other)重定向到该集合资源，集合资源表述包含所有新创建资源的链接。使用PUT请求更新或DELETE请求删除若干资源与创建过程类似，以上操作必须是原子化的。 客户端需要执行批量作业的用例不是少数。例如，为前一天销售订单做汇总、将一个或多个文档打包、批准选择的购买订单集合等等都需要批量执行。服务器需要设计一个控制器资源用于执行批量操作。如果客户端需要跟踪操作或客户端需要提交大量用于操作的数据，返回状态码202 (Accepted)以进行异步操作，否则返回200 (OK)或204 (No Content)。 将几个HTTP组合成一个HTTP请求以支持批量处理的用例不是少数。下面列举了一些通常使用的技术实现： - 客户端将几个HTTP请求序列化到一个JSON对象、或一个XML文档、或multipart/mixed消息的一部分。 - 客户端创建一个信封跟是将多个请求组合进入一个消息。 - 客户端向服务器的分批终点（batch end point）资源发送POST请求。 - 服务器接收到消息，打开信封，重构多个HTTP请求并分发到服务器的相关URI。或者服务器绕过HTTP将请求直接派发到能处理这种请求的代码。 - 服务器收集每个请求的响应并序列化为一个消息返回到客户端。 - 客户端打开信封并处理每个响应消息。
避免这种将多个HTTP请求封装入一个POST请求隧道的做法。因为通常隧道方案有以下不利之处：
|特性|介绍 |&amp;mdash;&amp;ndash; |并发|HTTP通过Last-Modified和ETag头来实现乐观并发检查。将多个HTTP请求封装进一个HTTP请求隧道的批量操作使并发检查变得困难，因为服务器需要为批量操作中每一个任务进行并发检查。 |原子性|HTTP请求是原子性的。每个请求执行单个任务，服务器在错误发生时确保数据的原子性和一致性。将多个任务混入一个请求、尤其是某些操作依赖于同一请求的前一操作是否成功的批量操作使Web服务很难确保原子性和进行错误恢复。 |可见性|将多个操作封装到一个HTTP请求隧道使中间节点无法对批量处理内的操作响应可见。此外检测请求防止拒绝攻击的典型安全方法几乎不可能捕捉到批量操作中的可疑请求，因此可能导致拒绝服务攻击。 |错误处理|用于批量操作的错误处理和报告更为复杂。单个批量请求的结构可能混杂成功和失败响应。 |可扩展性|一般用于批量操作的理由依赖批量处理比执行每个单个请求更可扩展这样的假设。当单个服务器收到非常多的批量处理，请求会降低服务器的响应能力。发送很多批量客户端处理到单个服务器的应用比不支持批量处理的相同应用性能可能更低。
分析导致促成使用隧道技术的用例，设计应用特有的控制器资源支持相同需求。由于请求使用的是处理请求资源的单个URI，所以请求可见。由于仅返回一个状态码，所以响应可见。 RESTful web服务在下列场景可能会需要处理事务： - 客户端执行操作流的一些列步骤。客户端在取消操作流时要撤销所有已完成的数据变动。 - 客户端同若干服务器顺序交互以实现应用操作流，客户端可能希望恢复任何状态改变或持久化存储状态。
可以提供对数据进行原子化操作的资源。将未提交状态视为应用状态（并将其加入URI中）。如果服务器需要允许客户端撤销操作，使用适当的PUT、DELETE或POST抵消已有的改变。
安全 系统安全可能需要： - 确保仅认证过的用户访问资源。 - 确保信息从采集到存储及之后展现给授权实体或用户过程中信息的可靠性和完整性。 - 防止未授权或恶意客户端滥用资源和数据。 - 维持私密性并符合当地安全法规。
认证协议，如基本认证和摘要认证，使用一种挑战-应答机制的协议。当客户端访问受限资源，服务器使用WWW-Authenticate头挑战客户端请求其应答，应答是客户端和服务器之间共享的密钥功能。认证可用于两类场景：客户端代表自己访问受限资源、客户端代表用于访问受限资源。 基本认证（RFC 2617）中客户端会通过标识符和共享密钥来向服务器认证请求。当服务器收到客户端访问受限资源的请求，返回状态码401 (Authorization Required)及WWW-Authenticate头，WWW-Authenticate：Basic realm=&amp;ldquo;some name&amp;rdquo;。客户端会将客户端标识符（例如用户名）和共享密钥（例如密码）连接成:并通过Base64编码到Authorization头，Authorization: Basic 。服务器会对文本进行解码并验证密钥是否一致。如果客户端提前知道服务器对某资源需要基本认证，可以在请求中加入Authorization头以免收到401 (Unauthorized)状态码及WWW-Authoricate头。服务器文档可包含认证需求以帮助客户端开发人员了解这些信息。 摘要认证（RFC 2617）同基本认证类似，客户端向服务器发送的是证书摘要而不是共享密钥。摘要认证也提供了防止重放攻击的机制。当服务器收到客户端访问受限资源的请求，返回状态码401 (Authorization Required)及WWW-Authenticate头，摘要认证方案、必要的realm和nonce指令及其他指令。nonce是仅一次或有限次数使用的数字或token。客户端会将客户端或用户标识符摘要、realm和共享密钥放入到Authorization头。服务器会将请求中的摘要与存储在服务器的证书摘要验证，并在响应中包含Authentication-Info头（Authorization头在服务器侧的等同体）。默认客户端使用MD5计算摘要，不同于基本认证，这种技术不会交换未加密的共享密钥。
# Request GET /photos HTTP/1.1 Host: www.example.org # Response 401 Unauthorized WWW-Authenticate: Digest realm=&amp;quot;Sample app&amp;quot;, nonce=&amp;quot;6cf093043215da528d7b5039ed4694d3&amp;quot;, qop=&amp;quot;auth&amp;quot; Content-Type: application/xml;charset=UTF-8 Unauthorized. # Request GET /photos HTTP/1.1 Host: www.example.org Authorization: Digest username=&amp;quot;photoapp.001&amp;quot;, realm=&amp;quot;Sample app&amp;quot;, 12.2 How to Use Digest Authentication to Authenticate Clients | 221 nonce=&amp;quot;6cf093043215da528d7b5039ed4694d3&amp;quot;, uri=&amp;quot;/photos&amp;quot;, response=&amp;quot;89fba5bf5e5f9dd69865258c21860956&amp;quot;, cnonce=&amp;quot;c019e396409afe784ae9f203b8dfdf7e&amp;quot;, nc=00000001, qop=&amp;quot;auth&amp;quot; # Response HTTP/1.1 200 OK Content-Type: application/xml;charset-UTF8 ...  OAuth（http://oauth.net）是2007年开发的一种代理认证协议。使用该协议，用户可以不用泄漏自己的证书，让客户端访问其在服务器上的数据。OAuth认证协议由于协议中包含三种角色，所以称为三方认证：服务提供者（例如服务器）、OAuth消费者（例如客户端）和用户。 OAuth依赖服务器向客户端发布的三套令牌和密钥。 - 消费者键值和消费者密钥：消费者键值是客户端的唯一标识符。客户端使用消费者密钥签署获得请求令牌的请求。 - 请求令牌和令牌密钥：请求令牌是服务器发布的一次性临时标识符，用于请求用户向客户端授予权限。令牌密钥是用于签署获得访问令牌的请求。 - 访问令牌和令牌密钥：访问令牌是客户端用于访问用户资源的标识符。拥有访问令牌的客户端能在令牌有效时访问用户资源。服务器可以由于令牌到期或用户撤销权限而随时撤销访问令牌。令牌密钥用于签署访问受限用户资源的请求。
使用三方OAuth涉及以下步骤以获得访问令牌和密钥。服务器可能会授予对特定用户资源一段时间或一定访问次数的访问令牌。 - 客户端带外向服务器请求消费者键值和消费者密钥。 - 客户端使用消费者键值获得请求令牌和密钥。 - 客户端重定向用户到服务器获得让客户端访问用户资源的权限，该过程产生认证过的请求令牌。 - 客户端请求服务器提供访问令牌和密钥。 - 当客户端发送请求访问受限资源时，客户端请求包含Authorization头（或查询参数），其含有消费者键值、访问令牌、签名方法和签名、时戳、nonce和可选的OAuth协议版本号。
由于OAuth是HTTP层之上的协议，服务器文档应该提供：获得请求令牌的URI、鉴权服务器的URI和获得访问令牌的URI。OAuth建议使用POST获得请求和访问令牌。 两方OAuth与客户端通过使用基本或摘要认证的Authorization头向服务器提供认证相类似，没有引入代理。注意OAuth协议没有指定这种认证方式，但是被广泛用于客户端与服务器之间的认证。 使用三方OAuth涉及以下步骤： - 客户端带外向服务器请求消费者键值和消费者密钥。消费者键值是客户端的标识符。消费者密钥是客户端和服务器之间共享的密钥。 - 当客户端发送请求访问受限资源时，客户端请求包含Authorization头，其含有消费者键值、访问令牌、签名方法和签名、时戳、nonce和可选的OAuth协议版本号。 - 服务器在授予资源访问权限之前验证签名。
服务器可能将应用程序状态编码到URI。在有些情况下，这些状态可能是敏感的。当URI被通过网络传输时使用TLS有助于状态的完整性，但是服务器无法控制客户端如何管理URI。在这种情况下，服务器需要确保URI不会被篡改且URI中的消息是可靠的。为了检测URI篡改，使用HMAC-SHA1和RSA-SHA1之类的机制对URI的数据计算摘要签名，将签名作为查询参数加入资源URI中。如果URI中数据是机密的，使用AES、Blowfish、DES、Triple DES、Serpent、Twofish等机制加密数据。确保在将其加入到URI之前使用对加密结果Base64进行编码。 为了维持资源表述的可靠性和完整性，使用TLS，仅基于HTTPS的请求可以访问服务器受限资源。
可扩展性与版本控制 在任何分布式CS环境管理变化都是困难的。在这些环境中，客户端依赖服务器来遵守契约，RESTfulweb服务也不例外。对于web服务，契约包含URI、资源、表述的结构和内容、格式及对每个资源所用的HTTP方法。任何对服务器的改变在后向兼容之前似乎都是良性的。当变化是后向兼容的，当修改服务器时无需升级客户端，客户端除了在服务器升级时宕机之外能够继续按照以往的方式使用服务器。当多个客户端和服务器在不同时刻升级，另一个比较重要的兼容性是前向兼容。在某些情况下，一些新客户端可能会和老服务器交互。前向兼容的目的是确保新客户端即使在老服务器功能缺失时能够继续使用老服务器。应用是否需要考虑后向兼容和前向兼容取决于操作环境，维护兼容性的手段是可扩展性。可扩展性是解决未来变化的设计过程。作为传输协议，HTTP是可扩展的，可以添加新方法或标头来扩展HTTP（须谨慎使用），但不意味着HTTP之上的应用也自动地可扩展。尽管维护兼容性是值得的，但并不总是有可能实现。 为了使URI变化跟现有客户端兼容，必须保持URI持久不变。将相同查询参数但排序不同的请求URI视为相同。客户端必须能够对不同查询参数排序获得相同的结果。当对URI添加新参数，继续使用现有参数并将新参数视为可选。当改变查询参数数据格式，继续使用现有格式。如果行不通，通过新的查询参数或新URI引入格式变化。默认情况下，将URI中的查询参数视为可选，除非这些参数基于并发或安全使用。 为了使XML/JSON表述变化跟现有客户端兼容，设计XML/JSON格式保持子节点无序。当对XML/JSON进行改动，保持现有分层结构以使客户端能够继续使用相同结构提取数据。使请求中新创建的数据节点可选以同现有客户端兼容。如果客户端发送新数据字段，服务器也必须能继续处理。不要删除或重命名响应体中表述的任何数据字段。 Atom格式被设计成支持未来的扩展。Atom格式中的所有元素允许外来的XML元素和属性。可以通过下列方式扩展Atom： - 添加链接关系类型，例如&amp;rdquo;种子分页和打包&amp;rdquo;扩展（RFC5005）引入了fist、last、previous和next链接关系类型。 - 在诸如atom:entry、atom:feed和atom:link之类的Atom元素内增加新元素。例如&amp;rdquo;Atom跟帖扩展&amp;rdquo;（RFC4685）引入了in-reply-to和total元素。 - 在atom:content元素内嵌套外来的XML或其他文本内容。
对atom:feed和atom:entry元素添加子元素或属性进行扩展，只要这些扩展不破坏客户端自身的功能且其他软件不了解这些扩展。当在atom:content元素下添加外来内容，在atom:summary元素下添加可读文本或XHTML。 为了使链接变化跟现有客户端兼容，避免删除链接，不要改变链接的rel和href属性值。当引入新资源，使用链接向客户端提供资源的URI。 当服务器作出兼容性改变，为了使客户端不操作失败，可让客户端解析表述体，仅寻找已知数据。不要假设从服务器接收到的表述是固定媒体类型、字符编码、内容语言或内容编码。 当服务器无法维持兼容性时或某些客户端需要与其他客户端不同的行为或功能时考虑对某些或所有资源版本化以对客户端隔离改变。当对RESTfulweb服务版本化时，当资源行为或表述所含信息发生变化时使用新URI添加新资源，在子域名、路径片段或查询参数中使用例如v1或v2这样易于检测的模式分辨URI。避免对同一资源使用不同媒体类型的新表述视为一个版本。
服务发现 当创建RESTful web服务时，需要解决设计时可发现性和运行时可发现性这两类可发现性。设计时可发现性有助于其他设计和创建客户端，它描述了客户端开发组和管理员用于构建和启动客户端的基本信息。运行时可发现性有助于维护客户端和服务器之间的松散耦合并使能即插即用式自动化。运行时可发现性解决了HTTP统一接口、媒体类型、链接和链接关系类型。本章是关于设计时可发现性的。 设计时发现简单来说就是将web服务用散文描述，不管这些散文诗有工具生成还是设计者手工写作。客户端开发者可以查询散文以理解资源语义、媒体类型、链接类型等信息以实现客户端。 下面这些信息需要在RESTful web服务文档中描述： - 所有资源及每个资源支持的方法 - 请求和响应中资源的媒体类型和表述格式 - 使用的链接关系、商业意义、所用的HTTP方法及链接标识的资源 - 所用不通过链接提供的固定URI - 用于所有固定URI的查询参数 - URI模板和符号替换规则 - 访问资源的认证和安全证书
对于XML表述，如果客户端和服务器支持XML schema，使用schema语言描述用于请求和响应中表述的XML结构。对于其他格式，使用散文描述表述。 通过支持HTTP OPTIONS方法，有助于工具了解web服务中的资源。在服务器端，实现OPTIONS方法通过Allow头返回支持方法列表。当资源支持PATCH方法，添加Accept-Patch头列举支持PATCH请求的媒体类型。可选择性添加含有资源描述文档的链接的Link头。
</content>
    </entry>
    
     <entry>
        <title>RESTful Web Services Cookbook笔记（二）</title>
        <url>https://mryqu.github.io/post/restful_web_services_cookbook%E7%AC%94%E8%AE%B0%E4%BA%8C/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>rest</tag><tag>最佳实践手册</tag><tag>webservice</tag><tag>http</tag>
        </tags>
        <content type="html">  Atom和AtomPub Atom聚合格式（RFC 4287）和Atom发布协议（也称为AtomPub，RFC5023）定义了条目和种子等资源及其表述和操作协议。Atom主要用于基于文本的、意图让人们去阅读的博客、讨论论坛、评论系统等资源。AtomPub描述了允许客户端创建和修改Atom格式资源的语义，并引入有助于应用程序发现的服务和分类资源。 Atom和AtomPub被用于很多应用场景。尽管Atom通常用于博客种子，也能进行格式扩展以用于用户简介、搜索结果、相簿等应用数据。 下面列举了Atom条目和种子内的一些元素。Atom条目和种子都是可扩展的，也可以引入新的属性和元素。
|元素|描述 |&amp;mdash;&amp;ndash; |atom:author|存在于atom:feed和atom:entry内，表现创建条目/种子的作者，包含至少一个atom:name及可选的atom:uri和atom:email子元素 |atom:content|存在于atom:entry内，提供普通文本、HTML或XHTML条目内容或带媒体类型的其他内容，使用src和type属性链接到任意媒体 |atom:summary|存在于atom:entry内，提供条目摘要或描述。与atom:tile相似，提供type属性。 |atom:id|存在于atom:entry内，包含条目的URN格式的全局唯一标识符（例如urn:guid:550e8400-e29b-41d4-a716-446655440123）。其值在条目/种子更新或移动后必须改变。 |atom:link|存在于atom:feed和atom:entry内，每个条目/种子必须包含一个rel值为self的atom:link元素，可以包含relf值为alternate的多个type和hreflang属性唯一的atom:link元素组合，也可以包含链接关联资源的其他atom:link元素。 |atom:title|存在于atom:feed、atom:entry和atom:source内，包含条目/种子的文本标题表述。支持type属性，值为text（默认）、thml或xhtml。 |atom:update|存在于atom:feed和atom:entry内，包含条目/种子的最新更新时间。 |atom:category|存在于atom:feed和atom:entry内，对条目和种子进行分类。 |atom:contributor|每个Atom条目可以包含一个或多个atom:contributor元素。 |atom:generator|存在于atom:entry和atom:source内，指示生成种子的软件或条目来源。 |atom:icon|存在于atom:feed内，每个种子可以包含一个atom:icon元素。 |atom:logo|存在于atom:feed内，每个种子可以包含一个atom:logo元素。 |atom:published|存在于atom:entry内，每个条目可以包含一个atom:published元素，用于指示条目第一次发布的时间。 |atom:rights|存在于atom:entry内，每个条目可以包含一个atom:rights元素，描述权利例如著作权。 |atom:subtitle|存在于atom:feed和atom:source内，每个条目/源可以包含一个atom:subtitle元素。
使用Atom的好处在于互通性。为了使用Atom，将资源建模成条目，集合建模成种子。这些元素在http://www.w3.org/2005/Atom命名空间下定义，该命名空间常用的前缀为atom。 Atom种子和条目的默认内容模型包括文本、HTML或XHTML内容和摘要、标识符、链接、作者、分类等。该内容模型最适合发布和聚合作为种子的信息片。然而，由于其格式获取的基本概念对大多数应用程序有益，可被用于各种场景而不是仅仅用于内容种子。 当资源的信息模型或元数据与Atom种子和条目的语法和语义自然匹配时使用Atom。即使资源的信息模型无法匹配Atom，考虑为其提供由短文本、HTML或XHTML资源摘要和链接。用户可以通过种子阅读器等工具了解资源。 AtomPub引入了服务文档和媒体资源等额外资源，服务文档有助于客户端发现Web服务提供的集合。服务器能够使用媒体资源将语音、视频、图像媒体或任意文档与Atom条目进行关联。 使用服务文档资源将集合汇入工作空间。该资源表述是XML文档，定义在http://www.w3.org/2007/app命名空间的service是文档的根节点。该命名空间常用的前缀为app。表述的媒体类型是application/atomsvc&#43;xml。服务（app:service）包含一个或多个工作空间（app:workspace）。每个工作空间包含多个的集合（app:collection），列举了所有种子URI、可接受媒体类型（app:accept）和分类（app:category）。 分类资源列举了集合内资源的分类，表述是category作为根节点的XML文档，有atom:category元素组成。表述的媒体类型是application/atomcat&#43;xml。 AtomPub是修改Atom文档的应用协议。它描述如何创建、更新和删除Atom条目，也支持编辑诸如图片、打包文件等关联的非文本媒体。如果正在使用Atom格式发布可编辑资源，考虑支持AtomPub。 允许客户端通过提交消息体为Atom条目文档的POST请求来创建新资源。客户端可以接下来对edit关系类型的链接用PUT方法修改或用DELETE方法删除资源。 当表述是Atom条目文档时在媒体类型上添加参数type=entry。 AtomPub引入的资源类型之一是媒体资源。媒体资源是除了Atom条目文档之外的其他资源，可用于表现文档、图片、音频和视频文件等。由于媒体资源不是Atom条目文档且可能是二进制资源，AtomPub对每个媒体资源关联一个媒体链接资源（描述并链接媒体资源的Atom条目）。 客户端通过发送POST请求来创建媒体资源。服务器创建媒体资源和媒体链接资源，并在响应的通过Location头返回媒体链接资源的URI。在媒体链接资源表述中，通过atom:conteng元素的src属性提供新创建的媒体资源URI。
内容协商 内容协商有时也称以为conneg，是当多种表述（/变体）可用时为客户端选择资源的最佳表述。尽管内容协商经常与指示媒体类型优先级相关，它也能用于指示语言本地化、字符编码和压缩编码的优先级。HTTP指定了两种内容协商：服务器驱动协商和代理驱动协商。服务器驱动协商使用request头选择一种变体，代理驱动协商为每一种变体使用不同URI。 当实现一个客户端时，对客户端来说向服务器指示自身能够处理的表述格式、语言、字符编码和压缩编码偏好和能力是非常重要的。即使能够通过带外了解响应中上诉信息，清楚指示客户端的偏好和能力有助于客户端面对变化。否则，当服务器决定提供资源的替换表述，HTTP库的任何默认偏好可能提示服务器返回了不同的表述并中断客户端。 在发送请求时，添加一个Accept头，包含逗号分隔的媒体类型优先级列表。如果媒体类型优先级不一样，对每个媒体类型添加一个q参数，以表示相关优先级（1.0～0.0，优先级越高值越大）。如果客户端仅能处理特定格式，在Accept头添加*;q=0.0以表明无法处理Accept头媒体列表之外的媒体。 如果客户端仅能处理特定字符编码，添加带有偏好字符集的Accept-Charset头，否则避免添加Accept-Charset头。为表述的偏好语言添加Accept-Language头。如果客户端能够解压缩诸如gzip、compress或deflate编码的表述，添加带有支持的压缩编码的Accept-Encoding头，否则，不要使用该头。
# Request headers Accept: application/atom&#43;xml;q=1.0, application/xml;q=0.6, **;q=0.0 # Response HTTP/1.1 200 OK Content-Language: en Vary: Accept-Language ... # Request for German representation GET /status HTTP/1.1 Host: www.example.org Accept-Language: de;q=1.0,**;q=0.0  当服务器无法满足客户端偏好且客户端显式包含**;q=0.8。这样很难在浏览器获得内容协商的表述。 代理驱动协商当客户端无法使用Accept-*头来表示偏好时很有效，它通过为每个变体提供不同URI，客户端使用URI来选择期望的表述。在代理驱动协商中，客户端通过从服务器获得的带外信息判断要使用的URI。如果表述存在，服务器返回表述，否则，返回404(Not Found)状态码。尽管所有Accept-*头内要协商的信息都可在代理驱动协商中实现，通常用于媒体类型和语言类型。下面是代理驱动协商的常用做法： - 查询参数，例如http://www.example.org/status?format=json和http://www.example.org/status?format=xml - URI扩展，例如http://www.example.org/status.json和http://www.example.org/status.xml - 子域，例如en.wikipedia.org和de.wikipedia.org
内容协商并不总是适合的，需要考虑Web服务支持多种格式的代价。当客户端需要多种变体或每个变体包含相同信息时，支持多种变体，否则为每个信息使用不同的URI。 在考虑为每个资源支持多种表述之前，需要考虑： - 应用程序流可能对每种表述格式都不同。 - 内容协商仅在开发框架支持的时候代价才会很小，并不是所有代发框架都支持通过带有多个媒体类型及不同q参数的Accept头返回表述变体的。 - 在某些情况下，法律和商业需求可能是区域性的，代理驱动语言协商可能是更好的途径。 - 缓存可能无法很多好地处理内容协商响应。一些缓存可能会忽略或限制任意给定资源的存储变体个数。
查询 查询信息是HTTP GET方法的一种常见应用，查询通常涉及三个组成部分，即过滤（filtering）、排序（sorting）和投影（projection）。过滤是基于一些过滤条件选择实体的一个子集的过程。排序会影响服务器是如何排列响应中结果的。投影是选择实体中哪些字段将被包含到结果的过程。只要关注URI和表述，查询设计还是相对简单的。客户端负责运行查询，服务器的职责包括设计URI来支持过滤、排序和投影，设计表述，设置合适的缓存头。 使用查询参数来设计查询是一种常用惯例，根据自己的用例，可能需要支持以下一种或全部情况的查询参数： - 从可用资源中选择数据 - 指定排序条件 - 罗列要包含在响应中资源的字段
http://www.example.org/book/978-0374292881/reviews?after=2009-08-15&amp;amp;sortbyAsc=date&amp;amp;fields=title # view参数值是一个预定义的查询，可在服务器优化常用查询，提供更快的响应速度。 http://www.example.org/book/978-0374292881/reviews?after=2009-08-15&amp;amp;view=summary # 获取所有标题包含“war”，发行于2000年后、至少有100条评论的电影，按年份排序 http://www.example.org/movies$contains(&#39;war&#39;)$compare(year&amp;gt;2000)$compare(count(comments)&amp;gt;100)?$sortby=year # 将查询参数值作为SQL WHERE子句的一部分 http://www.example.org/movies?query=&#39;.title like &#39;war&#39; and year &amp;gt; 2000 order by year&#39; # 使用XPath表达式选择电影标题 http://www.example.org/movies[year&amp;gt;2000&amp;amp;genre=&#39;war&#39;]/title  后三个特定查询（ad hoc query）对客户端来说很灵活，但是削弱了服务器优化数据存储和后端缓存的能力，从而降低了性能，并可能造成URI和数据存储方式的紧耦合。所以要避免使用通用查询语言（例如SQL或XPath）的特定查询。 HTTP头中一般断点下载时才用到Range和Content-Range实体头进行字节范围请求（Range:bytes=1102-1311）。有一些服务器在查询上也使用范围请求（Range: query:after=2009-08-15&amp;amp;sortByAsc=date），缓存可能会忽略这种非字节范围请求，应该避免使用，而查询参数则更容易实现与支持。 服务器将查询响应的表述设计为集合资源，当没有查询到任何匹配的资源，返回一个空集合。 尽管HTTP没有限制URI的长度，但它的一些实现处于安全原因对此进行限制，以避免缓存溢出，阻止用户将大量过滤条件编码到URI。使用HTTP POST来支持大查询。使用POST处理查询削弱了HTTP的统一接口。根据定义，GET才是用于安全、幂等地获取消息的。而且缓存把HOST方法的响应当成是不可缓存的，其后果是丧失了缓存能力，尤其增加了分页查询时的时延。然而在遇到实际限制时，这种权衡也是必不可少的。 服务器可以使用查询存储让那些使用POST方式发送的查询变得可以缓存。当客户端使用POST发起一个查询请求时，服务器创建一个包含查询条件的新查询资源，返回一个带有（指向新查询资源）Location头的响应码201(Created)。客户端对新查询资源发起GET请求，返回查询结果。如果客户端再用POST发起相同查询请求，服务器会找到匹配该请求的查询资源，客户端被重定向到该资源的URI上。存储查询弥补了使用POST方式处理查询的一些局限，缺点是不得不将查询永久保存为一个资源。此外，如果查询数量很大，服务器最终很有可能积聚大量不频繁使用的查询，需要频繁清理这些查询。而大量查询也会造成缓存命中率低下，缓存被很快占满，废弃很多不太使用的URI。
web缓存 当缓存能在不联系原服务器并能提供响应时，缓存会非常有效率地工作。有到期机制的缓存用于降低原服务器接受请求个数并降低应用所耗带宽。有到期机制的缓存基于Cache-Control和Expires头。这些头指导客户端和缓存在一定时间段内保留服务器返回的表述副本。缓存在时间窗内甚至在时间窗外不联系原服务器使用缓存的表述副本服务后继请求。 基于更新频率，决定缓存到期时间。此时间段后，缓存将认为缓存的表述是陈旧的。Cache-Control头是HTTP 1.1头，其max-value值是以秒为单位的新鲜生命期。为了支持遗留的HTTP 1.0缓存，也要包含Expires头及到期时间。如果决定缓存不应保留副本，使用值为no-cache的Cache-Control头。为了支持支持遗留的HTTP 1.0缓存，也要包含Pragma:no-cache头。 下面列举了Cache-Control指令：
|指令|应用 |&amp;mdash;&amp;ndash; |public|默认值。当请求是鉴权过的但仍希望允许共享缓存提供缓存响应服务，也可以用此指令 |private|当响应对客户端或用户私有或基于鉴权时使用。当此指令存在时，客户端缓存（例如浏览器缓存和转发代理）可以缓存表述，但服务器上或网络中的共享缓存不能进行缓存。 |no-cache和no-store|此指令防止任何缓存存储或提供缓存的表述。 |max-age|此指令是以秒为单位的新鲜生命期。 |s-maxage|此指令类似于max-age但仅用于共享缓存。当原服务器同事设置了max-age和s-maxage，缓存使用那个s-maxage。实践中，单设max-age就够了。 |must-revalidate|使用此指令请求缓存在提供陈旧表述之前检查原服务器。 |proxy-revalidate|此指令类似于must-revalidate除了它仅作用于共享缓存。
# Response HTTP/1.1 200 OK Date: Sun, 09 Aug 2009 00:56:14 GMT Last-Modified: Sun, 09 Aug 2009 00:56:14 GMT Expires: Sun, 09 Aug 2009 01:56:14 GMT Cache-Control: max-age=3600,must-revalidate Content-Type: application/xml; charset=UTF-8  像Squid之类的Cache为Cache-Control头提供了两个扩展指令stale-if-error和stale-if-revalidate。服务器使用stale-if-error告知缓存在max-age超时后仍可是使用一段时间的陈旧表述。服务器使用stale-if-revalidate告知缓存在max-age超时后在异步检查服务器响应的同时仍可是使用一段时间的陈旧表述。 并不是所有HTTP响应都被缓存。关于HTTP 1.1，GET、HEAD和POSt方法的响应可以缓存，但缓存认为该方法不可被缓存。对GET和HEAD请求的带有成功状态码的响应设置到期缓存头。无需对其他方法设置到期缓存头。除了带有200 (OK)状态码的成功响应设置到期缓存头，也可以考虑下面的3xx和4xx响应码。这有助于减少来自客户端的错误触发流量。这称之为消极缓存。
|状态码|介绍 |&amp;mdash;&amp;ndash; |300 (Multiple Choices)|带有这个状态码的表述可能很少频繁改变。将此响应缓存可以降低服务器负载。 |301 (Move Permanently)|当资源永久搬移，将URI存储在数据库的客户端肯能不会更新。在这种情况下，缓存转发响应可以不联系原服务器。 |400 (Bad Request)|当服务器返回此状态码，假定客户端就不会重发请求了。但有些客户端由于软件bug或者故意会重发请求。 |403 (Forbidden)|如果服务器永久拒绝服务此资源时添加。 |404 (Not Found)|资源不存在时添加 |405 (Method Not Allowed)|客户端可能由于软件bug重发请求。 |410 (Gone)|资源不再存在，因此缓存应尽可能为此返回错误响应。
除非使用线程软件，否则避免在客户端应用程序支持到期缓存，而是在客户端网络部署转发代理缓存，并且避免在客户端代码实现自己的缓存层（工作量大、维护复杂且耦合度高）。 复合资源中有一些数据是不经常改变的，而有一些数据可能是频繁改变的。对到期缓存处理和过期头设置基于最易于改变数据的最强新鲜需求制定。 支持缓存的一个挑战是在客户端没有发送请求时保持缓存新鲜（数据最新）且温暖（缓存不空）。当客户端上传一个新资源，所有缓存都没有这个资源，因此服务器必须为请求生成表述。一个新部署的缓存，必然是空的，只有随着客户端开始请求后才能进行填充。温暖的缓存避免冷启动问题。尽可能将超时与更新频率同步。如果不可能，实现监控数据库等新、定时通过无条件GET请求更新缓存的后台进程。如果使用Squid，使用HTTP缓存通道扩展将资源更新复制到缓存。
条件请求 HTTP条件请求有助于解决两个问题。对于GET请求，条件请求帮助客户端和缓存检验缓存的表述是否新鲜。对于PUT、POST和DELETE等不安全的请求，条件请求提供了并发控制。不支持条件GET请求会将降低性能。但对于并发，不支持条件POST、PUT和DELETE请求不安全而且可能影响应用程序完整性。缺乏足够的并发控制检查，服务器容易“丢失更新”或“陈旧删除”。当客户端它基于自认为的资源当前状态提交请求修改或删除资源，但在并发条件下，资源的当前状态并不是静态的，服务器（通过后端方式）或其他客户端都有可能已经修改或删除了资源。 并发控制确保客户端对数据的并发操作被正确处理。有两种并发控制实现方式： - 悲观并发控制：锁机制。 - 乐观并发控制：此模式下，客户端首先获得令牌，之后在写请求中携带此令牌，如果令牌有效则操作成功，否则操作失败。HTTP以此模式工作。
服务器使用Last-Modified和ETag响应头驱动条件请求。客户端使用If-Modified-Since和If-None-Match验证缓存表述，使用If-Unmodified-Since和If-Match进行并发控制预处理。 如果对存储资源的数据存储区能够控制，修改每个资源的schema以包含用来跟踪版本的修改时戳或序列号。如果数据存储区是数据库，使用触发器在数据修改时自动更新上述字段。如果无法修改存储schema或数据存储区不允许维护时戳或序列号，使用资源数据生成ETag头数值，并存储到单独的表或存储区。如果表述不是很大，使用表述体生成MD5哈希值或每次随资源变动的某些字段用于ETag。Last-Modified是一种弱验证，ETag是一种强验证，两者不必同时使用。 如果客户端在本地存储表述体，可以将响应的Last-Modified和ETag头一同存储。客户端基于上次请求响应的Last-Modified和ETag头发送带有If-Modified-Since或If-Non-Match的条件GET请求，服务器发现表述没有改变，可以省去发送表述体仅发送状态码304 (Not Modified)，否则发送包含新ETag或Last-Modified头的最新表述体。
# 客户端在一小时后发出第三个相同请求 GET /person/joe HTTP/1.1 Host: www.example.org # 缓存向原服务器发送请求 GET /person/joe HTTP/1.1 Host: www.example.org If-Modified-Since: Sun, 09 Aug 2009 00:40:14 GMT If-None-Match: &amp;quot;3f4a74db207d0447d46710a64971e777&amp;quot; # 服务器产生的响应 HTTP/1.1 304 Not Modified Date: Sun, 09 Aug 2009 01:54:14 GMT Last-Modified: Sun, 09 Aug 2009 00:56:14 GMT Expires: Sun, 09 Aug 2009 02:54:14 GMT Cache-Control: max-age=3600,must-revalidate E-Tag: &amp;quot;3f4a74db207d0447d46710a64971e777&amp;quot; Content-Type: application/xml; charset=UTF-8 # 缓存返回的响应 HTTP/1.1 200 OK Date: Sun, 09 Aug 2009 00:54:14 GMT Last-Modified: Sun, 09 Aug 2009 00:40:14 GMT Expires: Sun, 09 Aug 2009 01:44:14 GMT Cache-Control: max-age=3600,must-revalidate ...  服务器处理条件PUT请求 服务器处理条件DELETE请求 当客户端使用条件GET请求查询失败，获得412 (Precondition Failed)状态码。HTTP 1.1允许客户端修改到期缓存、使用包含Cache-Control:no-Cache和Pragma:no-cache的无条件GET请求获取新鲜表述。 当客户端使用PUT创建新资源时，或服务器在同一资源的上一个GET或PUT请求响应中没有返回Last-Modified或ETag头，客户端像和平时一样发送PUT请求。 如果客户端拥有之前对资源请求响应的Last-Modified和ETag头，客户端基于上次请求响应的Last-Modified和ETag头发送带有If-Unmodified-Since或If-Match的条件PUT/DELETE请求。如果服务器返回412 (Precondition Failed)状态码，客户端可通过无条件GET请求获取新鲜表述，决定是否通过重发PUT/DELETE实现自己的需求。 与PUT或DELETE不同，对一个资源的POST请求可能不会对请求URI中的资源造成任何改变。服务器可能会创建一个新资源（状态码201）或使用不同URI（状态码303）标识输出。因此，客户端不会在本地存储表述和条件头。为了让服务器检测和防止客户端重复发送POST请求，可以通过一次性URI实现条件或不可重复的POST请求。 客户端实现通过GET请求获取包含token链接，如果URI的目的是创建新资源，基于序列号、时戳和随机数的连接串生成token。如果URI的目的是修改资源，基于这些资源的实体标签和标识符生成token。 客户端发送服务器响应中提供的一次性URI进行POST请求，服务器查看token是否已经在服务器的事务日志中存在，以检测POST请求有效性，如果存在返回状态码403 (Forbidden)并解释原因，否则根据输出返回状态码201 (Created)或303 (See Other)。
</content>
    </entry>
    
     <entry>
        <title>RESTful Web Services Cookbook笔记（一）</title>
        <url>https://mryqu.github.io/post/restful_web_services_cookbook%E7%AC%94%E8%AE%B0%E4%B8%80/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>rest</tag><tag>最佳实践手册</tag><tag>webservice</tag><tag>http</tag>
        </tags>
        <content type="html">  使用统一接口 HTTP是一种应用层协议，它定义了客户端与服务器之间的转移操作的表述形式。在此协议中，诸如GET，POST和DELETE之类的方法是对资源的操作。有了它，无须创造createOrder,getOrder,updateOrder等应用程序特定的操作了。 作为应用协议，HTTP的设计目标是在客户端和服务器之间保持对库、服务器、代理、缓存和其他工具的可见性。可见性是HTTP的一个核心特征。 一旦识别并设计资源，就可以使用GET方法获取资源的表述，使用PUT方法更新资源，使用DELETE方法删除资源，以及使用POST方法执行各种不安全和非幂等的操作。可以添加适当的HTTP标头来描述请求和相应。 以下特性完全取决于保持请求和相应的可见性： - 缓存：缓存响应内容，并在资源修改时使缓存自动失效。 - 乐观并发控制：检测并发写入，并在操作过期的表述时防止资源发生变更。 - 内容协商：在给定资源的多个可用表述中，选择合适的表述。 - 安全性和幂等性：确保客户端可以重复或重试特定的HTTP请求。
HTTP通过以下途径来实现可见性： - HTTP的交互是无状态的，任何HTTP中介都可以推断出给定请求和响应的意义，而无须关联过去和将来的请求和响应。 - HTTP使用一个统一接口，包括有OPTIONS，GET，HEAD，POST，DELETE和TRACE方法。接口中的每一个方法操作一个且仅一个资源。每个方法的语法和含义不会因应用程序和资源的不同而发生改变。 - HTTP使用一种与MIME类似的信封格式进行表述编码。这种格式明确区分标头和内容。标头是可见的，除了创建、处理消息的部分，软件的其他部分都可以不用关心消息的内容。
保持可见性的另一方面是使用适当的状态码和状态消息，以便代理、缓冲和客户端可以决定请求的结果。 在某些情况下，可能需要权衡其他特性，如网络效率、客户端的便利性以及分离关注点，为此放弃可见性。当进行这种权衡时，应仔细分析对缓存、幂等性、安全性等特性的影响。 当有多个共享数据的资源，或一个操作修改多个资源时，需要权衡是否降低可见性（例如是否禁止缓存）以便获得更好的信息抽象、更松散的耦合程度、更好地网络效率、更好地资源粒度，或纯粹为了方便客户端使用。 可以通过带有应用程序状态的URI链接来保持应用程序状态而无需依赖服务器中内存中的会话。 安全性和幂等性是服务器要实现的HTTP方法的特征。当客户端发送GET、HEAD、OPTIONS、PUT或DELETE请求时，如果没有使用并发条件限制时，确保服务器提供相同响应。
|方法|是否安全?|是否幂等? |&amp;mdash;&amp;ndash; |GET|是|是 |HEAD|是|是 |OPTIONS|是|是 |PUT|否|是 |DELETE|否|是 |POST|否|否
客户端通过下列方法实现幂等的/安全的HTTP请求： - 将GET、OPTIONS和HEAD视为只读操作，可按需随时可发送请求。 - 在网络或软件异常的情况下，通过If-Unmodified-Since/If-Match条件标头重发GET、PUT和DELETE请求。 - 不要重发POST请求，除非客户端（通过服务器文档）知道对特定资源的POST实现是幂等的。
Web基础设施严重依赖于GET方法的幂等性和安全性。客户端期望能够重复发起GET请求，而不必担心造成副作用。缓存依赖于不需访问源服务器就能提供已缓存表述的能力。 不要把GET方法用于不安全和非幂等操作。因为这样做可能造成永久性的、意想不到的、不符合需要的资源改变。 可以使用POST方法或PUT方法创建新资源。只有在客户端可以决定资源的URI时才使用PUT方法创建新资源；否则使用POST，由服务器决定新创建资源的URI（客户端请求可以使用Slug头建议新资源的URI）。 在以下场合中使用POST方法： - 创建新的资源，把资源作为一个工厂 - 通过一个控制器资源来修改一个或多个资源 - 执行需要大数据输入的查询 - 在其他HTTP方法看上去不合适时，执行不安全或非幂等的操作。（缓存不会缓存这一方法的响应）
使用POST方式实现异步任务：服务器在接受到POST请求时，返回状态码202（Accepted），并包含一个让客户端可以跟踪异步任务状态的资源表述和客户端稍后检查状态的建议时间（ping-after）。 客户端使用GET请求查询异步任务状态，如服务器还在执行中，返回响应码200（OK）及包含当前状态的任务资源表述；如服务器成功完成，返回响应码303（SeeOther）以及包含新资源URL的Location头；如服务器任务失败，返回响应码200（OK）及任务失败的表述。 使用DELETE方法实现异步请求：服务器在收到DELETE请求，返回状态码202（Accepted），并包含一个让客户端可以跟踪异步任务状态的资源表述和客户端稍后检查状态的建议时间（ping-after）。 客户端使用GET请求查询异步任务状态，服务器返回响应码200（OK）及包含当前状态的任务资源表述。 避免使用非标准的自定义HTTP方法。当前比较有名的自定义方法包括WebDAV定义的方法、PATCH和MERGE。 HTTP服务器可能会使用自定义HTTP标头，比较有名的自定义HTTP包括X-Powered-By、X-Cache、X-Pingback、X-Forwarded-For及X-HTTP-Method-Override。实现客户端和服务器时，要让他们在没有发现需要的自定义标头时也不会失败。避免使用自定义HTTP标头改变HTTP方法的行为。
识别资源 从领域名词中识别资源。 直接将领域实体映射为资源可能导致资源效率低下且难以使用，可以通过网络效率、表述的多少以及客户端的应用程度来帮助确定资源的粒度。 粗粒度设计便于富客户端应用程序，更精细的资源颗粒可以更好地忙族缓存的要求。因此，应从客户端和网络的角度确定资源的粒度。下列原书可能会进一步影响资源粒度： - 可缓存性 - 修改频率 - 可变性
仔细设计资源粒度，以确保使用更多缓存，减少修改频率，或将不可变数据从使用缓存较少、修改频率更高或可变数据分离出来，这样可以改善客户端和服务器端的效率。 基于应用程序特有的条件来识别相似的资源（例如共享同一数据库schema的资源，有相同特性或属性的资源），可以将这些有共性的资源组织成为集合。 基于客户端的使用模式、性能和延时要求，确定一些新的聚合其他资源的复合资源，来减少客户端与服务器的交互。 符合资源降低了统一接口的可见性，应为它们的表述中包含了和其他资源相重叠的资源。因此，在提供复合资源前,需要考虑一下几点： - 如果在应用程序的请求很少，那么它可能不是一个好的选择。依赖缓存代理，从缓存中获取这些资源，也许能让客户端收益匪浅。 - 另一个因素是网络开销&amp;ndash;客户端与服务器之间的网络开销，服务区和后端服务或他所依赖的数据存储之间的网络开销。如果后者开销很大，那获取大量数据并在服务器上将他们组合成复合资源可能会增加客户端的延时，降低服务器的吞吐量。 - 想要改善延时，可以在客户端和服务器之间增加一个缓存层，并避免复合资源，进行一些负载测试来验证复合资源是否能起到改善作用。
最后，为每个客户端创建特定目标的复合资源并非是注重实效的做法。选择对Web服务最重要的客户端，设计复合资源来满足它们的需要。 像计算两地距离、行车路线、信用卡验证之类的计算或处理函数可被当作资源处理，并使用带有查询参数的HTTP GET获取函数输出表述。 当需要原子性修改多个资源时，可以为每个不同的操作指派一个控制器。客户端通过HTTP POST方法提交请求触发操作。如果操作结果是创建一个新资源，返回响应码201（Created）并在Location头里包含新资源的URL。如果操作结果是对一个或多个已有资源的修改，返回响应码303（See Other）并在Location头里包含客户端可用户获取修改表述的URL。如果服务器无法提供所有修改资源的单个URI，返回状态码200（OK）并在消息体内包含客户选可以用于了解操作结果的表述。 在RESTful Web服务中，控制器有助于对服务器和客户端之间进行关注分离，增进网络效率，让服务器端原子性地实现复杂操作。
设计表述 在HTTP设计中，发送发可以用一些名为实体头的标头来描述表述正文（也成为实体正文或消息正文）。有了这些标头，接收方可能在无须查看正文的情况下决定如何处理正文，还可以将解析正文所需要提前了解及猜测的内容尖刀最小程度。 使用以下标头来注解包含消息正文的表述：
标头用途解析处理Content-Type用于描述表述类型，即通常所说的media-type或MIME类型，包含charset参数或其他针对该媒体类型而定义的参数。
媒体类型格式参考规范application/xml通用XML格式RFC 3023application/*&#43;xml使用XML格式的特殊用途媒体格式RFC 3023application/atom&#43;xml用于Atom文档的XML格式RFC 4287
RFC 5023application/json通用JSON格式RFC 4627application/javascriptJavaScript, 用于可以处理JavaScript的客户端RFC 4329application/x-www-form-urlencoded查询字符串格式HTML 4.01application/pdfPDFRFC 3778text/html多种版本的HTMLRFC 2854text/csv逗号分隔值，一种通用格式RFC 4180
这个标头告诉接收方如何解析数据。例如，如果标头是application/xml或其他以&#43;xml结尾的值，就可以用XML解析器来解析消息。如果是application/json，就可以用JSON解析器。没有此标头时，就只能解析正文的格式了。当收到一个不太Content-Type的表述，避免猜测表述的类型。当客户端发送不带该标头的请求时，返回错误码400（Bad Request）。当从服务器接收到一个不带标头的响应时，将其视为不正确的响应。Content-Length最早从HTTP 1.0中被引入，用于指定表述正文的大小。
发送方需要在写正文前计算出表述的大小并设置该标头，接收方用它来判断自己是否从连接中读取了正确的字节数。
HTTP 1.1支持一种名为分块传输编码的更有效机制，这让Content-Length头变的有点多余。如果客户端不支持HTTP 1.1，需要包含Content-Length。此外对于POST和PUT请求就算使用分块传输编码,也要在客户端应用程序的请求中同时包含Content-Length头，因为有些代理会拒绝缺失Content-Length和Transfer-Encoding: chuncked的POST和PUT请求。
HTTP/1.1 200 OK Last-Modified: Thu, 02 Apr 2009 02:32:28 GMT Content-Type: application/xml;charset=UTF-8 Transfer-Encoding: chunked FF [some bytes here] 58 [some bytes here] 0在没有确定接收到表述不带Transfer-Encoding: chunked前，不要检查Content-Length头是否存在。Content-Language如果使用某种语言对表述进行本地化，使用该标头来指定语言。值是两个字母的RFC5646语言标签，还可以在谋面带上连字符（-）和任意两个字母的国家代码。如en-US或kr。如果存在该标头，读取并存储它的值，记录下使用的语言。Content-MD5工具/软件在处理或存储表述时可能存在错误，需要提供一致性校验来验证实体正文的完整性，用该标头的值是表述正文（在进行内容压缩编码之后，分块传输编码之间计算）的MD5摘要。请注意，TCP使用checksum在传输层提供一致性校验，因此此标头对非可靠网络发送或接受大的表述时非常有用。Content-Encoding当使用gzip、compress或deflate对表述正文进行编码时，使用该标头。接收方在解析正文前需要先解压缩消息。客户端可以用Accept-Encoding头来表明自己偏好的Content-Encoding。然而，并没有一个标准的方式让客户端了解到服务器是否可以处理用给定编码压缩过的表述。让网络库代码来解压这些压缩过的表述。Last-Modified仅用在响应上的标头，值是一个时间戳，表示服务器最后修改表述或资源的时间。 大多数情况下，客户端应用程序只需检查Content-Type头和字符编码，以此决定如何解析表述的正文。 一定要基于Content-Type、Content-Language和Content-Encoding头的值来处理响应的表述。 在发送表述时，如果媒体类型允许使用charset参数，则包含一个带字符编码值的charset参数，该参数值将被用于将字符转为字节。当接收到一个表述时，如果带有支持charset参数的媒体类型，使用其指定编码将表述正文的字节构造成字符流；如果收到一个不带charset参数的XML、JSON或HTML表述，让解析器通过相应格式规范的算法检查头几个字节来确定字符集。 JSON媒体类型application/json不指定charset参数，而是使用UTF-8作为默认编码。 另一个引入字符编码不匹配的常见途径是在XML表述的Content-Type头中给定一个编码，正文却又给定另一个编码。这时需要使用charset参数而不是中文中的编码。 还要比避免对XML格式的表述使用text/xml媒体类型，text/xml的默认字符是us-ascii，而application/xml使用UTF-8。 如果需要设计新的格式和媒体类型，考虑以下指导方针： - 如果媒体类型是基于XML的，使用&#43;xml结尾的子类型。 - 如果媒体类型是私有的，使用vnd.开头的子类型，例如application/vnd.example.org.user&#43;xml。 - 如果打算使媒体类型公共，按照RFC 4288向IANA注册新的媒体类型。
尽管自定义的媒体类型能改善协议级可见性，但现有的用于监控、国旅、路由HTTP流量的协议级工具可能不太关注，甚至不关注媒体类型。因此，没有必要仅仅为了协议层面的可见性而使用自定义媒体类型。 在XML（JSON）表述中，包含一个指向资源自身的self链接。对于那些组成资源的应用程序领域实体，在表述中包含它们的标识符。如果表述中的某个部分包含自然语言文本，添加xml：lang属性（增加一个属性），表示元素的内容用的是本地化语言。 在集合表述中包含以下内容： - 一个指向集合资源的self链接。 - 如果集合是分页的，包含指向可能的上一页和下一页的链接。
设计集合的表述，以使集合成员在结构和语法上类似（同构）。 除了文本对最终用户的表述有意义之外，避免使用语言、区域或国家特定的格式或格式识别符。而是使用下列可移植的格式： - 使用W3C XML模式中定义的decimal、float和double数据类型格式化包含汇率的数字。 - 使用ISO 3166中的国际和地区代码 - 使用ISO 4217中的货币代码 - 使用RFC 3339中的日期和时间值 - 使用BCP 47的语言标识符标签 - 使用Olson时区信息数据库中的时区标识符
对于RESTful Web服务，URI是资源的唯一标识符。然而，应用程序代码经常必须处理领域实体标识符（比如数据库中的ID）。应用程序领域实体标识符可作为资源表述中的URN加入。 有些表述可能需要在文本表述中包含二进制数据，可以使用如下的多部分媒体类型。避免对文本格式内的二进制数据使用Base64编码。
|媒体类型|用途 |&amp;mdash;&amp;ndash; |multipart/form-data|对数据的名值对及混合多部分任意媒体类数据进行编码，用于通过HTML表单上传文件。 |multipart/mixed|对多部分任意媒体类型进行打包。例如作为application/xml的视频元数据和作为video/mpeg的视频二进制数据被组合进入一个单独的HTTP消息内。 |multipart/alternative|当对相同资源使用不同媒体类型传送替代表述时使用。例如使用text/plain的普通文本和text/html的HTML格式发送邮件 |multipart/related|当各部分互相挂念并需要一同处理时使用。第一个部分是首部分，且通过Content-ID头引用其他部分。
下例展示用户上传file1.txt和file2.gif
Content-type: multipart/form-data, boundary=AaB03x --AaB03x content-disposition: form-data; name=&amp;quot;field1&amp;quot; Joe Blow --AaB03x content-disposition: form-data; name=&amp;quot;pics&amp;quot; Content-type: multipart/mixed, boundary=BbC04y --BbC04y Content-disposition: attachment; filename=&amp;quot;file1.txt&amp;quot; Content-Type: text/plain ... file1.txt 的内容... --BbC04y Content-disposition: attachment; filename=&amp;quot;file2.gif&amp;quot; Content-type: image/gif Content-Transfer-Encoding: binary ... file2.gif的内容... --BbC04y-- --AaB03x--  对于那些希望能被最终用户使用的资源，应该为他们提供HTML表述。避免为机器客户端设计HTML表述。以HTML文档的相识提供部分或全部表述时，考虑使用为格式或RDFa来注解HTML，可以让Web爬虫和同类软件从HTML文档中提取信息，而无须依赖文档的结构。 对于那些由客户端输入造成的错误，返回带4xx状态码的表述。对那些由于服务器实现或其当前状态造成的错误，则返回带5xx状态码的表述。这两种情况下，都要包含一个Date头，以表示错误发生时间。 除非请求的方法是HEAD，否则都应该在表述中包含一段正文，使用内容协商或是和阅读的HTML或村文本对其进行格式化和本地化。 如果能以独立的、适合阅读的文档形式来提供纠正或调试错误的信息，就包含一个指向该文档的链接，可以使用Link头，也可以使用正文中的链接。 如果为了后期最终或分析，在服务器上记录了错误日志，应该提供一个可以找到该错误的标识符或链接。 响应正文要具有描述性，但不应该包含注入错误堆栈、数据库连接错误之类的详细信息。如果可以的话，说明客户端可以采取的后续措施。
|错误码|描述|客户端解决方案 |&amp;mdash;&amp;ndash; |400(Bad Request)|当服务器由于语法错误无法解读请求时，返回该错误码。|查看错误表述的正文，了解问题的根本原因。 |401(Unauthorized)|当客户端无权访问资源，但在身份验证后可以获得访问权限时返回错误码。如果服务器就算是在身份验证后也不允许客户访问资源，那应该返回403(Forbidden)错误码。返回该错误码时，应该包含一个带有身份验证方法的WWW-Authenticate头。通常使用的方法是Basic和Digest。|如果客户端是面向用户的，提示用户提供身份信息。其他情况下，获取必要的安全身份信息。用带有Authorization头的请求进行重试，其中包含了身份信息。 |403(Forbidden)|当服务器不让客户端获得资源的访问权限，就算通过身份验证也没用时，返回该错误码。|这个错误意味着禁止客户端用这个请求方法来访问资源，不要重复引起该错误的请求。 |404(Not Found)|当没有找到资源时返回该错误码。如有可能，在消息体中说明原因。|资源在服务器端已经不存在了。如果客户端保存了资源的数据，清楚数据或将其标记为已删除。 |405(Not Allowed)|当资源不允许使用某个HTTP方法时返回该错误码。返回一个Allow头，其中带有该资源的有效HTTP方法。|查看Allow头来寻找适用于该资源的方法，然后做适当的代码变更，只用那些方法来访问资源。 |406(Not Acceptable)|内容协商失败|调整Accept-*头 |409(Conflict)|当请求与资源的当前状态有冲突时返回该错误码，并包含一段正文解释原因。|查看PUT的表述正文中列出的冲突。 |410(Gone)|资源以前存在，但今后不会再存在，返回该错误码。除非记录了被删除的资源，否则不能返回这个错误码。如果没有在服务器记录被删除的资源，应该用404(Not Found)取代。|将其等同于404(Not Found)。 |412(Precondition Failed)|条件请求失败|HTTP1.1允许客户端修改到期缓存、使用包含Cache-Control:no-Cache和Pragma:no-cache的无条件GET请求获取新鲜表述。之后进行后继处理。 |413(Request Entity Too Large)|当POST或PUT请求的消息体过大时返回该错误码。如有可能，在正文中说明允许哪些内容，提供一个备选方案。|在响应正文里寻找有效长度的提示。 |414(Unsupported Media Type)|当客户端用一种服务器不理解的格式来发送消息体时返回该错误。|在响应正文里了解请求支持的媒体类型。 |500(Internal Server Error)|由于某些实现上的问题，代码在服务器端失败时返回该错误码是最好的选择。|记录该错误日志，随后通知服务器开发者。 |503(Service Unavailable)|服务器在某个特定间隔或一段不确定的时间内无法完成请求时，返回该错误码。抛出该错误的两个常见场合是后端服务器失败（例如数据库连接失败）或是客户端请求达到了某个服务器设定的频率上限。如有可能，包含一个带日期或秒数的Retry-After响应头，用它的值来做提示。|如果响应中有Retry-After头，在到达时间之后进行重试。
设计URI 在设计URI时遵循常用惯例具有下列优势： - 遵循惯例的URI一般容易调试和管理。 - 服务器可以集中从请求URI中提取数据的代码。 - 可以避免花费宝贵的设计与实现时间来发明处理URI的新惯例和规则。 - 通过跨域、子域和路径来对服务器的URI进行分区，以实现负载分配、监控、路由和安全方面的操作灵活性。
针对本地化、分布式、强化多种监控及安全策略等方面的需求，可以使用域及子域对资源进行合理的分组或划分。（例如基于本地化或客户端进行子域划分） - 在URI的路径部分使用斜杠分隔符（/）来表示资源之间的层次关系。 - 在URI的路径部分使用逗号（,）和分号（;）来表示非层次元素。分号通常用于表示矩阵参数。 - 使用连字符（-）和下划线（_）来改善长路径中名称的可读性。 - 在URI的查询部分使用“与”符号（&amp;amp;）来分隔参数。 - 在URI中避免出现文件扩展名（例如.php、.aspx和.jsp）。 - 慎用空格和大写字符，以免造成问题。空格是有效的URI字符，但是RFC 3986会将其编码为%20，而applicaton/x-www-form-urlencoded媒体类型会将其编码为&#43;。RFC 3986定义URI除了协议和主机之外的其他部分是大小写敏感，而基于Windows的Web服务器却大小写的影响。
将URI视为不透明的标识符，及需要确保每个资源具有唯一的URI。然而，这可能造成超负荷的URI。在这种情况下，URI可能会变成未定信息和操作的通用网关。这会导致不正确的缓存响应，甚至没有适当鉴权不应该共享的安全数据被泄露。 - 仅使用URI来判断处理请求的资源。例如不要使用定制HTTP头来判断资源。 - 不要重复的状态变化封装入使用相同URI或定制头的POST隧道以造成URI超载。定制头仅用于信息告知用途。
为了让客户端将URI视为不透明的标识符，尽可能在运行时表述消息体和标头中提供URI，这有助于降低服务器和客户端之间的耦合。如果不能提供可能要用到的URI全集，考虑使用URI模板（半耦合）或建立带外规则（紧耦合）以便客户端能够以编程的方式构建URI。 URI应该设计用于很长时间。客户端可能将URI存储到数据库或配置文件，甚至在代码中硬编码。服务器改变了URI，可能会导致客户端无法正常工作。 应该基于稳定概念、标识符和信息来设计URI。如果URI必须改变，来在原有URI的请求可以通过301 (Moved Permanently)转移到新的URI。如果URI被废除，使用401 (Gone)表示其不再有效。
web链接 HTML、XHTML及Atom建立了在表述中包含链接的规则。理解这些格式语义的客户端可以发现表述中的链接。然而，XML是通用格式，应该由服务器负责设计在XML格式表述中包含链接并将其设计文档给客户端。 XML表述可以使用Atom中定义的link元素。该元素在http://www.w3.org/2005/Atom命名空间定义且具有下列属性：
|属性|介绍 |&amp;mdash;&amp;ndash; |href|包含链接的RUI |rel|指示链接的类型 |title（可选）|人可读的链接标题。 |type（可选）|服务器为链接URI返回表述的媒体类型 |hreflang（可选）|服务器为链接URI返回表述的内容语言 |length（可选）|服务器为链接URI返回表述的内容长度
href既可以是绝对URI也可以是相对URI（需要在link元素中包含xml:base属性）。对应Atom的link定义，在JSON表述中可以使用link（或links）特性。 Link标头提供了一种格式无关的方式来传送链接。除了使用表述体内的嵌入链接，也可以使用link标头。Link标头适用于下列场景： - 表述使用二进制格式，例如图像、富文本文档、表单等。 - 表述的格式不容易很容易发现链接（例如普通文本文档）。 - 当客户端/服务器软件需要不解析表述体添加或读取链接。对链接中的URI没有分配有意义的语义，链接自身就没有什么左右。链接关系类型传送了链接的角色和目的。一旦客户端与服务器对这些类型的含义达成一致，客户端就能发现并使用链接中的URI了。有两种方式为链接关系类型分配值。 - 当链接目的与下列表中的标准类型匹配，使用该值。 名称用途self使用此类型链接资源的推荐URIalternate使用此类型提供相同资源替换版本的URI链接（例如某pdf文档的英文版和中文版）appendix使用此类型提供作为资源集合附录的资源URI链接bookmark在博客系统使用此类型提供摘要URI链接chapter、section和subsection使用这些类型用于链接资源集合中的章、节和子节URIcontents使用此类型用于链接资源集合的目录URIcopyright使用此类型用于链接资源的著作权声明URIcurrent使用此类型用于链接资源集合中最近条目URIdescribedby使用此类型用于链接描述链接上下文的URIedit使用此类型链接客户端能编辑资源的URIedit-media使用此类型用于与媒体类型关联的Atom条目文档enclosure使用此类型链接可能很大的相关资源URI(例如视频预览片段里提供完全版本视频的链接)first、last、next、next-archive、prev、previous、prev-archive和start使用这些类型提供用于滚动浏览资源有序序列的链接glossary使用此类型链接术语表URIhelp使用此类型链接帮助文档URIindex使用此类型链接索引URIlicense使用此类型链接许可权URIpayment使用此类型链接购买或支付的URIrelated使用此类型链接有关资源replies使用此类型链接回复本链接的URIservice使用此类型链接Atom种子的服务文档URIstylesheet使用此类型链接表单URIup客户端使用此类型来进入上一层资源的链接 URIvia使用此类型标识消息源的资源URI - 当链接的目的无法与标准类型匹配，使用下列惯例定义一个扩展链接关系类型。 - 将链接关系类型表达成URI，例如http://www.example.org/rels/create-po。 - 对该URI提供HTML文档方式的信息通告资源，描述链接关系类型语义和支持的HTTP方法、对请求和响应的支持表述格式及商业规则等细节。 - 如果链接关系类型用于公共使用，向IANA注册链接类型。
超媒体链接的一个关键应用能够是客户端摆脱学习服务区用于管理应用流程的商业逻辑规则。服务器能提供包含应用状态的链接，因此使用超媒体作为应用状态的引擎。 对每个表述进行设计，使其仅包含客户端可能转移到的下一步的链接。 Web完整性是基于永久URI的，然而有时URI会是临时的。例如，一个URI可能仅对单个用户有效或在特定时间后到期终止。下面列举了几种依赖临时URI的场景： - Web服务向客户端提供安全令牌。客户端使用该令牌能在短时间内访问某个资源。 - 保险报价Web服务生成报价，每个报价针对特定用户且仅在72小时内有效。报价到期终止后，客户端必须重新获取新报价。 - 当用户在网上注册，服务器通过邮件发送安全令牌给用户并期望用户用之验证邮箱地址。
为了支持短期存活的URI，需要在链接内交互短期URI。为这些链接分配扩展关系类型，并将URI有效期和到期终止后客户端所需操作文档化。当客户端对到期终止URI发送请求，返回适当的4xx错误并在消息体内指示客户端能够采取的操作。 当服务区无法为链接提供用于生成有效且完整URI信息时，服务器可以向客户端提供URI模板。URI模板是在符号外括上大括号的字符串。 为了让符号易于匹配和替换，符号仅限用于URI的下列部分： - 路径段，例如http://example.org/segment1/{token1}/segment2 - 查询参数的值，例如http://example.org/path?param1={p1}¶m2={p2} - 矩阵参数的值，例如http://example.org/path?param1={p1};param2={p2}
为了在表述中包含URI模板，使用下列方式： - 对于XML表述，使用自己的应用程序XML命名空间内定义的link-template元素 - 对于JSON表述，使用link-template或link-templates特性。
Web浏览器是客服端使用链接进行浏览的最好例子。 为了支持服务器提供的URI和URI模板，基于已知链接关系类型从链接中抽取URI和URI模板。这些链接及其他资源数据构成了应用程序的当前状态。 如果应用程序长时间运行，将URI和关系类型同其他表述数据一同存储。 基于链接存在与否决定程序流程。 检查链接关系文档以学习任何关联的商业规则、鉴权、URI长期性、支持的方法和媒体类型等等。
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] Ubuntu 13.10下构建Mahout项目</title>
        <url>https://mryqu.github.io/post/hadoop_ubuntu_13.10%E4%B8%8B%E6%9E%84%E5%BB%BAmahout%E9%A1%B9%E7%9B%AE/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>jdk</tag><tag>eclipse</tag><tag>m2eclipse</tag><tag>mahout</tag><tag>ubuntu</tag>
        </tags>
        <content type="html">  JDK  在Oracle网站下载jdk-6u45-linux-i586.bin到/opt目录 进入/opt目录安装JDK:  chmod &#43;x jdk-6u45-linux-i586.bin sudo ./jdk-6u45-linux-i586.bin  进入/etc目录配置profile文件: sudo vi profile 在文件末尾添加：  JAVA_HOME=/opt/jdk1.6.0_45 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH  运行source /etc/profile 使其生效。 运行java -version 检验:  java version &amp;quot;1.6.0_45&amp;quot;   Eclipse  在Eclipse网站下载eclipse-jee-juno-SR2-linux-gtk.tar.gz到/opt目录 进入/opt目录解压缩Eclipse:  $ sudo tar -zxvf eclipse-jee-juno-SR2-linux-gtk.tar.gz$ sudo rm ./eclipse-jee-juno-SR2-linux-gtk.tar.gz  创建Eclipse启动快捷方式： $ sudo gedit /usr/share/applications/eclipse.desktop  [Desktop Entry] Type=Application Name=Eclipse Comment=Eclipse IDE Icon=/opt/eclipse/icon.xpm Exec=/opt/eclipse/eclipse Terminal=false StartupNotify=true Type=Application Categories=Development;IDE;Java; Exec=env UBUNTU_MENUPROXY= /opt/eclipse/eclipse   m2eclipse  通过下列update site安装:http://download.eclipse.org/technology/m2e/releases  Mahout  在Mahout网站下载mahout-distribution-0.8-src.tar.gz到自己的Eclipseworkspace中 进入worksapce解压缩mahout:  $ tar -zxvf mahout-distribution-0.8-src.tar.gz $ rm ./mahout-distribution-0.8-src.tar.gz  导入Mahout: File - Import - Maven - Existing MavenProjects   </content>
    </entry>
    
     <entry>
        <title>Linux包管理速查表</title>
        <url>https://mryqu.github.io/post/linux%E5%8C%85%E7%AE%A1%E7%90%86%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>linux</tag><tag>apt</tag><tag>yum</tag><tag>zypper</tag><tag>package</tag>
        </tags>
        <content type="html">  管理软件包 |任务|apt (deb)|yum (rpm)|zypper (rpm) |&amp;mdash;&amp;ndash; |通过仓库安装软件包|apt-get install {pkg}|yum install {pkg}|zypper install {pkg} |更新软件包|apt-get install {pkg}|yum update {pkg}|zypper update -t package {pkg} |移除软件包|apt-get remove {pkg}|yum erase {pkg}|zypper remove {pkg} |通过文件安装软件包|dpkg -i {pkg}|yum localinstall {pkg}|zypper install {pkg}
搜索软件包 |任务|apt (deb)|yum (rpm)|zypper (rpm) |&amp;mdash;&amp;ndash; |通过包名搜索|apt-cache search {pkg}|yum list {pkg}|zypper search {pkg} |通过模式搜索|apt-cache search pattern|yum search pattern|zypper search -t pattern pattern |通过文件名搜索|apt-file search path|yum provides file|zypper wp file |列举已安装软件包|dpkg -l|rpm -qa|zypper search -is |显示软件包信息|apt-cache show pgk-name|yum info {pkg}|zypper info {pkg}
更新系统 |任务|apt (deb)|yum (rpm)|zypper (rpm) |&amp;mdash;&amp;ndash; |更新软件包列表|apt-get update|yum check-update|zypper refresh |更新系统|apt-get upgrade|yum update|zypper update
软件仓库 |任务|apt (deb)|yum (rpm)|zypper (rpm) |&amp;mdash;&amp;ndash; |列举仓库|cat /etc/apt/sources.list|yum repolist|zypper repos |添加仓库|edit /etc/apt/sources.list|add to /etc/yum.repos.d/|zypper addrepo URI name |移除仓库|edit /etc/apt/sources.list|remove from /etc/yum.repos.d/|zypper removerepo name
英文原文：Linux Package Management Cheatsheet
</content>
    </entry>
    
     <entry>
        <title>Debian软件包管理速查表：dpkg、apt-get、apt-cache</title>
        <url>https://mryqu.github.io/post/debian%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86%E9%80%9F%E6%9F%A5%E8%A1%A8dpkgapt-getapt-cache/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>dpkg</tag><tag>apt-get</tag><tag>apt-cache</tag><tag>aptitude</tag><tag>synaptic</tag>
        </tags>
        <content type="html">  dpkg是Debian系统底层包管理器，apt-get是高层包管理工具，apt-cache是高层包查询工具。
dpkg速查表 dpkg是Debian Linux用于安装/管理单个软件包的命令行工具：
语法描述示例dpkg -i {.deb package}安装软件包dpkg -i zip_2.31-3_i386.debdpkg -i {.deb package}安装新的软件包。如果软件包已安装则尝试更新到最新版本dpkg -i zip_2.31-3_i386.debdpkg -R {Directory-name}递归地安装目录下所有软件包dpkg -R /tmp/downloadsdpkg -r {package}移除一个已安装的软件包，保留配置文件dpkg -r zipdpkg -P {package}移除一个已安装的软件包及配置dpkg -P apache-perl dpkg -l列举所有安装的软件包、及包版本和简短描述dpkg -l dokg -l | less dpkg -l &#39;*apache*&#39; dpkg -l | grep -i &#39;sudo&#39;dpkg -l {package}列举单个安装的软件包、及包版本和简短描述dpkg -l apache-perldpkg -L {package}找出安装的软件包所提供的文件，例如列出安装的文件dpkg -L apache-perl dpkg -L perldpkg -c {.Deb package}列出软件包所提供的文件，例如deb包文件内的所有文件，这对找出将要安装什么文件非常有帮助dpkg -c dc_1.06-19_i386.debdpkg -S {/path/to/file}找出拥有该文件的包，例如找出该文件归属的包dpkg -S /bin/netstat
dpkg -S /sbin/ippooldpkg -p {package}显示包的详细信息，包组、版本、维护者、架构、依赖包、描述等dpkg -p lsofdpkg -s {package} | grep Status找出Debian包是否安装(状态)dpkg -s lsof | grep Status apt-get速查表 apt-get是Debian Linux用于管理软件包的命令行工具：
 安装/管理单个软件包 升级软件包 打安全补丁 使Debian系统更新到最新状态 下载源.deb文件 前端有很多GUI和应用  |语法|描述|示例 |&amp;mdash; |apt-get install {package}|安装新的软件包。如果软件包已安装则尝试更新到最新版本|apt-get install zipapt-get install lsof samba mysql-client |apt-get remove {package}|移除一个已安装的软件包，保留配置文件|apt-get remove zip |apt-get &amp;ndash;purge remove {package}|移除一个已安装的软件包及配置|apt-get &amp;ndash;purge remove mysql-server |apt-get updateapt-get upgrade|重新同步包索引文件并升级Debian Linux系统及安全更新 (需要访问因特网)|apt-get updateapt-get upgrade |apt-get updateapt-get dist-upgrade|经常用于升级Debian分发。例如Woody升级到Sarge，&amp;rsquo;dist-upgrade&amp;rsquo;除了执行升级功能，也只能处理包新版本改变了的依赖关系；apt-get具有&amp;rdquo;智能&amp;rdquo;冲突解决系统，在必要时会试图以重要性较小的包为代价升级最重要的包。|apt-get updateapt-get dist-upgrade
apt-cache速查表 |语法|描述|示例 |&amp;mdash; |apt-cache pkgnames|列举所有有效包| |apt-cache depends {package}|检查包的依赖|apt-cache depends lsofapt-cache depends mysql-server |apt-cache search {progname}|通过关键字搜索包列表|apt-cache search mysqlapt-cache search &amp;ldquo;Network Security&amp;rdquo; |apt-cache show {package}|显示包的信息|apt-cache show screen
此外，也可以使用下列GUI工具： - aptitude：Debian GNU/Linux包系统基于文本的界面接口。- synaptic：APT的前端基于图形的界面接口。参考 dpkg Man Page
apt-get Man Page
apt-cache Man Page
aptitude Man Page
synaptic Man Page
dpkg command cheat sheet for Debian Linux
apt-get command cheat sheet for Debian Linux
Jon&amp;rsquo;s Apt-Get Cheat Sheet
Ubuntu Cheat Sheet - Package Management
[](http://blog.packagecloud.io/eng/2015/03/30/apt-cheat-sheet/)
</content>
    </entry>
    
     <entry>
        <title>FilenameFilter和FileFilter介绍</title>
        <url>https://mryqu.github.io/post/filenamefilter%E5%92%8Cfilefilter%E4%BB%8B%E7%BB%8D/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>file</tag><tag>list</tag><tag>filenamefilter</tag><tag>filefilter</tag>
        </tags>
        <content type="html">  FilenameFilter和FileFilter说明 java.io.File类提供了四个方法用于列举某个路径下的文件和目录，但不会递归列举子目录下的内容。其中两个是列举路径下的所有文件和目录。 - String[] list() - File[] listFiles()另外两个是列举路径满足指定过滤器的文件和目录。 - String[] list(FilenameFilter filter) - File[] listFiles(FileFilter filter)
示例 要求：返回当前目录下所有以yqu开头且以.tmp结尾的文件和目录。
代码 package com.yqu.file; import java.io.File; import java.io.FileFilter; import java.io.FilenameFilter; public class HelloFileListing { public static void main(String[] args) { File f = new File(&amp;quot;c:/test&amp;quot;); System.out.println(&amp;quot;\n====Method listFiles() example====&amp;quot;); File[] files = f.listFiles(); for (File fl : files) { String type = fl.isFile() ? &amp;quot;File: &amp;quot; : &amp;quot;Directory: &amp;quot;; try { System.out.println(type &#43; fl.getCanonicalPath()); } catch (Exception e) { e.printStackTrace(); } } System.out.println(&amp;quot;\n====Method listFiles(FileFilter) example====&amp;quot;); files = f.listFiles(new HelloFileFilter(&amp;quot;yqu&amp;quot;, &amp;quot;.tmp&amp;quot;)); for (File fl : files) { String type = fl.isFile() ? &amp;quot;File: &amp;quot; : &amp;quot;Directory: &amp;quot;; try { System.out.println(type &#43; fl.getCanonicalPath()); } catch (Exception e) { e.printStackTrace(); } } System.out.println(&amp;quot;\n====Method list(FilenameFilter) example====&amp;quot;); String[] fileNames = f.list(new HelloFilenameFilter(&amp;quot;yqu&amp;quot;, &amp;quot;.tmp&amp;quot;)); for (String fileName : fileNames) { System.out.println(fileName); } } public static class HelloFilenameFilter implements FilenameFilter { private String prefix; private String suffix; public HelloFilenameFilter(String prefix, String suffix) { this.prefix = prefix; this.suffix = suffix; } public boolean accept(File dir, String name) { return name.startsWith(prefix) &amp;amp;&amp;amp; name.endsWith(suffix); } } public static class HelloFileFilter implements FileFilter { private String prefix; private String suffix; public HelloFileFilter(String prefix, String suffix) { this.prefix = prefix; this.suffix = suffix; } public boolean accept(File pathname) { String name = pathname.getName(); return name.startsWith(prefix) &amp;amp;&amp;amp; name.endsWith(suffix); } } }  输出 ====Method listFiles() example==== File: C:\test\222.zip Directory: C:\test\Next Directory: C:\test\QE Directory: C:\test\yqu123.tmp File: C:\test\yqu321.tmp ====Method listFiles(FileFilter) example==== Directory: C:\test\yqu123.tmp File: C:\test\yqu321.tmp ====Method list(FilenameFilter) example==== yqu123.tmp yqu321.tmp  </content>
    </entry>
    
     <entry>
        <title>玩玩无序列表ul和有序列表ol</title>
        <url>https://mryqu.github.io/post/%E7%8E%A9%E7%8E%A9%E6%97%A0%E5%BA%8F%E5%88%97%E8%A1%A8ul%E5%92%8C%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8ol/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>html</tag><tag>css</tag><tag>列表</tag><tag>ul</tag><tag>ol</tag>
        </tags>
        <content type="html">  写博客有时候用到列表，这里好好玩一玩。
CSS list-style-type 属性 属性介绍 |值|描述 |&amp;mdash;&amp;ndash; |none|无标记。 |disc|默认。标记是实心圆。 |circle|标记是空心圆。 |square|标记是实心方块。 |decimal|标记是数字。 |decimal-leading-zero|0开头的数字标记。(01, 02, 03, 等。) |lower-roman|小写罗马数字(i, ii, iii, iv, v, 等。) |upper-roman|大写罗马数字(I, II, III, IV, V, 等。) |lower-alpha|小写英文字母The marker is lower-alpha (a, b, c, d, e, 等。) |upper-alpha|大写英文字母The marker is upper-alpha (A, B, C, D, E, 等。) |lower-greek|小写希腊字母(alpha, beta, gamma, 等。) |lower-latin|小写拉丁字母(a, b, c, d, e, 等。) |upper-latin|大写拉丁字母(A, B, C, D, E, 等。) |hebrew|传统的希伯来编号方式 |armenian|传统的亚美尼亚编号方式 |georgian|传统的乔治亚编号方式(an, ban, gan, 等。) |cjk-ideographic|简单的表意数字 |hiragana|标记是：a, i, u, e, o, ka, ki, 等。（日文片假名） |katakana|标记是：A, I, U, E, O, KA, KI, 等。（日文片假名） |hiragana-iroha|标记是：i, ro, ha, ni, ho, he, to, 等。（日文片假名） |katakana-iroha|标记是：I, RO, HA, NI, HO, HE, TO, 等。（日文片假名） |inherit|规定应该从父元素继承 list-style-type 属性的值。
示例 &amp;lt;ul&amp;gt; &amp;lt;li style=&amp;quot;list-style: none !important;&amp;quot;&amp;gt;none&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: disc !important;&amp;quot;&amp;gt;disc&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: circle !important;&amp;quot;&amp;gt;circle&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: square !important;&amp;quot;&amp;gt;square&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: decimal !important;&amp;quot;&amp;gt;decimal&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: decimal-leading-zero !important;&amp;quot;&amp;gt;decimal-leading-zero&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: lower-roman !important;&amp;quot;&amp;gt;lower-roman&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: upper-roman !important;&amp;quot;&amp;gt;upper-roman&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: lower-alpha !important;&amp;quot;&amp;gt;lower-alpha&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: upper-alpha !important;&amp;quot;&amp;gt;upper-alpha&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: lower-greek !important;&amp;quot;&amp;gt;lower-greek&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: lower-latin !important;&amp;quot;&amp;gt;lower-latin&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: upper-latin !important;&amp;quot;&amp;gt;upper-latin&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: hebrew !important;&amp;quot;&amp;gt;hebrew&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: armenian !important;&amp;quot;&amp;gt;armenian&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: georgian !important;&amp;quot;&amp;gt;georgian&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: cjk-ideographic !important;&amp;quot;&amp;gt;cjk-ideographic&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: hiragana !important;&amp;quot;&amp;gt;hiragana&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: katakana !important;&amp;quot;&amp;gt;katakana&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: hiragana-iroha !important;&amp;quot;&amp;gt;hiragana-iroha&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: katakana-iroha !important;&amp;quot;&amp;gt;katakana-iroha&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style: inherit !important;&amp;quot;&amp;gt;inherit&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt;  nonedisccirclesquaredecimaldecimal-leading-zerolower-romanupper-romanlower-alphaupper-alphalower-greeklower-latinupper-latinhebrewarmeniangeorgiancjk-ideographichiraganakatakanahiragana-irohakatakana-irohainherit CSS list-style-position 属性 属性介绍 |值|描述 |&amp;mdash;&amp;ndash; |inside|列表项目标记放置在文本以内，且环绕文本根据标记对齐。 |outside|默认值。保持标记位于文本的左侧。列表项目标记放置在文本以外，且环绕文本不根据标记对齐。 |inherit|规定应该从父元素继承 list-style-position 属性的值。
示例 &amp;lt;ul&amp;gt; &amp;lt;li style=&amp;quot;list-style-position: inside !important;&amp;quot;&amp;gt;inside&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style-position: outside !important;&amp;quot;&amp;gt;outside&amp;lt;/li&amp;gt; &amp;lt;li style=&amp;quot;list-style-position: inherit !important;&amp;quot;&amp;gt;inherit&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt;  insideoutsideinherit CSS list-style-image 属性 属性介绍 |值|描述 |&amp;mdash;&amp;ndash; |URL|图像的路径。 |none|默认。无图形被显示。 |inherit|规定应该从父元素继承 list-style-image 属性的值。
</content>
    </entry>
    
     <entry>
        <title>数据库非XA驱动和XA驱动列表</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9D%9Exa%E9%A9%B1%E5%8A%A8%E5%92%8Cxa%E9%A9%B1%E5%8A%A8%E5%88%97%E8%A1%A8/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>non-xa</tag><tag>xa</tag><tag>jdbc</tag><tag>数据库驱动</tag><tag>transaction</tag>
        </tags>
        <content type="html"> 数据库非XA驱动XA驱动Postgresorg.postgresql.Driverorg.postgresql.xa.PGXADataSourceMySQLcom.mysql.jdbc.Drivercom.mysql.jdbc.jdbc2.optional.MysqlXADataSourceOracleoracle.jdbc.OracleDriveroracle.jdbc.xa.client.OracleXADataSourceDB2com.ibm.db2.jcc.DB2Drivercom.ibm.db2.jcc.DB2XADataSourceSQL Servercom.microsoft.sqlserver.jdbc.SQLServerDrivercom.microsoft.sqlserver.jdbc.SQLServerXADataSourceTeradatacom.teradata.jdbc.TeraDriver
 </content>
    </entry>
    
     <entry>
        <title>vFabric Web Server配置：多tc Server负载均衡</title>
        <url>https://mryqu.github.io/post/vfabric_web_server%E9%85%8D%E7%BD%AE%E5%A4%9Atc_server%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Web Application Server</category>
        </categories>
        <tags>
          <tag>vfabric</tag><tag>webserver</tag><tag>loadbalance</tag><tag>tcserver</tag><tag>mod_proxy</tag>
        </tags>
        <content type="html">  HTTP请求需要分发到tc Server集群成员。为了实现这一目的，可以采用硬件负载均衡硬件或软件(Apache HTTP Server、vFabric Web Server和Microsoft IIS)。本文介绍用vFabric Web Server的mod_proxy模块实现tcServer负载均衡。
vFabric Web Server:加载mod_proxy动态共享对象 在httpsd.conf文件对下列行进行添加或去除注释: LoadModule proxy_module &amp;quot;VFWS-INSTALL/httpd-2.2/modules/mod_proxy.so&amp;quot; LoadModule proxy_http_module &amp;quot;VFWS-INSTALL/httpd-2.2/modules/mod_proxy_http.so&amp;quot; LoadModule proxy_balancer_module &amp;quot;VFWS-INSTALL/httpd-2.2/modules/mod_proxy_balancer.so&amp;quot;  vFabric Web Server:创建额外的配置文件 在vFabric Web Server实例的conf目录创建mod_proxy.conf: # Enable capturing of proxy statistics ProxyStatus on SetHandler balancer-manager Order Deny,Allow Deny from all Allow from 127.0.0.1 # These apps aren&#39;t clustered -- requests go to dedicated server ProxyPass /my-app1 balancer://my-standalone/my-app1 ProxyPass /my-app2 balancer://my-standalone/my-app2 # Clustered apps get directed to loadbalanced worker ProxyPass /my-app3 balancer://my-balancer/my-app3 ProxyPass /my-app4 balancer://my-balancer/my-app4 # Standalone &amp;quot;balancer&amp;quot; for standalone apps that aren&#39;t clustered BalancerMember http://MYSERVER1:8080 # Load balanced &amp;quot;balancer&amp;quot; for clustered apps BalancerMember http://MYSERVER1:8080 route=aGVsbG8gcWUh_MyServer1_1 loadfactor=1 BalancerMember http://MYSERVER1:8180 route=aGVsbG8gcWUh_MyServer1_2 loadfactor=2 ProxySet lbmethod=byrequests stickysession=aGVsbG8gcWUh_Cluster1|aGVsbG8gcWUh_cluster1 # Configure cache timeouts for static content AddType text/javascript .js # Configure browser cache timeouts for static content ExpiresActive On ExpiresByType image/gif &amp;quot;access plus 1 hour&amp;quot; ExpiresByType text/javascript &amp;quot;access plus 1 hour&amp;quot; ExpiresByType image/x-icon &amp;quot;access plus 1 hour&amp;quot; ExpiresByType text/css &amp;quot;access plus 1 hour&amp;quot;  tc Server:设置Engine的jvmRoute 对两个tc Server下的两个tc Server实例的Server进行配置，文件为:
/MyServer1_1/conf/server.xml /MyServer1_2/conf/server.xml	 </content>
    </entry>
    
     <entry>
        <title>vFabric Web Server 5.2模块和库</title>
        <url>https://mryqu.github.io/post/vfabric_web_server_5.2%E6%A8%A1%E5%9D%97%E5%92%8C%E5%BA%93/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Web Application Server</category>
        </categories>
        <tags>
          <tag>vfabric</tag><tag>apache</tag><tag>webserver</tag><tag>httpd</tag><tag>module</tag>
        </tags>
        <content type="html">  vFabric Web Server 是VMware的vFabric套件中的Web服务器和负载均衡组件。vFabric WebServer 5.2基于Apache HTTP Server 2.2版本。 其大部分模块可见链接
核心功能和多处理模块 |模块|介绍 |&amp;mdash;&amp;ndash; |core|Apache HTTP服务器核心提供的功能，始终有效。 |mpm_common|收集了被多个多路处理模块(MPM)实现的公共指令。 |beos|专门针对BeOS优化过的多路处理模块(MPM) |event|一个标准workerMPM的实验性变种。 |mpm_netware|专门为Novell NetWare优化的线程化的多路处理模块(MPM) |mpmt_os2|专门针对OS/2优化过的混合多进程多线程多路处理模块(MPM) |prefork|一个非线程型的、预派生的MPM |mpm_winnt|用于Windows NT/2000/XP/2003 系列的MPM |worker|线程型的MPM，实现了一个混合的多线程多处理MPM，允许一个子进程中包含多个线程。
其它普通模块 |模块|介绍 |&amp;mdash;&amp;ndash; |mod_actions|根据特定的媒体类型或请求方法，激活特定的CGI脚本 |mod_alias|提供从文件系统的不同部分到文档树的映射和URL重定向 |mod_asis|发送自己包含HTTP头内容的文件 |mod_auth_basic|使用基本认证 |mod_auth_digest|使用MD5摘要认证(更安全，但是只有最新的浏览器才支持) |mod_authn_alias|基于实际认证支持者创建扩展的认证支持者，并为它起一个别名以便于引用 |mod_authn_anon|提供匿名用户认证支持 |mod_authn_dbd|使用SQL数据库为认证提供支持 |mod_authn_dbm|使用DBM数据库为认证提供支持 |mod_authn_default|在未正确配置认证模块的情况下简单拒绝一切认证信息 |mod_authn_file|使用纯文本文件为认证提供支持 |mod_authnz_ldap|允许使用一个LDAP目录存储用户名和密码数据库来执行基本认证和授权 |mod_authz_dbm|使用DBM数据库文件为组提供授权支持 |mod_authz_default|在未正确配置授权支持模块的情况下简单拒绝一切授权请求 |mod_authz_groupfile|使用纯文本文件为组提供授权支持 |mod_authz_host|供基于主机名、IP地址、请求特征的访问控制 |mod_authz_owner|基于文件的所有者进行授权 |mod_authz_user|基于每个用户提供授权支持 |mod_autoindex|自动对目录中的内容生成列表，类似于&amp;rdquo;ls&amp;rdquo;或&amp;rdquo;dir&amp;rdquo;命令 |mod_cache|基于URI键的内容动态缓冲(内存或磁盘) |mod_cern_meta|允许Apache使用CERN httpd元文件，从而可以在发送文件时对头进行修改 |mod_cgi|在非线程型MPM(prefork)上提供对CGI脚本执行的支持 |mod_cgid|在线程型MPM(worker)上用一个外部CGI守护进程执行CGI脚本 |mod_charset_lite|允许对页面进行字符集转换 |mod_dav|允许Apache提供DAV协议支持 |mod_dav_fs|为mod_dav访问服务器上的文件系统提供支持 |mod_dav_lock|为mod_dav锁定服务器上的文件提供支持 |mod_dbd|管理SQL数据库连接，为需要数据库功能的模块提供支持 |mod_deflate|压缩发送给客户端的内容 |mod_dir|指定目录索引文件以及为目录提供&amp;rdquo;尾斜杠&amp;rdquo;重定向 |mod_disk_cache|基于磁盘的缓冲管理器 |mod_dumpio|将所有I/O操作转储到错误日志中 |mod_echo|一个很简单的协议演示模块 |mod_env|允许Apache修改或清除传送到CGI脚本和SSI页面的环境变量 |mod_example|一个很简单的Apache模块API演示模块 |mod_expires|允许通过配置文件控制HTTP的&amp;rdquo;Expires:&amp;ldquo;和&amp;rdquo;Cache-Control:&amp;ldquo;头内容 |mod_ext_filter|使用外部程序作为过滤器 |mod_file_cache|提供文件描述符缓存支持，从而提高Apache性能 |mod_filter|根据上下文实际情况对输出过滤器进行动态配置 |mod_headers|允许通过配置文件控制任意的HTTP请求和应答头信息 |mod_ident|实现RFC1413规定的ident查找 |mod_imagemap|处理服务器端图像映射 |mod_include|实现服务端包含文档(SSI)处理 |mod_info|生成Apache配置情况的Web页面 |mod_isapi|仅限于在Windows平台上实现ISAPI扩展 |mod_ldap|为其它LDAP模块提供LDAP连接池和结果缓冲服务 |mod_log_config|允许记录日志和定制日志文件格式 |mod_log_forensic|实现&amp;rdquo;对比日志&amp;rdquo;，即在请求被处理之前和处理完成之后进行两次记录 |mod_logio|对每个请求的输入/输出字节数以及HTTP头进行日志记录 |mod_mem_cache|基于内存的缓冲管理器 |mod_mime|根据文件扩展名决定应答的行为(处理器/过滤器)和内容(MIME类型/语言/字符集/编码) |mod_mime_magic|通过读取部分文件内容自动猜测文件的MIME类型 |mod_negotiation|提供内容协商支持 |mod_nw_ssl|仅限于在NetWare平台上实现SSL加密支持 |mod_proxy|提供HTTP/1.1的代理/网关功能支持 |mod_proxy_ajp|mod_proxy的扩展，提供Apache JServ Protocol支持 |mod_proxy_balancer|mod_proxy的扩展，提供负载平衡支持 |mod_proxy_connect|mod_proxy的扩展，提供对处理HTTP CONNECT方法的支持 |mod_proxy_ftp|mod_proxy的FTP支持模块 |mod_proxy_http|mod_proxy的HTTP支持模块 |mod_proxy_scgi|mod_proxy的SCGI网关支持模块 |mod_reqtimeout|设置接受请求的超时或最小数据速率 |mod_rewrite|一个基于一定规则的实时重写URL请求的引擎 |mod_setenvif|根据客户端请求头字段设置环境变量 |mod_so|允许运行时加载DSO模块 |mod_speling|自动纠正URL中的拼写错误 |mod_ssl|使用安全套接字层(SSL)和传输层安全(TLS)协议实现高强度加密传输 |mod_status|生成描述服务器状态的Web页面 |mod_substitute|提供对响应体执行正则表达式和固定字符串替换机制 |mod_suexec|使用与调用web服务器的用户不同的用户身份来运行CGI和SSI程序 |mod_unique_id|为每个请求生成唯一的标识以便跟踪 |mod_userdir|允许用户从自己的主目录中提供页面(使用&amp;rdquo;/~username&amp;rdquo;) |mod_usertrack|使用Session跟踪用户(会发送很多Cookie)，以记录用户的点击流 |mod_version|提供基于版本的配置段支持 |mod_vhost_alias|提供大批量虚拟主机的动态配置支持
少量模块和库收集在下面介绍：
|模块和库|介绍 |&amp;mdash;&amp;ndash; |mod_bmx|用于提供监控支持的Hyperic插件模块 |mod_fcgid|CGI脚本执行的支持模块，较mod_cgi性能更高 |mod_jk|Tomcat连接器模块 |libexpat|提供XML解析器的函数库 |libpcre|与Perl兼容的正则表达式函数库 |libaprlibaprutil|Apache Portable Runtime |libiconv|提供在多种国际编码格式之间进行文本内码转换的函数库 |zlib|提供数据压缩用的函式库
</content>
    </entry>
    
     <entry>
        <title>vFabric Web Server控制台命令</title>
        <url>https://mryqu.github.io/post/vfabric_web_server%E6%8E%A7%E5%88%B6%E5%8F%B0%E5%91%BD%E4%BB%A4/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Web Application Server</category>
        </categories>
        <tags>
          <tag>vfabric</tag><tag>apache</tag><tag>httpd</tag><tag>webserver</tag><tag>httpdctl</tag>
        </tags>
        <content type="html">  检查并设置PowerShell 在Windows中，vFabric WebServer控制台命令需要在PowerShell下执行。默认状态下，PowerShell的脚本处理是被禁止的。 通过如下命令检查当前的PowerShell设置:
PS prompt&amp;gt; Get-ExecutionPolicy  如果命令返回Restricted,这意味着PowerShell还没有使能。通过执行如下命令使它允许最低限度执行本地脚本：
PS prompt&amp;gt; Set-ExecutionPolicy RemoteSigned  按照需要设置不同的执行策略并使用组和用户策略使能PowerShell。通常，仅有管理员使用vFabric WebServer脚本，因此RemoteSigned执行策略在大多情况下是足够的。
通过执行如下命令设置编码为UTF-8，以便更好地显示httpctl输出、更容易检查日志文件。
PS prompt&amp;gt; chcp 65001  使用vFabric Web Server控制台 可以使用httpdctl 脚本控制vFabric Web Server实例，其命令如下:
|命令|描述 |&amp;mdash;&amp;ndash; |start|启动vFabric Web Server实例。如果实例已启动，该命令返回错误。 |stop|强制停止vFabric Web Server实例。当前所有打开的连接将中断。 |gracefulstop|优雅停止vFabric Web Server实例，脚本会等到所有打开的连接关闭后才停止vFabric Web Server实例。 |restart|重启实例。如果实例之前没有启动，脚本会启动实例，此外也会在启动实例前运行configtest。 |graceful|优雅重启实例。与restart命令的区别在于当前打开的连接不会中断，副作用就是老的日志文件不会立即关闭。 |status|显示实例状态信息，例如是否运行中、运行中的进程标识符 (PID)。 |install|安装实例成Windows或UNIX服务。服务可以用Windows服务控制台、sc命令手动启动或停止，或随Windows自动启动或停止。
install命令可用参数：
服务名: vFabrichttpd_实例名_ 例子: vFabrichttpdmyserver显示名: vFabric httpd 实例名
例子: vFabric httpd myserverUnix下，实例安装成/etc/init.d目录下名为vFabric-httpd-&amp;lt;实例名&amp;gt;的脚本文件。服务随Unix自动启动或停止。 |uninstall|卸载作为Windows或UNIX服务的实例。Windows下，实例会从服务注册表中删除。Unix下，命令会删除/etc/init.d/vFabric-httpd-_instance-name_脚本文件。 |configtest|对例如conf/httpd.conf之类的配置文件运行语法检查。脚本解析配置文件，如果语法正确返回OK，否则返回特定语法错误的详细信息。
示例如下：
PS C:\sas\Config\Lev1\Web\WebServer\bin&amp;gt; .\httpdctl.ps1 status httpdctl.ps1 - manage the SAS [Config-Lev1] httpd - WebServer server instance Copyright c 2012 VMware, Inc. All rights reserved. SAS [Config-Lev1] httpd - WebServer (pid 19924) RUNNING  </content>
    </entry>
    
     <entry>
        <title>开源事务管理器列表</title>
        <url>https://mryqu.github.io/post/%E5%BC%80%E6%BA%90%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%E5%99%A8%E5%88%97%E8%A1%A8/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>开源</tag><tag>transaction</tag><tag>jotm</tag><tag>atomikos</tag><tag>narayana</tag>
        </tags>
        <content type="html"> WebLogic、WebSphere和JBoss这些Web应用服务器都自带事务管理器，而Tomcat和tcServer需要额外使用第三方事务管理器。公司选择了AtomikosTransactionEssentials作为JTA/XA提供者，这里列一下当前使用较多的开源事务管理器。
|事务管理器|开源许可证|当前版本|介绍 |&amp;mdash;&amp;ndash; |JOTM|BSD样式|2.1.9 (2010-1-14)|JOTM(Java Open TransactionManager)是由ObjectWeb协会开发的功能完整的且资源开放的独立的事务管理器。 它提供了JAVA 应用程序的事务支持，而且与JTA（JAVA事务API）兼容。
JOTM内嵌了一些开源项目。例如CAROL用于事务上下文传播；XAPool是一个XA兼容的JDBC连接池；HOWL是用于事务恢复的日志。 |Bitronix JTA Transaction Manager|LGPL|2.1.4 (2013-9-15)|Bitronix Transaction Manager (BTM)是JTA 1.1 API的一个简单但完整的实现。 |SimpleJTA|Apache 2.0|2.02 (2007-4-10)|SimpleJTA实现了一个单独的JTA兼容事务处理器，仅支持Oracle 9i和ApacheDerby数据库，不支持JTS。 |Atomikos TransactionsEssentials|Apache 2.0|3.9.0.M1 (2013-6-8)|Atomikos分两个：一个是开源的TransactionEssentials，一个是商业的ExtremeTransactions。（功能对比）
Atomikos TransactionsEssentials是对JDBC/XA池,JMS/XA池和JTA/XA提供基本支持的开源事务处理系统。 |Narayana(JBossTS)|LGPL v2.1|5.0.0.M6 (2013-10-11)|Narayana（JBossTS）前身是最初纽卡斯尔大学在1986到1995开发的Arjuna系统，后由JBoss从Arjuna和HP手中收购并开源的。
它支持下列事务处理协议标准:JTA、JTS、Web服务事务、REST事务、STM、XATMI/TX
Stack Overflow有一篇文章对上面的一些事务管理器进行了对比： - JOTM 用户抱怨的比较多 - GeronimoTM/Jencks(Jencks是Apache Geronimo中使用的JCA容器，并使用ApacheGeronimo的事务管理器) 缺乏文档。 - SimpleJTA 没有实现JTS (Java Transaction Service)而且不是活跃的。 - Bitronix 有不错的文档但是不提供技术支持。 - Atomikos 是一个另人钦佩的产品。有丰富的文档，而且提供技术支持。 - JBossTS 从收购通告可见肯定是一个成熟的产品，而且提供技术支持。
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 源码分析mapred.mapper.new-api/mapred.reducer.new-api设置与区别</title>
        <url>https://mryqu.github.io/post/hadoop_%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90mapred.mapper.new-api%E5%92%8Cmapred.reducer.new-api%E8%AE%BE%E7%BD%AE%E4%B8%8E%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>mapper</tag><tag>reducer</tag><tag>new-api</tag>
        </tags>
        <content type="html">  即mapred和mapreduce包的区别后，本文再次从源码角度分析新老API（mapred.mapper.new-api/ mapred.reducer.new-api）的设置与区别。 mapred.mapper.new-api /mapred.reducer.new-api这两个参数很少显式去设置，默认为false，即使用mapred包下的老API。 不过MapReduce框架也会去自动识别应该使用老API还是新API。作业提交有两种方式，异步方式用submit，同步方式用waitForCompletion。不过org.apache.hadoop.mapreduce.Job.waitForCompletion(boolean)里调用了org.apache.hadoop.mapreduce.Job.submit()方法，submit方法又调用了org.apache.hadoop.mapreduce.Job.setUseNewAPI()方法。setUseNewAPI方法里面对新老API做了判断： - 是否设置了mapred.mapper.class属性，则mapred.mapper.new-api为true，否则为false。说白了就是用org.apache.hadoop.mapreduce.Job.setMapperClass(Class)还是org.apache.hadoop.mapred.JobConf.setMapperClass(Class)设置的Mapper，前者设置的是mapreduce.job.map.class属性，后者设置的是mapred.mapper.class属性。 - 如果mapreduce.job.reduces属性值不为0，则看是否设置了mapred.reducer.class属性，则mapred.reducer.new-api为true，否则为false。说白了就是用org.apache.hadoop.mapreduce.Job.setReducerClass(Class)还是org.apache.hadoop.mapred.JobConf.setReducerClass(Class)设置的Mapper，前者设置的是mapreduce.job.reducer.class属性，后者设置的是mapred.reducer.class属性。
new-api相关区别 使用new-api不使用new-api不允许设置下列属性：
mapred.input.format.classmapred.mapper.classmapred.partitioner.classmapred.reducer.classmapred.output.format.class不允许设置下列属性：
mapreduce.job.inputformat.classmapreduce.job.map.classmapreduce.job.partitioner.classmapreduce.job.reducer.classmapreduce.job.outputformat.class使用下列类或接口的实现：
o.a.h.conf.Configurationo.a.h.mapreduce.Mapper抽象类o.a.h.mapreduce.Reducer抽象类o.a.h.mapreduce.OutputFormat抽象类o.a.h.mapreduce.OutputCommitter抽象类o.a.h.mapreduce.TaskIDo.a.h.mapreduce.TaskAttemptIDo.a.h.mapreduce.TaskAttemptContext接口o.a.h.mapreduce.InputFormat抽象类o.a.h.mapreduce.InputSplit抽象类使用下列类或接口的实现：
o.a.h.mapred.JobConfo.a.h.mapred.Mapper接口o.a.h.mapred.Reducer接口o.a.h.mapred.OutputFormat接口o.a.h.mapred.OutputCommitter抽象类o.a.h.mapred.TaskIDo.a.h.mapred.TaskAttemptIDo.a.h.mapred.TaskAttemptContext接口o.a.h.mapred.InputFormat接口o.a.h.mapred.InputSplit接口使用方法：
o.a.h.mapred.MapTask.runNewMappero.a.h.mapreduce.JobSubmitter.writeNewSplitso.a.h.mapred.ReduceTask.runNewReducer使用方法：
o.a.h.mapred.MapTask.runOldMappero.a.h.mapreduce.JobSubmitter.writeOldSplitso.a.h.mapred.ReduceTask.runOldReducer </content>
    </entry>
    
     <entry>
        <title>Servlet URL映射模式</title>
        <url>https://mryqu.github.io/post/servlet_url%E6%98%A0%E5%B0%84%E6%A8%A1%E5%BC%8F/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>servlet</tag><tag>url</tag><tag>mapping</tag><tag>pattern</tag><tag>匹配</tag>
        </tags>
        <content type="html">  Servlet 2.5规范中的映射规则:  完全匹配URL 匹配通配符路径 匹配扩展名 匹配默认servlet  特殊URL模式: url-pattern:/* servlet上的/* 会压制所有其他servlet。无论什么请求都会被该servlet处理。这是一种不好的URL模式。通常，仅将/* 用于过滤器。它能通过调用FilterChain#doFilter()让请求继续由监听另外一个特定URL模式的任何servlet处理。
url-pattern:/ / 不会压制其他servlet。它仅会替换servlet容器内建的默认servlet，用于无法匹配任何已注册servlet的所有请求。一般仅调用在静态资源(CSS/JS/image/etc)和列举目录上。servlet容器内建默认servlet也能处理HTTP缓存请求、媒体（音视频）流和文件重新下载。由于必须负责默认servlet的所有任务，工作量不小，通常不会想要替换默认servlet。这也是一种不好的URL模式。关于为什么JSP页面不会调用这个servlet，是因为servlet容器的内建JSPservlet默认映射到*.jsp并被调用。
url-pattern: 这也有一个空字符串URL模式。当上下文根被请求时会被调用。这不同于welcome-file方法，因为它对任何子目录请求不会被调用，而welcome-file方法对任何局部有效但没有匹配上servlet的请求都会被调用。这更像需要“主页servlet”所要用到的URL模式。.
</content>
    </entry>
    
     <entry>
        <title>tc Server与Atomikos集成</title>
        <url>https://mryqu.github.io/post/tc_server%E4%B8%8Eatomikos%E9%9B%86%E6%88%90/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>tcserver</tag><tag>atomikos</tag><tag>tomcat</tag><tag>database</tag><tag>jms</tag>
        </tags>
        <content type="html">  tc Server是基于Apache的Tomcat的，Atomikos有篇文档介绍Tomcat与Atomikos集成，同样适用于tcServer。
Atomikos安装配置 复制JAR文件 复制下列JAR文件到TCS_HOME/lib目录： - atomikos-util.jar - transactions.jar - transactions-api.jar - transactions-jdbc.jar - transactions-jdbc-deprecated.jar - transactions-jms.jar - transactions-jms-deprecated.jar - transactions-jta.jar - transactions-osgi.jar - geronimo-jms_1.1_spec.jar - geronimo-jta_1.0.1B_spec.jar - JDBC驱动 - 如果使用Hibernate：transactions-hibernate3.jar和/或transactions-hibernate2.jar
复制Atomikos配置文件 将jta.properties复制到TCS_HOME/lib目录并做适当修改。
com.atomikos.icatch.service=com.atomikos.icatch.standalone.UserTransactionServiceFactory com.atomikos.icatch.console_file_limit=10240000 com.atomikos.icatch.output_dir=${catalina.base}/logs com.atomikos.icatch.log_base_dir=${catalina.base}/logs com.atomikos.icatch.max_actives=-1 com.atomikos.icatch.default_jta_timeout=3600000 com.atomikos.icatch.max_timeout=3600000 com.atomikos.icatch.tm_unique_name=tm com.atomikos.icatch.console_log_level=WARN com.atomikos.icatch.force_shutdown_on_vm_exit=false  复制类文件 创建AtomikosLifecycleListener和BeanFactory两个类并放置在TCS_HOME/lib目录： 创建用于Atomikos的tc Server生命期监视器：当tcServer实例启动时，创建UserTransactionManager并初始化；当tcServer关闭时，关闭UserTransactionManager。
package com.atomikos.tomcat; import org.apache.catalina.Lifecycle; import org.apache.catalina.LifecycleEvent; import org.apache.catalina.LifecycleListener; import com.atomikos.icatch.jta.UserTransactionManager; public class AtomikosLifecycleListener implements LifecycleListener { private UserTransactionManager utm; public void lifecycleEvent(LifecycleEvent event) { try { if (Lifecycle.START_EVENT.equals(event.getType())) { if (utm == null) { utm = new UserTransactionManager(); } utm.init(); } else if (Lifecycle.AFTER_STOP_EVENT.equals(event.getType())) { if (utm != null) { utm.close(); } } } catch (Exception e) {} } }  创建用于Atomikos的tc Server JNDIbean工厂类：为数据库数据源创建AtomikosDataSourceBean，为JMS数据源创建AtomikosConnectionFactoryBean
package com.atomikos.tomcat; import java.lang.reflect.Method; import java.util.Enumeration; import java.util.Hashtable; import javax.naming.Context; import javax.naming.Name; import javax.naming.NamingException; import javax.naming.RefAddr; import javax.naming.Reference; import javax.naming.spi.ObjectFactory; import com.atomikos.beans.PropertyUtils; public class BeanFactory implements ObjectFactory { public Object getObjectInstance(Object obj, Name name, Context nameCtx, Hashtable environment) throws NamingException { if ((obj == null) || !(obj instanceof Reference)) { return null; } Reference ref = (Reference)obj; String beanClassName = ref.getClassName(); try { Class&amp;lt;?&amp;gt; beanClass = Class.forName(beanClassName); System.out.println(&amp;quot;Creating instance of &amp;quot; &#43; beanClassName); Object bean = beanClass.newInstance(); Enumeration&amp;lt;RefAddr&amp;gt; properties = ref.getAll(); while (properties.hasMoreElements()) { RefAddr refAddr = properties.nextElement(); String propertyName = refAddr.getType(); if ((!propertyName.equals(&amp;quot;factory&amp;quot;)) &amp;amp;&amp;amp; (!propertyName.equals(&amp;quot;singleton&amp;quot;)) &amp;amp;&amp;amp; (!propertyName.equals(&amp;quot;description&amp;quot;)) &amp;amp;&amp;amp; (!propertyName.equals(&amp;quot;scope&amp;quot;)) &amp;amp;&amp;amp; (!propertyName.equals(&amp;quot;auth&amp;quot;))) { String propertyValue = (String)refAddr.getContent(); System.out.println(&amp;quot;Setting &amp;quot; &#43; propertyName &#43; &amp;quot;=&amp;quot; &#43; propertyValue); PropertyUtils.setProperty(bean, propertyName, propertyValue); } } System.out.println(&amp;quot;Initializing &amp;quot; &#43; bean); // initialize the bean Method method = beanClass.getMethod(&amp;quot;init&amp;quot;, (Class&amp;lt;?&amp;gt;[])null); method.invoke(bean, (Object[]) null); return bean; } catch (Exception e) { System.out.println(e.getClass().getName() &#43; &amp;quot;: &amp;quot; &#43; e.getMessage()); throw (NamingException)(new NamingException(&amp;quot;Error creating instance of: &amp;quot; &#43; beanClassName).initCause(e)); } } }  </content>
    </entry>
    
     <entry>
        <title>dig笔记</title>
        <url>https://mryqu.github.io/post/dig%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>NetWork</category>
        </categories>
        <tags>
          <tag>dig</tag><tag>dns</tag>
        </tags>
        <content type="html">  dig是域信息搜索器（domain informationgroper）。dig和nslookup作用有些类似，都是DNS查询工具。它与nslookup的区别在于：dig使用操作系统的解析库而nslookup使用自己的一套。Bind开发组已经宣布废弃nslookup，但Unix和Linux系统一般都安装了这两个工具。一些专业的DNS管理员在追查DNS问题时，都乐于使用dig命令，是看中了dig设置灵活、输出清晰、功能强大的特点。
如果Ubuntu下没有dig命令，可通过如下命令安装：
sudo apt-get install dnsutils  dig使用说明 dig选项 -b address设置所要询问地址的源 IP 地址。这必须是主机网络接口上的某一合法的地址。-c class缺省查询类（IN forinternet）由选项-c重设。class可以是任何合法类，比如查询Hesiod记录的HS类或查询CHAOSNET记录的CH 类。-f filename使dig在批处理模式下运行，通过从文件filename读取一系列搜索请求加以处理。文件包含许多查询；每行一个。文件中的每一项都应该以和使用命令行接口对dig的查询相同的方法来组织。-h当使用选项-h时，显示一个简短的命令行参数和选项摘要。-k filename要签署由dig发送的DNS查询以及对它们使用事务签名（TSIG）的响应，用选项-k指定TSIG密钥文件。-n缺省情况下，使用IP6.ARPA域和RFC2874定义的二进制标号搜索IPv6地址。为了使用更早的、使用IP6.INT域和nibble标签的RFC1886方法，指定选项-n（nibble）。-p port#如果需要查询一个非标准的端口号，则使用选项-p。port#是dig将发送其查询的端口号，而不是标准的DNS端口号53。该选项可用于测试已在非标准端口号上配置成侦听查询的域名服务器。-t type设置查询类型为type。可以是BIND9支持的任意有效查询类型。缺省查询类型是A，除非提供-x选项来指示一个逆向查询。通过指定AXFR的type可以请求一个区域传输。当需要增量区域传输（IXFR）时，type设置为ixfr=N。增量区域传输将包含自从区域的SOA记录中的序列号改为N之后对区域所做的更改。-x addr逆向查询（将地址映射到名称）可以通过-x选项加以简化。addr是一个以小数点为界的IPv4地址或冒号为界的IPv6地址。当使用这个选项时，无需提供name、class和type参数。dig自动运行类似11.12.13.10.in-addr.arpa的域名查询，并分别设置查询类型和类为PTR和IN。-y name:key您可以通过命令行上的-y选项指定TSIG密钥；name是TSIG密码的名称，key是实际的密码。密码是64位加密字符串，通常由dnssec-keygen（8）生成。当在多用户系统上使用选项-y时应该谨慎，因为密码在ps（1）的输出或shell 的历史文件中可能是可见的。当同时使用dig和TSCG认证时，被查询的名称服务器需要知道密码和解码规则。在BIND中，通过提供正确的密码和named.conf中的服务器声明实现。
dig的查询选项 dig提供查询选项，它影响搜索方式和结果显示。一些在查询请求报头设置或复位标志位，一部分决定显示哪些回复信息，其它的确定超时和重试战略。每个查询选项被带前缀（&#43;）的关键字标识。一些关键字设置或复位一个选项。通常前缀是求反关键字含义的字符串no。其他关键字分配各选项的值，比如超时时间间隔。它们的格式形如&#43;keyword=value。查询选项是： &#43;[no]tcp查询域名服务器时使用[不使用]TCP。缺省行为是使用 UDP，除非是 AXFR 或 IXFR 请求，才使用 TCP连接。&#43;[no]vc查询名称服务器时使用[不使用]TCP。&#43;[no]tcp的备用语法提供了向下兼容。vc代表虚电路。&#43;[no]ignore忽略 UDP 响应的中断，而不是用 TCP 重试。缺省情况运行 TCP 重试。&#43;domain=somename设定包含单个域somename的搜索列表，好像被/etc/resolv.conf中的域伪指令指定，并且启用搜索列表处理，好像给定了&#43;search选项。&#43;[no]search使用[不使用]搜索列表或resolv.conf中的域伪指令（如果有的话）定义的搜索列表。缺省情况不使用搜索列表。&#43;[no]defname不建议看作&#43;[no]search的同义词。&#43;[no]aaonly该选项不做任何事。它用来提供对设置成未实现解析器标志的dig的旧版本的兼容性。&#43;[no]adflag在查询中设置 [不设置]AD（真实数据）位。目前AD位只在响应中有标准含义，而查询中没有，但是出于完整性考虑在查询中这种性能可以设置。&#43;[no]cdflag在查询中设置 [不设置] CD（检查禁用）位。它请求服务器不运行响应信息的DNSSEC合法性。&#43;[no]recursive切换查询中的 RD（要求递归）位设置。在缺省情况下设置该位，也就是说 dig正常情形下发送递归查询。当使用查询选项&#43;nssearch或&#43;trace时，递归自动禁用。&#43;[no]nssearch这个选项被设置时，dig试图寻找包含待搜名称的网段的权威域名服务器，并显示网段中每台域名服务器的SOA记录。&#43;[no]trace切换为待查询名称从根名称服务器开始的代理路径跟踪。缺省情况不使用跟踪。一旦启用跟踪，dig使用迭代查询解析待查询名称。它将按照从根服务器的参照，显示来自每台使用解析查询的服务器的应答。&#43;[no]cmd设定在输出中显示指出dig版本及其所用的查询选项的初始注释。缺省情况下显示注释。&#43;[no]short提供简要答复。缺省值是以冗长格式显示答复信息。&#43;[no]identify当启用&#43;short选项时，显示[或不显示]提供应答的IP地址和端口号。如果请求简短格式应答，缺省情况不显示提供应答的服务器的源地址和端口号。&#43;[no]comments切换输出中的注释行显示。缺省值是显示注释。&#43;[no]stats该查询选项设定显示统计信息：查询进行时，应答的大小等等。缺省显示查询统计信息。&#43;[no]qr显示 [不显示] 发送的查询请求。缺省不显示。&#43;[no]question当返回应答时，显示 [不显示] 查询请求的问题部分。缺省作为注释显示问题部分。&#43;[no]answer显示 [不显示] 应答的回答部分。缺省显示。&#43;[no]authority显示 [不显示] 应答的权限部分。缺省显示。&#43;[no]additional显示 [不显示] 应答的附加部分。缺省显示。&#43;[no]all设置或清除所有显示标志。&#43;time=T为查询设置超时时间为T秒。缺省是5秒。如果将T设置为小于1的数，则以1秒作为查询超时时间。&#43;tries=A设置向服务器发送 UDP 查询请求的重试次数为A，代替缺省的 3次。如果把A小于或等于 0，则采用 1 为重试次数。&#43;ndots=D出于完全考虑，设置必须出现在名称D的点数。缺省值是使用在/etc/resolv.conf中的ndots语句定义的，或者是 1，如果没有ndots语句的话。带更少点数的名称被解释为相对名称，并通过搜索列表中的域或文件/etc/resolv.conf 中的域伪指令进行搜索。&#43;bufsize=B设置使用 EDNS0 的 UDP消息缓冲区大小为B字节。缓冲区的最大值和最小值分别为65535和0。超出这个范围的值自动舍入到最近的有效值。&#43;[no]multiline以详细的多行格式显示类似SOA的记录，并附带可读注释。缺省值是每单个行上显示一条记录，以便于计算机解析dig的输出。
全局查询参数（global-d-opt）和（在主机名之前的）服务器影响所有查询。局部查询参数和（在主机名之后的）服务器仅影响当前查询。
dig使用 mryqu&amp;gt; dig ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.6.-ESV-R3 &amp;lt;&amp;lt;&amp;gt;&amp;gt; ;; global options: &#43;cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 13319 ;; flags: qr rd ra; QUERY: 1, ANSWER: 13, AUTHORITY: 0, ADDITIONAL: 13 ;; QUESTION SECTION: ;. IN NS ;; ANSWER SECTION: . 391557 IN NS l.root-servers.net. . 391557 IN NS i.root-servers.net. . 391557 IN NS f.root-servers.net. . 391557 IN NS m.root-servers.net. . 391557 IN NS j.root-servers.net. . 391557 IN NS b.root-servers.net. . 391557 IN NS e.root-servers.net. . 391557 IN NS h.root-servers.net. . 391557 IN NS d.root-servers.net. . 391557 IN NS g.root-servers.net. . 391557 IN NS a.root-servers.net. . 391557 IN NS c.root-servers.net. . 391557 IN NS k.root-servers.net. ;; ADDITIONAL SECTION: a.root-servers.net. 477957 IN A 198.41.0.4 a.root-servers.net. 477957 IN AAAA 2001:503:ba3e::2:30 b.root-servers.net. 477957 IN A 192.228.79.201 b.root-servers.net. 477957 IN AAAA 2001:500:84::b c.root-servers.net. 477957 IN A 192.33.4.12 c.root-servers.net. 477957 IN AAAA 2001:500:2::c d.root-servers.net. 477957 IN A 199.7.91.13 d.root-servers.net. 477957 IN AAAA 2001:500:2d::d e.root-servers.net. 477957 IN A 192.203.230.10 f.root-servers.net. 477957 IN A 192.5.5.241 f.root-servers.net. 477957 IN AAAA 2001:500:2f::f g.root-servers.net. 477957 IN A 192.112.36.4 h.root-servers.net. 477957 IN A 128.63.2.53 ;; Query time: 60 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat Oct 12 05:02:23 2013 ;; MSG SIZE rcvd: 496  当未指定任何命令行参数或选项时，dig 将对“.”（根）执行 NS 查询。输出为： - dig程序版本信息 - Got answer显示运行结果信息汇总，包括操作类型、查询及回复统计等等。 - QUESTION SECTION：显示查询请求的内容。 - ANSWER ECTION：查询目标信息。这里是13台根名称服务器的NS记录，对应Gotanswer中的ANSWER:13。后面的数值是相应的TTL值（秒）。 - ADDITIONAL SECTION：附加信息，这里是13台根名称服务器的NS记录对应的A和AAAA记录。 - 最后显示此次查询的操作信息，比如本次查询所用时间、查询服务器地址、当前系统时间、所接收消息的大小。
mryqu&amp;gt; dig www.baidu.com ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.6.-ESV-R3 &amp;lt;&amp;lt;&amp;gt;&amp;gt; www.baidu.com ;; global options: &#43;cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 15452 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 5, ADDITIONAL: 0 ;; QUESTION SECTION: ;www.baidu.com. IN A ;; ANSWER SECTION: www.baidu.com. 1200 IN CNAME www.a.shifen.com. www.a.shifen.com. 300 IN A 103.235.46.39 ;; AUTHORITY SECTION: a.shifen.com. 1200 IN NS ns4.a.shifen.com. a.shifen.com. 1200 IN NS ns3.a.shifen.com. a.shifen.com. 1200 IN NS ns1.a.shifen.com. a.shifen.com. 1200 IN NS ns5.a.shifen.com. a.shifen.com. 1200 IN NS ns2.a.shifen.com. ;; Query time: 777 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Sat Oct 12 05:50:10 2013 ;; MSG SIZE rcvd: 164  查询百度
mryqu&amp;gt; dig @8.8.8.8 www.baidu.com ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.6.-ESV-R3 &amp;lt;&amp;lt;&amp;gt;&amp;gt; @8.8.8.8 www.baidu.com ; (1 server found) ;; global options: &#43;cmd ;; Got answer: ;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 31371 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;www.baidu.com. IN A ;; ANSWER SECTION: www.baidu.com. 886 IN CNAME www.a.shifen.com. www.a.shifen.com. 264 IN A 103.235.46.39 ;; Query time: 18 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Sat Oct 12 05:52:38 2013 ;; MSG SIZE rcvd: 74  通过Google的DNS查询百度
mryqu&amp;gt; dig www.baidu.com &#43;trace ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.6.-ESV-R3 &amp;lt;&amp;lt;&amp;gt;&amp;gt; www.baidu.com &#43;trace ;; global options: &#43;cmd . 44153 IN NS c.root-servers.net. . 44153 IN NS e.root-servers.net. . 44153 IN NS a.root-servers.net. . 44153 IN NS l.root-servers.net. . 44153 IN NS k.root-servers.net. . 44153 IN NS b.root-servers.net. . 44153 IN NS d.root-servers.net. . 44153 IN NS i.root-servers.net. . 44153 IN NS f.root-servers.net. . 44153 IN NS m.root-servers.net. . 44153 IN NS h.root-servers.net. . 44153 IN NS g.root-servers.net. . 44153 IN NS j.root-servers.net. ;; Received 496 bytes from 127.0.0.1#53(127.0.0.1) in 20 ms com. 172800 IN NS l.gtld-servers.net. com. 172800 IN NS m.gtld-servers.net. com. 172800 IN NS b.gtld-servers.net. com. 172800 IN NS c.gtld-servers.net. com. 172800 IN NS f.gtld-servers.net. com. 172800 IN NS i.gtld-servers.net. com. 172800 IN NS k.gtld-servers.net. com. 172800 IN NS d.gtld-servers.net. com. 172800 IN NS j.gtld-servers.net. com. 172800 IN NS e.gtld-servers.net. com. 172800 IN NS a.gtld-servers.net. com. 172800 IN NS g.gtld-servers.net. com. 172800 IN NS h.gtld-servers.net. ;; Received 503 bytes from 192.5.5.241#53(f.root-servers.net) in 76 ms baidu.com. 172800 IN NS dns.baidu.com. baidu.com. 172800 IN NS ns2.baidu.com. baidu.com. 172800 IN NS ns3.baidu.com. baidu.com. 172800 IN NS ns4.baidu.com. baidu.com. 172800 IN NS ns7.baidu.com. ;; Received 201 bytes from 192.33.14.30#53(b.gtld-servers.net) in 106 ms www.baidu.com. 1200 IN CNAME www.a.shifen.com. a.shifen.com. 1200 IN NS ns3.a.shifen.com. a.shifen.com. 1200 IN NS ns2.a.shifen.com. a.shifen.com. 1200 IN NS ns5.a.shifen.com. a.shifen.com. 1200 IN NS ns4.a.shifen.com. a.shifen.com. 1200 IN NS ns1.a.shifen.com. ;; Received 228 bytes from 220.181.38.10#53(ns4.baidu.com) in 327 ms  从根名称服务器开始的代理路径跟踪百度域名。
参考 dig.c
</content>
    </entry>
    
     <entry>
        <title>Ubuntu下安装部署MySQL数据库</title>
        <url>https://mryqu.github.io/post/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2mysql%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>ubuntu</tag><tag>linux</tag><tag>mysql</tag><tag>database</tag><tag>install</tag>
        </tags>
        <content type="html">  安装MySQL mysql-server是MySQL数据库服务器，必选安装。mysql-client是MySQL数据库客户端，提供对MySQL服务器的查询工具及备份/恢复数据工具；libmysqlclient-dev是MySQL的C语言开发接口，libmysql-java是MySQL的JDBC驱动，按需安装。
sudo apt-get update sudo apt-get install mysql-server sudo apt-get isntall mysql-client sudo apt-get install libmysqlclient-dev sudo apt-get install libmysql-java  查看是否安装成功:
sudo netstat -tap | grep mysql  查看上述软件包所提供的文件:
dpkg -L mysql-server dpkg -L mysql-client dpkg -L libmysqlclient-dev  通过上述命令检查之后，如果看到有MySQL的socket处于LISTEN状态则表示安装成功。
MySQL服务操作 查看状态 - 使用service查看状态：sudo service mysql status - 使用mysql脚本查看状态：/etc/inint.d/mysql status - 使用mysqladmin查看状态：mysqladmin -u root -p status
启动 - 使用service启动：sudo service mysql start - 使用mysql脚本启动：/etc/inint.d/mysql start - 使用mysqld_safe启动：mysqld_safe&amp;amp;
停止 - 使用service停止：sudo service mysql stop - 使用mysql脚本停止：/etc/inint.d/mysql stop - 使用mysqladmin停止：mysqladmin -u root -p shutdown
重启 - 使用service重启：sudo service mysql restart - 使用mysql脚本重启：/etc/inint.d/mysql restart
去除禁止远程访问限制 编辑/etc/mysql/my.cnf，注释掉bind-address：
#bind-address = 127.0.0.1  在mysql数据库中通过selecthost,user from user;命令进行查询，如出现%则说明允许使用该用户远程访问此MySQL服务器。 如果对特定用户没有远程访问权限，可以执行grantall privileges on *.* to &#39;yourUserName&#39;@&#39;%&#39; identified by&#39;yourUserPwd&#39; with grant option;。
修改MySQL root密码 方法1： 用SET PASSWORD命令
mysql -u root mysql&amp;gt; SET PASSWORD FOR &#39;root&#39;@&#39;localhost&#39; = PASSWORD(&#39;newpass&#39;);  方法2：用mysqladmin
mysqladmin -u root password &amp;quot;newpass&amp;quot; mysqladmin -u root -p password &amp;quot;newpass&amp;quot; #root已有密码  方法3： 用UPDATE直接编辑user表
mysql -u root mysql&amp;gt; use mysql; mysql&amp;gt; UPDATE user SET Password = PASSWORD(&#39;newpass&#39;) WHERE user = &#39;root&#39;; mysql&amp;gt; FLUSH PRIVILEGES;  忘记已设root密码
mysqld_safe --skip-grant-tables&amp;amp; mysql -u root mysql mysql&amp;gt; UPDATE user SET password=PASSWORD(&amp;quot;new password&amp;quot;) WHERE user=&#39;root&#39;; mysql&amp;gt; FLUSH PRIVILEGES;  其他操作 查看数据库:
show databases;  查看当前字符集:
show variables like &#39;character%&#39;;  </content>
    </entry>
    
     <entry>
        <title>正则表达式风格与语法对比</title>
        <url>https://mryqu.github.io/post/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%A3%8E%E6%A0%BC%E4%B8%8E%E8%AF%AD%E6%B3%95%E5%AF%B9%E6%AF%94/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>正则表达式</tag><tag>regex</tag><tag>posix</tag><tag>perl</tag><tag>语法</tag>
        </tags>
        <content type="html">  现在很多编程语言都支持正则表达式，一般都会提到是Perl风格（PCRE，Perl兼容正则表达式）还是POSIX风格（IEEE制定的POSIXExtended 1003.2标准）。解析POSIX与Perl标准的正则表达式区别详细介绍了这两种风格正则表达式的区别。我对正则表达式的使用主要是Java语言中，其次在R、Python和Javascript中有不同程度的涉猎。不同软件的正则表达式语法汇总介绍了不同语言/软件之间的区别。参考 https://developer.mozilla.org/en/docs/web/javascript/guide/regular_expressions
https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html
https://docs.python.org/2/library/re.html
</content>
    </entry>
    
     <entry>
        <title>Shell中的source和.命令</title>
        <url>https://mryqu.github.io/post/shell%E4%B8%AD%E7%9A%84source%E5%92%8C.%E5%91%BD%E4%BB%A4/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>nix</tag><tag>shell</tag><tag>source</tag><tag>.命令</tag><tag>command</tag>
        </tags>
        <content type="html"> source是csh(C Shell)的内置命令: 标识读入并执行文件中的命令。 这与执行shell脚本是不一样的./script.sh会启动一个新的shell并执行script.sh中的命令。
source [-h] filename [arguments] The shell reads and executes commands from name. The commands are not placed on the history list. If any args are given, they are placed in argv. (&#43;) source commands may be nested; if they are nested too deeply the shell may run out of file descriptors. An error in a source at any level terminates all nested source commands. With -h, commands are placed on the history list instead of being executed, much like `history -L&#39;.  sh (Bourne Shell)和ksh (Korn Shell)有一个类似的命令 .
. filename [arguments] The commands in the specified file are read and executed by the shell. The return command may be used to return to the . command&#39;s caller. If file contains any &#39;&#39;/&#39;&#39; characters, it is used as is. Otherwise, the shell searches the PATH for the file. If it is not found in the PATH, it is sought in the current working directory.  bash (GNU Bourne-AgainShell)是组合了sh/ksh/csh的特征同时支持(source)命令和.命令。
</content>
    </entry>
    
     <entry>
        <title>获取Tomcat版本的简单办法</title>
        <url>https://mryqu.github.io/post/%E8%8E%B7%E5%8F%96tomcat%E7%89%88%E6%9C%AC%E7%9A%84%E7%AE%80%E5%8D%95%E5%8A%9E%E6%B3%95/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Web Application Server</category>
        </categories>
        <tags>
          <tag>version.bat</tag><tag>版本</tag><tag>查看</tag><tag>获取</tag><tag>tomcat</tag>
        </tags>
        <content type="html"> [Tomcat]\bin\version.bat可以用来查看Tomcat版本。
C:\tomcat6\bin&amp;gt;version.bat Using CATALINA_BASE: &amp;quot;C:\tomcat6&amp;quot; Using CATALINA_HOME: &amp;quot;C:\tomcat6&amp;quot; Using CATALINA_TMPDIR: &amp;quot;C:\tomcat6\temp&amp;quot; Using JRE_HOME: &amp;quot;C:\Program Files\Java\jdk1.6.0_45&amp;quot; Using CLASSPATH: &amp;quot;C:\tomcat6\bin\bootstrap.jar&amp;quot; Server version: Apache Tomcat/6.0.37 Server built: Apr 29 2013 11:34:47 Server number: 6.0.0.37 OS Name: Windows Server 2008 R2 OS Version: 6.1 Architecture: x86 JVM Version: 1.6.0_45-b06 JVM Vendor: Sun Microsystems Inc.  </content>
    </entry>
    
     <entry>
        <title>非概率抽样</title>
        <url>https://mryqu.github.io/post/%E9%9D%9E%E6%A6%82%E7%8E%87%E6%8A%BD%E6%A0%B7/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>非概率抽样</tag><tag>non-probability</tag><tag>sampling</tag>
        </tags>
        <content type="html">  非概率抽样：调查者根据自己的方便或主观判断抽取样本的方法。它不是严格按随机抽样原则来抽取样本，所以失去了大数定律的存在基础，也就无法确定抽样误差,无法正确地说明样本的统计值在多大程度上适合于总体。虽然根据样本调查的结果也可在一定程度上说明总体的性质，特征，但不能从数量上推断总体。非概率抽样主要有偶遇抽样，主观抽样，定额抽样，滚雪球抽样等类型。
定义 非概率抽样就是调查者根据自己的方便或主观判断抽取样本的方法。 它不是严格按随机抽样原则来抽取样本，所以失去了大数定律的存在基础，也就无法确定抽样误差，无法正确地说明样本的统计值在多大程度上适合于总体。虽然根据样本调查的结果也可在一定程度上说明总体的性质、特征，但不能从数量上推断总体。
分类 非概率抽样依抽样特点可分为方便抽样、定额抽样、立意抽样、滚雪球抽样和空间抽样。
方便抽样 样本限于总体中易于抽到的一部分。最常见的方便抽样是偶遇抽样，即研究者将在某一时间和环境中所遇到的每一总体单位均作为样本成员。“街头拦人法”就是一种偶遇抽样。某些调查对被调查者来说是不愉快的、麻烦的，这时为方便起见就采用以自愿被调查者为调查样本的方法。方便抽样是非随机抽样中最简单的方法，省时省钱，但样本代表性因受偶然因素的影响太大而得不到保证。
定额抽样 定额抽样也称配额抽样，是将总体依某种标准分层（群）；然后按照各层样本数与该层总体数成比例的原则主观抽取样本。定额抽样与分层概率抽样很接近，最大的不同是分层概率抽样的各层样本是随机抽取的，而定额抽样的各层样本是非随机的。总体也可按照多种标准的组合分层(群)，例如，在研究自杀问题时，考虑到婚姻与性别都可能对自杀有影响，可将研究对象分为未婚男性、已婚男性、未婚女性和已婚女性四个组，然后从各群非随机地抽样。定额抽样是通常使用的非概率抽样方法，样本除所选标识外无法保证代表性。
立意抽样 立意抽样又称判断抽样，研究人员从总体中选择那些被判断为最能代表总体的单位作样本的抽样方法。当研究者对自己的研究领域十分熟悉，对研究总体比较了解时采用这种抽样方法，可获代表性较高的样本。这种抽样方法多应用于总体小而内部差异大的情况，以及在总体边界无法确定或因研究者的时间与人力、物力有限时采用。
滚雪球抽样 以若干个具有所需特征的人为最初的调查对象，然后依靠他们提供认识的合格的调查对象，再由这些人提供第三批调查对象，……依次类推，样本如同滚雪球般由小变大。滚雪球抽样多用于总体单位的信息不足或观察性研究的情况。这种抽样中有些分子最后仍无法找到，有些分子被提供者漏而不提，两者都可能造成误差。
空间抽样 对非静止的、暂时性的空间相邻的群体的抽样方法。例如，游行与集会没有确定的总体，参加者从一地到另一地，一些人离去又有一些人进来，但这些事件是在一定范围内进行的。对这样的总体在同一时间内抽样十分重要，以便样本组成不会经历时间上的太大变化。具体作法是:若干调查员间隔均匀的距离,从某一方向开始，访问离他最近的人，然后每隔一定步数抽取一人为调查对象。
抽样列举 常用的非概率抽样有方便抽样、定额抽样、立意抽样、雪球抽样等。
方便抽样 方便抽样又称偶遇抽样。在这种抽样中，研究者选择那些最容易接近的人作为研究对象。此法常用于干预试验或预调查时，也可用于调查收尾时补缺。
立意抽样 立意抽样又称目的抽样和判断抽样。根据研究目的的需要和研究者的主观判断，选择研究对象。
雪球抽样 雪球抽样是指选择并调查几个具有研究目的所需要的特征的人，再依靠他们选择合乎研究需要的人，后者又可选择更多合乎研究需要的人，以此类推下去，样本就像滚雪球一样越来越大。
定额抽样 定额抽样是先将要研究的人群按某种特征划分成几个组别，然后，按照一定的比例，从每组人群中任意选择一定量的样本作为研究对象。由于抽样前先进行了分层处理，抽得的样本代表性比单纯的方便抽样要好。
优点 简单易行、成本低、省时间,在统计上也比概率抽样简单。但由于无法排除抽样者的主观性，无法控制和客观地测量样本代表性，因此样本不具有推论总体的性质。非概率抽样多用于探索性研究和预备性研究，以及总体边界不清难于实施概率抽样的研究。在实际应用中，非概率抽样往往与概率抽样结合使用。
方法 PPS抽样调查法;Q分类法;SEM模型;不重复抽样;专项调查;主观概率法;二手资料调研;二路焦点小组;产品留置测试;任意抽样;会议调查;典型调查法;分层抽样;分层最佳抽样;分层比例抽样;判断抽样;双重抽样;可行性研究;因果性调研;垃圾调研法;多维尺度法;多阶段抽样;威廉·戈塞;定性研究方法;定量研究方法;实地调研;家庭日记法;市场实验调查法;市场容量测定法;平衡量表法;投射研究;投影技法;抽样;抽样调查;抽签法;拐点调研;探索性调研;推销人员估计法;描述性调研;数值分配量表;整群抽样;文案调查法;文献调查法;无准备访问;案例研究法;案头调研;概率抽样;深层访谈法;滚雪球抽样;焦点访谈法;独立控制配额抽样;电话调查;留置调查;盲测;相互控制配额抽样;等比量表;等距抽样;等距量表;简单随机抽样;类别量表;经销商访谈;经验判断法;网上间接调查;网络调研;联合分析法;营销学术语英汉对照表;行踪分析;观察法;评价量表;询问法;辅助变量;辛迪加调研;逐户寻找法;邮寄调查;配对比较量表;配额抽样;重点调查;重置抽样;问卷调查法;随机号码表法;面谈访问法;顺序量表;&amp;hellip;
</content>
    </entry>
    
     <entry>
        <title>[HBase] Shell命令</title>
        <url>https://mryqu.github.io/post/hbase_shell%E5%91%BD%E4%BB%A4/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>HBase</category>
        </categories>
        <tags>
          <tag>hbase</tag><tag>shell</tag><tag>command</tag><tag>usage</tag>
        </tags>
        <content type="html">  HBase提供可扩展的基于jruby(JIRB)命令行已用于执行一些命令。HBase命令行主要归为六类。
1) 通用HBase命令 status 显示集群状态。可以为‘summary’、‘simple’或‘detailed’。默认为‘summary’。 用法:
hbase&amp;gt; status hbase&amp;gt; status ‘simple’ hbase&amp;gt; status ‘summary’ hbase&amp;gt; status ‘detailed’  version 输出HBase版本 用法:
hbase&amp;gt; version  whoami 显示当前HBase用户。 用法:
hbase&amp;gt; whoami  2) 表管理命令 alter 修改列族schema；提供表名和指定新列族schema的字典。字典必须包含所要修改的列族名。例如，
对表‘t1’修改或添加列族‘f1’从当前值到最大版本5：
hbase&amp;gt; alter ‘t1’, NAME =&amp;gt; ‘f1’, VERSIONS =&amp;gt; 5  对多个列族进行操作:
hbase&amp;gt; alter ‘t1’, ‘f1’, {NAME =&amp;gt; ‘f2’, IN_MEMORY =&amp;gt; true}, {NAME =&amp;gt; ‘f3’, VERSIONS =&amp;gt; 5}  删除表‘t1’中的列族‘f1’，使用下列方法之一：
hbase&amp;gt; alter ‘t1’, NAME =&amp;gt; ‘f1’, METHOD =&amp;gt; ‘delete’ hbase&amp;gt; alter ‘t1’, ‘delete’ =&amp;gt; ‘f1’  也可以修改诸如MAX_FILESIZE、READONLY、MEMSTORE_FLUSHSIZE、DEFERRED_LOG_FLUSH等表属性，例如将region最大容量设为128MB：
hbase&amp;gt; alter ‘t1’, MAX_FILESIZE =&amp;gt; ‘134217728’  可以通过设置表协处理器属性添加表协处理器：
hbase&amp;gt; alter ‘t1’, ‘coprocessor’=&amp;gt;’hdfs:///foo.jar|com.foo.FooRegionObserver|1001|arg1=1,arg2=2’  由于可以对一个表配置多个协处理器，一个序列号将自动附加到属性名后以唯一标识。为了理解如何加载协处理器类，协处理器属性必须匹配下列模式:
[coprocessor jar file location] | class name | [priority] | [arguments]  也可以设置表或列族特定配置：
hbase&amp;gt; alter ‘t1’, CONFIGURATION =&amp;gt; {‘hbase.hregion.scan.loadColumnFamiliesOnDemand’ =&amp;gt; ‘true’} hbase&amp;gt; alter ‘t1’, {NAME =&amp;gt; ‘f2’, CONFIGURATION =&amp;gt; {‘hbase.hstore.blockingStoreFiles’ =&amp;gt; ’10’}}  也可以移除表属性：
hbase&amp;gt; alter ‘t1’, METHOD =&amp;gt; ‘table_att_unset’, NAME =&amp;gt; ‘MAX_FILESIZE’ hbase&amp;gt; alter ‘t1’, METHOD =&amp;gt; ‘table_att_unset’, NAME =&amp;gt; ‘coprocessor$1’  上两个命令可以放在一个命令中：
hbase&amp;gt; alter ‘t1’, { NAME =&amp;gt; ‘f1’, VERSIONS =&amp;gt; 3 },{ MAX_FILESIZE =&amp;gt; ‘134217728’ }, { METHOD =&amp;gt; ‘delete’, NAME =&amp;gt; ‘f2’ },OWNER =&amp;gt; ‘johndoe’, METADATA =&amp;gt; { ‘mykey’ =&amp;gt; ‘myvalue’ }  create 创建表。提供表明和针对每个列族的字典及一个表配置的可选字典。
hbase&amp;gt; create ‘t1’, {NAME =&amp;gt; ‘f1’, VERSIONS =&amp;gt; 5} hbase&amp;gt; create ‘t1’, {NAME =&amp;gt; ‘f1’}, {NAME =&amp;gt; ‘f2’}, {NAME =&amp;gt; ‘f3’} hbase&amp;gt; # 上一命令的缩写方法如下： hbase&amp;gt; create ‘t1’, ‘f1’, ‘f2’, ‘f3’ hbase&amp;gt; create ‘t1’, {NAME =&amp;gt; ‘f1’, VERSIONS =&amp;gt; 1, TTL =&amp;gt; 2592000, BLOCKCACHE =&amp;gt; true} hbase&amp;gt; create ‘t1’, {NAME =&amp;gt; ‘f1’, CONFIGURATION =&amp;gt; {‘hbase.hstore.blockingStoreFiles’ =&amp;gt; ’10’}}  表配置选项可被放在参数的最后。
describe 描述特定表。
hbase&amp;gt; describe ‘t1’  disable 禁止特定表。
hbase&amp;gt; disable ‘t1’  disable_all 禁止匹配给定正则表达式的所有表。
hbase&amp;gt; disable_all ‘t.*’  is_disabled 判断特定表是否被禁止。
hbase&amp;gt; is_disabled ‘t1’  drop  删除特定表。表必须先被禁止。
hbase&amp;gt; drop ‘t1’  drop_all 删除匹配给定正则表达式的所有表。
hbase&amp;gt; drop_all ‘t.*’  enable 使能特定表。
hbase&amp;gt; enable ‘t1’  enable_all 使能匹配给定正则表达式的所有表。
hbase&amp;gt; enable_all ‘t.*’  is_enabled 判断特定表是否使能。
hbase&amp;gt; is_enabled ‘t1’  exists 判断特定表是否存在。
hbase&amp;gt; exists ‘t1’  list 列举HBase中所有表。可选的正则表达式参数可被用于过滤输出
hbase&amp;gt; list hbase&amp;gt; list ‘abc.*’  show_filters 显示HBase中所有过滤器。
hbase&amp;gt; show_filters  alter_status 获得alter命令状态。提供一个表名参数，返回表中收到更新的schema的region数目。
hbase&amp;gt; alter_status ‘t1’  alter_async 修改列族schema，不等待所有region收到schema变动。提供表名和指定新列族schema的字典。字典必须包含所要修改的列族名。
对表‘t1’修改或添加列族‘f1’从当前值到最大版本5：
hbase&amp;gt; alter_async ‘t1’, NAME =&amp;gt; ‘f1’, VERSIONS =&amp;gt; 5 删除表‘t1’中的列族‘f1’: hbase&amp;gt; alter_async ‘t1’, NAME =&amp;gt; ‘f1’, METHOD =&amp;gt; ‘delete’or a shorter version:hbase&amp;gt; alter_async ‘t1’, ‘delete’ =&amp;gt; ‘f1’  也可以修改诸如MAX_FILESIZE、READONLY、MEMSTORE_FLUSHSIZE、DEFERRED_LOG_FLUSH等表属性，例如将region最大容量设为128MB：
hbase&amp;gt; alter ‘t1’, METHOD =&amp;gt; ‘table_att’, MAX_FILESIZE =&amp;gt; ‘134217728’  可以把多个命令换成一个命令中：
hbase&amp;gt; alter ‘t1’, {NAME =&amp;gt; ‘f1’}, {NAME =&amp;gt; ‘f2’, METHOD =&amp;gt; ‘delete’}  检查是否所有region已被更新，使用alter_status
3) 数据操作命令 count 获得表中行数。该操作可能会耗费很长时间。(运行 ‘$HADOOP_HOME/bin/hadoop jar hbase.jar rowcount’以执行一个计数mapreduce作业)。默认当前行数每1000行显示一次，计数间隔可选指定。行数扫描默认是能缓存扫描，默认缓存容龄为10行。如果你的行大小较小，可以增大概参数。例如：
hbase&amp;gt; count ‘t1’ hbase&amp;gt; count ‘t1’, INTERVAL =&amp;gt; 100000 hbase&amp;gt; count ‘t1’, CACHE =&amp;gt; 1000 hbase&amp;gt; count ‘t1’, INTERVAL =&amp;gt; 10, CACHE =&amp;gt; 1000  相同的命令也可以运行在一个表引用上。假设有对表‘t1’的的引用t，相应命令为：
hbase&amp;gt; t.count hbase&amp;gt; t.count INTERVAL =&amp;gt; 100000 hbase&amp;gt; t.count CACHE =&amp;gt; 1000 hbase&amp;gt; t.count INTERVAL =&amp;gt; 10, CACHE =&amp;gt; 1000  delete 在特定表/行/列及可选的时戳坐标放置一个删除单元值。删除必须严格匹配被删除单元的标记。当扫描时，删除单元抑制更旧的版本。从表‘t1’的行‘r1’、列‘c1’删除一个单元并标识时间‘ts1’：
hbase&amp;gt; delete ‘t1’, ‘r1’, ‘c1’, ts1  相同的命令也可以运行在一个表引用上。假设有对表‘t1’的的引用t，相应命令为：
hbase&amp;gt; t.delete ‘r1’, ‘c1’, ts1  deleteall 删除给定行的全部单元。提供表名、行、及可选的列和时戳。例如：
hbase&amp;gt; deleteall ‘t1’, ‘r1’ hbase&amp;gt; deleteall ‘t1’, ‘r1’, ‘c1’ hbase&amp;gt; deleteall ‘t1’, ‘r1’, ‘c1’, ts1  相同的命令也可以运行在一个表引用上。假设有对表‘t1’的的引用t，相应命令为：
hbase&amp;gt; t.deleteall ‘r1’ hbase&amp;gt; t.deleteall ‘r1’, ‘c1’ hbase&amp;gt; t.deleteall ‘r1’, ‘c1’, ts1  get 获取行或单元内容。提供表名、行及可选的列、时戳、时间范围和版本的词典。例如
hbase&amp;gt; get ‘t1’, ‘r1’ hbase&amp;gt; get ‘t1’, ‘r1’, {TIMERANGE =&amp;gt; [ts1, ts2]} hbase&amp;gt; get ‘t1’, ‘r1’, {COLUMN =&amp;gt; ‘c1’} hbase&amp;gt; get ‘t1’, ‘r1’, {COLUMN =&amp;gt; [‘c1’, ‘c2’, ‘c3’]} hbase&amp;gt; get ‘t1’, ‘r1’, {COLUMN =&amp;gt; ‘c1’, TIMESTAMP =&amp;gt; ts1} hbase&amp;gt; get ‘t1’, ‘r1’, {COLUMN =&amp;gt; ‘c1’, TIMERANGE =&amp;gt; [ts1, ts2], VERSIONS =&amp;gt; 4} hbase&amp;gt; get ‘t1’, ‘r1’, {COLUMN =&amp;gt; ‘c1’, TIMESTAMP =&amp;gt; ts1, VERSIONS =&amp;gt; 4} hbase&amp;gt; get ‘t1’, ‘r1’, {FILTER =&amp;gt; “ValueFilter(=, ‘binary:abc’)”} hbase&amp;gt; get ‘t1’, ‘r1’, ‘c1’ hbase&amp;gt; get ‘t1’, ‘r1’, ‘c1’, ‘c2’ hbase&amp;gt; get ‘t1’, ‘r1’, [‘c1’, ‘c2’]  除了默认‘toStringBinary’格式，‘get’也支持基于列的定制格式。一个用户可在get规范中列名增加定义格式器。定义格式器方式如下： - 或者使用org.apache.hadoop.hbase.util.Bytes类中的方法名(例如toInt、toString) - 或者使用定制类及方法名: 例如‘c(MyFormatterClass).format’。
将cf:qualifier1和cf:qualifier2都格式化为整数的示例如下：
hbase&amp;gt; get ‘t1’, ‘r1’ {COLUMN =&amp;gt; [‘cf:qualifier1:toInt’, ‘cf:qualifier2:c(org.apache.hadoop.hbase.util.Bytes).toInt’] }  注意，你仅可以对一个列(cf:qualifer)指定格式器，不能指定一个用于一个列族所有列的格式器。相同的命令也可以运行在一个表引用上（通过get_table或create_table获得）。假设有对表‘t1’的的引用t，相应命令为：
hbase&amp;gt; t.get ‘r1’ hbase&amp;gt; t.get ‘r1’, {TIMERANGE =&amp;gt; [ts1, ts2]} hbase&amp;gt; t.get ‘r1’, {COLUMN =&amp;gt; ‘c1’} hbase&amp;gt; t.get ‘r1’, {COLUMN =&amp;gt; [‘c1’, ‘c2’, ‘c3’]} hbase&amp;gt; t.get ‘r1’, {COLUMN =&amp;gt; ‘c1’, TIMESTAMP =&amp;gt; ts1} hbase&amp;gt; t.get ‘r1’, {COLUMN =&amp;gt; ‘c1’, TIMERANGE =&amp;gt; [ts1, ts2], VERSIONS =&amp;gt; 4} hbase&amp;gt; t.get ‘r1’, {COLUMN =&amp;gt; ‘c1’, TIMESTAMP =&amp;gt; ts1, VERSIONS =&amp;gt; 4} hbase&amp;gt; t.get ‘r1’, {FILTER =&amp;gt; “ValueFilter(=, ‘binary:abc’)”} hbase&amp;gt; t.get ‘r1’, ‘c1’ hbase&amp;gt; t.get ‘r1’, ‘c1’, ‘c2’ hbase&amp;gt; t.get ‘r1’, [‘c1’, ‘c2’]  get_counter 获取特定表/行/列坐标上计数器单元值。一个单元在HBase上可用原子增加函数管理且数据为二进制编码。例如：
hbase&amp;gt; get_counter ‘t1’, ‘r1’, ‘c1’  相同的命令也可以运行在一个表引用上。假设有对表‘t1’的的引用t，相应命令为：
hbase&amp;gt; t.get_counter ‘r1’, ‘c1’  incr 在特定表/行/列坐标对单元值增值。从表‘t1’的行‘r1’、列‘c1’对单元值增值1（默认，可略）或10：
hbase&amp;gt; incr ‘t1’, ‘r1’, ‘c1’ hbase&amp;gt; incr ‘t1’, ‘r1’, ‘c1’, 1 hbase&amp;gt; incr ‘t1’, ‘r1’, ‘c1’, 10  相同的命令也可以运行在一个表引用上。假设有对表‘t1’的的引用t，相应命令为：
hbase&amp;gt; t.incr ‘r1’, ‘c1’ hbase&amp;gt; t.incr ‘r1’, ‘c1’, 1 hbase&amp;gt; t.incr ‘r1’, ‘c1’, 10  put 在特定表/行/列及可选的时戳坐标放置一个单元值。从表‘t1’的行‘r1’、列‘c1’放置一个单元值并标识时间‘ts1’：
hbase&amp;gt; put ‘t1’, ‘r1’, ‘c1’, ‘value’, ts1  相同的命令也可以运行在一个表引用上。假设有对表‘t1’的的引用t，相应命令为：
hbase&amp;gt; t.put ‘r1’, ‘c1’, ‘value’, ts1  scan 扫描一个表；提供表名和可选的扫描器规范词典。扫描器规范可能包含下列一或多个项:TIMERANGE、FILTER、LIMIT、STARTROW、STOPROW、TIMESTAMP、MAXLENGTH、COLUMNS、CACHE。
如果没有指定列，所有列将被扫描。要扫描列族所有成员，将‘col_family:’中的限定符置空。过滤器可以两种方式指定： - 使用过滤器语句 - 更多信息见HBASE-4176 JIRA所附的过滤器语言文档 - 使用过滤器的整个包名
例如：
hbase&amp;gt; scan ‘.META.’ hbase&amp;gt; scan ‘.META.’, {COLUMNS =&amp;gt; ‘info:regioninfo’} hbase&amp;gt; scan ‘t1’, {COLUMNS =&amp;gt; [‘c1’, ‘c2’], LIMIT =&amp;gt; 10, STARTROW =&amp;gt; ‘xyz’} hbase&amp;gt; scan ‘t1’, {COLUMNS =&amp;gt; ‘c1’, TIMERANGE =&amp;gt; [1303668804, 1303668904]} hbase&amp;gt; scan ‘t1’, {FILTER =&amp;gt; “(PrefixFilter (‘row2’) AND (QualifierFilter (&amp;gt;=, ‘binary:xyz’))) AND (TimestampsFilter ( 123, 456))”} hbase&amp;gt; scan ‘t1’, {FILTER =&amp;gt; org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)}  对于专家，有个用于开关扫描器块缓存的额外选项 — CACHE_BLOCKS, 默认为开。例如：
hbase&amp;gt; scan ‘t1’, {COLUMNS =&amp;gt; [‘c1’, ‘c2’], CACHE_BLOCKS =&amp;gt; false}  此外对于专家，还有一个用于命令扫描器返回（包含删除标记和未分配删除单元）所有单元的高级选项 — RAW，默认为禁止。该选项不能与指定特定列的请求共用。例如：
hbase&amp;gt; scan ‘t1’, {RAW =&amp;gt; true, VERSIONS =&amp;gt; 10}  除了默认‘toStringBinary’格式，‘get’也支持基于列的定制格式。一个用户可在get规范中列名增加定义格式器。定义格式器方式如下： - 或者使用org.apache.hadoop.hbase.util.Bytes类中的方法名(例如toInt、toString) - 或者使用定制类及方法名: 例如‘c(MyFormatterClass).format’。
将cf:qualifier1和cf:qualifier2都格式化为整数的示例如下：
hbase&amp;gt; get ‘t1’, ‘r1’ {COLUMN =&amp;gt; [‘cf:qualifier1:toInt’, ‘cf:qualifier2:c(org.apache.hadoop.hbase.util.Bytes).toInt’] }  注意，你仅可以对一个列(cf:qualifer)指定格式器，不能指定一个用于一个列族所有列的格式器。 相同的命令也可以运行在一个表引用上，首先需要获得表的引用，相应命令为：
hbase&amp;gt; t = get_table ‘t’ hbase&amp;gt; t.scan  注意在使用表引用时，仍可以提供所有过滤器、列、选项等。
truncate 禁止、删除并重生特定表。例如：
hbase&amp;gt;truncate ‘t1’  4) HBase工具命令 assign 分配一个region。使用须注意。如果一个region已经被分配，该命令会强制重新分配。仅供专家使用。例如：
hbase&amp;gt; assign ‘REGION_NAME’  balancer 触发集群均衡器。 如均衡器运行且能够让region服务器取消所有region以进行均衡（重新分配自身是异步的），则返回true。否则返回false（如果region处于状态变迁则不会运行）。 例如：
hbase&amp;gt; balancer  balance_switch 使能/禁止均衡器。返回上一均衡器状态。例如：
hbase&amp;gt; balance_switch true hbase&amp;gt; balance_switch false  close_region 关闭单个region。请求master将集群上一个region关闭。如果提供‘SERVER_NAME’的话，请求所在regionserver直接关闭region。关闭region，master期望‘REGIONNAME’是完全限定region名。当请求所在regionserver直接关闭一个region，仅能提供region的编码名。 例如一个region名为TestTable,0094429456,1289497600452.527db22f95c8a9e0116f0cc13c680396。 编码的region名则为527db22f95c8a9e0116f0cc13c680396。服务器名是主机名、端口名及regionserver的启动代码。例如：host187.example.com,60020,1289493121758。例如：该命令以关闭运行在regionserver上region作为结束。该操作无需引入master（master不会了解该关闭）。一旦关闭，region将保持关闭状态。使用assign来重新打开/分配。使用unassign或move来分配集群内其他位置的region。使用须谨慎，仅供专家使用。 例如：
hbase&amp;gt; close_region ‘REGIONNAME’ hbase&amp;gt; close_region ‘REGIONNAME’, ‘SERVER_NAME’  compact 提供一个表参数压缩所有region，或提供一个region row参数压缩单个region。也可以压缩region内单个列族。例如：
压缩一个表中所有region：
hbase&amp;gt; compact ‘t1’  压缩整个region：
hbase&amp;gt; compact ‘r1’  仅压缩一个region中一个列族：
hbase&amp;gt; compact ‘r1’, ‘c1’  压缩一个表中一个列族：
hbase&amp;gt; compact ‘t1’, ‘c1’  flush 提供一个表参数flush所有region，或提供一个region row参数flush单个region。例如：
hbase&amp;gt; flush ‘TABLENAME’ hbase&amp;gt; flush ‘REGIONNAME’  major_compact 提供一个表参数重量级压缩所有region，或提供一个region row参数重量级压缩单个region。也可以重量级压缩region内单个列族。例如：
压缩一个表中所有region:
hbase&amp;gt; major_compact ‘t1’  压缩整个region:
hbase&amp;gt; major_compact ‘r1’  压缩一个region内单个列族:
hbase&amp;gt; major_compact ‘r1’, ‘c1’  压缩一个表内单个列族:
hbase&amp;gt; major_compact ‘t1’, ‘c1’  move 移动region。可以可选地指定目标regionserver或随机选择一个。注意：需要传递编码后的region名，而不是region名。因此这个命令与其他命令稍有不同。编码的region名是region名的哈希后缀。例如一个region名为TestTable,0094429456,1289497600452.527db22f95c8a9e0116f0cc13c680396. 编码的region名则为527db22f95c8a9e0116f0cc13c680396。服务器名是主机名、端口名及regionserver的启动代码。例如：host187.example.com,60020,1289493121758。例如：
hbase&amp;gt; move ‘ENCODED_REGIONNAME’ hbase&amp;gt; move ‘ENCODED_REGIONNAME’, ‘SERVER_NAME’  split 分割整表或将一个region。使用第二个参数，可以为region显式指定分割键。例如：
split ‘tableName’ split ‘regionName’ # format: ‘tableName,startKey,id’ split ‘tableName’, ‘splitKey’ split ‘regionName’, ‘splitKey’  unassign 取消分配一个region。unassign将关闭当前位置的region，之后重新打开。传递true强制取消分配（‘强制’将在重新分配前清除其在master内存内的所有状态。如果导致双重分配，使用hbck-fix解决。） 使用须谨慎，仅供专家使用。例如：
hbase&amp;gt; unassign ‘REGIONNAME’ hbase&amp;gt; unassign ‘REGIONNAME’, true  hlog_roll 滚动日志记录器。即开始向新文件写日志消息。regionserver应作为一个参数提供。服务器名是主机名、端口名及regionserver的启动代码。例如：host187.example.com,60020,1289493121758?(在masterui或shell中查询详细状态可获知服务器名)。
hbase&amp;gt;hlog_roll  zk_dump 导出ZooKeeper所见的HBase集群状态。例如：
hbase&amp;gt;zk_dump  5) 集群复制命令 添加所要复制到的对端集群，集群键组成类似于：hbase.zookeeper.quorum:hbase.zookeeper.property.clientPort:zookeeper.znode.parent， 这提供了HBase连接其他集群的全路径。例如：
hbase&amp;gt; add_peer ‘1’, “server1.mryqu.com:2181:/hbase” hbase&amp;gt; add_peer ‘2’, “zk1,zk2,zk3:2182:/hbase-prod”  remove_peer 停止特定复制流并删除所有元数据信息。例如：
hbase&amp;gt; remove_peer ‘1’  list_peers 列举所有复制对端集群。
hbase&amp;gt; list_peers  enable_peer 重启已被禁止的向特定对端集群的复制。例如：
hbase&amp;gt; enable_peer ‘1’  disable_peer 停止向特定对端集群的复制，但是仍保留复制相关元数据。例如：
hbase&amp;gt; disable_peer ‘1’  start_replication 重启所有复制功能。每个启动的流的状态不确定。 警告：启动/停止复制仅在临界负荷情况下可能被使用。 示例:
hbase&amp;gt; start_replication  stop_replication 停止所有复制功能。每个停掉的流的状态不确定。 警告：启动/停止复制仅在临界负荷情况下可能被使用。 示例:
hbase&amp;gt; stop_replication  6) 安全命令 赋予用户特定权限。 语法：所赋予的权限可以是“RWXCA”中零或多个字母。READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’)
hbase&amp;gt; grant ‘bobsmith’, ‘RW’, ‘t1’, ‘f1’, ‘col1’  #### revoke 取消一个用户访问权限。 语法：revoke
hbase&amp;gt; revoke ‘bobsmith’, ‘t1’, ‘f1’, ‘col1’  user_permission 显示特定用户的所有权限。 语法：user_permission
hbase&amp;gt; user_permission ‘table1’  英文原文：HBase shell commands
</content>
    </entry>
    
     <entry>
        <title>安装Python的simplejson库</title>
        <url>https://mryqu.github.io/post/%E5%AE%89%E8%A3%85python%E7%9A%84simplejson%E5%BA%93/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>pip</tag><tag>simplejson</tag><tag>ubuntu</tag><tag>install</tag>
        </tags>
        <content type="html"> 在Ubuntu下运行一个Python程序，遇到如下问题：ImportError: No module named simplejson。
首先查看一下Python和pip的版本：
python -V pip -V  竟然没有装pip，解决方案如下：
sudo apt-get install python-pip pip2 install simplejson  </content>
    </entry>
    
     <entry>
        <title>[Git] 预览远程仓库与本地仓库的差异</title>
        <url>https://mryqu.github.io/post/git_%E9%A2%84%E8%A7%88%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E4%B8%8E%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E7%9A%84%E5%B7%AE%E5%BC%82/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>preview</tag><tag>diff</tag><tag>remote</tag><tag>local</tag>
        </tags>
        <content type="html"> 首先使用git fetch更新远程分支的本地副本，这不会对任何本地分支造成影响。
使用git log HEAD..origin可以显示本地分支与origin远程分支之间的提交日志。
使用git log -p HEAD..origin除了显示上述提交日志外，还会显示每个提交的补丁。
使用git diff HEAD...origin显示整个补丁。此外如果有本地未提交的修改，可以使用git diff origin/master显示整个补丁。
如果不想使用git pull来合并所有远程提交，可以使用git cherry-pick接受所需要的指定远程提交。最后当准备好接受所有远程提交再使用git pull合并剩余远程提交。
</content>
    </entry>
    
     <entry>
        <title>尝试Protocol Buffers支持的各种数据类型</title>
        <url>https://mryqu.github.io/post/%E5%B0%9D%E8%AF%95protocol_buffers%E6%94%AF%E6%8C%81%E7%9A%84%E5%90%84%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>protobuf</tag><tag>protocol</tag><tag>buffers</tag><tag>google</tag><tag>序列化</tag>
        </tags>
        <content type="html">  Protocol Buffers(即protobuf)是Google开源的序列化库，是一种轻便高效的结构化数据存储格式，可以用于结构化数据序列化/反序列化。它很适合做数据存储或RPC的数据交换格式，常用作通信协议、数据存储等领域。 相比于常见的XML格式，ProtocolBuffers官方网站这样描述它的优点： - 平台无关、语言无关； - 高性能； - 体积小； - 使用简单； - 兼容性好。
现在尝试一下Protocol Buffers支持的各种数据类型。
test.proto package com.yqu.proto; option java_package = &amp;quot;com.yqu.proto&amp;quot;; option java_outer_classname=&amp;quot;TestProtos&amp;quot;; message Test { required double doubleVar = 1; required float floatVar = 2; required int32 int32Var = 3; required int64 int64Var = 4; required uint32 uint32Var = 5; required uint64 uint64Var = 6; required sint32 sint32Var = 7; required sint64 sint64Var = 8; required fixed32 fixed32Var = 9; required fixed64 fixed64Var = 10; required sfixed32 sfixed32Var = 11; required sfixed64 sfixed64Var = 12; required bool booleanVar = 13; required string stringVar = 14; required bytes bytesVar = 15; enum Suit { SPADES = 0; HEARTS = 1; DIAMONDS = 2; CLUBS = 3; } required Suit enumVar = 16 [default = HEARTS]; repeated int32 int32ArrayVar = 17; repeated uint32 uint32ArrayVar = 18 [packed=true]; repeated string stringArrayVar = 19; message MsgVar { required string url = 1; optional string title = 2; } repeated MsgVar msgVar = 20; }  使用Google提供的Protocol Buffers编译器生成Java语言：
protoc -java_ out=. person. proto  TestProtos.java 参考 Protocol Buffers官网
</content>
    </entry>
    
     <entry>
        <title>[Git] 获取两个版本间所有变更的文件列表</title>
        <url>https://mryqu.github.io/post/git_%E8%8E%B7%E5%8F%96%E4%B8%A4%E4%B8%AA%E7%89%88%E6%9C%AC%E9%97%B4%E6%89%80%E6%9C%89%E5%8F%98%E6%9B%B4%E7%9A%84%E6%96%87%E4%BB%B6%E5%88%97%E8%A1%A8/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>diff</tag><tag>--stat</tag><tag>--name-status</tag>
        </tags>
        <content type="html"> git diff commit-SHA1 commit-SHA2 &amp;ndash;name-status返回变更的文件列表，每个文件前带有变更状态： - &amp;lsquo; &amp;rsquo; = unmodified - M = modified - A = added - D = deleted - R = renamed - C = copied - U = updated but unmergedgit diff commit-SHA1 commit-SHA2 &amp;ndash;stat返回变更的文件列表，每个文件后面带有变更统计信息。
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 安装protobuf</title>
        <url>https://mryqu.github.io/post/hadoop_%E5%AE%89%E8%A3%85protobuf/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>protobuf</tag><tag>安装</tag>
        </tags>
        <content type="html">  Protocol Buffers (即protobuf)是Google的语言无关、平台无关、结构数据序列化的可扩展机制。 在Window平台编译Hadoop需要protobuf，下载所需的protoc-2.5.0-win32.zip，将protoc.exe复制到某目录，加入PATH变量即可。
参考 Hadoop WIKI: How to Contribute to Hadoop
Hadoop WIKI: ProtocolBuffers
protobuf releases
GitHub: google/protobuf
protobuf documents
</content>
    </entry>
    
     <entry>
        <title>Python安装oauth2库</title>
        <url>https://mryqu.github.io/post/python%E5%AE%89%E8%A3%85oauth2%E5%BA%93/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>oauth2</tag><tag>twitter</tag>
        </tags>
        <content type="html"> 玩一下用python和twitterAPI访问Twitter数据，首先需要安装oauth2库以获得身份验证。pip是PythonPackaging Authority(PyPA)推荐用于安装Python包的工具。首先下载get-pip.py，然后通过下面的命令安装python get-pip.py pip.exe会被安装到\script\目录下。之后就可以用pip安装oauth2包了.</content>
    </entry>
    
     <entry>
        <title>SSH keys for Git System</title>
        <url>https://mryqu.github.io/post/ssh_keys_for_git_system/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>ssh</tag><tag>key</tag><tag>git</tag><tag>github</tag><tag>gitlab</tag>
        </tags>
        <content type="html">  SSH keys[](http://gitlab.sas.com/help/ssh/README#ssh-keys) An SSH key allows you to establish a secure connection betweenyour computer and Git system such as GitHub, GitLab. Before generating an SSH key, check if your system already hasone by running cat ~/.ssh/id_rsa.pub . If you see a long string startingwith ssh-rsa or ssh-dsa ,you can skip the ssh-keygen step. To generate a new SSH key, just open your terminal and use codebelow. The ssh-keygen command prompts you for a location andfilename to store the key pair and for a password. When promptedfor the location and filename, you can press enter to use thedefault. It is a best practice to use a password for an SSH key, but itis not required and you can skip creating a password by pressingenter. Note that the password you choose here can&amp;rsquo;t be altered orretrieved.
ssh-keygen -t rsa -C &amp;quot;quyandong@yahoo.com&amp;quot;  Use the code below to show your public key.
cat ~/.ssh/id_rsa.pub  Copy-paste the key to the &amp;lsquo;My SSH Keys&amp;rsquo; section under the &amp;lsquo;SSH&amp;rsquo;tab in your user profile. Please copy the complete key startingwith ssh- andending with your username and host.
Use code below to copy your public key to the clipboard.Depending on your OS you&amp;rsquo;ll need to use a different command:
Windows:
clip &amp;lt; ~/.ssh/id_rsa.pub  Mac:
pbcopy &amp;lt; ~/.ssh/id_rsa.pub  Linux (requires xclip):
xclip -sel clip &amp;lt; ~/.ssh/id_rsa.pub  Deploy keys[](http://gitlab.sas.com/help/ssh/README#deploy-keys) Deploy keys allow read-only access to multiple projects with asingle SSH key. This is really useful for cloning repositories to yourContinuous Integration (CI) server. By using deploy keys, you don&amp;rsquo;thave to setup a dummy user account. If you are a project master or owner, you can add a deploy keyin the project settings under the section &amp;lsquo;Deploy Keys&amp;rsquo;. Press the&amp;rsquo;New Deploy Key&amp;rsquo; button and upload a public SSH key. After this,the machine that uses the corresponding private key has read-onlyaccess to the project. You can&amp;rsquo;t add the same deploy key twice with the &amp;lsquo;New DeployKey&amp;rsquo; option. If you want to add the same key to another project,please enable it in the list that says &amp;lsquo;Deploy keys from projectsavailable to you&amp;rsquo;. All the deploy keys of all the projects you haveaccess to are available. This project access can happen throughbeing a direct member of the projecti, or through a group. See def accessible_deploy_keys in app/models/user.rb for more information.
</content>
    </entry>
    
     <entry>
        <title>尝试Apache Avro支持的各种数据类型</title>
        <url>https://mryqu.github.io/post/%E5%B0%9D%E8%AF%95apache_avro%E6%94%AF%E6%8C%81%E7%9A%84%E5%90%84%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
        <categories>
          <category>Hadoop&#43;Spark</category>
        </categories>
        <tags>
          <tag>apache</tag><tag>avro</tag><tag>序列化</tag><tag>idl</tag>
        </tags>
        <content type="html">  Apache Avro是一个独立与编程语言的数据序列化系统，该项目由Doug Cutting（Hadoop之父）牵头创建的。它可以提供： - 丰富的数据结构类型 - 快速可压缩的二进制数据形式 - 存储持久数据的文件容器 - 远程过程调用（RPC） - 同动态语言的简单集成。读写数据文件和使用RPC协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。
今天尝试一下Apache Avro支持的各种数据类型。
test.avsc {&amp;quot;namespace&amp;quot;: &amp;quot;com.yqu.avro&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;record&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Test&amp;quot;, &amp;quot;fields&amp;quot;: [ {&amp;quot;name&amp;quot;: &amp;quot;stringVar&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;}, {&amp;quot;name&amp;quot;: &amp;quot;bytesVar&amp;quot;, &amp;quot;type&amp;quot;: [&amp;quot;bytes&amp;quot;, &amp;quot;null&amp;quot;]}, {&amp;quot;name&amp;quot;: &amp;quot;booleanVar&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;boolean&amp;quot;}, {&amp;quot;name&amp;quot;: &amp;quot;intVar&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;int&amp;quot;, &amp;quot;order&amp;quot;:&amp;quot;descending&amp;quot;}, {&amp;quot;name&amp;quot;: &amp;quot;longVar&amp;quot;, &amp;quot;type&amp;quot;: [&amp;quot;long&amp;quot;, &amp;quot;null&amp;quot;], &amp;quot;order&amp;quot;:&amp;quot;ascending&amp;quot;}, {&amp;quot;name&amp;quot;: &amp;quot;floatVar&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot;}, {&amp;quot;name&amp;quot;: &amp;quot;doubleVar&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;double&amp;quot;}, {&amp;quot;name&amp;quot;: &amp;quot;enumVar&amp;quot;, &amp;quot;type&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;enum&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Suit&amp;quot;, &amp;quot;symbols&amp;quot; : [&amp;quot;SPADES&amp;quot;, &amp;quot;HEARTS&amp;quot;, &amp;quot;DIAMONDS&amp;quot;, &amp;quot;CLUBS&amp;quot;]}}, {&amp;quot;name&amp;quot;: &amp;quot;strArrayVar&amp;quot;, &amp;quot;type&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;, &amp;quot;items&amp;quot;: &amp;quot;string&amp;quot;}}, {&amp;quot;name&amp;quot;: &amp;quot;intArrayVar&amp;quot;, &amp;quot;type&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;array&amp;quot;, &amp;quot;items&amp;quot;: &amp;quot;int&amp;quot;}}, {&amp;quot;name&amp;quot;: &amp;quot;mapVar&amp;quot;, &amp;quot;type&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;map&amp;quot;, &amp;quot;values&amp;quot;: &amp;quot;long&amp;quot;}}, {&amp;quot;name&amp;quot;: &amp;quot;fixedVar&amp;quot;, &amp;quot;type&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;fixed&amp;quot;, &amp;quot;size&amp;quot;: 16, &amp;quot;name&amp;quot;: &amp;quot;md5&amp;quot;}} ] }  使用下列命令将schema编译成代码
java -jar avro-tools-1.7.5.jar compile schema test.avsc .  Test.java package com.yqu.avro; @SuppressWarnings(&amp;quot;all&amp;quot;) @org.apache.avro.specific.AvroGenerated public class Test extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord { public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse( &amp;quot;{&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Test&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;com.yqu.avro&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;stringVar&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;bytesVar&amp;quot;,&amp;quot;type&amp;quot;:[&amp;quot;bytes&amp;quot;,&amp;quot;null&amp;quot;]},{&amp;quot;name&amp;quot;:&amp;quot;booleanVar&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;boolean&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;intVar&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;,&amp;quot;order&amp;quot;:&amp;quot;descending&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;longVar&amp;quot;,&amp;quot;type&amp;quot;:[&amp;quot;long&amp;quot;,&amp;quot;null&amp;quot;]},{&amp;quot;name&amp;quot;:&amp;quot;floatVar&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;float&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;doubleVar&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;double&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;enumVar&amp;quot;,&amp;quot;type&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;enum&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Suit&amp;quot;,&amp;quot;symbols&amp;quot;:[&amp;quot;SPADES&amp;quot;,&amp;quot;HEARTS&amp;quot;,&amp;quot;DIAMONDS&amp;quot;,&amp;quot;CLUBS&amp;quot;]}},{&amp;quot;name&amp;quot;:&amp;quot;strArrayVar&amp;quot;,&amp;quot;type&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;array&amp;quot;,&amp;quot;items&amp;quot;:&amp;quot;string&amp;quot;}},{&amp;quot;name&amp;quot;:&amp;quot;intArrayVar&amp;quot;,&amp;quot;type&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;array&amp;quot;,&amp;quot;items&amp;quot;:&amp;quot;int&amp;quot;}},{&amp;quot;name&amp;quot;:&amp;quot;mapVar&amp;quot;,&amp;quot;type&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;map&amp;quot;,&amp;quot;values&amp;quot;:&amp;quot;long&amp;quot;}},{&amp;quot;name&amp;quot;:&amp;quot;fixedVar&amp;quot;,&amp;quot;type&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;fixed&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;md5&amp;quot;,&amp;quot;size&amp;quot;:16}}]}&amp;quot;); public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; } @Deprecated public java.lang.CharSequence stringVar; @Deprecated public java.nio.ByteBuffer bytesVar; @Deprecated public boolean booleanVar; @Deprecated public int intVar; @Deprecated public java.lang.Long longVar; @Deprecated public float floatVar; @Deprecated public double doubleVar; @Deprecated public com.yqu.avro.Suit enumVar; @Deprecated public java.util.List&amp;lt;java.lang.charsequence&amp;gt; strArrayVar; @Deprecated public java.util.List&amp;lt;java.lang.integer&amp;gt; intArrayVar; @Deprecated public java.util.Map&amp;lt;java.lang.charsequence,java.lang.long&amp;gt; mapVar; @Deprecated public com.yqu.avro.md5 fixedVar; public Test() {} public Test(java.lang.CharSequence stringVar, java.nio.ByteBuffer bytesVar, java.lang.Boolean booleanVar, java.lang.Integer intVar, java.lang.Long longVar, java.lang.Float floatVar, java.lang.Double doubleVar, com.yqu.avro.Suit enumVar, java.util.List&amp;lt;java.lang.charsequence&amp;gt; strArrayVar, java.util.List&amp;lt;java.lang.integer&amp;gt; intArrayVar, java.util.Map&amp;lt;java.lang.charsequence,java.lang.long&amp;gt; mapVar, com.yqu.avro.md5 fixedVar) { this.stringVar = stringVar; this.bytesVar = bytesVar; this.booleanVar = booleanVar; this.intVar = intVar; this.longVar = longVar; this.floatVar = floatVar; this.doubleVar = doubleVar; this.enumVar = enumVar; this.strArrayVar = strArrayVar; this.intArrayVar = intArrayVar; this.mapVar = mapVar; this.fixedVar = fixedVar; } public org.apache.avro.Schema getSchema() { return SCHEMA$; } // Used by DatumWriter. Applications should not call. public { switch (field$) { case 0: return stringVar; case 1: return bytesVar; case 2: return booleanVar; case 3: return intVar; case 4: return longVar; case 5: return floatVar; case 6: return doubleVar; case 7: return enumVar; case 8: return strArrayVar; case 9: return intArrayVar; case 10: return mapVar; case 11: return fixedVar; default: throw new org.apache.avro.AvroRuntimeException(&amp;quot;Bad index&amp;quot;); } } // Used by DatumReader. Applications should not call. @SuppressWarnings(value=&amp;quot;unchecked&amp;quot;) public void put(int field$, { switch (field$) { case 0: stringVar = (value$; break; case 1: bytesVar = (value$; break; case 2: booleanVar = (value$; break; case 3: intVar = (value$; break; case 4: longVar = (value$; break; case 5: floatVar = (value$; break; case 6: doubleVar = (value$; break; case 7: enumVar = (com.yqu.avro.Suit)value$; break; case 8: strArrayVar = (value$; break; case 9: intArrayVar = (value$; break; case 10: mapVar = (value$; break; case 11: fixedVar = (com.yqu.avro.md5)value$; break; default: throw new org.apache.avro.AvroRuntimeException(&amp;quot;Bad index&amp;quot;); } } public java.lang.CharSequence getStringVar() { return stringVar; } public void setStringVar(java.lang.CharSequence value) { this.stringVar = value; } public java.nio.ByteBuffer getByteVar() { return bytesVar; } public void setBytesVar(java.nio.ByteBuffer value) { this.bytesVar = value; } public boolean getBooleanVar) { return booleanVar; } public void setBooleanVar(boolean value) { this.booleanVar = value; } public int getIntVar() { return intVar; } public void setIntVar(int value) { this.intVar = value; } public long getLongVar() { return longVar; } public void setLongVar(long value) { this.longVar = value; } public float getFloatVar() { return floatVar; } public void setFloatVar(float value) { this.floatVar = value; } public double getDoubleVar() { return doubleVar; } public void setDoubleVar(double value) { this.doubleVar = value; } public com.yqu.avro.Suit getEnumVar() { return enumVar; } public void setEnumVar(com.yqu.avro.Suit value) { this.enumVar = value; } public java.util.List&amp;lt;java.lang.charsequence&amp;gt; getStrArrayVar() { return strArrayVar; } public void setStrArrayVar(java.util.List&amp;lt;java.lang.charsequence&amp;gt; value) { this.strArrayVar = value; } public public java.util.List&amp;lt;java.lang.integer&amp;gt; getIntArrayVar() { return intArrayVar; } public void setIntArrayVar(java.util.List&amp;lt;java.lang.integer&amp;gt; value) { this.intArrayVar = value; } public java.util.Map&amp;lt;java.lang.charsequence,java.lang.long&amp;gt; getMapVar() { return mapVar; } public void setMapVar(java.util.Map&amp;lt;java.lang.charsequence,java.lang.long&amp;gt; value) { this.mapVar = value; } public com.yqu.avro.md5 getFixedVar() { return fixedVar; } public void setFixedVar(com.yqu.avro.md5 value) { this.fixedVar = value; } public static com.yqu.avro.Test.Builder newBuilder() { return new com.yqu.avro.Test.Builder(); } public static com.yqu.avro.Test.Builder newBuilder(com.yqu.avro.Test.Builder other) { return new com.yqu.avro.Test.Builder(other); } public static com.yqu.avro.Test.Builder newBuilder(com.yqu.avro.Test other) { return new com.yqu.avro.Test.Builder(other); } public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase&amp;lt;test&amp;gt; implements org.apache.avro.data.RecordBuilder&amp;lt;test&amp;gt; { private java.lang.CharSequence stringVar; private java.nio.ByteBuffer bytesVar; private boolean booleanVar; private int intVar; private java.lang.Long longVar; private float floatVar; private double doubleVar; private com.yqu.avro.Suit enumVar; private java.util.List&amp;lt;java.lang.charsequence&amp;gt; strArrayVar; private java.util.List&amp;lt;java.lang.integer&amp;gt; intArrayVar; private java.util.Map&amp;lt;java.lang.charsequence,java.lang.long&amp;gt; mapVar; private com.yqu.avro.md5 fixedVar; private Builder() { super(com.yqu.avro.Test.SCHEMA$); } private Builder(com.yqu.avro.Test.Builder other) { super(other); if (isValidValue(fields()[0], other.stringVar)) { this.stringVar = data().deepCopy(fields()[0].schema(), other.stringVar); fieldSetFlags()[0] = true; } if (isValidValue(fields()[1], other.bytesVar)) { this.bytesVar = data().deepCopy(fields()[1].schema(), other.bytesVar); fieldSetFlags()[1] = true; } if (isValidValue(fields()[2], other.booleanVar)) { this.booleanVar = data().deepCopy(fields()[2].schema(), other.booleanVar); fieldSetFlags()[2] = true; } if (isValidValue(fields()[3], other.intVar)) { this.intVar = data().deepCopy(fields()[3].schema(), other.intVar); fieldSetFlags()[3] = true; } if (isValidValue(fields()[4], other.longVar)) { this.longVar = data().deepCopy(fields()[4].schema(), other.longVar); fieldSetFlags()[4] = true; } if (isValidValue(fields()[5], other.floatVar)) { this.floatVar = data().deepCopy(fields()[5].schema(), other.floatVar); fieldSetFlags()[5] = true; } if (isValidValue(fields()[6], other.doubleVar)) { this.doubleVar = data().deepCopy(fields()[6].schema(), other.doubleVar); fieldSetFlags()[6] = true; } if (isValidValue(fields()[7], other.enumVar)) { this.enumVar = data().deepCopy(fields()[7].schema(), other.enumVar); fieldSetFlags()[7] = true; } if (isValidValue(fields()[8], other.strArrayVar)) { this.strArrayVar = data().deepCopy(fields()[8].schema(), other.strArrayVar); fieldSetFlags()[8] = true; } if (isValidValue(fields()[9], other.intArrayVar)) { this.intArrayVar = data().deepCopy(fields()[9].schema(), other.intArrayVar); fieldSetFlags()[9] = true; } if (isValidValue(fields()[10], other.mapVar)) { this.mapVar = data().deepCopy(fields()[10].schema(), other.mapVar); fieldSetFlags()[10] = true; } if (isValidValue(fields()[11], other.fixedVar)) { this.fixedVar = data().deepCopy(fields()[11].schema(), other.fixedVar); fieldSetFlags()[11] = true; } } private Builder(com.yqu.avro.Test other) { super(com.yqu.avro.Test.SCHEMA$); if (isValidValue(fields()[0], other.stringVar)) { this.stringVar = data().deepCopy(fields()[0].schema(), other.stringVar); fieldSetFlags()[0] = true; } if (isValidValue(fields()[1], other.bytesVar)) { this.bytesVar = data().deepCopy(fields()[1].schema(), other.bytesVar); fieldSetFlags()[1] = true; } if (isValidValue(fields()[2], other.booleanVar)) { this.booleanVar = data().deepCopy(fields()[2].schema(), other.booleanVar); fieldSetFlags()[2] = true; } if (isValidValue(fields()[3], other.intVar)) { this.intVar = data().deepCopy(fields()[3].schema(), other.intVar); fieldSetFlags()[3] = true; } if (isValidValue(fields()[4], other.longVar)) { this.longVar = data().deepCopy(fields()[4].schema(), other.longVar); fieldSetFlags()[4] = true; } if (isValidValue(fields()[5], other.floatVar)) { this.floatVar = data().deepCopy(fields()[5].schema(), other.floatVar); fieldSetFlags()[5] = true; } if (isValidValue(fields()[6], other.doubleVar)) { this.doubleVar = data().deepCopy(fields()[6].schema(), other.doubleVar); fieldSetFlags()[6] = true; } if (isValidValue(fields()[7], other.enumVar)) { this.enumVar = data().deepCopy(fields()[7].schema(), other.enumVar); fieldSetFlags()[7] = true; } if (isValidValue(fields()[8], other.strArrayVar)) { this.strArrayVar = data().deepCopy(fields()[8].schema(), other.strArrayVar); fieldSetFlags()[8] = true; } if (isValidValue(fields()[9], other.intArrayVar)) { this.intArrayVar = data().deepCopy(fields()[9].schema(), other.intArrayVar); fieldSetFlags()[9] = true; } if (isValidValue(fields()[10], other.mapVar)) { this.mapVar = data().deepCopy(fields()[10].schema(), other.mapVar); fieldSetFlags()[10] = true; } if (isValidValue(fields()[11], other.fixedVar)) { this.fixedVar = data().deepCopy(fields()[11].schema(), other.fixedVar); fieldSetFlags()[11] = true; } } public java.lang.CharSequence getStringVar { return stringVar; } public com.yqu.avro.Test.Builder setStringVar(java.lang.CharSequence value) { validate(fields()[0], value); this.stringVar = value; fieldSetFlags()[0] = true; return this; } public boolean hasStringVar() { return fieldSetFlags()[0]; } public com.yqu.avro.Test.Builder clearStringVar() { stringVar = null; fieldSetFlags()[0] = false; return this; } public java.nio.ByteBuffer getByteVar() { return bytesVar; } public com.yqu.avro.Test.Builder setBytesVar(java.nio.ByteBuffer value) { validate(fields()[1], value); this.bytesVar = value; fieldSetFlags()[1] = true; return this; } public boolean hasBytesVar() { return fieldSetFlags()[1]; } public com.yqu.avro.Test.Builder clearBytesVar() { bytesVar = null; fieldSetFlags()[1] = false; return this; } public boolean getBooleanVar() { return booleanVar; } public com.yqu.avro.Test.Builder setBooleanVar(boolean value) { validate(fields()[2], value); this.booleanVar = value; fieldSetFlags()[2] = true; return this; } public boolean hasBooleanVar() { return fieldSetFlags()[2]; } public com.yqu.avro.Test.Builder clearBooleanVar() { fieldSetFlags()[2] = false; return this; } public int getIntVar() { return intVar; } public com.yqu.avro.Test.Builder setIntVar(int value) { validate(fields()[3], value); this.intVar = value; fieldSetFlags()[3] = true; return this; } public boolean hasIntVar() { return fieldSetFlags()[3]; } public com.yqu.avro.Test.Builder clearIntVar() { fieldSetFlags()[3] = false; return this; } public long getLongVar() { return longVar; } public com.yqu.avro.Test.Builder setLongVar(long value) { validate(fields()[4], value); this.longVar = value; fieldSetFlags()[4] = true; return this; } public boolean hasLongVar() { return fieldSetFlags()[4]; } public com.yqu.avro.Test.Builder clearLongVar() { longVar = null; fieldSetFlags()[4] = false; return this; } public float getFloatVar() { return floatVar; } public com.yqu.avro.Test.Builder setFloatVar(float value) { validate(fields()[5], value); this.floatVar = value; fieldSetFlags()[5] = true; return this; } public boolean hasFloatVar() { return fieldSetFlags()[5]; } public com.yqu.avro.Test.Builder clearFloatVar() { fieldSetFlags()[5] = false; return this; } public double getDoubleVar() { return doubleVar; } public com.yqu.avro.Test.Builder setDoubleVar(double value) { validate(fields()[6], value); this.doubleVar = value; fieldSetFlags()[6] = true; return this; } public boolean hasDoubleVar() { return fieldSetFlags()[6]; } public com.yqu.avro.Test.Builder clearDoubleVar() { fieldSetFlags()[6] = false; return this; } public com.yqu.avro.Suit getEnumVar() { return enumVar; } public com.yqu.avro.Test.Builder setEnumVar(com.yqu.avro.Suit value) { validate(fields()[7], value); this.enumVar = value; fieldSetFlags()[7] = true; return this; } public boolean hasEnumVar() { return fieldSetFlags()[7]; } public com.yqu.avro.Test.Builder clearEnumVar() { enumVar = null; fieldSetFlags()[7] = false; return this; } public java.util.List&amp;lt;java.lang.charsequence&amp;gt; getStrArrayVar() { return strArrayVar; } public com.yqu.avro.Test.Builder setStrArrayVar(java.util.List&amp;lt;java.lang.charsequence&amp;gt; value) { validate(fields()[8], value); this.strArrayVar = value; fieldSetFlags()[8] = true; return this; } public boolean hasStrArrayVar() { return fieldSetFlags()[8]; } public com.yqu.avro.Test.Builder clearStrArrayVar() { strArrayVar = null; fieldSetFlags()[8] = false; return this; } public java.util.List&amp;lt;java.lang.integer&amp;gt; getIntArrayVar() { return intArrayVar; } public com.yqu.avro.Test.Builder setIntArrayVar(java.util.List&amp;lt;java.lang.integer&amp;gt; value) { validate(fields()[9], value); this.intArrayVar = value; fieldSetFlags()[9] = true; return this; } public boolean hasIntArrayVar() { return fieldSetFlags()[9]; } public com.yqu.avro.Test.Builder clearIntArrayVar() { intArrayVar = null; fieldSetFlags()[9] = false; return this; } public java.util.Map&amp;lt;java.lang.charsequence,java.lang.long&amp;gt; getMapVar() { return mapVar; } public com.yqu.avro.Test.Builder setMapVar(java.util.Map&amp;lt;java.lang.charsequence,java.lang.long&amp;gt; value) { validate(fields()[10], value); this.mapVar = value; fieldSetFlags()[10] = true; return this; } public boolean hasMapVar() { return fieldSetFlags()[10]; } public com.yqu.avro.Test.Builder clearMapVar() { mapVar = null; fieldSetFlags()[10] = false; return this; } public com.yqu.avro.md5 getFixedVar() { return fixedVar; } public com.yqu.avro.Test.Builder setFixedVar(com.yqu.avro.md5 value) { validate(fields()[11], value); this.fixedVar = value; fieldSetFlags()[11] = true; return this; } public boolean hasFixedVar() { return fieldSetFlags()[11]; } public com.yqu.avro.Test.Builder clearFixedVar() { fixedVar = null; fieldSetFlags()[11] = false; return this; } @Override public Test build() { try { Test record = new Test(); record.stringVar = fieldSetFlags()[0] ? this.stringVar : (; record.bytesVar = fieldSetFlags()[1] ? this.bytesVar : (; record.booleanVar = fieldSetFlags()[2] ? this.booleanVar : (; record.intVar = fieldSetFlags()[3] ? this.intVar : (; record.longVar = fieldSetFlags()[4] ? this.longVar : (; record.floatVar = fieldSetFlags()[5] ? this.floatVar : (; record.doubleVar = fieldSetFlags()[6] ? this.doubleVar : (; record.enumVar = fieldSetFlags()[7] ? this.enumVar : (com.yqu.avro.Suit) defaultValue(fields()[7]); record.strArrayVar = fieldSetFlags()[8] ? this.strArrayVar : (; record.intArrayVar = fieldSetFlags()[9] ? this.intArrayVar : (; record.mapVar = fieldSetFlags()[10] ? this.mapVar : (; record.fixedVar = fieldSetFlags()[11] ? this.fixedVar : (com.yqu.avro.md5) defaultValue(fields()[11]); return record; } catch (Exception e) { throw new org.apache.avro.AvroRuntimeException(e); } } } }  Suit.java package com.yqu.avro; @SuppressWarnings(&amp;quot;all&amp;quot;) @org.apache.avro.specific.AvroGenerated public enum Suit { SPADES, HEARTS, DIAMONDS, CLUBS ; public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse( &amp;quot;{&amp;quot;type&amp;quot;:&amp;quot;enum&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Suit&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;com.yqu.avro&amp;quot;,&amp;quot;symbols&amp;quot;:[&amp;quot;SPADES&amp;quot;,&amp;quot;HEARTS&amp;quot;,&amp;quot;DIAMONDS&amp;quot;,&amp;quot;CLUBS&amp;quot;]}&amp;quot;); public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; } }  Md5.java package com.yqu.avro; @SuppressWarnings(&amp;quot;all&amp;quot;) @org.apache.avro.specific.FixedSize(16) @org.apache.avro.specific.AvroGenerated public class Md5 extends org.apache.avro.specific.SpecificFixed { public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse( &amp;quot;{&amp;quot;type&amp;quot;:&amp;quot;fixed&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Md5&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;com.yqu.avro&amp;quot;,&amp;quot;size&amp;quot;:16}&amp;quot;); public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; } public Md5() { super(); } public Md5(byte[] bytes) { super(bytes); } }  参考：
Apache Avro规范
Apache Avro 与 Thrift 比较
Avro总结(RPC/序列化)
</content>
    </entry>
    
     <entry>
        <title>[算法] Trie（数字树、字典树、前缀树）</title>
        <url>https://mryqu.github.io/post/%E7%AE%97%E6%B3%95_trie%E6%95%B0%E5%AD%97%E6%A0%91%E5%AD%97%E5%85%B8%E6%A0%91%E5%89%8D%E7%BC%80%E6%A0%91/</url>
        <categories>
          <category>Algorithm.DataStruct</category>
        </categories>
        <tags>
          <tag>trie</tag><tag>数字树</tag><tag>字典树</tag><tag>前缀树</tag><tag>算法</tag>
        </tags>
        <content type="html">  术语trie取自retrieval，也被称为数字树、字典树或前缀树，是一种有序树数据结构，哈希树的变种。 与二叉查找树不同，树中节点不存储与节点关联的键，而是通过树中的位置定义键。一个节点的所有子孙节点拥有与该节点相同的字符串前缀，根节点与空字符串相关联。并不是每个节点都与值关联，仅叶节点和部分内部节点与值关联。 含有键为&amp;rdquo;A&amp;rdquo;、&amp;rdquo;to&amp;rdquo;、&amp;rdquo;tea&amp;rdquo;、&amp;rdquo;ted&amp;rdquo;、&amp;rdquo;ten&amp;rdquo;、&amp;rdquo;i&amp;rdquo;、&amp;rdquo;in&amp;rdquo;和&amp;rdquo;inn&amp;rdquo;的trie示例。 trie 中的键通常是字符串，但也可以是其它的结构。trie的算法可以很容易地修改为处理其它结构的有序序列，比如一串数字或者形状的排列。比如，bitwise trie中的键是一串位元，可以用于表示整数或者内存地址。
性质  根节点不包含字符，除根节点外每一个节点都只包含一个字符； 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串； 每个节点的所有子节点包含的字符都不相同。  应用 替代其他数据结构 trie较二叉查找树有很多优点，trie可用于替代哈希表，优点如下： - trie数据查找与不完美哈希表（链表实现，完美哈希表为数组实现）在最差情况下更快：对于trie，最差情况为O(m)，m为查找字符串的长度；对于不完美哈希表，会有键冲突（不同键哈希相同），最差情况为O(N)，N为全部字符产集合个数。典型情况下是O(m)用于哈希计算、O(1)用于数据查找。 - trie中不同键没有冲突 - trie的桶与哈希表用于存储键冲突的桶类似，仅在单个键与多个值关联时需要 - 当更多的键加入trie，无需提供哈希方法或改变哈希方法 - tire通过键为条目提供了字母顺序Trie也有一些缺点： - trie数据查找在某些情况下（尤其当数据直接从磁盘或随机访问时间远远高于主内存的辅助存储设备时）比哈希表慢 - 当键为某些类型时（例如浮点数）之类的键，前缀链很长且前缀不是特别有意义。然而bitwisetrie能够处理标注IEEE单精度和双精度浮点数。 - 一些trie会比哈希表消耗更多空间：对于trie，每个字符串的每个字符都可能需要分配内存；对于大多数哈希表，为整个条目分配一块内存。
字典表示 典型应用是预测文本排序（常被搜索引擎系统用于文本词频统计）、字典自动完成、字符串近似匹配（拼写检查、断字）。
实现 trie基本操作有：查找、插入和删除。 trie数据查找的方法为 - 从根结点开始一次搜索； - 取得要查找关键词的第一个字母，并根据该字母选择对应的子树并转到该子树继续进行检索； - 在相应的子树上，取得要查找关键词的第二个字母,并进一步选择对应的子树进行检索。 - 迭代过程…… - 在某个结点处，关键词的所有字母已被取出，则读取附在该结点上的信息，即完成查找。
public class Trie { private Node root = new Node(&amp;quot;&amp;quot;); public Trie() {} public Trie(List argInitialWords) { for (String word:argInitialWords) { addWord(word); } } public void addWord(String argWord) { char argChars[] = argWord.toCharArray(); Node currentNode = root; for (int i = 0; i &amp;lt; argChars.length; i&#43;&#43;) { if (!currentNode.containsChildValue(argChars[i])) { currentNode.addChild(argChars[i], new Node(currentNode.getValue() &#43; argChars[i])); } currentNode = currentNode.getChild(argChars[i]); } currentNode.setIsWord(true); } public boolean containsPrefix(String argPrefix) { return contains(argPrefix, false); } public boolean containsWord(String argWord) { return contains(argWord, true); } public Node getWord(String argString) { Node node = getNode(argString); return node != null &amp;amp;&amp;amp; node.isWord() node : null; } public Node getPrefix(String argString) { return getNode(argString); } private boolean contains(String argString, boolean argIsWord) { Node node = getNode(argString); return (node != null &amp;amp;&amp;amp; node.isWord() &amp;amp;&amp;amp; argIsWord) || (!argIsWord &amp;amp;&amp;amp; node != null); } private Node getNode(String argString) { Node currentNode = root; char argChars[] = argString.toCharArray(); for (int i = 0; i &amp;lt; argChars.length &amp;amp;&amp;amp; currentNode != null; i&#43;&#43;) { currentNode = currentNode.getChild(argChars[i]); if (currentNode == null) { return null; } } return currentNode; } } class Node { private final String value; private Map children = new HashMap(); private boolean isValidWord; public Node(String argValue) { value = argValue; } public boolean addChild(char c, Node argChild) { children.put(c, argChild); return true; } public boolean containsChildValue(char c) { return children.containsKey(c); } public String getValue() { return value.toString(); } public Node getChild(char c) { return children.get(c); } public boolean isWord() { return isValidWord; } public void setIsWord(boolean argIsWord) { isValidWord = argIsWord; } public String toString() { return value; } } public class Test { private static BufferedReader br = new BufferedReader( new InputStreamReader(System.in)); public static void main(String[] args) { String words[] = { &amp;quot;a&amp;quot;, &amp;quot;apple&amp;quot;, &amp;quot;argument&amp;quot;, &amp;quot;aptitude&amp;quot;, &amp;quot;ball&amp;quot;, &amp;quot;bat&amp;quot; }; Trie trie = new Trie(Arrays.asList(words)); try { while (true) { System.out.print(&amp;quot;Word to lookup: &amp;quot;); String word = br.readLine().trim(); if (word.isEmpty()) break; if (trie.containsWord(word)) System.out.println(word &#43; &amp;quot; found&amp;quot;); else if (trie.containsPrefix(word)) { if (confirm(word &#43; &amp;quot; is a prefix. Add it as a word &amp;quot;)) trie.addWord(word); } else { if (confirm(&amp;quot;Add &amp;quot; &#43; word &#43; &amp;quot; &amp;quot;)) trie.addWord(word); } } } catch (IOException e) { e.printStackTrace(); } } public static boolean confirm( String question ) throws IOException { while (true) { System.out.print(question &#43; &amp;quot; &amp;quot;); String ans = br.readLine().trim(); if (ans.equalsIgnoreCase(&amp;quot;N&amp;quot;) || ans.equalsIgnoreCase(&amp;quot;NO&amp;quot;)) return false; else if (ans.equalsIgnoreCase(&amp;quot;Y&amp;quot;) || ans.equalsIgnoreCase(&amp;quot;YES&amp;quot;)) return true; System.out.println(&amp;quot;Please answer Y, YES, or N, NO&amp;quot;); } } }  引用 维基百科：Trie字典树
百度百科:字典树前缀树
An Implementation of Double-Array Trie
</content>
    </entry>
    
     <entry>
        <title>JDK7中的双端队列Deque实现</title>
        <url>https://mryqu.github.io/post/jdk7%E4%B8%AD%E7%9A%84%E5%8F%8C%E7%AB%AF%E9%98%9F%E5%88%97deque%E5%AE%9E%E7%8E%B0/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>jdk7</tag><tag>deque</tag><tag>双端队列</tag>
        </tags>
        <content type="html">  双端队列Deque（全名double-endedqueue）是一种数据结构，可在双端队列的两端插入、获取或删除元素。队列和栈可以认为是双端队列的特列。 Deque常用的方法： First Element (Head)Last Element (Tail)Throws exceptionSpecial valueThrows exceptionSpecial valueInsertaddFirst(e)offerFirst(e)addLast(e)offerLast(e)RemoveremoveFirst()pollFirst()removeLast()pollLast()ExaminegetFirst()peekFirst()getLast()peekLast()
Deque扩展了Queue接口，当Deque用作FIFO队列时，元素从双端队列队尾加入，从队首移出。从Queue接口继承的方法等同于Deque如下方法： Queue&amp;nbsp;MethodEquivalent&amp;nbsp;Deque&amp;nbsp;Methodadd(e)addLast(e)offer(e)offerLast(e)remove()removeFirst()poll()pollFirst()element()getFirst()peek()peekFirst()
Deques也可作为LIFO栈。该接口应该优先于遗留的Stack类使用。当双端队列用作栈时，元素从队首入栈和出栈。Stack方法等同与Deque如下方法： Stack MethodEquivalent&amp;nbsp;Deque&amp;nbsp;Methodpush(e)addFirst(e)pop()removeFirst()peek()peekFirst() 注意：当deque用作队列或堆栈时，peek方法也可正常工作，都从deque起始位置移除元素。
JDK6加入的Deque实现 LinkedList: 一个基于链接节点实现的无界双端队列。允许null元素。 ArrayDeque: 一个基于可变长度数组实现的无界双端队列。不允许null元素。
就效率而言，ArrayDeque在两端添加或删除元素时比LinkedList更高效。ArrayDeque用作栈时比Stack更快，用作队列时比LinkedList更快。 LinkedList实现比ArrayDeque实现更复杂，耗费更多的内存。当遍历双端列表时删除当前元素，LinkedList比ArrayDeque更高效。 继承BlockingQueue的BlockingDeque接口及其实现LinkedBlockingDeque：一个基于链接节点实现的可选有界双端队列。如果LinkedBlockingDeque在构造时没有设定容量大小，添加元素永远不会有阻塞队列的等待（至少在其中有Integer.MAX_VALUE元素之前不会）。 LinkedBlockingDeque实现采用一个独占锁，所有对队列的操作都进行了锁保护，因而很好的支持双向阻塞的特性。缺点是由于独占锁，所以不能同时进行两个操作，这样性能上就大打折扣。 LinkedBlockingDeque实现具有显对低的开销及相对低的可扩展性。如果仅需要FIFO队列功能，最好使用LinkedBlockingQueue，LinkedBlockingQueue具有相同的开销但是有更好的伸缩性(例如很多线程竞争时保持更好的性能)。 深入浅出 Java Concurrency (24): 并发容器 part 9 双向队列集合 Deque
深入浅出 Java Concurrency (25): 并发容器 part 10 双向并发阻塞队列 BlockingDeque
JDK7加入的Deque实现 ConcurrentLinkedDeque：一个基于链接节点实现的无界无阻塞双端队列。收集关于队列大小的信息会很慢，需要遍历队列。 ConcurrentLinkedDeque具有跟LinkedBlockingDeque相反的表现：相对高的开销及很好的可伸缩性（使用CAS操作进行非堵塞操作，减少了锁的开销，避免序列化瓶颈）。 [concurrency-interest] BlockingDeque and revised Deque
Deque应用：回文（Palindrome）检查 Deque的应用不是很多，很容易使用双端队列解决的一个有趣问题是经典的回文问题。回文是正着读和倒着读都一样的字符串，例如radar、toot和madam。 回文检查方案是采用Deque存储字符串的字符，将字符串中的字符从左到右插入Deque尾部，则deque头部将持有字符串的首字符，deque尾部将持有字符串的末字符。然后从Deque两端移出字符进行比较，直到Deque内字符数为0或1为止。 public class PalindromeChecker { public static boolean palchecker(String str) { if(str==null || str.isEmpty()) return false; ArrayDeque aDeque = new ArrayDeque (str.length()); for(int i=0;i char ch = str.charAt(i); aDeque.addLast(new Character(ch)); } boolean res = true; while (aDeque.size()&amp;gt;1 &amp;amp;&amp;amp; res) { Character first = aDeque.removeFirst(); Character last = aDeque.removeLast(); if(!first.equals(last)) res = false; } return res; } public static void main(String[] args) { System.out.println(palchecker(&amp;quot;lsdkjfskf&amp;quot;)); System.out.println(palchecker(&amp;quot;radar&amp;quot;)); } }  </content>
    </entry>
    
     <entry>
        <title>序列化压缩实现及对比测试</title>
        <url>https://mryqu.github.io/post/%E5%BA%8F%E5%88%97%E5%8C%96%E5%8E%8B%E7%BC%A9%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%AF%B9%E6%AF%94%E6%B5%8B%E8%AF%95/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>序列化</tag><tag>压缩</tag><tag>zlib</tag><tag>gzip</tag><tag>zip</tag>
        </tags>
        <content type="html">  网上介绍序列化压缩的用gzip比较多。写个测试代码，测试一下四种序列化方式： - 无压缩 - zlib压缩 - gzip压缩 - zip压缩
测例结果显示压缩效果：gzip压缩 &amp;gt; zlib压缩 &amp;gt; zip压缩&amp;gt; 无压缩 测例结果显示压缩速度：zlib压缩&amp;gt; gzip压缩&amp;gt; zip压缩 = 无压缩 确实用gzip性价比比较高！
### zlib介绍
zlib是一个开源库，提供了在内存中压缩和解压的函数。zlib它的设计目标是处理单纯的数据（而不管数据的来源是什么）。
### gzip介绍
gzip是UNIX下的一种数据格式(.tar.gz)。gzip是在zlib之上，包了一层，在头和尾添加了一些额外的信息。 gzip是一种文件压缩工具（或该压缩工具产生的压缩文件格式），它的设计目标是处理单个的文件。gzip在压缩文件中的数据时使用的就是zlib。为了保存与文件属性有关的信息，gzip需要在压缩文件（.gz）中保存更多的头信息内容，而zlib不用考虑这一点。但gzip只适用于单个文件，所以我们在UNIX/Linux上经常看到的压缩包后缀都是.tar.gz或*.tgz，也就是先用tar把多个文件打包成单个文件，再用gzip压缩的结果。
### zip介绍
zip只是一种数据结构，跟rar同级别的。zip是适用于压缩多个文件的格式（相应的工具有PkZip和WinZip等），因此，zip文件还要进一步包含文件目录结构的信息，比gzip的头信息更多。但需要注意，zip格式可采用多种压缩算法，我们常见的zip文件大多不是用zlib的算法压缩的，其压缩数据的格式与gzip大不一样。 Java SDK提供了对上述三种压缩技术的支持：Inflater类和Deflater类直接用zlib库对数据压缩/解压缩，GZIPInputStream类和GZIPOutputStream类提供了对gzip格式的支持，ZipFile、ZipInputStream、ZipOutputStream则用于处理zip格式的文件。 所以，你应当根据你的具体需求，选择不同的压缩技术：如果只需要压缩/解压缩数据，你可以直接用zlib实现，如果需要生成gzip格式的文件或解压其他工具的压缩结果，你就必须用gzip或zip等相关的类来处理了。
测试代码 import java.io.ByteArrayInputStream; import java.io.ByteArrayOutputStream; import java.io.File; import java.io.FileInputStream; import java.io.FileOutputStream; import java.io.IOException; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.io.Serializable; import java.util.Arrays; import java.util.zip.DataFormatException; import java.util.zip.Deflater; import java.util.zip.GZIPInputStream; import java.util.zip.GZIPOutputStream; import java.util.zip.Inflater; import java.util.zip.ZipEntry; import java.util.zip.ZipInputStream; import java.util.zip.ZipOutputStream; public class TestCompressedSerializaion { public static BigObject createBigObject() { final int SIZE = 1 &amp;lt;&amp;lt; 12; int[] bigArray = new int[SIZE]; for (int i = 0; i &amp;lt; SIZE; &#43;&#43;i) { bigArray[i] = (int) (Math.random() * 100); } return new BigObject(bigArray); } public static void serializeObject(Object obj, String flName) { FileOutputStream fileOut = null; ObjectOutputStream out = null; try { fileOut = new FileOutputStream(flName); out = new ObjectOutputStream(fileOut); out.writeObject(obj); } catch (IOException e) { e.printStackTrace(); } finally { if (out != null) { try { out.close(); } catch (Exception e) { } } if (fileOut != null) { try { fileOut.close(); } catch (Exception e) { } } } } public static Object deserializeObject(String flName) { FileInputStream fileIn = null; ObjectInputStream in = null; Object res = null; try { fileIn = new FileInputStream(flName); in = new ObjectInputStream(fileIn); res = in.readObject(); } catch (IOException ioe) { ioe.printStackTrace(); } catch (ClassNotFoundException cnfe) { cnfe.printStackTrace(); } finally { if (in != null) { try { in.close(); } catch (Exception e) { } } if (fileIn != null) { try { fileIn.close(); } catch (Exception e) { } } } return res; } public static void testCompressedSerializaion(CompressType compressType, BigObject bigObj) { CompressedSerializaion.compressType = compressType; String flName = compressType.name() &#43; &amp;quot;.ser&amp;quot;; long beginTime = System.currentTimeMillis(); serializeObject(new CompressedSerializaion(bigObj), flName); CompressedSerializaion obj = (CompressedSerializaion) deserializeObject(flName); long usedTime = System.currentTimeMillis()-beginTime; System.out.println(flName &#43; &amp;quot; length:&amp;quot; &#43; new File(flName).length() &#43; &amp;quot; usedTime:&amp;quot;&#43;usedTime &#43; &amp;quot; serialization correctness:&amp;quot;&#43; bigObj.equals(obj.getBigObj())); } public static void main(String[] args) { BigObject bigObj = createBigObject(); testCompressedSerializaion(CompressType.UNCOMPRESSED, bigObj); testCompressedSerializaion(CompressType.ZLIB, bigObj); testCompressedSerializaion(CompressType.GZIP, bigObj); testCompressedSerializaion(CompressType.ZIP, bigObj); } } enum CompressType { UNCOMPRESSED, ZLIB, GZIP, ZIP }; @SuppressWarnings(&amp;quot;serial&amp;quot;) class BigObject implements Serializable { private int[] bigArray; public BigObject(int[] bigArray) { this.bigArray = bigArray; } public int[] getBigArray() { return bigArray; } public void setBigArray(int[] bigArray) { this.bigArray = bigArray; } @Override public int hashCode() { return Arrays.hashCode(bigArray); } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; BigObject other = (BigObject) obj; if (!Arrays.equals(bigArray, other.bigArray)) return false; return true; } } @SuppressWarnings(&amp;quot;serial&amp;quot;) class CompressedSerializaion implements Serializable { public static CompressType compressType = CompressType.UNCOMPRESSED; private BigObject bigObj; public CompressedSerializaion(BigObject bigObj) { this.bigObj = bigObj; } public BigObject getBigObj() { return bigObj; } public void setBigObj(BigObject bigObj) { this.bigObj = bigObj; } @Override public int hashCode() { return bigObj == null 0 : bigObj.hashCode(); } @Override public boolean equals(Object obj) { if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; CompressedSerializaion other = (CompressedSerializaion) obj; if (!bigObj.equals(other.bigObj)) return false; return true; } public void writeMyObject(ObjectOutputStream out) throws IOException { out.writeObject(bigObj); } private void readMyObject(ObjectInputStream ois) throws ClassNotFoundException, IOException { bigObj = (BigObject) ois.readObject(); } private void writeObject(ObjectOutputStream out) throws IOException { if (compressType == CompressType.UNCOMPRESSED) { writeMyObject(out); } else if (compressType == CompressType.ZLIB) { ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream baOos = new ObjectOutputStream(baos); writeMyObject(baOos); baOos.flush(); baOos.close(); byte unCompressedBytes[] = baos.toByteArray(); baos.close(); Deflater compresser = new Deflater(Deflater.BEST_SPEED); compresser.setInput(unCompressedBytes); compresser.finish(); byte bytes[] = new byte[unCompressedBytes.length]; int nz = compresser.deflate(bytes); out.writeInt(nz); out.write(bytes, 0, nz); } else { ByteArrayOutputStream baos = new ByteArrayOutputStream(); if (compressType == CompressType.GZIP) { GZIPOutputStream gzos = new GZIPOutputStream(baos); ObjectOutputStream gzOos = new ObjectOutputStream(gzos); writeMyObject(gzOos); gzOos.flush(); gzos.close(); } else { ZipOutputStream zos = new ZipOutputStream(baos); ZipEntry ze = new ZipEntry(&amp;quot;testZipSerializaion&amp;quot;); zos.setLevel(Deflater.BEST_SPEED); zos.putNextEntry(ze); ObjectOutputStream zipOos = new ObjectOutputStream(zos); writeMyObject(zipOos); zipOos.flush(); zos.closeEntry(); zos.close(); } byte bytes[] = baos.toByteArray(); int nz = baos.size(); baos.close(); out.writeInt(nz); out.write(bytes, 0, nz); } } private void readObject(ObjectInputStream ois) throws ClassNotFoundException, IOException { if (compressType == CompressType.UNCOMPRESSED) { readMyObject(ois); } else { int nz = ois.readInt(); byte bytes[] = new byte[nz]; int totalSize = 0; while (totalSize &amp;lt; nz) { // attempt to read the entire buffer in a single read int bytesRead = ois.read(bytes, totalSize, (nz - totalSize)); if (bytesRead == -1) // EOF break; totalSize &#43;= bytesRead; } if (compressType == CompressType.ZLIB) { Inflater decompresser = new Inflater(); decompresser.setInput(bytes, 0, nz); byte[] unCompressedBytes; ByteArrayOutputStream baos = new ByteArrayOutputStream(); try { byte[] buf = new byte[1024]; while (!decompresser.finished()) { int i = decompresser.inflate(buf); baos.write(buf, 0, i); } unCompressedBytes = baos.toByteArray(); } catch (DataFormatException e) { throw new IOException(e); } finally { try { baos.close(); decompresser.end(); } finally { } } ByteArrayInputStream bais = new ByteArrayInputStream( unCompressedBytes); ObjectInputStream baOis = new ObjectInputStream(bais); readMyObject(baOis); bais.close(); } else if (compressType == CompressType.GZIP) { ByteArrayInputStream bais = new ByteArrayInputStream(bytes); GZIPInputStream zis = new GZIPInputStream(bais); ObjectInputStream zipOis = new ObjectInputStream(zis); readMyObject(zipOis); bais.close(); } else if (compressType == CompressType.ZIP) { ByteArrayInputStream bais = new ByteArrayInputStream(bytes); ZipInputStream zis = new ZipInputStream(bais); // open first zip file entry ZipEntry ze = zis.getNextEntry(); ObjectInputStream zipOis = new ObjectInputStream(zis); readMyObject(zipOis); bais.close(); } } } }  测试结果 UNCOMPRESSED.ser length:16582 usedTime:14 serialization correctness:true ZLIB.ser length:6108 usedTime:4 serialization correctness:true GZIP.ser length:5682 usedTime:5 serialization correctness:true ZIP.ser length:6254 usedTime:14 serialization correctness:true  </content>
    </entry>
    
     <entry>
        <title>JDK7中的队列实现</title>
        <url>https://mryqu.github.io/post/jdk7%E4%B8%AD%E7%9A%84%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>jdk7</tag><tag>queue</tag><tag>队列</tag>
        </tags>
        <content type="html">  JDK7之前已有的队列实现 JDK7之前已有的队列实现分为两类：用于一般用途的实现和用于并发的实现。
用于一般用途的队列实现 LinkedList实现了Queue接口，为offer、poll等方法提供了先入先出队列操作。 PriorityQueue类是基于堆（数据结构）的优先队列。如果PriorityQueue在构造时指定比较器Comparator，则用比较器对元素排序，否则使用元素的自然排序（通过其java.util.Comparable实现）。队列的取操作（poll、remove、peek和element）访问队列头部的元素。队列头部是顺序上最小的元素或具有相同最小值的元素之一。PriorityQueue的iterator方法提供的爹抬起不保证按特定顺序遍历PriorityQueue中的元素。
并发队列实现 java.util.concurrent包下包含一系列同步的Queue接口和类。 ConcurrentLinkedQueue基于链接节点的、线程安全的队列。并发访问不需要同步。因为它在队列的尾部添加元素并从头部删除它们，所以只要不需要知道队列的大小。ConcurrentLinkedQueue对公共集合的共享访问就可以工作得很好。收集关于队列大小的信息会很慢，需要遍历队列。 http://www.cs.rochester.edu/research/synchronization/pseudocode/queues.html JDK5加入了BlockingQueue接口和五个阻塞队列类。阻塞队列BlockingQueue扩展了Queue的操作，元素添加操作会在没有空间可用时阻塞，而元素获取操作会在队列中没有任何东西时阻塞。 五个队列所提供的各有不同： - ArrayBlockingQueue：一个基于数组实现的有界（大小有限）队列。 - LinkedBlockingQueue：一个基于链接节点实现的可选有界队列。如果LinkedBlockingQueue在构造时没有设定容量大小，添加元素永远不会有阻塞队列的等待（至少在其中有Integer.MAX_VALUE元素之前不会）。 - PriorityBlockingQueue：一个基于堆实现的无界优先级队列。 - DelayQueue：一个基于堆实现的、基于时间的调度队列。 - SynchronousQueue：一个利用BlockingQueue接口的会合（rendezvous）机制。它没有内部容量。它就像线程之间的手递手机制，类似于生活中一手交钱一手交货这种情况。在队列中加入一个元素的生产者会等待另一个线程的消费者。当这个消费者出现时，这个元素就直接在消费者和生产者之间传递，永远不会加入到阻塞队列中。公平模式下等待线程按照FIFO顺序访问队列，非公平模式下等待线程访问顺序不定。
JDK7的TransferQueue JDK7加入了继承自BlockingQueue的TransferQueue接口和及其实现LinkedTransferQueue：一个基于链接节点实现的无界TransferQueue。 TransferQueue可以让使用者决定使用正常的BlockingQueue语义还是有保障的手递手机制，因而比SynchronousQueue更通用和有效。当队列内已经存在元素时，调用transfer会确保所有已有元素在此传递元素之前被处理。DougLea称之为容量智能化，LinkedTransferQueue实际上是ConcurrentLinkedQueue,(在公平模式下)SynchronousQueue, 无界的LinkedBlockingQueues等的超集。 TransferQueue混合了若干高级特性的同时，也提供了更高的性能。LinkedTransferQueue相比不公平模式SynchronousQueue，性能超过3倍；相比公平模式SynchronousQueue，性能超过14倍。SynchronousQueueJDK5实现是使用两个队列（用于等待生产者和等待消费者）,用一个锁保护这两个队列。而LinkedTransferQueue实现使用CAS操作进行非堵塞操作，减少了锁的开销，避免序列化瓶颈。 获得TransferQueue队列大小会很慢，需要遍历队列。 http://tech.puredanger.com/2009/02/28/java-7-transferqueue/
http://www.blogjava.net/yongboy/archive/2012/02/04/369575.html
</content>
    </entry>
    
     <entry>
        <title>JDK7的Fork/Join并发框架</title>
        <url>https://mryqu.github.io/post/jdk7%E7%9A%84forkjoin%E5%B9%B6%E5%8F%91%E6%A1%86%E6%9E%B6/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>jdk7</tag><tag>fork</tag><tag>join</tag><tag>并发</tag><tag>框架</tag>
        </tags>
        <content type="html"> 硬件的发展趋势非常清晰；Moore定律表明不会出现更高的时钟频率，但是每个芯片上会集成更多的内核。很容易想象让十几个处理器繁忙地处理一个粗粒度的任务边界（比如一个用户请求），但是这项技术不会扩大到数千个处理器——在这种环境下短时间内流量可能会呈指数级增长，但最终硬件趋势将会占上风。当跨入多内核时代时，我们需要找到更细粒度的并行性，否则将面临即便有许多工作需要去做而处理器却仍处于空闲的风险。如果希望跟上技术发展的脚步，软件平台也必须配合主流硬件平台的转变。最终，Java7包含的一种框架，用于表示某种更细粒度级别的并行算法：Fork/Join框架。 Fork/Join融合了分而治之（divide-and-conquer）编程技术；获取问题后，递归地将它分成多个子问题，直到每个子问题都足够小，以至于可以高效地串行地解决它们。递归的过程将会把问题分成两个或者多个子问题，然后把这些问题放入队列中等待处理（fork步骤），接下来等待所有子问题的结果（join步骤），把多个结果合并到一起。 假如充分分解任务的大小，那么创建一个线程的开销有可能超出执行该任务的开销。因此，fork/join框架使用与可用核数相匹配的适当大小的线程池，以减少这种频繁交换的开销。为避免线程空闲，框架包含了一个工作窃取方法，该方法可以使空闲线程从一个执行较慢的线程中窃取等待其处理的工作。 Java教程 - Fork/Join
分解和合并：Java 也擅长轻松的并行编程！
JDK 7 中的 Fork/Join 模式
Doug Lea： &amp;ldquo;A Java Fork/Join Framework&amp;ldquo;：了解 Fork/Join 模式的实现机制和执行性能。 InfoQ: Doug Lea谈Fork/Join框架
</content>
    </entry>
    
     <entry>
        <title>Java NaN小结</title>
        <url>https://mryqu.github.io/post/java_nan%E5%B0%8F%E7%BB%93/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>double</tag><tag>NaN</tag><tag>user-defined</tag><tag>自定义</tag>
        </tags>
        <content type="html">  Double.NAN介绍 Double类有个NaN常量，是Not a Number的缩写，其值等于Double.longBitsToDouble(0x7ff8000000000000L) ，用于表示非数值。NaN必须使用isNaN(double)方法来判断，NaN与任何double数值进行加减乘除等数学运算后的结果仍然是NaN，且NaN使用==运算符与自身进行判断返回结果为false。测试代码如下：测试结果如下：
Double.NaN==Double.NaN :false Double.isNaN(Double.NaN*0) :true Double.NaN*0==0 :false Double.isNaN(Double.NaN/0) :true Double.NaN/0==0 :false Double.isNaN(0/Double.NaN) :true 0/Double.NaN==0 :false Double.isNaN(Double.NaN&#43;0) :true Double.NaN&#43;0==0 :false Double.isNaN(Double.NaN-0) :true Double.NaN-0==0 :false Double.isNaN(Double.NaN*Double.NaN) :true Double.NaN*Double.NaN==0 :false  自定义NAN介绍 Double.NaN可以用来表示计算结果发生异常，但是无法获知异常原因。我们可以通过自定义NAN来解决这一问题。使用Double类的isNaN(double)方法判断自定义NAN，其返回结果为true，我们可以通过自己的方法判决到底是那种NaN。测试代码如下：测试结果如下：
Double.isNaN(Double.NaN) :true MyNaN.isMyNaN(Double.NaN) :false Double.isNaN(MyNaN.INVALID_PARAMETER_NAN) :true MyNaN.isMyNaN(MyNaN.INVALID_PARAMETER_NAN) :true Double.isNaN(Double.longBitsToDouble(0xffff000000000123L)) :true MyNaN.isMyNaN(Double.longBitsToDouble(0xffff000000000123L)) :false  </content>
    </entry>
    
     <entry>
        <title>在Ubuntu上修改主机名</title>
        <url>https://mryqu.github.io/post/%E5%9C%A8ubuntu%E4%B8%8A%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>ubuntu</tag><tag>hostname</tag>
        </tags>
        <content type="html">  显示主机名 hostname -s  更多细节见hostname帮助文档。
修改主机名 sudo hostname your-new-name # Ubuntu专有 hostnamectl set-hostname new-hostname  更多细节见hostnamectl帮助文档。 上述命令重启服务器后失效。
修改主机名配置 sudo -H vi /etc/hostname sudo -H vi /etc/hosts  </content>
    </entry>
    
     <entry>
        <title>Java线程是否会被垃圾回收？</title>
        <url>https://mryqu.github.io/post/java%E7%BA%BF%E7%A8%8B%E6%98%AF%E5%90%A6%E4%BC%9A%E8%A2%AB%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>thread</tag><tag>gc</tag><tag>weakreference</tag><tag>线程</tag><tag>垃圾回收</tag>
        </tags>
        <content type="html">  如果将线程启动后，然后线程变量置空，线程会怎么样？
import java.lang.ref.WeakReference; public class TestThread { public static void testUnreferencedThread() { // anonymous class extends Thread Thread t = new Thread() { public void run() { // infinite loop while (true) { try { Thread.sleep(1000); } catch (InterruptedException e) {} // as long as this line printed out, you know it is alive. System.out.println(&amp;quot;thread is running...&amp;quot;); } } }; t.start(); WeakReference&amp;lt;Thread&amp;gt; wr = new WeakReference&amp;lt;Thread&amp;gt;(t); t = null; // no more references for Thread t // another infinite loop while (true) { try { Thread.sleep(3000); } catch (InterruptedException e) {} System.gc(); StringBuilder sb = new StringBuilder(); sb.append(&amp;quot;Executed System.gc(),&amp;quot;); if(wr.get()==null) { sb.append(&amp;quot; thread variable has been GCed&amp;quot;); } else { sb.append(&amp;quot;WeakReference still keep &amp;quot;).append(wr.get()); } System.out.println(sb.toString()); } // The program will run forever until you use ^C to stop it } public static void main(String[] s) { testUnreferencedThread(); } }  上面的例程运行结果是两个线程在程序被强制终止之前一直运行。
thread is running... thread is running... thread is running... Executed System.gc(),WeakReference still keep Thread[Thread-0,5,main] thread is running... thread is running... thread is running... Executed System.gc(),WeakReference still keep Thread[Thread-0,5,main] thread is running... thread is running... ... ... ...  运行中的线程是称之为垃圾回收根对象的一种，不会被垃圾回收。当垃圾回收器判断一个对象是否可达，总是使用垃圾回收根对象作为参考点。 例如，主线程并没有被引用，但是不会被垃圾回收。 垃圾回收根对象是可在堆之外被访问的对象。一个对象可由于下列原因成为GC根对象： - System Class 由自举/系统类加载器加载的类。例如，rt.jar中所有诸如java.util.*的类。 - JNI Local 原生代码中的本地变量，例如用户定义的JNI代码或JVM内部代码。 - JNI Global 原生代码中的全局变量，例如用户定义的JNI代码或JVM内部代码。 - Thread Block 当前活跃的线程块中引用的对象。 - Thread 启动且未停止的线程。 - Busy Monitor 其wait()或notify()方法被调用，或被同步synchronized的对象。例如，通过调用synchronized(Object)或者进入其某个synchronized方法。静态方法对应类，非静态方法对应对象。 - Java Local 本地变量。例如，仍在线程的栈中的方法输入参数或本地创建的对象。 - Native Stack （例如用户定义的JNI代码或JVM内部代码这样的）原生代码的入或出参数。通常发生在许多方法有原生部分，方法参数处理的对象成为GC根对象。例如，参数用于文件、网络I/O或反射。 - Finalizer 在队列中等待其finalizer运行的对象。 - Unfinalized 拥有finalize方法，但是还没有被终结且不在finalizer队列的对象。 - Unreachable 从其他根对象不可达的对象，但是被内存分析器标记为根对象。 - Unknown 没有根类型的对象。一些转储(dump)，例如IBM可移植对转储文件，没有根信息。对于这些转储，内存分析器解析程序将没有被其他根对象引用的对象标记为此类根对象。
参考 Java Thread Garbage collected or not
Garbage Collection Roots
</content>
    </entry>
    
     <entry>
        <title>显示jar文件中某个类的公开方法</title>
        <url>https://mryqu.github.io/post/%E6%98%BE%E7%A4%BAjar%E6%96%87%E4%BB%B6%E4%B8%AD%E6%9F%90%E4%B8%AA%E7%B1%BB%E7%9A%84%E5%85%AC%E5%BC%80%E6%96%B9%E6%B3%95/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>jar</tag><tag>jarp</tag><tag>类</tag><tag>公开方法</tag>
        </tags>
        <content type="html">  使用jar命令显示jar文件的内容 C:\&amp;gt;jar tvf log4j.jar 0 SatAug 25 00:29:46 GMT&#43;08:00 2007 META-INF/ 262 Sat Aug 25 00:29:44GMT&#43;08:00 2007 META-INF/MANIFEST.MF 0 SatAug 25 00:10:04 GMT&#43;08:00 2007 org/ 0 SatAug 25 00:10:04 GMT&#43;08:00 2007 org/apache/ 0 SatAug 25 00:10:06 GMT&#43;08:00 2007 org/apache/log4j/ ................. 676 Sat Aug 25 00:10:04GMT&#43;08:00 2007 org/apache/log4j/Appender.class 2567 Sat Aug 25 00:10:04 GMT&#43;08:00 2007org/apache/log4j/Logger.class 1038 Sat Aug 25 00:10:04 GMT&#43;08:00 2007org/apache/log4j/Layout.class ................. 0 SatAug 25 00:29:46 GMT&#43;08:00 2007 META-INF/maven/ 0 SatAug 25 00:29:46 GMT&#43;08:00 2007 META-INF/maven/log4j/ 0 SatAug 25 00:29:46 GMT&#43;08:00 2007 META-INF/maven/log4j/log4j/ 17780 Sat Aug 25 00:09:44 GMT&#43;08:00 2007META-INF/maven/log4j/log4j/pom.xml 96 Sat Aug 25 00:29:44GMT&#43;08:00 2007 META-INF/maven/log4j/log4j/pom.properties  使用javap命令显示jar文件中某个类的公开方法 C:\&amp;gt;%JAVA_HOME%\bin\javap -classpath log4j.jarorg.apache.log4j.Appender Compiled from &amp;quot;Appender.java&amp;quot; public interface org.apache.log4j.Appender{ public abstract voidaddFilter(org.apache.log4j.spi.Filter); public abstractorg.apache.log4j.spi.Filter getFilter(); public abstract voidclearFilters(); public abstract voidclose(); public abstract voiddoAppend(org.apache.log4j.spi.LoggingEvent); public abstractjava.lang.String getName(); public abstract voidsetErrorHandler(org.apache.log4j.spi.ErrorHandler); public abstractorg.apache.log4j.spi.ErrorHandler getErrorHandler(); public abstract voidsetLayout(org.apache.log4j.Layout); public abstractorg.apache.log4j.Layout getLayout(); public abstract voidsetName(java.lang.String); public abstract booleanrequiresLayout(); }  </content>
    </entry>
    
     <entry>
        <title>Java RMI 客户端回调（callback）</title>
        <url>https://mryqu.github.io/post/java_rmi_%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%9B%9E%E8%B0%83callback/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>rmi</tag><tag>回调</tag><tag>callback</tag>
        </tags>
        <content type="html"> 项目的代码用到了Java RMI 客户端回调，了解一下这方面资料。 http://docs.oracle.com/cd/E13211_01/wle/rmi/callbak.htm
http://blog.sina.com.cn/s/blog_6a1928130100mmk5.html
</content>
    </entry>
    
     <entry>
        <title>Python dictionary practice</title>
        <url>https://mryqu.github.io/post/python_dictionary_practice/</url>
        <categories>
          <category>Python</category>
        </categories>
        <tags>
          <tag>python</tag><tag>dict</tag><tag>键值互换</tag><tag>top</tag>
        </tags>
        <content type="html">  reverse Dict (Swap Key and Value) &amp;gt;&amp;gt;&amp;gt; a={&amp;quot;a&amp;quot;:123,&amp;quot;b&amp;quot;:222,&amp;quot;c&amp;quot;:30,&amp;quot;d&amp;quot;:6,&amp;quot;e&amp;quot;:1} &amp;gt;&amp;gt;&amp;gt; print a {&#39;a&#39;: 123, &#39;c&#39;: 30, &#39;b&#39;: 222, &#39;e&#39;: 1, &#39;d&#39;: 6} &amp;gt;&amp;gt;&amp;gt; res = dict((v,k) for k,v in a.iteritems()) &amp;gt;&amp;gt;&amp;gt; print res {1: &#39;e&#39;, 6: &#39;d&#39;, 123: &#39;a&#39;, 222: &#39;b&#39;, 30: &#39;c&#39;}  &amp;gt;&amp;gt;&amp;gt; a={&amp;quot;a&amp;quot;:123,&amp;quot;b&amp;quot;:222,&amp;quot;c&amp;quot;:30,&amp;quot;d&amp;quot;:6,&amp;quot;e&amp;quot;:1} &amp;gt;&amp;gt;&amp;gt; print a {&#39;a&#39;: 123, &#39;c&#39;: 30, &#39;b&#39;: 222, &#39;e&#39;: 1, &#39;d&#39;: 6} &amp;gt;&amp;gt;&amp;gt; res = dict(zip(a.values(), a.keys())) &amp;gt;&amp;gt;&amp;gt; print res {1: &#39;e&#39;, 6: &#39;d&#39;, 123: &#39;a&#39;, 222: &#39;b&#39;, 30: &#39;c&#39;}  Top N by Dict Value &amp;gt;&amp;gt;&amp;gt; from collections import Counter &amp;gt;&amp;gt;&amp;gt; a={&amp;quot;a&amp;quot;:123,&amp;quot;b&amp;quot;:222,&amp;quot;c&amp;quot;:30,&amp;quot;d&amp;quot;:6,&amp;quot;e&amp;quot;:1} &amp;gt;&amp;gt;&amp;gt; top_three = Counter(a).most_common(3) &amp;gt;&amp;gt;&amp;gt; for key, value in top_three: print key, float(value) ... b 222.0 a 123.0 c 30.0  &amp;gt;&amp;gt;&amp;gt; a={&amp;quot;a&amp;quot;:123,&amp;quot;b&amp;quot;:222,&amp;quot;c&amp;quot;:30,&amp;quot;d&amp;quot;:6,&amp;quot;e&amp;quot;:1} &amp;gt;&amp;gt;&amp;gt; res = sorted(a.items(), key=lambda x:x[1], reverse=True) &amp;gt;&amp;gt;&amp;gt; res [(&#39;b&#39;, 222), (&#39;a&#39;, 123), (&#39;c&#39;, 30), (&#39;d&#39;, 6), (&#39;e&#39;, 1)] &amp;gt;&amp;gt;&amp;gt; for idx in range(3): print res[idx][0], float(res[idx][1]) ... b 222.0 a 123.0 c 30.0  Top N by Dict Key &amp;gt;&amp;gt;&amp;gt; a = {1: &#39;e&#39;, 6: &#39;d&#39;, 123: &#39;a&#39;, 222: &#39;b&#39;, 30: &#39;c&#39;} &amp;gt;&amp;gt;&amp;gt; res = sorted(a.items(), reverse=True) &amp;gt;&amp;gt;&amp;gt; print res [(222, &#39;b&#39;), (123, &#39;a&#39;), (30, &#39;c&#39;), (6, &#39;d&#39;), (1, &#39;e&#39;)] &amp;gt;&amp;gt;&amp;gt; for idx in range(3): print res[idx][1], float(res[idx][0]) ... b 222.0 a 123.0 c 30.0  </content>
    </entry>
    
     <entry>
        <title>[Hadoop] 在RACE虚拟机上安装单节点Hadoop</title>
        <url>https://mryqu.github.io/post/hadoop_%E5%9C%A8race%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8A%E5%AE%89%E8%A3%85%E5%8D%95%E8%8A%82%E7%82%B9hadoop/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>linux</tag><tag>race</tag><tag>安装</tag>
        </tags>
        <content type="html">  RACE（Remote Access ComputerEnvironment）是SAS公司内使用的虚拟机集成系统。通过RACE系统申请虚拟机，使用自己或别的项目组、同事创建的RACEimage安装操作系统和应用程序，省心省力。
RACE安装及配置 申请一台RACE虚拟机，使用RACE Image（Id：579290，STG_LAX_RHEL6_SAS94_16G_Ora112 ）安装了RHEL linux操作系统。 启动后，使用下列脚本替换主机信息
/nfs/cardio/vol/vol1/sasinside/setup/changehost94.sh  安装Java JDK 如果RACE Image没有安装Java JDK的话，需要自己安装：
yum install java-1.6.0-openjdk java-1.6.0-openjdk-devel  幸好在/sasjdk/jdk发现很多版本的Java JDK，最后决定使用下列位置的openjdk:
/usr/lib/jvm/java-openjdk/  创建帐号 原系统中没有安装hadoop，但是有hadoop帐号。我没有找到密码，只好重做一把：
userdel hadoop useradd hadoop passwd hadoop  下载并解压缩Hadoop 因为Hadoop 2.0采用YARN，hive、mahout等需要MapReduce V1的可能无法使用，这里安装的是Hadoop 1.2.1。
# mkdir /opt/hadoop # cd /opt/hadoop/ # wget http://download.nextag.com/apache/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz # tar -xzf hadoop-1.2.1.tar.gz # chown -R hadoop /opt/hadoop # cd /opt/hadoop/hadoop-1.2.1/  配置Hadoop 下列为Hadoop的单节点伪分布模式配置。 conf/core-site.xml:
fs.default.name hdfs://localhost:9000/  conf/hdfs-site.xml:
dfs.replication 1  conf/mapred-site.xml:
mapred.job.tracker localhost:9001  conf/hadoop-env.sh:
export JAVA_HOME=/usr/lib/jvm/java-openjdk/ export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true  设置无密码登录（passphraseless）ssh Hadoop集群中节点都是通过ssh相互联系，进行数据传输，我们不可能为每次连接输入访问密码（常规ssh需要访问密码），所以我们需要进行相应配置，使节点之间的ssh连接不需要密码。 我们可以通过设置密钥来实现。 由于部署单节点的时候，当前节点既是namenode又是datanode，所以此时需要生成无密码登录的ssh。 首先检查是否可以无密码登录本机ssh
$ ssh localhost  如果不能无密码登录本机ssh，执行下列命令
$ ssh-keygen -t dsa -P &#39;&#39; -f ~/.ssh/id_dsa $ cat ~/.ssh/id_dsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys  配置环境变量 查看当前Shell，发现是Korn shell。
$ echo $0 -ksh  将JAVA_HOME和Hadoop信息加入环境变量：
echo &#39;export JAVA_HOME=/usr/lib/jvm/java-openjdk/&#39; &amp;gt;&amp;gt; ~/.profile echo &#39;export PATH=$JAVA_HOME/bin:/opt/hadoop/hadoop-1.2.1/bin:/opt/hadoop/hadoop-1.2.1/sbin:$PATH&#39; &amp;gt;&amp;gt; ~/.profile  无须重启，使环境变量生效：
source ~/.profile  格式化NameNode # su - hadoop $ cd /opt/hadoop/hadoop-1.2.1 $ bin/hadoop namenode -format  此时会初始化好hadoop的文件系统，在/tmp/hadoop-hadoop/dfs发现data、name和namesecondary三个目录。
启动Hadoop服务 $ start-all.sh  测试和访问Hadoop服务 使用jps命令检查是否所有服务正常启动。
$ jps 29784 SecondaryNameNode 29878 JobTracker 29993 TaskTracker 23699 Jps 29658 DataNode 29549 NameNode  服务的Web访问链接
http://localhost:50030/ 用于Jobtracker http://localhost:50070/ 用于Namenode http://localhost:50060/ 用于Tasktracker  </content>
    </entry>
    
     <entry>
        <title>ActiveMQ集群</title>
        <url>https://mryqu.github.io/post/activemq%E9%9B%86%E7%BE%A4/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>MQ</category>
        </categories>
        <tags>
          <tag>ActiveMQ</tag><tag>集群</tag><tag>clustering</tag><tag>ha</tag><tag>failover</tag>
        </tags>
        <content type="html">  ActiveMQ Introduction ActiveMQ is an open source (Apache 2.0 licensed) message brokerwhich fully implements the Java Message Service 1.1. It providesadvanced features like clustering, multiple message stores, andability to use file systems, and databases as a JMS persistenceprovider.
ActiveMQ HA ActiveMQ support reliable high performance load balancing ofmessages on a queue across consumers. If a consumer dies, anyunacknowledged messages are redelivered to other consumers on thequeue. If one consumer is faster than the others it gets moremessages etc. If any consumer slows down, other consumers pick upthe slack. So you can have a reliable load balanced cluster ofconsumers on a queue processing messages. ActiveMQ support many types clustering, for example: - Network of Brokers - Advantage - Networks of Brokers provides store and forward to move messagesfrom brokers with producers to brokers with consumers which allowsus to support distributed queues and topics across a network ofbrokers. - The client uses failover protocol to connect to the network toachieve high performance. - Disadvantage Messages are owned by a single physical broker at any point intime. If the broker goes down, the messages on this broker are notable to be consumed until the broker restarts again. - Master/Slave - Advantage Master/Slave provides a high availability mechanism. If themaster broker fails down one slave will take up the masterposition. Then the messages can always be insured to be deliveredto consumers unless all the brokers are down. - Disadvantage At any time there is only one master that means all the timethere is only one broker which is working to receive and delivermessages. Then the performance should not be as good as network ofbrokers. - A combination of Network of Brokers and Master/Slave
Reference ActiveMQ topologies
ActiveMQ clustering
ActiveMQ:Networks of Brokers
ActiveMQ:Master/Slave
ActiveMQ:Transport configuration options
ActiveMQ:How do distributed queues work
</content>
    </entry>
    
     <entry>
        <title>PostgreSQL数据库分区的update操作</title>
        <url>https://mryqu.github.io/post/postgresql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%8C%BA%E7%9A%84update%E6%93%8D%E4%BD%9C/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>postgresql</tag><tag>数据库</tag><tag>分区</tag><tag>update</tag><tag>操作</tag>
        </tags>
        <content type="html"> CREATE TABLE measurement ( city_id int not null, unitsales int ); CREATE TABLE measurement_1 ( CHECK (unitsales &amp;lt; 100 ) ) INHERITS (measurement); CREATE TABLE measurement_2 ( CHECK (unitsales &amp;gt;= 100 ) ) INHERITS (measurement); CREATE OR REPLACE FUNCTIONmeasurement_insert_trigger() RETURNS TRIGGER AS $$ BEGIN IF (NEW.unitsales &amp;lt; 100) THEN INSERT INTO measurement_1 VALUES (NEW.*); ELSE INSERT INTO measurement_2 VALUES (NEW.*); END IF; RETURNNULL; END; $$ LANGUAGE plpgsql; CREATE TRIGGER insert_measurement_trigger BEFOREINSERT ON measurement FOR EACH ROWEXECUTE PROCEDURE measurement_insert_trigger(); INSERT INTO measurement VALUES (1, 1); INSERT INTO measurement VALUES (2, 2); INSERT INTO measurement VALUES (3, 300); INSERT INTO measurement VALUES (4, 400); mysdm=# select * from measurement_1; city_id | unitsales ---------&#43;----------- 1| 1 2| 2 (2 rows) mysdm=# select * from measurement_2; city_id | unitsales ---------&#43;----------- 3| 300 4| 400 (2 rows)  Postgres没有智能到在update时根据记录的新值变动分区，而是报错！
mysdm=# updatemeasurement set unitsales=5 where city_id = 3; ERROR: new row for relation&amp;quot;measurement_2&amp;quot; violates check constraint&amp;quot;measurement_2_unitsales_check&amp;quot;  如果新值符合原分区的约束，则更新成功。 这也证明了仅对insert操作设置触发器，无需对update操作设置触发器。。
mysdm=# update measurement set unitsales=500where city_id = 3; UPDATE 1 mysdm=# select * from measurement_2; city_id | unitsales ---------&#43;----------- 4| 400 3| 500 (2 rows)  </content>
    </entry>
    
     <entry>
        <title>Hibernate shards数据库分片</title>
        <url>https://mryqu.github.io/post/hibernate_shards%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E7%89%87/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Hibernate</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>shards</tag><tag>数据库</tag><tag>分片</tag><tag>水平分区</tag>
        </tags>
        <content type="html">  简介 Hibernate Shards是Hibernate的一个子项目，由Google工程师Max Ross创建并捐献给Hibernate社区。 http://www.hibernate.org/subprojects/shards.html https://github.com/hibernate/hibernate-shards
Hibernate Shards是对Hibernate Core提供水平分区支持的一个框架。 - 标准Hibernate编程模型 - 灵活的分片策略 - 支持虚拟分片 - 免费/开源
实现Hibernate Shards Hibernate Shards几乎可以与现有Hibernate项目无缝结合使用。 Hibernate Shards的首要目标是让程序员使用标准Hibernate Core API查询和处理已分片的数据库,因此Hibernate Shards主要由大家已经熟知的Hibernate Core接口的实现（分片感知）组成，大多数Hibernate应用程序使用Hibernate Core提供的接口，因此无需对已有代码做过多重构。
|Hibernate Core接口|Hibernate Shards实现 |&amp;mdash;&amp;ndash; |org.hibernate.Session|org.hibernate.shards.session.ShardedSession |org.hibernate.SessionFactory|org.hibernate.shards.ShardedSessionFactory |org.hibernate.Criteria|org.hibernate.shards.criteria.ShardedCriteria |org.hibernate.Query|org.hibernate.shards.query.ShardedQuery
唯一问题是 Hibernate Shards 需要一些特定信息和行为。比如，需要一个分片访问策略、一个分片选择策略和一个分片解析策略。这些是您必须实现的接口，虽然部分情况下，您可以使用默认策略。我们将在后面的部分逐个了解各个接口。 首先让我们看一下《HibernateShard 参考指南》中所用的数据库模式、对象模型及映射。
气象报告数据库模式 CREATE TABLE WEATHER_REPORT ( REPORT_ID INT NOT NULL AUTO_INCREMENT PRIMARY KEY, CONTINENT ENUM(&#39;AFRICA&#39;, &#39;ANTARCTICA&#39;, &#39;ASIA&#39;, &#39;AUSTRALIA&#39;, &#39;EUROPE&#39;, &#39;NORTH AMERICA&#39;, &#39;SOUTH AMERICA&#39;), LATITUDE FLOAT, LONGITUDE FLOAT, TEMPERATURE INT, REPORT_TIME TIMESTAMP );  气象报告对象模型 @Entity @Table(name=&amp;quot;WEATHER_REPORT&amp;quot;) public class WeatherReport { @Id @GeneratedValue(generator=&amp;quot;WeatherReportIdGenerator&amp;quot;) @GenericGenerator(name=&amp;quot;WeatherReportIdGenerator&amp;quot;, strategy=&amp;quot;org.hibernate.shards.id.ShardedUUIDGenerator&amp;quot;) @Column(name=&amp;quot;REPORT_ID&amp;quot;) private Integer reportId; @Column(name=&amp;quot;CONTINENT&amp;quot;) private String continent; @Column(name=&amp;quot;LATITUDE&amp;quot;) private BigDecimal latitude; @Column(name=&amp;quot;LONGITUDE&amp;quot;) private BigDecimal longitude; @Column(name=&amp;quot;TEMPERATURE&amp;quot;) private int temperature; @Column(name=&amp;quot;REPORT_TIME&amp;quot;) private Date reportTime; ... // getters and setters }  Hibernate分片支持任何ID生成策略，唯一的要求是对象ID必须在所有分片中是唯一的。下面是满足要求的一些简单ID生成策略： - Native ID生成 - 使用Hibernate的native ID生成策略，且配置数据库以便ID不冲突。例如，如果使用自增ID生成且分片有5个数据库分发数据，期望不会超过1百万记录，数据库#0返回的ID从0起始，数据库#1然会的ID从200000起始&amp;hellip;只要对数据的假设正确，主键就不会冲突。 - 应用级UUID生成 - 通过定义无需担心ID冲突，但是很可能需要为对象解决难处理的主键。 Hibernate Shards提供了一个简单的、分片感知的UUID生成器实现 - ShardedUUIDGenerator。 - 分布式hilo生成 - 该思路是仅在一个分片上使用hilo表，其确保高/低位机制所生成的ID在所有分片内唯一。这种方式有两个缺陷：访问hilo表可能成为ID生成的并经；在单个数据库上存储hilo表会造成系统的单点失效。 Hibernate Shards提供了一个分布式hilo生成机制实现 - ShardedTableHiLoGenerator。 该实现基于org.hibernate.id.TableHiLoGenerator。 ID生成与分片解析紧密相关。分片解析是使用一个对象ID找到对象所在的分片。有两种方式完成这一目的： - 使用ShardResolutionStrategy - 在ID生成时将分片ID放入对象ID，并在分片解析时获得分片ID。其优势在于Hibernate Shards无需查找数据库表就可以更快速地从对象ID解析出分片ID。Hibernate Shards无需任何对分片ID编码/解码的特殊机制，只需要使用实现ShardEncodingIdentifierGenerator接口的ID生成器。Hibernate Shards所含有的两个ID生成器中，ShardedUUIDGenerator实现了该接口。
配置及加载 &amp;lt;!-- Contents of shard0.hibernate.cfg.xml --&amp;gt; &amp;lt;hibernate-configuration&amp;gt; &amp;lt;session-factory name=&amp;quot;HibernateSessionFactory0&amp;quot;&amp;gt; &amp;lt;!-- note the different name --&amp;gt; &amp;lt;property name=&amp;quot;dialect&amp;quot;&amp;gt;org.hibernate.dialect.MySQLInnoDBDialect&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.driver_class&amp;quot;&amp;gt;com.mysql.jdbc.Driver&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.url&amp;quot;&amp;gt;jdbc:mysql://dbhost0:3306/mydb&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.username&amp;quot;&amp;gt;my_user&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.password&amp;quot;&amp;gt;my_password&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;hibernate.connection.shard_id&amp;quot;&amp;gt;0&amp;lt;/property&amp;gt; &amp;lt;!-- new --&amp;gt; &amp;lt;property name=&amp;quot;hibernate.shard.enable_cross_shard_relationship_checks&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt; &amp;lt;!-- new --&amp;gt; &amp;lt;/session-factory&amp;gt; &amp;lt;/hibernate-configuration&amp;gt;  &amp;lt;!-- Contents of shard1.hibernate.cfg.xml --&amp;gt; &amp;lt;hibernate-configuration&amp;gt; &amp;lt;session-factory name=&amp;quot;HibernateSessionFactory1&amp;quot;&amp;gt; &amp;lt;!-- note the different name --&amp;gt; &amp;lt;property name=&amp;quot;dialect&amp;quot;&amp;gt;org.hibernate.dialect.MySQLInnoDBDialect&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.driver_class&amp;quot;&amp;gt;com.mysql.jdbc.Driver&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.url&amp;quot;&amp;gt;jdbc:mysql://dbhost1:3306/mydb&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.username&amp;quot;&amp;gt;my_user&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;connection.password&amp;quot;&amp;gt;my_password&amp;lt;/property&amp;gt; &amp;lt;property name=&amp;quot;hibernate.connection.shard_id&amp;quot;&amp;gt;1&amp;lt;/property&amp;gt; &amp;lt;!-- new --&amp;gt; &amp;lt;property name=&amp;quot;hibernate.shard.enable_cross_shard_relationship_checks&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt; &amp;lt;!-- new --&amp;gt; &amp;lt;/session-factory&amp;gt; &amp;lt;/hibernate-configuration&amp;gt;  public SessionFactory createSessionFactory() { AnnotationConfiguration prototypeConfig = new AnnotationConfiguration().configure(&amp;quot;shard0.hibernate.cfg.xml&amp;quot;); prototypeConfig.addAnnotatedClass(WeatherReport.class); List shardConfigs = new ArrayList(); shardConfigs.add(buildShardConfig(&amp;quot;shard0.hibernate.cfg.xml&amp;quot;)); shardConfigs.add(buildShardConfig(&amp;quot;shard1.hibernate.cfg.xml&amp;quot;)); shardConfigs.add(buildShardConfig(&amp;quot;shard2.hibernate.cfg.xml&amp;quot;)); ShardStrategyFactory shardStrategyFactory = buildShardStrategyFactory(); ShardedConfiguration shardedConfig = new ShardedConfiguration( prototypeConfig, shardConfigs, shardStrategyFactory); return shardedConfig.buildShardedSessionFactory(); } ShardStrategyFactory buildShardStrategyFactory() { ShardStrategyFactory shardStrategyFactory = new ShardStrategyFactory() { public ShardStrategy newShardStrategy(List shardIds) { RoundRobinShardLoadBalancer loadBalancer = new RoundRobinShardLoadBalancer(shardIds); ShardSelectionStrategy pss = new RoundRobinShardSelectionStrategy(loadBalancer); ShardResolutionStrategy prs = new AllShardsShardResolutionStrategy(shardIds); ShardAccessStrategy pas = new SequentialShardAccessStrategy(); return new ShardStrategyImpl(pss, prs, pas); } }; return shardStrategyFactory; } ShardConfiguration buildShardConfig(String configFile) { Configuration config = new Configuration().configure(configFile); return new ConfigurationToShardConfigurationAdapter(config); }  在上述示例中，一共分配了四个配置。第一个分配的配置是AnnotationConfiguration对象，用于构造与分片无关的信息；之后分配的三个配置是ShardConfiguration对象将仅用于构造分片特定的数据库url、数据库用户和密码、数据库标识符和缓存region前缀。 因此shard1.hibernate.cfg.xml与shard1.hibernate.cfg.xml中与分片无关的信息将被忽略。
分片访问策略 Hibernate Shards使用ShardAccessStrategy决定如何对多个分片实行数据库操作。Hibernate Shards 无需确定查询什么（这是 Hibernate Core 和基础数据库需要做的），但是它确实意识到，在获得答案之前可能需要对多个分片进行查询。因此，Hibernate Shards提供了两种极具创意的逻辑实现方法：一种方法是根据序列机制（一次一个）对切分进行查询，直到获得答案为止；另一种方法是并行访问策略，这种方法使用一个线程模型一次对所有分片进行查询。
顺序策略 SequentialShardAccessStrategy会让查询在分片上顺序执行。取决于所执行的查询类型，有可能因为它每次都按相同的顺序在分片执行查询避免使用这种实现。 如果执行很多限制结果集行数、非排序的查询，就会导致分片上糟糕的使用率(排在前面的分片很忙，排在后面的分片很闲。在这种情况下，可以考虑使用LoadBalancedSequentialShardAccessStrategy，它会在每次调用后收到分片的轮询视图，因此可以平衡地分发查询负载。
并行策略 ParallelShardAccessStrategy会让查询在分片上并行执行。当使用这种实现，需要提供适合应用程序性能和吞吐量的线程池执行器java.util.concurrent.ThreadPoolExecutor。 示例如下:
ThreadFactory factory = new ThreadFactory() { public Thread newThread(Runnable r) { Thread t = Executors.defaultThreadFactory().newThread(r); t.setDaemon(true); return t; } }; ThreadPoolExecutor exec = new ThreadPoolExecutor( 10, 50, 60, TimeUnit.SECONDS, new SynchronousQueue(), factory); return new ParallelShardAccessStrategy(exec);  分片选择策略 Hibernate Shards使用ShardSelectionStrategy判断新增对象应该存储到哪个分片。该接口的实现完全取决于开发者，Hibernate Shards默认提供了一个轮询选择策略RoundRobinShardSelectionStrategy,但一般不这样使用。 通常采用基于属性的分片。一般是用户根据表自己实现一个基于对象属性分片的策略类ShardSelectionStrategy ，例如，以下WeatherReport基于“大陆”属性选择分区：
public class WeatherReportShardSelectionStrategy implements ShardSelectionStrategy { public ShardId selectShardIdForNewObject(Object obj) { if(obj instanceof WeatherReport) { return ((WeatherReport)obj).getContinent().getShardId(); } throw new IllegalArgumentException(); } }  使用Hibernate级联功能存储多级对象时，仅在存储顶级对象时考虑ShardSelectionStrategy。所有子对象自动存储在父对象所在的分片。 如果阻止开发人员在对象层级超过一级之上创建新对象，ShardSelectionStrategy会更容易实现。实现方式为让ShardSelectionStrategy实现感知模型中的顶级对象，如果遇到一个对象不在顶级对象集合就抛出异常。 如果不希望采用这个限制，需要记得对于基于对象属性分片选择，每个对象都必须存在用于选择分片的属性并传递给session.save()。
分片解析策略 Hibernate Shards通过对象ID使用ShardResolutionStrategy判断对象所在的分片集合。对于气象报告应用程序，每个大陆关联一些ID。每当给WeatherReport分配ID，就会选取落入WeatherReport所属大陆的合法范围内的ID。ShardResolutionStrategy可以通过ID和上述信息识别WeatherReport所在的分片。
public class WeatherReportShardResolutionStrategy extends AllShardsShardResolutionStrategy { public WeatherReportShardResolutionStrategy(List shardIds) { super(shardIds); } public List selectShardIdsFromShardResolutionStrategyData( ShardResolutionStrategyData srsd) { if(srsd.getEntityName().equals(WeatherReport.class.getName())) { return Continent.getContinentByReportId(srsd.getId()).getShardId(); } return super.selectShardIdsFromShardResolutionStrategyData(srsd); } }  值得指出的是当我们还没有实现对实体名/ID与分片进行映射的缓存，ShardResolutionStrategy是放入这一缓存的绝佳之所。 分区解析与ID生成紧密绑定，如果选择的ID生成器(实现ShardEncodingIdentifierGenerator接口的生成器，例如Hibernate Shards提供的ShardedUUIDGenerator)生成的对象ID包含分片ID，则ShardResolutionStrategy将不被调用。 Hibernate Shards提供的默认AllShardsShardResolutionStrategy，会返回所有分区ID。
重分片和虚拟分片 当应用程序的数据集增长超出原先分配给应用程序的数据库能力，需要增加更多的数据库，通常（为了实现合适的负载均衡或满足应用程序不变性）也需要在分片上重新分布数据，这就是重分片。 重分片是个复杂的问题，为了减轻重分片带来的痛苦，Hibernate Shards对虚拟分片提供支持。 通常，每个对象存储在一个分片上，重分片包括两个任务：将对象移到另一个分片，并改变对象与分片的映射。对象与分片的映射可能是对象ID包含分片ID，或对象所使用的分片解析策略的内部逻辑。 前者，重分片需要改变所有对象ID和外键。后者重分片需要需要改变给定分片解析策略的运行时配置来改变分片解析策略的机制。 不幸的是，一旦我们考虑到Hibernate Shards不支持跨分片的关系，改变对象与分片的映射的问题会更加严重。这一限制让我们无法将对象图的子集从一个分片移到另一个分片。 改变对象与分片的映射可以通过增加一层重定向来简化 - 每个对象存在于一个虚拟分片，每个虚拟分片映射到一个物理分片。开发人员在设计时必须决定应用程序所需的最大物理分片数。这个物理分片数将被用作虚拟分片的个数，然后虚拟分片映射到应用程序当前所需的物理分片。因为Hibernate Shards的分片选择策略、分片解析策略和ShardEncodingIdentifierGenerator都做用于虚拟分片，对象将在虚拟分片上正确分布。当重分片时，可以通过修改虚拟分片与物理分片的关系就很容易地改变对象与分片的映射。 如果担心无法正确估计应用程序将来所需的最大物理分片数，可以尽量往高估。虚拟分片很廉价，这种方式比必须增加虚拟分片性价比高的多。 为了使能虚拟分片，需要在创建ShardedConfiguration传入一个虚拟分片ID与物理分片ID的映射。下例是将4个虚拟分片映射到2个物理分片。
Map virtualShardMap = new HashMap(); virtualShardMap.put(0, 0); virtualShardMap.put(1, 0); virtualShardMap.put(2, 1); virtualShardMap.put(3, 1); ShardedConfiguration shardedConfig = new ShardedConfiguration( prototypeConfiguration, configurations, strategyFactory, virtualShardMap); return shardedConfig.buildShardedSessionFactory();  此后想要改变虚拟分片与物理分片的映射，仅需要改变传给此构造器的virtualShardToShardMap。 我们提到在重分片的第二个任务是将数据从一个物理分片移到另一个。由于这是应用程序特定的、且随着应用程序热重分片、部署框架等潜在需求而复杂多变的，HibernateShards对比不提供支持。
限制  通过JPA配置SessionFactory目前无法使用Hibernate Shards。 不支持查询中的distinct、order-by和聚集函数 对Hibernate API的不完全实现，一部分较少使用Hibernate API没有实现 不支持跨分区的关系操作。无法创建存在于不同分片上对象A与B的绑定。 不支持在非受管环境（例如Servlet容器，应用程序需要负责获得数据库的连接）下的分布事物。 使用有状态拦截器org.hibernate.Interceptor需要更多处理。 不支持ID为基本类型的实体对象。 将复制数据（相同的数据存在于所有分片）与分片数据绑定可能会有问题，无法确保复制数据的分片与分片数据所在分片是同一分片。  </content>
    </entry>
    
     <entry>
        <title>PostgreSQL与MySQL数据库分区</title>
        <url>https://mryqu.github.io/post/postgresql%E4%B8%8Emysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%8C%BA/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>数据库</tag><tag>分区</tag><tag>partitioning</tag><tag>mysql</tag><tag>postgresql</tag>
        </tags>
        <content type="html">  数据库分区是逻辑数据库的分割。分区可以通过创建独立的较小数据库（每个有自己的表、索引和事物日志）或分割所选的元素（例如一个表）来实现。数据库分区通常是为了易管理性、性能和数据有效性。
分类 分区主要有两种形式：
水平分区 水平分区是将不同的行放入不同的表中。比如将邮编小于50000的客户放入“东部客户”表中，将邮编等于或大于50000的客户放入“西部客户”表中。该例中有两个分区表“东部客户”和“西部客户”，其合集视图就是所有客户的完整视图。 通过这样的方式不同分区里面的物理列分割的数据集得以组合，从而进行个体分割（单分区）或集体分割（1个或多个分区）。所有在表中定义的列在每个数据集中都能找到，所以表的特性依然得以保持。
垂直分区 垂直分区创建含有（主键加上）较少列的表并使用额外的表存储（主键加上）剩余的列，每个分区表中都含有其中列所对应的行。范式化是内在包含垂直分区的过程。 垂直分区被称为“行分割”，通常用于分割表中（查找很慢的）动态数据和（查找很快的、使用较动态数据更频繁的）静态数据。这样在保证数据相关性的同时，在诸如统计分析之类的查询中访问静态数据还能提高性能。 不同的物理存储也可以用于垂直分区，例如不频繁使用的列或者宽列被存入不同的设备。 其缺点是需要管理冗余列，查询所有数据需要join操作。
分区标准 当前高端关系数据库管理系统提供分割数据库的不同标准。这些标准使用分区键基于一定标准分配分区。常用的标准为：
基于范围的分区 通过判断分区键是否在一定范围内选择分区。例如所有邮编列在70000和79999之间的行可以是一个分区。
基于列表的分区 一个分区分配给一列数值。如果分区键为这些值之一，该分区被选中。例如所有国家列为冰岛、挪威、瑞典、芬兰或丹麦的行可以是一个北欧国家分区。
基于哈希的分区 哈希函数的返回值决定分区归属。假设有四个分区，哈希函数返回值为0到3。 组合分区允许上述分区方案的一定组合。例如先使用基于范围的分区，然后使用基于哈希的分区。
PostgreSQL数据库分区 PostgreSQL支持基本的数据库分区，本文以PostgreSQL 9.1为例介绍一下PostgreSQL数据库分区。
优点 分区具有下列优点： - 在某些情况下查询性能能显著提升，特别是表中频繁访问的行在一个单独分区或者少数量分区中。分区替代索引起始列会减少索引大小，使频繁使用的索引更有可能放入内存。 - 当查询或更新访问单个分区的大部分记录时，通过对分区的顺序扫描取代对全表的索引读或随机读，也会提升性能。 - 如果批量加载和删除需求付诸于分区设计，这些需求可以通过添加或删除分区来完成。ALTER TABLE NOINHERIT和DROP TABLE都比批量操作更快，而且完全可以避免批量删除导致的VACUUM负担。 - 很少使用的数据可被移往更便宜更慢的存储媒体。
这些优点仅在表非常大时是真正有价值的。对表采用分区的收益取决于应用程序，一般经验法则是表超过了数据库服务器的物理内存时使用分区。
表继承 PostgreSQL通过表继承支持分区，每个分区作为单个父表的子表创建。父表本身通常为空，仅用于代表整个数据集。
CREATE TABLE cities ( name text, population float, altitude int -- in feet ); CREATE TABLE capitals ( state char(2) ) INHERITS (cities);  父表的所有check约束和not-null约束自动被子表继承，其他类型的约束（unique、主键和外键约束）不会被继承。
SELECT name, altitude FROM cities WHERE altitude &amp;gt; 500; SELECT name, altitude FROM ONLY cities WHERE altitude &amp;gt; 500; -- 通过ONLY关键词，第二个查询仅作用于表cities而不会用于cities的子表。  支持的分区标准 PostgreSQL数据库支持基于范围的分区和基于列表的分区。
分区实现案例  创建主表measurement  CREATE TABLE measurement ( city_id int not null, logdate date not null, peaktemp int, unitsales int );  创建分区表 ``` CREATE TABLE measurement_y2006m02 ( CHECK ( logdate &amp;gt;= DATE &amp;rsquo;2006-02-01&amp;rsquo; AND logdate &amp;lt; DATE &amp;rsquo;2006-03-01&amp;rsquo; ) ) INHERITS (measurement);   CREATE TABLE measurement_y2006m03 ( CHECK ( logdate &amp;gt;= DATE &amp;rsquo;2006-03-01&amp;rsquo; AND logdate &amp;lt; DATE &amp;rsquo;2006-04-01&amp;rsquo; ) ) INHERITS (measurement);
&amp;hellip;
CREATE TABLE measurement_y2007m11 ( CHECK ( logdate &amp;gt;= DATE &amp;rsquo;2007-11-01&amp;rsquo; AND logdate &amp;lt; DATE &amp;rsquo;2007-12-01&amp;rsquo; ) ) INHERITS (measurement);
CREATE TABLE measurement_y2007m12 ( CHECK ( logdate &amp;gt;= DATE &amp;rsquo;2007-12-01&amp;rsquo; AND logdate &amp;lt; DATE &amp;rsquo;2008-01-01&amp;rsquo; ) ) INHERITS (measurement);
CREATE TABLE measurement_y2008m01 ( CHECK ( logdate &amp;gt;= DATE &amp;rsquo;2008-01-01&amp;rsquo; AND logdate &amp;lt; DATE &amp;rsquo;2008-02-01&amp;rsquo; ) ) INHERITS (measurement);
- 为分区表的键列创建索引（可选）  CREATE INDEX measurement_y2006m02_logdate ON measurement_y2006m02 (logdate); CREATE INDEX measurement_y2006m03_logdate ON measurement_y2006m03 (logdate);
&amp;hellip;
CREATE INDEX measurement_y2007m11_logdate ON measurement_y2007m11 (logdate); CREATE INDEX measurement_y2007m12_logdate ON measurement_y2007m12 (logdate); CREATE INDEX measurement_y2008m01_logdate ON measurement_y2008m01 (logdate);
- 创建触发器  CREATE OR REPLACE FUNCTION measurement_insert_trigger() RETURNS TRIGGER AS $$ BEGIN IF ( NEW.logdate &amp;gt;= DATE &amp;rsquo;2006-02-01&amp;rsquo; AND NEW.logdate &amp;lt; DATE &amp;rsquo;2006-03-01&amp;rsquo; ) THEN INSERT INTO measurement_y2006m02 VALUES (NEW.); ELSIF ( NEW.logdate &amp;gt;= DATE &amp;rsquo;2006-03-01&amp;rsquo; AND NEW.logdate &amp;lt; DATE &amp;rsquo;2006-04-01&amp;rsquo; ) THEN INSERT INTO measurement_y2006m03 VALUES (NEW.); &amp;hellip; ELSIF ( NEW.logdate &amp;gt;= DATE &amp;rsquo;2008-01-01&amp;rsquo; AND NEW.logdate &amp;lt; DATE &amp;rsquo;2008-02-01&amp;rsquo; ) THEN INSERT INTO measurement_y2008m01 VALUES (NEW.*); ELSE RAISE EXCEPTION &amp;rsquo;Date out of range. Fix the measurement_insert_trigger() function!&amp;lsquo;; END IF;
 RETURN NULL; END;
$$
LANGUAGE plpgsql;
CREATE TRIGGER insert_measurement_trigger BEFORE INSERT ON measurement FO EACH ROW EXECUTE PROCEDURE measurement_insert_trigger();
 约束排除是一种能够提高分区表性能的查询优化技术。当使用约束排除，下列查询会仅扫描分区表measurement_y2008m01；当禁止掉约束排除，查询会扫描measurement的所有分区表。  SET constraint_exclusion = on; SELECT count(*) FROM measurement WHERE logdate &amp;gt;= DATE &amp;rsquo;2008-01-01&amp;rsquo;;
 ## MySQL数据库分区 MySQL从5.1开始支持数据库分区，本文以MySQL 5.7为例介绍一下MySQL数据库分区。 ### 支持的分区类型 - 基于范围的分区 - 基于列表的分区 - 基于哈希的分区 - 基于键的分区 - 子分区--组合分区 基于键的分区之外的其他分区类型所使用的列受限于INT或NULL类型(RANGE COLUMNS和LIST COLUMNS还可处理DATE和DATETIME类型)，基于键的分区没有这个限制。 对于NULL，MySQL的分区实现把它当作任何非空的值都小来处理 ### 基于范围的分区 使用PARTITION BY RANGE关键词的分区表达式返回结果必须是整数型的。  CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &amp;rsquo;1970-01-01&amp;rsquo;, separated DATE NOT NULL DEFAULT &amp;rsquo;9999-12-31&amp;rsquo;, job_code INT NOT NULL, store_id INT NOT NULL )
PARTITION BY RANGE (store_id) ( PARTITION p0 VALUES LESS THAN (6), PARTITION p1 VALUES LESS THAN (11), PARTITION p2 VALUES LESS THAN (16), PARTITION p3 VALUES LESS THAN MAXVALUE );
  CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &amp;rsquo;1970-01-01&amp;rsquo;, separated DATE NOT NULL DEFAULT &amp;rsquo;9999-12-31&amp;rsquo;, job_code INT, store_id INT )
PARTITION BY RANGE ( YEAR(separated) ) ( PARTITION p0 VALUES LESS THAN (1991), PARTITION p1 VALUES LESS THAN (1996), PARTITION p2 VALUES LESS THAN (2001), PARTITION p3 VALUES LESS THAN MAXVALUE );
  CREATE TABLE quarterly_report_status ( report_id INT NOT NULL, report_status VARCHAR(20) NOT NULL, report_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP )
PARTITION BY RANGE ( UNIX_TIMESTAMP(report_updated) ) ( PARTITION p0 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2008-01-01 00:00:00&amp;rsquo;) ), PARTITION p1 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2008-04-01 00:00:00&amp;rsquo;) ), PARTITION p2 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2008-07-01 00:00:00&amp;rsquo;) ), PARTITION p3 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2008-10-01 00:00:00&amp;rsquo;) ), PARTITION p4 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2009-01-01 00:00:00&amp;rsquo;) ), PARTITION p5 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2009-04-01 00:00:00&amp;rsquo;) ), PARTITION p6 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2009-07-01 00:00:00&amp;rsquo;) ), PARTITION p7 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2009-10-01 00:00:00&amp;rsquo;) ), PARTITION p8 VALUES LESS THAN ( UNIX_TIMESTAMP(&amp;lsquo;2010-01-01 00:00:00&amp;rsquo;) ), PARTITION p9 VALUES LESS THAN (MAXVALUE) );
 使用PARTITION BY RANGE COLUMNS关键词的分区表达式可以是DATE或DATETIME类型，并且可以接受多列作为一个元组进行分区。  CREATE TABLE members ( firstname VARCHAR(25) NOT NULL, lastname VARCHAR(25) NOT NULL, username VARCHAR(16) NOT NULL, email VARCHAR(35), joined DATE NOT NULL )
PARTITION BY RANGE COLUMNS(joined) ( PARTITION p0 VALUES LESS THAN (&amp;lsquo;1960-01-01&amp;rsquo;), PARTITION p1 VALUES LESS THAN (&amp;lsquo;1970-01-01&amp;rsquo;), PARTITION p2 VALUES LESS THAN (&amp;lsquo;1980-01-01&amp;rsquo;), PARTITION p3 VALUES LESS THAN (&amp;lsquo;1990-01-01&amp;rsquo;), PARTITION p4 VALUES LESS THAN MAXVALUE );
  CREATE TABLE rangecolumn ( a INT, b INT )
PARTITION BY RANGE COLUMNS(a,b) ( PARTITION p0 VALUES LESS THAN (0,10), PARTITION p1 VALUES LESS THAN (10,20), PARTITION p2 VALUES LESS THAN (10,30), PARTITION p3 VALUES LESS THAN (10,35), PARTITION p4 VALUES LESS THAN (20,40), PARTITION p5 VALUES LESS THAN (MAXVALUE,MAXVALUE) );
 ### 基于列表的分区 使用PARTITION BY LIST关键词的分区表达式返回结果必须是整数型的。  CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &amp;rsquo;1970-01-01&amp;rsquo;, separated DATE NOT NULL DEFAULT &amp;rsquo;9999-12-31&amp;rsquo;, job_code INT, store_id INT )
PARTITION BY LIST(store_id) ( PARTITION pNorth VALUES IN (3,5,6,9,17), PARTITION pEast VALUES IN (1,2,10,11,19,20), PARTITION pWest VALUES IN (4,12,13,14,18), PARTITION pCentral VALUES IN (7,8,15,16) );
 使用PARTITION BY LIST COLUMNS关键词的分区表达式可以是DATE或DATETIME类型，并且可以接受多列作为一个元组进行分区。  CREATE TABLE customers_2 ( first_name VARCHAR(25), last_name VARCHAR(25), street_1 VARCHAR(30), street_2 VARCHAR(30), city VARCHAR(15), renewal DATE )
PARTITION BY LIST COLUMNS(renewal) ( PARTITION pWeek_1 VALUES IN(&amp;lsquo;2010-02-01&amp;rsquo;, &amp;rsquo;2010-02-02&amp;rsquo;, &amp;rsquo;2010-02-03&amp;rsquo;, &amp;rsquo;2010-02-04&amp;rsquo;, &amp;rsquo;2010-02-05&amp;rsquo;, &amp;rsquo;2010-02-06&amp;rsquo;, &amp;rsquo;2010-02-07&amp;rsquo;), PARTITION pWeek_2 VALUES IN(&amp;lsquo;2010-02-08&amp;rsquo;, &amp;rsquo;2010-02-09&amp;rsquo;, &amp;rsquo;2010-02-10&amp;rsquo;, &amp;rsquo;2010-02-11&amp;rsquo;, &amp;rsquo;2010-02-12&amp;rsquo;, &amp;rsquo;2010-02-13&amp;rsquo;, &amp;rsquo;2010-02-14&amp;rsquo;), PARTITION pWeek_3 VALUES IN(&amp;lsquo;2010-02-15&amp;rsquo;, &amp;rsquo;2010-02-16&amp;rsquo;, &amp;rsquo;2010-02-17&amp;rsquo;, &amp;rsquo;2010-02-18&amp;rsquo;, &amp;rsquo;2010-02-19&amp;rsquo;, &amp;rsquo;2010-02-20&amp;rsquo;, &amp;rsquo;2010-02-21&amp;rsquo;), PARTITION pWeek_4 VALUES IN(&amp;lsquo;2010-02-22&amp;rsquo;, &amp;rsquo;2010-02-23&amp;rsquo;, &amp;rsquo;2010-02-24&amp;rsquo;, &amp;rsquo;2010-02-25&amp;rsquo;, &amp;rsquo;2010-02-26&amp;rsquo;, &amp;rsquo;2010-02-27&amp;rsquo;, &amp;rsquo;2010-02-28&amp;rsquo;) );
 ### 基于哈希的分区 使用PARTITION BY HASH关键字的哈希算法是简单地对分区数取模。  CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &amp;rsquo;1970-01-01&amp;rsquo;, separated DATE NOT NULL DEFAULT &amp;rsquo;9999-12-31&amp;rsquo;, job_code INT, store_id INT )
PARTITION BY HASH(store_id) PARTITIONS 4;
 使用PARTITION BY LINEAR HASH关键字的线性哈希算法： V是大于等于分区数的最接近的2的幂数。计算方法为：V = POWER(2, CEILING(LOG(2, 分区数))) Set N = F(column_list) &amp;amp; (V - 1). 当N大于等于分区数时，做如下循环计算直至小于分区数： V = CEIL(V / 2) N = N &amp;amp; (V - 1)  CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT &amp;rsquo;1970-01-01&amp;rsquo;, separated DATE NOT NULL DEFAULT &amp;rsquo;9999-12-31&amp;rsquo;, job_code INT, store_id INT )
PARTITION BY LINEAR HASH( YEAR(hired) ) PARTITIONS 6;
 如果一个员工2003年入职，则分区为#3。 V = POWER(2, CEILING( LOG(2,6) )) = 8 N = YEAR(&#39;2003-04-14&#39;) &amp;amp; (8 - 1) = 2003 &amp;amp; 7 = 3 如果一个员工1998年入职，则分区为#2。 V = 8 N = YEAR(&#39;1998-10-19&#39;) &amp;amp; (8-1) = 1998 &amp;amp; 7 = 6 N = 6 &amp;amp; CEILING(8 / 2) = 6 &amp;amp; 3 = 2 ### 基于键的分区 基于键的分区与基于哈希的分区类似，除了基于哈希的分区使用用户自定义的表达式，而基于键的分区使用MySQL提供的与PASSWORD()相类似的哈希算法。 键列表可以为空，也可以为多个列名。 键列表为空，使用主键id。  CREATE TABLE k1 ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20) )
PARTITION BY KEY() PARTITIONS 2;
 键列表为空，使用UNIQUE键KEY。  CREATE TABLE k1 ( id INT NOT NULL, name VARCHAR(20), UNIQUE KEY (id) )
PARTITION BY KEY() PARTITIONS 2;
 键列表为s1,此例效果等同于空的键列表。  CREATE TABLE tm1 ( s1 CHAR(32) PRIMARY KEY )
PARTITION BY KEY(s1) PARTITIONS 10;
 使用PARTITION BY LINEAR KEY关键字与使用PARTITION BY LINEAR HASH关键字具有一样的效果。  CREATE TABLE tk ( col1 INT NOT NULL, col2 CHAR(5), col3 DATE )
PARTITION BY LINEAR KEY (col1) PARTITIONS 3;
 ### 子分区--组合分区 子分区隐形定义：  CREATE TABLE ts (id INT, purchased DATE) PARTITION BY RANGE( YEAR(purchased) ) SUBPARTITION BY HASH( TO_DAYS(purchased) ) SUBPARTITIONS 2 ( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (2000), PARTITION p2 VALUES LESS THAN MAXVALUE );
 子分区显性定义：  CREATE TABLE ts (id INT, purchased DATE) PARTITION BY RANGE( YEAR(purchased) ) SUBPARTITION BY HASH( TO_DAYS(purchased) ) ( PARTITION p0 VALUES LESS THAN (1990) ( SUBPARTITION s0, SUBPARTITION s1 ), PARTITION p1 VALUES LESS THAN (2000) ( SUBPARTITION s2, SUBPARTITION s3 ), PARTITION p2 VALUES LESS THAN MAXVALUE ( SUBPARTITION s4, SUBPARTITION s5 ) );  ### 分区修剪（Partition Pruning） 分区修剪与PostgreSQL数据库的约束排除功能一样，就是不扫描与值不匹配的分区。 ### 分区选择 PostgreSQL数据库需要为分区创建表，而MySQL不需要。那如何仅对某个分区进行查询呢？分区选择就是解决这一需求的方案。  CREATE TABLE employees ( id INT NOT NULL AUTO_INCREMENT PRIMARY KEY, fname VARCHAR(25) NOT NULL, lname VARCHAR(25) NOT NULL, store_id INT NOT NULL, department_id INT NOT NULL ) PARTITION BY RANGE(id) ( PARTITION p0 VALUES LESS THAN (5), PARTITION p1 VALUES LESS THAN (10), PARTITION p2 VALUES LESS THAN (15), PARTITION p3 VALUES LESS THAN MAXVALUE );
 下面的查询语句仅应用于分区p1。  SELECT * FROM employees PARTITION (p1); ```
分区约束与限制 MySQL分区的约束与限制很多，这里就不列举了，详见MySQL手册。
</content>
    </entry>
    
     <entry>
        <title>R语言字符处理</title>
        <url>https://mryqu.github.io/post/r%E8%AF%AD%E8%A8%80%E5%AD%97%E7%AC%A6%E5%A4%84%E7%90%86/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>R语言</tag><tag>字符</tag><tag>处理</tag><tag>函数</tag>
        </tags>
        <content type="html"> 字符处理Encoding(x)
Encoding(x)  ## x is intended to be in latin1  x Encoding(x) [1] &#34;latin1&#34;  Encoding(x) xx Encoding(c(x, xx)) [1] &#34;latin1&#34; &#34;UTF-8&#34;  Encoding(xx) cat(&#34;xx = &#34;, xx, &#34;\n&#34;, sep = &#34;&#34;) xx = fa\xc3\xa7ilenchar(x, type = &#34;chars&#34;, allowNA = FALSE)
返回字符长度，在我的测试中allowNA参数没有作用？
nzchar(x) 判断是否空字符
对于缺失值NA，nchar和nzchar函数认为是字符数为2的字符串。
所以在对字符串进行测量之前，最好先使用is.na()函数判断一下是否是NA。
对于NULL，nchar和nzchar函数会忽略掉。 nchar(c(&#34;em&#34;,&#34;yqu&#34;,&#34;&#34;,NA)) [1] 2 3 0 2  nzchar(c(&#34;em&#34;,&#34;yqu&#34;,&#34;&#34;,NA)) [1] TRUE TRUE FALSE TRUE  nzchar(c(&#34;em&#34;,&#34;yqu&#34;,NULL,&#34;&#34;,NA)) [1] TRUE TRUE FALSE TRUE  nchar(c(&#34;em&#34;,&#34;yqu&#34;,NULL,&#34;&#34;,NA)) [1] 2 3 0 2  nchar(NULL) integer(0)  nzchar(NULL) logical(0)substr(x, start, stop)
substring(text, first, last = 1000000L)
substr(x, start, stop)  substr(&#34;abcdef&#34;, 2, 4) [1] &#34;bcd&#34;  substr(&#34;abcdef&#34;, -3, 9) [1] &#34;abcdef&#34;  substring(&#34;abcdef&#34;, 1:6, 1:6) [1] &#34;a&#34; &#34;b&#34; &#34;c&#34; &#34;d&#34; &#34;e&#34; &#34;f&#34;  x substring(x, 2, 4:5) [1] &#34;sfe&#34; &#34;wert&#34; &#34;uio&#34; &#34;&#34; &#34;tuf&#34;strtrim(x, width)
按显示宽度截断字符串 xstrtrim(x,c(2,1,3)) [1] &#34;ab&#34; NA &#34;66&#34;paste (..., sep = &#34; &#34;, collapse = NULL)
paste0(..., collapse = NULL)
通过sep连接间隔连接对象,返回字符串向量
设定collapse的话，会通过collapse连接间隔
将上一步的字符串向量连接成一个字符串
paste0(..., collapse)等同于paste(..., sep = &#34;&#34;, collapse) paste(1:6) # same as as.character(1:6) [1] &#34;1&#34; &#34;2&#34; &#34;3&#34; &#34;4&#34; &#34;5&#34; &#34;6&#34;  paste(&#34;A&#34;, 1:6, sep = &#34;=&#34;) [1] &#34;A=1&#34; &#34;A=2&#34; &#34;A=3&#34; &#34;A=4&#34; &#34;A=5&#34; &#34;A=6&#34;  paste(&#34;A&#34;, 1:6, sep = &#34;=&#34;, collapse=&#34;;&#34;) [1] &#34;A=1;A=2;A=3;A=4;A=5;A=6&#34;strsplit(x, split, fixed = FALSE, perl = FALSE, useBytes = FALSE)
基于split子句分割字符向量x
fixed为TRUE的话，完全匹配split；
否则，基于正则表达式
可以使用split=NULL来分割每个字符。 x strsplit(x, &#34;e&#34;) $as [1] &#34;mf&#34; $qu [1] &#34;qw&#34; &#34;rty&#34; [[3]] [1] &#34;70&#34; [[4]] [1] &#34;y&#34; &#34;s&#34;  strsplit(&#34;Hello world!&#34;, NULL) [[1]] [1] &#34;H&#34; &#34;e&#34; &#34;l&#34; &#34;l&#34; &#34;o&#34; &#34; &#34; &#34;w&#34; &#34;o&#34; &#34;r&#34; &#34;l&#34; &#34;d&#34; &#34;!&#34;  ## Note that &#39;split&#39; is a regexp!  unlist(strsplit(&#34;a.b.c&#34;, &#34;.&#34;)) [1] &#34;&#34; &#34;&#34; &#34;&#34; &#34;&#34; &#34;&#34;  ## If you really want to split on &#39;.&#39;, use  unlist(strsplit(&#34;a.b.c&#34;, &#34;[.]&#34;)) [1] &#34;a&#34; &#34;b&#34; &#34;c&#34;  unlist(strsplit(&#34;a.b.c&#34;, &#34;.&#34;, TRUE)) [1] &#34;a&#34; &#34;b&#34; &#34;c&#34;字符转换和大小写转换chartr(old, new, x)
将x中的字符old变换为字符new
 x chartr(&#34;iXs&#34;, &#34;why&#34;, x) [1] &#34;MwheD cAyE 123&#34;  chartr(&#34;a-cX&#34;, &#34;D-Fw&#34;, x) [1] &#34;MiweD FAsE 123&#34;tolower(x)
toupper(x)
casefold(x, upper = FALSE)
casefold是为了兼容S-PLUS而实现的tolower和toupper函数封装器。 x tolower(x) [1] &#34;mixed case 123&#34;  toupper(x) [1] &#34;MIXED CASE 123&#34;格式化输出sprintf(fmt, ...) 系统C库函数sprintf封装器 sprintf(&#34;%s is %f feet tall\n&#34;, &#34;Sven&#34;, 7.1) [1] &#34;Sven is 7.100000 feet tall\n&#34;format 格式化输出
formatC 格式化（C语言风格）输出strwrap(x, width = 0.9 * getOption(&#34;width&#34;),
indent = 0, exdent = 0, prefix = &#34;&#34;,
simplify = TRUE, initial = prefix)
将字符串封装成格式化段落 str strwrap(str, width=60,indent=1) [1] &#34; Now is the time&#34;  strwrap(str, width=60,indent=2) [1] &#34; Now is the time&#34;  strwrap(str, width=60,indent=3) [1] &#34; Now is the time&#34;  strwrap(str, prefix=&#34;kx&#34;) [1] &#34;kxNow is the time&#34;字符串匹配pmatch(x, table, nomatch = NA_integer_, duplicates.ok = FALSE)
局部字符串匹配，返回匹配的下标。
pmatch的行为因duplicates.ok参数而异。
当duplicates.ok为TRUE，有完全匹配的情况返回第一个完全匹配的下标，否则有唯一一个局部匹配的情况返回该唯一一个局部匹配的下标，没有匹配则返回nomatch参数值。
空字符串与任何字符串都不匹配，甚至是空字符串。
当duplicates.ok为FALSE，table中的值一旦匹配都被排除用于后继匹配，空字符串例外。
NA被视为字符常量&#34;NA&#34;。 pmatch(c(&#34;&#34;, &#34;ab&#34;, &#34;ab&#34;), c(&#34;abc&#34;, &#34;ab&#34;), dup = FALSE) [1] NA 2 1  pmatch(c(&#34;&#34;, &#34;ab&#34;, &#34;ab&#34;), c(&#34;abc&#34;, &#34;ab&#34;), dup = TRUE) [1] NA 2 2  pmatch(&#34;m&#34;, c(&#34;mean&#34;, &#34;median&#34;, &#34;mode&#34;)) # returns NA [1] NA charmatch(x, table, nomatch = NA_integer_)
局部字符串匹配，返回匹配的下标。
charmatch与uplicates.ok为TRUE的pmatch近似，当有单个完全匹配的情况返回该完全匹配的下标，否则有唯一一个局部匹配的情况返回该唯一一个局部匹配的下标，有多个完全匹配或局部匹配返回0，没有匹配则返回nomatch参数值。
charmatch允许匹配空字符串。
NA被视为字符常量&#34;NA&#34;。  charmatch(c(&#34;&#34;, &#34;ab&#34;, &#34;ab&#34;), c(&#34;abc&#34;,&#34;ab&#34;)) [1] 0 2 2  charmatch(&#34;m&#34;, c(&#34;mean&#34;, &#34;median&#34;, &#34;mode&#34;)) # returns 0 [1] 0match(x, table, nomatch = NA_integer_, incomparables = NULL)
x %in% table
值匹配，不限于字符串 sstr sstr[sstr %in% c(letters, LETTERS)] [1] &#34;e&#34; &#34;M&#34; &#34;P&#34;模式匹配和替换grep(pattern,x,ignore.case=FALSE,
perl=FALSE,value=FALSE,fixed=FALSE,
useBytes=FALSE,invert=FALSE)
返回匹配下标
grepl(pattern,x,ignore.case=FALSE,
perl=FALSE,fixed=FALSE,useBytes=FALSE)
返回匹配逻辑结果
sub(pattern,replacement,x,ignore.case=FALSE,
perl=FALSE,fixed=FALSE,useBytes=FALSE)
替换第一个匹配的字符串
gsub(pattern,replacement,x,ignore.case=FALSE,
perl=FALSE,fixed=FALSE,useBytes=FALSE)
替换全部匹配的字符串
regexpr(pattern,text,ignore.case=FALSE,
perl=FALSE,fixed=FALSE,useBytes=FALSE)
返回第一个匹配的下标和匹配长度
gregexpr(pattern,text,ignore.case=FALSE,
perl=FALSE,fixed=FALSE,useBytes=FALSE)
返回全部匹配的下标和匹配长度
regexec(pattern,text,ignore.case=FALSE,
fixed=FALSE,useBytes=FALSE)
返回第一个匹配的下标和匹配长度
这些函数(除了不支持Perl风格正则表达式的regexec函数)可以工作在三种模式下:fixed = TRUE: 使用精确匹配perl = TRUE: 使用Perl风格正则表达式fixed = FALSE且perl = FALSE: 使用POSIX 1003.2扩展正则表达式useBytes = TRUE时逐字节匹配，否则逐字符匹配。
其主要作用是避免对多字节字符码中无效输入和虚假匹配的错误/告警,但是对于regexpr，它改变了输出的解释。
它会阻止标记编码的输入进行转换，尤其任一输入被标记为“字节”时强制禁止转换。 strgrep(&#34; &#43;&#34;, str) [1] 1 3  grepl(&#34; &#43;&#34;, str) [1] TRUE FALSE TRUE  sub(&#34; &#43;&#34;, &#34;&#34;, str) [1] &#34;Nowis &#34; &#34;the&#34; &#34;time &#34;  sub(&#34;[[:space:]]&#43;&#34;, &#34;&#34;, str) ## white space, POSIX-style [1] &#34;Nowis &#34; &#34;the&#34; &#34;time &#34;  sub(&#34;\\s&#43;&#34;, &#34;&#34;, str, perl = TRUE) ## Perl-style white space [1] &#34;Nowis &#34; &#34;the&#34; &#34;time &#34;  gsub(&#34; &#43;&#34;, &#34;&#34;, str) [1] &#34;Nowis&#34; &#34;the&#34; &#34;time&#34;  regexpr(&#34; &#43;&#34;, str) [1] 4 -1 1 attr(,&#34;match.length&#34;) [1] 1 -1 1 attr(,&#34;useBytes&#34;) [1] TRUE  gregexpr(&#34; &#43;&#34;, str) [[1]] [1] 4 7 attr(,&#34;match.length&#34;) [1] 1 1 attr(,&#34;useBytes&#34;) [1] TRUE [[2]] [1] -1 attr(,&#34;match.length&#34;) [1] -1 attr(,&#34;useBytes&#34;) [1] TRUE [[3]] [1] 1 6 attr(,&#34;match.length&#34;) [1] 1 2 attr(,&#34;useBytes&#34;) [1] TRUE  regexec(&#34; &#43;&#34;, str) [[1]] [1] 4 attr(,&#34;match.length&#34;) [1] 1 [[2]] [1] -1 attr(,&#34;match.length&#34;) [1] -1 [[3]] [1] 1 attr(,&#34;match.length&#34;) [1] 1regmatches(x, m, invert = FALSE)
regmatches(x, m, invert = FALSE)  strmregmatches(str,m)str [1] &#34;Nowkxis &#34; &#34;the&#34; &#34;kxtime &#34;   strmregmatches(str,m, invert=TRUE)str [1] &#34;kx kx kx&#34; &#34;kx&#34; &#34;kx kx kx&#34;agrep(pattern, x, max.distance = 0.1,
costs = NULL, ignore.case = FALSE,
value = FALSE, fixed = TRUE,
useBytes = FALSE)
agrepl(pattern, x, max.distance = 0.1,
costs = NULL, ignore.case = FALSE,
fixed = TRUE, useBytes = FALSE)
使用广义Levenshtein编辑距离进行字符串近似匹配待进一步研究 str agrep(&#34;laysy&#34;, str, max = 2) [1] 1grepRaw(pattern, x, offset = 1L,
ignore.case = FALSE, value = FALSE,
fixed = FALSE, all = FALSE,
invert = FALSE)
对原始数据向量进行模式匹配 raws raws [1] 4e 6f 77 20 69 73 20 74 68 65 20 74 69 6d 65 20  grepRaw(charToRaw(&#34; &#43;&#34;),raws) [1] 4glob2rx(pattern, trim.head = FALSE, trim.tail = TRUE)
将通配符模式变成正则表达式 glob2rx(&#34;abc.*&#34;) [1] &#34;^abc\\.&#34;  glob2rx(&#34;a?b.*&#34;) [1] &#34;^a.b\\.&#34;  glob2rx(&#34;a?b.*&#34;, trim.tail = FALSE) [1] &#34;^a.b\\..*$&#34;  glob2rx(&#34;*.doc&#34;) [1] &#34;^.*\\.doc$&#34;  glob2rx(&#34;*.doc&#34;, trim.head = TRUE) [1] &#34;\\.doc$&#34;  glob2rx(&#34;*.t*&#34;) [1] &#34;^.*\\.t&#34;  glob2rx(&#34;*.t??&#34;) [1] &#34;^.*\\.t..$&#34;  glob2rx(&#34;*[*&#34;) [1] &#34;^.*\\[&#34; </content>
    </entry>
    
     <entry>
        <title>R语言数值计算</title>
        <url>https://mryqu.github.io/post/r%E8%AF%AD%E8%A8%80%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>R语言</tag><tag>数值</tag><tag>计算</tag><tag>运算</tag>
        </tags>
        <content type="html"> R中数值计算的对象一般是向量或列表，不同长度的对象进行计算时，短的对象元素将被循环使用。
运算操作符&#43; - * /
&amp; | ！
== !=  =  aba/b [1] 1.000000 7.000000 4.166667 4.000000 Warning message: In a/b : longer object length is not a multiple of shorter object length  2^5 [1] 32  25%%6 [1] 1  13%/%5 [1] 2  7&amp;8 [1] TRUE无归类的函数sign 取符号 sign(-2:2) [1] -1 -1 0 1 1abs 取绝对值sqrt 取平方根 sqrt(-2:2) [1] NaN NaN 0.000000 1.000000 1.414214 Warning message: In sqrt(-2:2) : NaNs produced对数与指数函数log(x, base = exp(1)) 取base为底的对数，base缺省的情况下取自然对数
logb(x, base = exp(1)) 是为了兼容S语言而实现的log函数封装器
log10(x) 取常用对数
log2(x) 取2为底的对数 log(64, 8) [1] 2log1p(x)
计算log(1&#43;x)exp(x) 取指数 exp(1) [1] 2.718282expm1(x)
计算exp(x) - 1取整舍入ceiling(x)
向上取整 ceiling(3.666666) [1] 4  ceiling(-3.666666) [1] -3floor(x)
向下取整 floor(3.666666) [1] 3  floor(-3.666666) [1] -4trunc(x, ...)
取整 trunc(3.666666) [1] 3  trunc(-3.666666) [1] -3round(x, digits = 0)
四舍五入，正数的digits为小数个数，负数的digits为整数的个数 round(3.666666,3) [1] 3.667  round(-3.666666,3) [1] -3.667  round(266.6666,-2) [1] 300signif(x, digits = 6)
四舍五入，正数的digits为整数和小数的个数，负数的digits为整数的个数 signif(3.666666,3) [1] 3.67  signif(-3.666666,3) [1] -3.67  signif(266.6666,-2) [1] 300zapsmall(x, digits = getOption(&#34;digits&#34;))
zapsmall选取round的digits参数，以使向量中与最大绝对值相比接近0的数值被剃掉 zapsmall function (x, digits = getOption(&#34;digits&#34;)) { if (length(digits) == 0L) stop(&#34;invalid &#39;digits&#39;&#34;) if (all(ina 0) max(0L, digits -log10(mx)) else digits) }三角函数sin(x) 正弦函数 sin(pi/6) [1] 0.5cos(x) 余弦函数tan(x) 正切函数asin(x) 反正弦函数 asin(0.5) #-pi/6 [1] 0.5235988acos(x) 反余弦函数atan(x) 反正切函数atan2 反正切函数
atan2(y, x)=atan(y/x).sinpi(x) 正弦函数
sinpi(x)=sin(pi*x) sinpi(1/6) [1] 0.5cospi(x) 余弦函数
cospi(x)=cos(pi*x)tanpi(x) 正切函数
tanpi(x)=tan(pi*x)双曲函数sinh(x) 双曲正弦函数
sinh(x)=(exp(x)-exp(-x))/2 sinh(pi/6) [1] 0.5478535  (exp(pi/6)-exp(-pi/6))/2 [1] 0.5478535cosh(x) 双曲余弦函数
cosh(x)=(exp(x)&#43;exp(-x))/2tanh(x) 双曲正切函数
tanh(x)=sinh(x)/cosh(x)asinh(x) 反双曲正弦函数acosh(x) 反双曲余弦函数atanh(x) 反双曲正切函数汇总函数max(..., na.rm = FALSE)
取最大值 max(5:1, pi) #- one number [1] 5min(..., na.rm = FALSE)
取最小值 min(5:1, pi) #- one number [1] 1pmax(..., na.rm = FALSE)
取并行最大值 pmax(5:1, pi) #- 5 numbers [1] 5.000000 4.000000 3.141593 3.141593 3.141593pmin(..., na.rm = FALSE)
取并行最小值 pmin(5:1, pi) #- 5 numbers [1] 3.141593 3.141593 3.000000 2.000000 1.000000range(..., na.rm = FALSE, finite = FALSE)
返回含最大值和最小值的向量 x range(x) [1] NA NA  range(x, na.rm = TRUE) [1] -Inf Inf  range(x, finite = TRUE) [1] 1 3which.min(x)
返回最小值下标 x which.min(x) [1] 5which.max(x)
返回最大值下标 x which.max(x) [1] 11any(..., na.rm = FALSE)
任一个元素值为真则返回TRUE;
所有元素值为假则返回FALSE;
na.rm参数为FALSE时，没有元素值为真且数据中有NA值则返回NA any(c(-2:0)0) [1] FALSE  any(c(1:2,NA)0) [1] TRUE  any(c(-2:0,NA)0) [1] NAall(..., na.rm = FALSE)
所有元素值为真则返回TRUE;
任一个元素值为假则返回FALSE;
na.rm参数为FALSE时，没有元素值为假且数据中有NA值则返回NA all(c(1:2)0) [1] TRUE  all(c(-2:0,NA)0) [1] FALSE  all(c(1:2,NA)0) [1] NAsum(..., na.rm = FALSE)
任一个元素值为NA则返回NA;
元素不含NA，任意元素值为NaN则返回NaN；
元素不含NA和NaN，返回元素和，NULL视为整数0 sum(1:3) [1] 6  sum(1:3,NA) [1] NA  sum(1:3,NaN) [1] NaN  sum(1:3,NaN,NA) [1] NAprod(..., na.rm = FALSE)
任一个元素值为NA则返回NA;
元素不含NA，任意元素值为NaN则返回NaN；
元素不含NA和NaN，返回元素乘积，NULL视为数字0 prod(1:3) [1] 6  prod(1:3,NA) [1] NA  prod(1:3,NaN) [1] NaN  prod(1:3,NaN,NA) [1] NA累计函数cumsum 取累计和
遇到NA或NaN元素，返回NA
NULL元素会被忽略掉 cumsum(1:5) [1] 1 3 6 10 15  cumsum(c(1:3,NA,4:5)) [1] 1 3 6 NA NA NA  cumsum(c(1:3,NaN,4:5)) [1] 1 3 6 NA NA NA  cumsum(c(1:3,NULL,4:5)) [1] 1 3 6 10 15cumprod 取累计乘积
遇到NA或NaN元素，返回NA
NULL元素会被忽略掉 cumprod(1:5) [1] 1 2 6 24 120cummin 取累计最小值
如果累计最小值不为NA，遇到NaN元素输出NaN；
遇到NA元素输出NA
NULL元素会被忽略掉 #对于列表元素3 2 1 2 1 0 4 3 2，cummin第四个输入为2，  #比累计最小值1大，所以第四个输出为1  cummin(c(3:1, 2:0, 4:2)) [1] 3 2 1 1 1 0 0 0 0  cummin(c(3:1, NaN, 2:0, NA, 4:2)) [1] 3 2 1 NaN NaN NaN NaN NA NA NA NAcummax 取累计最大值
如果累计最小值不为NA，遇到NaN元素输出NaN；
遇到NA元素输出NA
NULL元素会被忽略掉 #对于列表元素3 2 1 2 1 0 4 3 2，cummin前六个输出都为第一个输入3，  #第七个输入为4，比累计最小值3大，所以第七个输出为4  cummax(c(3:1, 2:0, 4:2)) #3 2 1 2 1 0 4 3 2 [1] 3 3 3 3 3 3 4 4 4  cummax(c(3:1, NA, 2:0, NaN, 4:2)) [1] 3 3 3 NA NA NA NA NA NA NA NAbeta和gamma相关函数 ![R语言数值计算](/images/2013/7/0026uWfMgy6PF9i0WUI64.jpg)此外，还有Bessel函数:besselI(x, nu, expon.scaled = FALSE)besselK(x, nu, expon.scaled = FALSE)besselJ(x, nu)besselY(x, nu)其中，排列组合函数choose和阶乘函数factorial比较常用。复数函数Re(z) 取实部
Im(z) 取虚部
Mod(z) 取模
Arg(z) 取幅角
Conj(z) 共轭复数
对于一个复数z=x &#43; iy，
r=Mod(z)=√(x^2&#43;y^2)，
φ=Arg(z)，
x=r*cos(φ)且y=r*sin(φ) 1:2 &#43; 1i*(8:9) [1] 1&#43;8i 2&#43;9i  z Re(z) [1] 1 2 3  Im(z) [1] 7 8 9  Mod(z) [1] 7.071068 8.246211 9.486833  Arg(z) [1] 1.428899 1.325818 1.249046  Conj(z) [1] 1-7i 2-8i 3-9i排序函数rev 逆序 x rev(x) [1] 3 4 5 5 4 3 2 1sort(x, decreasing = FALSE, ...)
向量、列表或因子排序 x sort(x) [1] 1 3 4 15 92order(..., na.last = TRUE, decreasing = FALSE)
次序下标，可用于多个变量，例如数据框
第一个输出为5，是由于第五个输入按增序排第一 set.seed(1)  x order(x) [1] 5 1 2 3 4rank(x, na.last = TRUE,
ties.method = c(&#34;average&#34;, &#34;first&#34;, &#34;random&#34;, &#34;max&#34;, &#34;min&#34;))
排名 set.seed(1)  x rank(x) [1] 2 3 4 5 1 </content>
    </entry>
    
     <entry>
        <title>git资料</title>
        <url>https://mryqu.github.io/post/git%E8%B5%84%E6%96%99/</url>
        <categories>
          <category>Tool</category><category>Git</category>
        </categories>
        <tags>
          <tag>git</tag><tag>gh-pages</tag><tag>资料</tag><tag>版本控制</tag><tag>代码库</tag>
        </tags>
        <content type="html">  综合 Pro Git 英文版 中文版
Git Community Book 英文版 中文版
GotGitHub
Git使用详解
项目主页 Creating Project Pages manually
Setup GitHub Pages &amp;ldquo;gh-pages&amp;rdquo; branch and &amp;ldquo;master&amp;rdquo; branch as subfolders of a parent project folder (&amp;ldquo;grandmaster&amp;rdquo;).
Setup GitHub Pages &amp;ldquo;gh-pages&amp;rdquo; branch as a subfolder within the &amp;ldquo;master&amp;rdquo; project on your local checkout - a step-by-step guide.
其他 Collaborative Github Workflow
如何理解git reset 取消提交的操作？
Git Document
Atlassian Git Tutorial
版本管理svn,git,cvs比较
learnGitBranching
a successful git branching model
Git下的冲突解决
学习笔记-git
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] mapred和mapreduce包的区别</title>
        <url>https://mryqu.github.io/post/hadoop_mapred%E5%92%8Cmapreduce%E5%8C%85%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapred</tag><tag>mapreduce</tag><tag>package</tag><tag>difference</tag>
        </tags>
        <content type="html">  背景介绍 在Hadoop的代码中，存在org.apache.hadoop.mapred和org.apache.hadoop.mapreduce两个包。mapred包下是老的API，在Hadoop0.20时被废弃了，引入了新包mapreduce，但是由于新的API迟迟没有完成，所以在Hadoop0.21中取消了mapred包的废弃状态。原来的设想中老包mapred在Hadoop0.22和1.0中将再次设成废弃状态，但时至今日也没有被废弃。
区别 本文将通过WordCount示例代码，介绍一下二者的区别。WordCount示例代码分别取自0.19和0.23.9版本的Hadoop源码。
0.19版WordCount示例 0.23.9版WordCount示例 区别新API老API包新API位于org.apache.hadoop.mapreduce包内老API位于org.apache.hadoop.mapred.包内Mapper和Reducer类型新API使用Mapper和Reducer抽象类
抽象类更容易扩展，Hadoop实现可以轻松向其抽象类中添加方法(用默认的实现)而不会对已有Hadoop应用造成影响老API使用Mapper和Reduceer接口使用对象新API使用Configuration和一些Helper类完成作业配置；
新API使用Job完成作业控制；
新API使用Context完成用户代码与MapReduce系统的通信。老API使用JobConf
完成作业配置，它是Configuration子类；
老API使用JobClient完成作业控制；
老API使用OutputCollector和Reporter完成用户代码与MapReduce系统的通信。
方法map() reduce() clearup() setup() run()；
所有方法可抛IOException或InterruptedException；
Reduce()输入值为java.lang.Iterable；键值对输出通过Context对象的write方法实现；
map() reduce()；
所有方法可抛IOException；
Reduce()输入值为java.lang.Iterator；
键值对输出通过OutputCollector对象的collect方法实现；输出文件part-m-nnnnn和part-r-nnnnn
(nnnnn为从0开始的整数)part-nnnnn
注意事项 尽量使用新API。在mapred和mapreduce两个包下存在FileInputFormat、FileOutputFormat等名字一样的类，如果引入错误的话，程序会无法通过编译。
参考 Upgrading To The New Map Reduce API
Difference between Hadoop OLD API and NEW API
</content>
    </entry>
    
     <entry>
        <title>Eclipse RCP资料</title>
        <url>https://mryqu.github.io/post/eclipse_rcp%E8%B5%84%E6%96%99/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>rcp</tag><tag>资料</tag>
        </tags>
        <content type="html"> Lubos: Eclipse plugin and RCP development notes
</content>
    </entry>
    
     <entry>
        <title>分布式事务处理</title>
        <url>https://mryqu.github.io/post/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>事务</tag><tag>transaction</tag><tag>jta</tag><tag>2pc</tag><tag>dtp</tag>
        </tags>
        <content type="html">  事务 概念 企业级应用程序经常需要访问多个组件共享的分布式数据并执行操作。这些程序应该在下列情况下保持（由应用程序商业规则所定义的）数据完整性： - 分布式访问单个数据资源 - 从单个应用组件访问分布式资源
在这种情况下，对（分布式）资源上的一组操作可能需要被作为一个工作单元来对待。在一个工作单元中的所有操作必须全部成功或在失败时一起回滚到之前的状态。
下列情况下情况会更加复杂，需要应用程序维护工作单元的成功或失败信息： - 在一组分布式组件上实现的工作单元对多个资源上的数据进行操作 - 串行或在并行线程中执行的操作需要协调或同步
事务(Transaction)以及事务管理器（或事务处理服务）降低了企业级分布式应用程序构建难度，维护了数据的完整性。
特性 事务是恢复和并发控制的基本单位。 事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。 - 原子性（atomicity）。一个事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。当任一操作失败，事务中的所有操作需要撤销，数据恢复到之前的状态。 - 一致性（consistency）。事务必须保持定义在数据上的不变属性（例如完整性约束）。在事务成功结束后，数据必须处于一致状态。换句话说，事务必须是使资源从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 - 隔离性（isolation）。一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。这需要：在事务执行期间，数据的（可能不一致的）中间状态不会暴露给其他事务；两个并发事务不能对同一数据进行操作。数据库管理系统通常使用锁机制实现这一功能。 - 持久性（durability）。持续性也称永久性（permanence），指一个事务一旦提交，它对资源中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。
嵌套事务 事务处理组件 应用程序组件 应用程序组件是事务型资源的客户端，包含商业事务程序。在事务管理器的帮助下，这些组件创建全局事务并划分事务界限，在必要时传播事务上下文，在事务范围内通过资源管理器对事务型资源进行操作。这些组件不负责实现事务语义。然而，作为应用逻辑的一部分，这些组件通常决定是否提交还是回滚事务。
资源管理器 资源管理器是管理持久化和稳定数据存储系统的组件，并同事务管理器一起参与两阶段提交和恢复协议。 资源管理器通常是稳定数据存储系统的驱动或者包装器，提供两套接口接口：一套接口用于应用程序组件获得连接并操作数据、另一套接口用于事务管理器参与两阶段提交和恢复协议。该组件可能也直接或间接向事务管理器注册资源，以便事务管理器能够跟踪所有参与事务的资源。该过程称之为资源征集（enlistment）。为了实现两阶段提交和恢复协议，资源管理器应该实现辅助机制用于事务恢复。
事务管理器 事务管理器是事务处理环境中的核心组件。主要职责是应应用程序组件请求创建事务、允许资源征集（enlistment）和遣散（delistment）、同资源管理器一起实施两阶段提交和恢复协议。 一个典型事务型应用程序发起事务，向事务管理器发出请求初始化一个事务。作为应答，事务管理器启动一个事务并与当前线程关联。事务管理器也会创建一个事务上下文。所有参与事务的应用程序组件/线程共享事务上下文。发起事务的线程或其他线程在事务管理器允许的情况下通过发出提交或回滚请求来终止事务。 在事务终止之前，任意数量的组件或线程可在事务管理器已知的任意数量事务型资源上执行事务型操作。在资源管理器允许的情况下，事务在最终终止之前可以被挂起或恢复。 一旦应用程序发出提交请求，事务管理器（通过投票）准备用于提交操作的所有资源，并基于是否所有资源准备好提交向所有资源发出提交或回滚请求。在两阶段提交和恢复协议处理过程前后调用应用程序组件synchronization回调。
两阶段提交（2PC） 两阶段提交分为两个阶段：投票阶段和决策阶段。 - 在投票（或准备）阶段，事务管理器会询问每个资源管理器是否同意成功执行。资源管理器有可能返回否定应答，例如当超时导致数据库回滚。如果资源管理器做肯定应答的话，它应该确信它始终可以将工作持久化（这暗示它不会应为内部超时取消工作）， - 事务管理器接收到所有应答（也称之为选票），会对事务结果作出全局决策。该决策取决于收集的应答: - 如果所有应答都是肯定的（意味着每个稳定数据存储系统都可以将工作持久化），事务管理器会指示每个资源管理器进行提交。 - 如果有一个应答是否定的（或丢失），则回滚决策会发给剩余的资源，剩余资源取消该事务中的工作。
有两件事需要注意: - 每个资源必须有能力理解两阶段提交：它需要回复来自事务管理器的准备请求，并能在事务管理器决定回滚时取消工作。 - 如果资源在准备阶段投肯定票之后跟事务管理器联系中断（例如事务管理器崩溃），它将不知道如何去做。由于两阶段提交协议规则它不能自己取消，因此它需要无期限地记着这个事务。此外，这限制了其他事务的并发访问。在这种情况下，该资源被称为不确定的。 资源处于不确定状态并限制并发访问令很多厂商烦恼。为了减少限制，两阶段提交协议的实践变种包含一种叫做试探性决策：太长时间处于不确定状态的资源会决定单方面回滚（或提交）事务，有可能导致违背全部接受或全部不接受的特性。
事务处理标准和技术 X/Open 分布式事务处理(DTP)模型是厂商协会OpenGroup提出的一个分布式处理模型，在事务处理和数据库领域中多数商业厂商间的一个标准。 对象事务服务(OTS)是由对象管理组织(OMG)规定的分布式事务处理服务。这个规范扩展了 CORBA 模型并定义了一系列跨越多个CORBA 对象完成事务处理的接口。 JTA和JTS是Sun提出的事务处理和事务服务的Java规范。 微软事务服务器（MTS）是一个基于构件的事务服务器，它的构件基于微软的构件对象模型(COM)。MTS编程模型为建造事务性COM构件提供接口，而MTS运行环境提供一个部署和管理这些构件和管理事务的方法。使用了MTS，由多个COM构件做的工作可被组合在一个单一的事务中。 企业级Java Beans (EJB)是Sun提出的一个技术规范。它规定了一个建造基于构件的分布式应用的框架。
JTA 概念 JTA是Java事务API的缩写，定义了在事务管理器与分布式事务系统的资源管理器、应用服务器、事务型应用程序之间的标准Java接口，包括javax.transaction和javax.transaction.xa包。在实际系统中还需要厂商专有的JTA实现。JTA是JavaEE平台的标准部分和每个EJB应用服务器都包含了JTA实现。
JTA组件 JTA的主要组件为：
TransactionManager TransactionManager可用于创建事务，获取、挂起和恢复当前事务,设置事务超时值属性。其方法为线程安全的，多个并发线程可以创建自己的事务，获取仅自己创建的事务。
|begin|创建一个新事务并与当前线程关联 |commit|提交当前线程关联的事务 |getStatus|获取当前线程关联的事务状态 |getTransaction|获取当前线程关联的事务 |resume|使用挂起的事务对象代表的事务恢复调用线程的事务上下文 |rollback|回滚当前线程关联的事务 |setRollbackOnly|修改当前线程关联的事务以致事务的唯一可能结果是回滚 |setTransactionTimeout|修改当前线程使用begin方法启动的事务的超时值 |suspend|挂起调用线程当前关联的事务并返回代表正在挂起事务上下文的事务对象
Transaction Transaction允许对有效的事务进行操作。一个事务对象对应着一个全局事务，它可用于资源征集/遣散、synchronization注册、结束事务和状态查询操作。
|commit|提交事务对象代表的事务 |delistResource|接触资源与目标事务对象代表的事务之间的关联 |enlistResource|关联资源与目标事务对象代表的事务 |getStatus|获得目标事务对象代表的事务 |registerSynchronization|向目标事务对象代表的事务注册Synchronization |rollback|回滚事务对象代表的事务 |setRollbackOnly|修改目标事务对象代表的事务以致事务的唯一可能结果是回滚
Xid 该接口对于事务管理器和在XAResource后面的稳定数据存储系统之间的通信十分重要。XAResource基本上是同稳定数据存储系统之间的连接，任意多个不同事务使用相同的连接。 每次事务管理器发起和结束事务，它需要使用稳定数据存储系统能够理解的事务标识。最后要说的是，一个事务可能关联一个甚至多个Xid实例。
XAResource XAResource是事务管理器同数据源的连接。对于每个应用级别连接，需要一个XAResource让应用程序通过JTA事务的连接部分进行工作。
Synchronization 该接口是注册应用级回调的一种方式，它允许应用程序接收两阶段提交事件通知。可以在应用程序中通过实现该接口来使用上述功能。 注意：synchronizations不是持久化的。在事务崩溃并恢复后synchronizations将丢失。
 beforeCompletion: 该方法在事务开始提交前调用。该方法的一个典型使用是向数据库写挂起的更新。 afterCompletion: 该方法在提交或回滚结束后调用，指示事务是否成功。  UserTransaction 该接口是JTA功能的简单受限版本，是EJB中处理的典型应用级事务服务，使用该接口仅向应用程序暴露JTA功能的一个子集。
Exceptions JTA中有异常是关于试探性（heuristic）终止的。只要发生试探性错误，事务管理器应该保留该事务的日志，以便管理员能够人力解决冲突。 - HeuristicCommitException：所有资源在两阶段提交的准备阶段返回肯定应答，之后跟事务管理器联系中断、长时间处于不确定状态，可能通过试探性决策提交或回滚。如果事务管理器之后跟这部分资源重新建立联系并指示这些资源回滚，则此类型异常抛给应用程序。它表明所有资源由于处于不确定状态选择试探性提交后的事务结果失常。意味着尽管期望回滚，整个事务却提交了。 - HeuristicRollbackException:尽管事务管理器的最终决策是提交，所有资源进行试探性回滚。意味着尽管期望提交，整个事务却回滚了。 - HeuristicMixedException:这是当部分资源提交而部分资源回滚的更复杂错误。它表明事务仅部分生效，明显违反事务语义。需要记住的是，更多信息需要记录到日志中。
JTA交互 本节主要介绍用于数据库数据源的典型JTA交互。对于其他资源（例如JMS队列），仅在XAResource获取上有所不同。
激活事务 事务提交 事务回滚 下面展示一个可能发生的回滚场景:应用程序请求提交，但是一个XAResource超时并在请求资源准备之前回滚。结果就是事务回滚并返回应用级回滚异常。 事务出错终止 下面展示一个可能发生的试探性场景:应用程序请求提交，但是一个XAResource收到资源准备请求之后不可达。结果就是试探性回滚（heuristicrollback）并返回应用级试探性混合异常。 参考 JTA规范
EJB规范(见事务章节)
JavaEE5规范(见事务章节)
JavaDocs: javax.transaction、javax.transaction.xa和JTS
Spring事务
Atomikos Transactions Guide Nuts and Bolts of Transaction Processing中文翻译版
Java事务设计策略
</content>
    </entry>
    
     <entry>
        <title>小玩Java序列化</title>
        <url>https://mryqu.github.io/post/%E5%B0%8F%E7%8E%A9java%E5%BA%8F%E5%88%97%E5%8C%96/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>序列化</tag>
        </tags>
        <content type="html"> 折腾GemFire，免不了要折腾序列化和发序列化。GemFire支持Java的序列化，同时也有自己的DataSerializable接口实现自己的序列化，此外还有Delta接口支持数据同步时仅传送上一次数据同步后的更新。 今天测试的实现先用Java的序列化，一开始玩java.io.Serializable接口，后来玩writeObject()和readObject()方法。顺便看看有writeObject方法后ObjectOutputStream调用堆栈与原有分支的不同。
private void writeSerialData(Object obj, ObjectStreamClass desc) throws IOException { ObjectStreamClass.ClassDataSlot[] slots = desc.getClassDataLayout(); for (int i = 0; i &amp;lt; slots.length; i&#43;&#43;) { ObjectStreamClass slotDesc = slots[i].desc; if (slotDesc.hasWriteObjectMethod()) { PutFieldImpl oldPut = curPut; curPut = null; if (extendedDebugInfo) { debugInfoStack.push(&amp;quot;custom writeObject data (class \&amp;quot;&amp;quot; &#43; slotDesc.getName() &#43; &amp;quot;\&amp;quot;)&amp;quot;); } SerialCallbackContext oldContext = curContext; try { curContext = new SerialCallbackContext(obj, slotDesc); bout.setBlockDataMode(true); slotDesc.invokeWriteObject(obj, this); bout.setBlockDataMode(false); bout.writeByte(TC_ENDBLOCKDATA); } finally { curContext.setUsed(); curContext = oldContext; if (extendedDebugInfo) { debugInfoStack.pop(); } } curPut = oldPut; } else { defaultWriteFields(obj, slotDesc); } } }  Java序列化会写入类描述符，在序列化后的字节数据占一定比例。GemFire支持Java序列化的同时有自己的序列化实现，GemFire自己的序列化实现更高效，而像Hadoop这样的项目则根本不使用Java的序列化，只支持自己的序列化实现。 为了让父类不可序列化的子类序列化，需要父类有public或protected无参构造器，子类需要负责存储和恢复父类的public、protected和（如可访问的）package字段。当父类没有无参构造器，会在运行态序列化子类时返回错误。 下面是Java序列化规范和两个博客连接（这两篇写的都非常细致，后一个一篇都3到5万字，很值得学习）。 Java Object Serialization Specification 理解Java对象序列化 Java序列化123
</content>
    </entry>
    
     <entry>
        <title>动态GemFire Region使用局限</title>
        <url>https://mryqu.github.io/post/%E5%8A%A8%E6%80%81gemfire_region%E4%BD%BF%E7%94%A8%E5%B1%80%E9%99%90/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category><category>GemFire</category>
        </categories>
        <tags>
          <tag>gemfire</tag><tag>动态region</tag><tag>局限</tag>
        </tags>
        <content type="html">  介绍 当前项目原有设计使用了嵌套的Map作为缓存。我在使用GemFire产品时使用普通region替代外层map，使用动态region替代内层map。
对于&amp;rdquo;/outer_region/Dimension&amp;rdquo;region中的条目，键是维ID（例如5），值是动态region&amp;rdquo;/inter_region/Dimension/5&amp;rdquo;. &amp;ldquo;/outer_region/Dimension&amp;rdquo; region的容量是256个条目，动态region&amp;rdquo;/inter_region/Dimension/5&amp;rdquo;的容量是10个条目。 当维#5被删除，&amp;rdquo;/outer_region/Dimension&amp;rdquo;相应条目应该删除，而动态region&amp;rdquo;/inter/Dimension/5&amp;rdquo;应该被销毁。
测试发现，当GemFire节点通过API创建了动态region后，它会将动态region列表发送给其他节点，此后的GemFire节点在创建Cache会失败。通过异常日志可知动态region会在Cache创建过程中被创建，但后继GemFire节点无法找到父region (在该节点创建Cache之前根本没机会创建region!),这是导致失败的原因。
如果对cache使用cache-xml-file， GemFire会先创建普通region之后创建动态region。这需要将一个cache下所有region定义都放到cache-xml-file里，上述问题可能会被避免。但是一个cache被多个项目共享，所有项目的region定义放在一起的话，配置耦合度很高，对于产品的灵活性和扩展性很不利。
日志 Caused by: com.gemstone.gemfire.cache.RegionDestroyedException: Error -- Could not find a region named: &#39;/inter_region/Dimension&#39; at com.gemstone.gemfire.cache.DynamicRegionFactory.createDynamicRegionImpl(DynamicRegionFactory.java) at com.gemstone.gemfire.cache.DynamicRegionFactory.createDefinedDynamicRegions(DynamicRegionFactory.java) at com.gemstone.gemfire.cache.DynamicRegionFactory._internalInit(DynamicRegionFactory.java) at com.gemstone.gemfire.internal.cache.DynamicRegionFactoryImpl.internalInit(DynamicRegionFactoryImpl.java) at com.gemstone.gemfire.internal.cache.GemFireCacheImpl.readyDynamicRegionFactory(GemFireCacheImpl.java) at com.gemstone.gemfire.internal.cache.GemFireCacheImpl.initializeDeclarativeCache(GemFireCacheImpl.java) at com.gemstone.gemfire.internal.cache.GemFireCacheImpl.init(GemFireCacheImpl.java) at com.gemstone.gemfire.internal.cache.GemFireCacheImpl.create(GemFireCacheImpl.java) at com.gemstone.gemfire.cache.CacheFactory.create(CacheFactory.java) at com.gemstone.gemfire.cache.CacheFactory.create(CacheFactory.java) at org.springframework.data.gemfire.CacheFactoryBean.createCache(CacheFactoryBean.java) at org.springframework.data.gemfire.CacheFactoryBean.init(CacheFactoryBean.java) at org.springframework.data.gemfire.CacheFactoryBean.getObject(CacheFactoryBean.java) at org.springframework.data.gemfire.CacheFactoryBean.getObject(CacheFactoryBean.java) at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java) ... 34 more  </content>
    </entry>
    
     <entry>
        <title>初探ANTLR</title>
        <url>https://mryqu.github.io/post/%E5%88%9D%E6%8E%A2antlr/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>antlr</tag><tag>eclipse</tag><tag>ant</tag><tag>maven</tag><tag>编译器</tag>
        </tags>
        <content type="html">  当前的项目是基于多维数据集市的RTOLAP财务系统，其中的公式使用ANTLR 3进行语法解析和编译。 此外看Hibernate源码的时候也接触到ANTLR，Hibernate使用ANTLR产生查询分析器。
ANTLR简介 ANTLR的全称是ANother Tool for LanguageRecognition，其前身是PCCTS，和YACC、LEX、JavaCC、Coco/R等工具一样，都是编译器的编译程序。 ANTLR对语法树构造、遍历和转换、错误修复和报告提供出色的支持，它为包括Java，C&#43;&#43;，C#和Python在内的语言提供了一个通过语法描述来自动构造自定义语言的识别器、解析器、编译器和转换器的框架。 ANTLR可以通过断言（Predicate）解决识别冲突；支持动作（Action）和返回值（ReturnValue）来；更棒的是，它可以根据输入自动生成语法树并可视化的显示出来。由此，计算机语言的翻译变成了一项普通的任务。 ANTLR是由旧金山大学的Terence Parr博士领导下完成的，最新版本为4.1。
编译器工作主要分有词法分析，语法分析，代码生成三个步骤。ANTLR分别提供了三个东西: 1. 词法分析器（Lexer） 词法分析器又称为Scanner，Lexicalanalyser和Tokenizer。程序设计语言通常由关键字和严格定义的语法结构组成。编译的最终目的是将程序设计语言的高层指令翻译成物理机器或虚拟机可以执行的指令。词法分析器的工作是分析量化那些本来毫无意义的字符流，将他们翻译成离散的字符组（也就是一个一个的Token），包括关键字、标识符、符号和操作符供语法分析器使用。 1. 语法分析器（Parser） 编译器又称为Syntacticalanalyser。在分析字符流的时候，Lexer不关心所生成的单个Token的语法意义及其与上下文之间的关系，而这就是Parser的工作。语法分析器将收到的Tokens组织起来，并转换成为目标语言语法定义所允许的序列。 无论是Lexer还是Parser都是一种识别器，Lexer是字符序列识别器而Parser是Token序列识别器。他们在本质上是类似的东西，而只是在分工上有所不同而已。 1. 抽象语法树遍历器 (Tree walker) 树分析器可以用于对语法分析生成的抽象语法树进行遍历，并能执行一些相关的操作，可以进行语义匹配生成代码。
ANTLR 3的Eclipse插件 ANTLR IDE用于ANTLR3的一个Eclipse插件。 - 支持ANTLR 3.0、3.1、3.2、3.3和3.4。 - ANTLR启动器和调试器(仅限Java) - ANTLR内建解析器。 - 代码格式化工具(Ctrl&#43;Shift&#43;F) - 语法图（铁路图） - 定制目标 - 自动生成资源 - 对语法文件中错误和告警自动标注 - 高级文本编辑器、代码选择(F3)和代码补全(Ctrl&#43;Space) - 对（Java、C#、Python和C等）目标语言自动语法高亮 - 标注生成的资源 - 高级字符串模板(StringTemplate)编辑器(*.st and .stg) - 高级语法单元测试(gUnit)编辑器(.gunit and *.testsuite)使用ANTLR生成代码的ANT设置 &amp;lt;property name=&amp;quot;lib.dir&amp;quot; value=&amp;quot;lib&amp;quot; /&amp;gt; &amp;lt;property name=&amp;quot;gensrc.dir&amp;quot; value=&amp;quot;gen-source/Java&amp;quot; /&amp;gt; &amp;lt;property name=&amp;quot;parser.dir&amp;quot; value=&amp;quot;/com/sas/yourgramarpackage&amp;quot;/&amp;gt; &amp;lt;target name=&amp;quot;gen-source&amp;quot;&amp;gt; &amp;lt;mkdir dir=&amp;quot;${gensrc.dir}/${parser.dir}&amp;quot; /&amp;gt; &amp;lt;antlr:antlr3 xmlns:antlr=&amp;quot;antlib:org/apache/tools/ant/antlr&amp;quot; target=&amp;quot;${src.dir}/${parser.dir}/YourGrammar.g&amp;quot; outputdirectory=&amp;quot;${gensrc.dir}/${parser.dir}&amp;quot;&amp;gt; &amp;lt;classpath&amp;gt; &amp;lt;fileset dir=&amp;quot;${lib.dir}&amp;quot;&amp;gt; &amp;lt;include name=&amp;quot;*.jar&amp;quot; /&amp;gt; &amp;lt;/fileset&amp;gt; &amp;lt;/classpath&amp;gt; &amp;lt;/antlr:antlr3&amp;gt; &amp;lt;/target&amp;gt;  使用ANTLR生成代码的Maven设置 &amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;maven-antlr-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;grammars&amp;gt;com/yqu/yourgramarpackage/yourgrammar.g&amp;lt;/grammars&amp;gt; &amp;lt;sourceDirectory&amp;gt;Source/Java&amp;lt;/sourceDirectory&amp;gt; &amp;lt;outputDirectory&amp;gt;target/generated/antlr/Source/Java&amp;lt;/outputDirectory&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;generate&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;/plugins&amp;gt; &amp;lt;/build&amp;gt;  </content>
    </entry>
    
     <entry>
        <title>博客链接（搜索、统计、数据挖掘）</title>
        <url>https://mryqu.github.io/post/%E5%8D%9A%E5%AE%A2%E9%93%BE%E6%8E%A5%E6%90%9C%E7%B4%A2%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>搜索技术</tag><tag>数据挖掘</tag><tag>博客</tag><tag>统计</tag>
        </tags>
        <content type="html">  统计（侧重R） |Name|Blog|Weibo |&amp;mdash;- |R-bloggers|http://www.r-bloggers.com/| |statMethods blog|http://statmethods.wordpress.com/| |谢益辉|http://yihui.name/|Weibo：@谢益辉 |刘思喆|http://www.bjt.name/|Weibo：@刘思喆 |邓一硕|http://yishuo.org/|Weibo：@邓一硕 |陈堰平|http://yanping.me/|Weibo：@平沙落雁 |邱怡轩|http://yixuan.cos.name/|Weibo：@解名缰 |魏太云|http://blog.cos.name/taiyun/|Weibo：@cloud_wei |陈丽云|http://www.loyhome.com/|Weibo：@cloudly |肖楠|http://www.road2stat.com/|Weibo：@road2stat |肖凯|http://xccds.github.io/|Weibo：@xccds |高涛|http://joegaotao.github.io/cn|Weibo： @三水成海 |陈刚|http://gossipcoder.com/| |李舰|http://jliblog.com/|Weibo：@lijian001 |熊熹|http://blog.cos.name/tracy/|Weibo：@熊熹91 |范建|http://blog.cos.name/fan/|Weibo：@thinkfan
搜索 百度搜索研发部官方博客
淘宝搜索技术
量子恒道官方博客
</content>
    </entry>
    
     <entry>
        <title>[Hadoop] 分布式缓存</title>
        <url>https://mryqu.github.io/post/hadoop_%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/</url>
        <categories>
          <category>Hadoop&#43;Spark</category><category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>分布式缓存</tag><tag>distributed</tag><tag>cache</tag>
        </tags>
        <content type="html">  一直在看分布式缓存，最近涉猎到Hadoop的分布式缓存，做个汇总以备后用。
adoop分布式缓存是Map-Reduce框架提供的用于缓存应用程序所需文件（文本文件、存档文件、Jar文件等）的工具。 应用程序通过URL（hdfs://或http://）指定通过JobConf进行缓存的文件。分布式缓存假定URL所指定的文件已经存在于Hadoop分布式文件系统或本地文件系统中并可被集群中所有机器访问到。Hadoop框架会在任何作业在节点执行之前将必须的缓存文件复制到任务节点以供使用。为了节省网络带宽，这些文件只会为每个作业复制一次，且归档类型的缓存文件会在任务节点中解压缩。分布式缓存能用于分发简单只读数据或文本文件及复杂文件（存档文件、Jar文件等）。归档文件（zip、tar和tgz/tar.gz文件）在任务节点中解压缩。Jar文件可选择加入任务的类路径，这是基本的软件分发机制。 分布式缓存跟踪缓存文件的修改时戳。很明显当作业执行时这些缓存文件不应被应用程序或外部修改。
下面的示例介绍了如何使用DistributedCache： 1. 将所需文件复制到FileSystem:
 $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz $ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz   设置应用程序的JobConf:  JobConf job = new JobConf(); DistributedCache.addCacheFile(new URI(&amp;quot;/myapp/lookup.dat&amp;quot;), job); DistributedCache.addCacheArchive(new URI(&amp;quot;/myapp/map.zip&amp;quot;, job); DistributedCache.addFileToClassPath(new Path(&amp;quot;/myapp/mylib.jar&amp;quot;), job); DistributedCache.addCacheArchive(new URI(&amp;quot;/myapp/mytar.tar&amp;quot;, job); DistributedCache.addCacheArchive(new URI(&amp;quot;/myapp/mytgz.tgz&amp;quot;, job); DistributedCache.addCacheArchive(new URI(&amp;quot;/myapp/mytargz.tar.gz&amp;quot;, job);  在Mapper或Reducer中使用缓存的文件:
public static class MapClass extends MapReduceBase implements Mapper{ private Path[] localArchives; private Path[] localFiles; public void configure(JobConf job) { // Get the cached archives/files File f = new File(&amp;quot;./map.zip/some/file/in/zip.txt&amp;quot;); } public void map(K key, V value, OutputCollector output, Reporter reporter) throws IOException { // Use data from the cached archives/files here // ... // ... output.collect(k, v); } }   通过GenericOptionsParser使用分布式缓存是一种常用的方式。用户可以使用-file选项指定待分发的文件，文件内包含以都放分隔的URL列表。文件可以放在本地文件系统、HDFS或其他Hadoop可读文件系统（例如S3）中。如果未指定文件系统，则这些文件默认是本地的（即使默认文件系统并非本地文件系统，也是成立的。）。用户可以使用-archieves选项向自己的任务复制存档文件，这些存档文件会在任务节点解压缩。-libjars选项会把JAR文件添加到mapper和reducer任务的类路径中。当库JAR文件没有打包进作业JAR文件时，会非常有帮助。
工作机制 当用户启动一个作业，Hadoop将由-files、-archieves和-libjars选项所指定的文件复制到jobtracker的文件系统（一般是HDFS）之中。接着，tasktracker在任务运行之前将文件从jobtracker的文件系统中复制到本地磁盘&amp;ndash;缓存—使任务能够访问文件。这些文件在这一刻可以被访问到了。从任务的角度来看，这些文件就已经在那里了（它并不关心这些文件是否来自HDFS） Tasktracker为缓存中的每个文件维护一个计数器来统计使用该文件的任务数。当任务运行之前，文件引用数加1；当所有任务结束后，计数器减1。当计数器值为0时，表明该文件没有被任何任务使用，可以从缓存中移除。当缓存超过一定容量—默认为10GB，无用的文件将被删除以腾出空间来装载新文件。缓存大小可以通过配置属性local.cache.size进行配置，以字节为单位。 尽管这个机制并不确保在同一tasktracker上运行的作业的后继任务能否在缓存中找到文件，但是成功的概率相当大。原因在于作业的多个任务在调度之后几乎同时开始运行，在此期间基本不可能有足够多的其他任务也在运行，以至于将该任务所需文件从缓存中移除出去。 文件存放在tasktracker的${mapred.loccal.dir}/taskTracker/archive目录下。但是应用程序无需了解这一点，因为这些文件可在任务工作目录通过符号链接获得。
分布式缓存 API 由于可以通过GenericOptionsParser间接使用分布式缓存，大多数应用程序不需要使用分布式缓存API。然而，一些应用程序可能需要使用分布式缓存更高级的个噢能能，因此直接使用API。API分为两部分：将数据放入缓存的方法（在Job中）和从缓存中获取数据的方法（在Jobcontext中） 作业API方法GenericOptionsParser等价项描述addCacheFile(URI uri)-files file1,file2,&amp;hellip;将文件加入分布式缓存、复制到任务节点。setCacheFiles(URI[] files)addCacheArchive(URI uri)-archives archive1,archive2,…将归档文件加入分布式缓存、复制到任务 节点并解压缩。setCacheArchives(URI[] files)addFileToClassPath(Path file)-libjars jar1,jar2,&amp;hellip;将文件加入分布式缓存并加入MapReduce 任务类路径。这些文件不会解压缩，因此 是将JAR文件加入类路径的非常有用的一种方式。addArchiveToClassPath(Path archive)&amp;nbsp;无将归档文件加入分布式缓存并解压缩后加 入MapReduce任务类路径。当需要将一个 目录下的所有文件加入类路径时非常有用，因为可以创建包含这些文件的归档文件。 另一种方式是创建JAR文件并使用 addFileToClassPath()方法。
</content>
    </entry>
    
     <entry>
        <title>嵌套的动态GemFire region研究</title>
        <url>https://mryqu.github.io/post/%E5%B5%8C%E5%A5%97%E7%9A%84%E5%8A%A8%E6%80%81gemfire_region%E7%A0%94%E7%A9%B6/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category><category>GemFire</category>
        </categories>
        <tags>
          <tag>gemfire</tag><tag>dynamic</tag><tag>nested</tag><tag>region</tag>
        </tags>
        <content type="html">  研究目的和结论  研究多级动态region是否可行，结论可行 研究嵌套region(一个region是另一个region的值)是否可行，结论可行  Java代码 import java.util.Set; import com.gemstone.gemfire.cache.Cache; import com.gemstone.gemfire.cache.CacheFactory; import com.gemstone.gemfire.cache.DynamicRegionFactory; import com.gemstone.gemfire.cache.Region; public class EmbededDynamicRegion { public static void main(String[] args) { System.out.println(&amp;quot;\nConnecting to the distributed system and creating the cache.&amp;quot;); Cache cache = null; try { // Create the cache which causes the cache-xml-file to be parsed cache = new CacheFactory().set(&amp;quot;name&amp;quot;, &amp;quot;yqu_test_cache&amp;quot;) .set(&amp;quot;cache-xml-file&amp;quot;, &amp;quot;xml/YquTest.xml&amp;quot;).create(); // Get the exampleRegion Region yquRegion = cache.getRegion(&amp;quot;yqu_region&amp;quot;); printRegionFullPath(yquRegion); DynamicRegionFactory dynRegFactory = DynamicRegionFactory.get(); for(int i=0;i&amp;lt;3;i&#43;&#43;) { Region keyRegion = dynRegFactory.createDynamicRegion(&amp;quot;/yqu_region&amp;quot;, &amp;quot;k&amp;quot;&#43;i); yquRegion.put(&amp;quot;k&amp;quot;&#43;i, keyRegion); for(int j=0;j&amp;lt;10;j&#43;&#43;) { Region asofRegion = dynRegFactory.createDynamicRegion(&amp;quot;/yqu_region/k&amp;quot;&#43;i,&amp;quot;asof&amp;quot;&#43;j); keyRegion.put(&amp;quot;asof&amp;quot;&#43;j, asofRegion); } } System.out.println(&amp;quot;\nSubregions under /yqu_region:&amp;quot;); Set&amp;lt;Region&amp;gt; regionSet = yquRegion.subregions(true); for(Region region:regionSet) { System.out.println(&amp;quot;&amp;quot;); printRegionFullPath(region); for(Object obj:region.keySet()) System.out.println(obj&#43;&amp;quot;:&amp;quot;&#43;region.get(obj)); } } catch (Throwable t) { t.printStackTrace(); } finally { // Close the cache and disconnect from GemFire distributed system System.out.println(&amp;quot;Closing the cache and disconnecting.&amp;quot;); if(cache!=null) cache.close(); } } public static void printRegionFullPath(Region region) { System.out.println(&amp;quot;The path of region &amp;quot; &#43; region.getName() &#43; &amp;quot; :&amp;quot; &#43;region.getFullPath()); } }  XML配置文件 </content>
    </entry>
    
     <entry>
        <title>nslookup笔记</title>
        <url>https://mryqu.github.io/post/nslookup%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>Tool</category><category>NetWork</category>
        </categories>
        <tags>
          <tag>nslookup</tag><tag>dns</tag>
        </tags>
        <content type="html">  nslookup命令是用来查询因特网域名服务器的。该命令在Unix、Linux和Windows平台都有提供。如果在Linux平台上找不到nslookup命令，检查是否安装bind-utils。
nslookup的使用模式 nslookup有两种模式：交互式和非交互式。 - 交互式模式允许向域名服务器查询各种主机、域名或打印域内主机列表。 - 非交互式模式用于查询一个主机或域的信息。
C:\&amp;gt;nslookup /? Usage: nslookup [-opt ...] # interactive mode using default server nslookup [-opt ...] - server # interactive mode using &#39;server&#39; nslookup [-opt ...] host # just look up &#39;host&#39; using default server nslookup [-opt ...] host server # just look up &#39;host&#39; using &#39;server&#39;  制定了查询对象就进入非交互式模式，否则进入交互模式。示例如下：这两次查询百度的命令区别在于：第一次使用了默认DNS，第二次指定了Google的DNS。返回结果是非权威答案，即从上连DNS服务器的本地缓存中读取出的值，而非实际去查询到的值。
nslookup选项 - set all：列出nslookup工具的常用选项的当前设置值。 - set class=[value]：可以更改查询类
 IN：Internet类（默认） CH：Chaos类 HS：Hesiod类 ANY：通配  Chaos和Hesiod现在几乎无人使用。 - set [no]debug：可以用来设置是否进入调试模式。如果setdebug，则会进入到调试模式，查询过程中会显示完整的响应包以及其中的交互包。 - set[no]d2：开启了高级调试模式，会输出很多nslookup内部工作的信息，包括了许多函数调用信息。 - set domain=[name]：用于设置默认的域。 - set [no]search：使用域搜索列表。 - setport=[value]：众所周知，DNS默认的服务端口是53。当某些特殊情况，此端口改变时，可以通过本命令来设置。 - set type=[value]：也可以写成setquerytype=[value]，用于更改信息查询类型。默认情况下，nslookup是查询域名所对应的A记录，而如果你想查询其对应的MX记录等信息时，就需要专门设置type值了。目前常用的type值如下：
 A：查看主机的IPv4地址 AAAA：查看主机的IPv6地址 ANY：查看关于主机域的所有信息 CNAME：查找与别名对应的正式名字 ISDN：域名对应的ISDN号码 HINFO：查找主机的CPU与操作系统类型 MB：存放指定邮箱的服务器 MG：邮件组记录 MINFO：查找邮箱信息 MR：改名的邮箱记录 MX：查找邮件交换信息 NS：查找主机域的域名服务器 PTR：查找与给定IP地址匹配的主机名 RP：查找域负责人记录 RT：路由穿透记录 SOA：查找域内的SOA地址 SRV：TCP服务器信息记录 TXT：域名对应的文本信息 UINFO：查找用户信息 X25：域名对应的X.25地址记录   retry=[number]：可以用来设置查询重试的次数。 timeout=[number]：可以用来设置每次查询的超时时限。示例如下：  nslookup -type=MX -debug www.baidu.com 8.8.8.8   DNS知识 什么叫DNS？ 域名管理系统DNS（Domain Name System）是域名解析服务器的意思.它在互联网的作用是：把域名转换成网络可以识别的ip地址，在通过IP地址访问主机。比如：我们上网时输入的www.baidu.com会自动转换成为220.181.112.244。
什么是A记录？ A记录是用来指定主机名（或域名）对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。
什么是NS记录？ NS（Name Server）记录是域名服务器记录，用来指定该域名由哪个DNS服务器来进行解析。
什么是别名记录(CNAME)？ 也被称为规范名字。这种记录允许您将多个名字映射到同一台计算机。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.domain.com”（A记录）。它同时提供WWW和MAIL服务，为了便于用户访问服务。可以为该计算机设置两个别名（CNAME）：WWW和MAIL。这两个别名的全称就是“www.domain.com”和“mail.domain.com”。实际上他们都指向“host.domain.com”。
什么是泛域名解析？ 泛域名解析定义为：用户的域名aaa.com，之下所设的*.aaa.com全部解析到同一个IP地址上去。比如客户设mail.aaa.com就会自已自动解析到与aaa.com同一个IP地址上去。
DNS查询的大致步骤  首先，客户端提出域名解析请求（无论以何种形式或方法），并将该请求发或转发给本地的DNS服务器。 接着，本地DNS服务器收到请求后就去查询自己的缓存，如果有该条记录，则会将查询的结果返回给客户端。（也就是我们看到的““非权威性”的应答”）。请注意，下面就开始递归查询了：反之，如果DNS服务器本地没有搜索到相应的记录，则会把请求转发到根DNS（13台根DNS服务器的IP信息默认均存储在DNS服务器中，当需要时就会去有选择性的连接）。 然后，根DNS服务器收到请求后会判断这个域名是谁来授权管理，并会返回一个负责该域名子域的DNS服务器地址。比如，查询news.baidu.com的IP，根DNS服务器就会在负责.com顶级域名的DNS服务器中选一个（并非随机，而是根据空间、地址、管辖区域等条件进行筛选），返回给本地DNS服务器。可以说根域对顶级域名有绝对管理权，自然也知道他们的全部信息，因为在DNS系统中，上一级对下一级有管理权限，毫无疑问，根DNS是最高一级了。 本地DNS服务器收到这个地址后，就开始联系对方并将此请求发给他。负责.com域名的某台服务器收到此请求后，如果自己无法解析，就会返回一个管理.com的下一级的DNS服务器地址给本地DNS服务器，也就是负责管理baidu.com的DNS。 当本地DNS服务器收到这个地址后，就会重复上面的动作，继续往下联系。 不断重复这样的轮回过程，直到有一台DNS服务器可以顺利解析出这个地址为止。在这个过程中，客户端一直处理等待状态，他不需要做任何事，也做不了什么。 直到本地DNS服务器获得IP时，才会把这个IP返回给客户端，到此在本地的DNS服务器取得IP地址后，递归查询就算完成了。本地DNS服务器同时会将这条记录写入自己的缓存，以备后用。到此，整个解析过程完成。客户端拿到这个地址后，就可以顺利往下进行了。但假设客户端请求的域名根本不存在，解析自然不成功，DNS服务器会返回此域名不可达，在客户端的体现就是网页无法浏览或网络程序无法连接等等。  </content>
    </entry>
    
     <entry>
        <title>常用HTML转义字符</title>
        <url>https://mryqu.github.io/post/%E5%B8%B8%E7%94%A8html%E8%BD%AC%E4%B9%89%E5%AD%97%E7%AC%A6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>html</tag><tag>escape</tag><tag>转义</tag>
        </tags>
        <content type="html">  最常用的字符实体 |显示|说明|实体名称|实体编号 |&amp;mdash;&amp;ndash; | |半方大的空白|&amp;amp; ensp;|&amp;amp; #8194; | |全方大的空白|&amp;amp; emsp;|&amp;amp; #8195; | |不断行的空白格|&amp;amp; nbsp;|&amp;amp; #160; |&amp;lt; |小于|&amp;amp; lt;|&amp;amp; #60; |&amp;gt;|大于|&amp;amp; gt;|&amp;amp; #62; |&amp;amp;|&amp;amp; 符号|&amp;amp; amp;|&amp;amp; #38; |&amp;ldquo;|双引号|&amp;amp; quot;|&amp;amp; #34; |©|版权|&amp;amp; copy;|&amp;amp; #169; |®|已注册商标|&amp;amp; reg;|&amp;amp; #174; |™|商标（美国）|&amp;amp; trade;|&amp;amp; #8482; |×|乘号|&amp;amp; times;|&amp;amp; #215; |÷|除号|&amp;amp; divide;|&amp;amp; #247;
ISO 8859-1 (Latin-1)字符集 HTML 4.01 支持 ISO 8859-1 (Latin-1) 字符集。
备注：为了方便起见，以下表格中，“实体名称”简称为“名称”，“实体编号”简称为“编号”
|显示|名称|编号|显示|名称|编号|显示|名称|编号 |&amp;mdash;&amp;ndash; | |&amp;amp; nbsp;|&amp;amp; #160;|¡|&amp;amp; iexcl;|&amp;amp; #161;|¢|&amp;amp; cent;|&amp;amp; #162; |£|&amp;amp; pound;|&amp;amp; #163;|¤|&amp;amp; curren;|&amp;amp; #164;|¥|&amp;amp; yen;|&amp;amp; #165; |¦|&amp;amp; brvbar;|&amp;amp; #166;|§|&amp;amp; sect;|&amp;amp; #167;|¨|&amp;amp; uml;|&amp;amp; #168; |©|&amp;amp; copy;|&amp;amp; #169;|ª|&amp;amp; ordf;|&amp;amp; #170;|«|&amp;amp; laquo;|&amp;amp; #171; |¬|&amp;amp; not;|&amp;amp; #172;|­|&amp;amp; shy;|&amp;amp; #173;|®|&amp;amp; reg;|&amp;amp; #174; |¯|&amp;amp; macr;|&amp;amp; #175;|°|&amp;amp; deg;|&amp;amp; #176;|±|&amp;amp; plusmn;|&amp;amp; #177; |²|&amp;amp; sup2;|&amp;amp; #178;|³|&amp;amp; sup3;|&amp;amp; #179;|´|&amp;amp; acute;|&amp;amp; #180; |µ|&amp;amp; micro;|&amp;amp; #181;|¶|&amp;amp; para;|&amp;amp; #182;|·|&amp;amp; middot;|&amp;amp; #183; |¸|&amp;amp; cedil;|&amp;amp; #184;|¹|&amp;amp; sup1;|&amp;amp; #185;|º|&amp;amp; ordm;|&amp;amp; #186; |»|&amp;amp; raquo;|&amp;amp; #187;|¼|&amp;amp; frac14;|&amp;amp; #188;|½|&amp;amp; frac12;|&amp;amp; #189; |¾|&amp;amp; frac34;|&amp;amp; #190;|¿|&amp;amp; iquest;|&amp;amp; #191;|À|&amp;amp; Agrave;|&amp;amp; #192; |Á|&amp;amp; Aacute;|&amp;amp; #193;|Â|&amp;amp; Acirc;|&amp;amp; #194;|Ã|&amp;amp; Atilde;|&amp;amp; #195; |Ä|&amp;amp; Auml;|&amp;amp; #196;|Å|&amp;amp; Aring;|&amp;amp; #197;|Æ|&amp;amp; AElig;|&amp;amp; #198; |Ç|&amp;amp; Ccedil;|&amp;amp; #199;|È|&amp;amp; Egrave;|&amp;amp; #200;|É|&amp;amp; Eacute;|&amp;amp; #201; |Ê|&amp;amp; Ecirc;|&amp;amp; #202;|Ë|&amp;amp; Euml;|&amp;amp; #203;|Ì|&amp;amp; Igrave;|&amp;amp; #204; |Í|&amp;amp; Iacute;|&amp;amp; #205;|Î|&amp;amp; Icirc;|&amp;amp; #206;|Ï|&amp;amp; Iuml;|&amp;amp; #207; |Ð|&amp;amp; ETH;|&amp;amp; #208;|Ñ|&amp;amp; Ntilde;|&amp;amp; #209;|Ò|&amp;amp; Ograve;|&amp;amp; #210; |Ó|&amp;amp; Oacute;|&amp;amp; #211;|Ô|&amp;amp; Ocirc;|&amp;amp; #212;|Õ|&amp;amp; Otilde;|&amp;amp; #213; |Ö|&amp;amp; Ouml;|&amp;amp; #214;|×|&amp;amp; times;|&amp;amp; #215;|Ø|&amp;amp; Oslash;|&amp;amp; #216; |Ù|&amp;amp; Ugrave;|&amp;amp; #217;|Ú|&amp;amp; Uacute;|&amp;amp; #218;|Û|&amp;amp; Ucirc;|&amp;amp; #219; |Ü|&amp;amp; Uuml;|&amp;amp; #220;|Ý|&amp;amp; Yacute;|&amp;amp; #221;|Þ|&amp;amp; THORN;|&amp;amp; #222; |ß|&amp;amp; szlig;|&amp;amp; #223;|à|&amp;amp; agrave;|&amp;amp; #224;|á|&amp;amp; aacute;|&amp;amp; #225; |â|&amp;amp; acirc;|&amp;amp; #226;|ã|&amp;amp; atilde;|&amp;amp; #227;|ä|&amp;amp; auml;|&amp;amp; #228; |å|&amp;amp; aring;|&amp;amp; #229;|æ|&amp;amp; aelig;|&amp;amp; #230;|ç|&amp;amp; ccedil;|&amp;amp; #231; |è|&amp;amp; egrave;|&amp;amp; #232;|é|&amp;amp; eacute;|&amp;amp; #233;|ê|&amp;amp; ecirc;|&amp;amp; #234; |ë|&amp;amp; euml;|&amp;amp; #235;|ì|&amp;amp; igrave;|&amp;amp; #236;|í|&amp;amp; iacute;|&amp;amp; #237; |î|&amp;amp; icirc;|&amp;amp; #238;|ï|&amp;amp; iuml;|&amp;amp; #239;|ð|&amp;amp; eth;|&amp;amp; #240; |ñ|&amp;amp; ntilde;|&amp;amp; #241;|ò|&amp;amp; ograve;|&amp;amp; #242;|ó|&amp;amp; oacute;|&amp;amp; #243; |ô|&amp;amp; ocirc;|&amp;amp; #244;|õ|&amp;amp; otilde;|&amp;amp; #245;|ö|&amp;amp; ouml;|&amp;amp; #246; |÷|&amp;amp; divide;|&amp;amp; #247;|ø|&amp;amp; oslash;|&amp;amp; #248;|ù|&amp;amp; ugrave;|&amp;amp; #249; |ú|&amp;amp; uacute;|&amp;amp; #250;|û|&amp;amp; ucirc;|&amp;amp; #251;|ü|&amp;amp; uuml;|&amp;amp; #252; |ý|&amp;amp; yacute;|&amp;amp; #253;|þ|&amp;amp; thorn;|&amp;amp; #254;|ÿ|&amp;amp; yuml;|&amp;amp; #255;
数学和希腊字母标志 |显示|名称|编号|显示|名称|编号|显示|名称|编号 |&amp;mdash;&amp;ndash; |ƒ|&amp;amp; fnof;|&amp;amp; #402;|Α|&amp;amp; Alpha;|&amp;amp; #913;|Β|&amp;amp; Beta;|&amp;amp; #914; |Γ|&amp;amp; Gamma;|&amp;amp; #915;|Δ|&amp;amp; Delta;|&amp;amp; #916;|Ε|&amp;amp; Epsilon;|&amp;amp; #917; |Ζ|&amp;amp; Zeta;|&amp;amp; #918;|Η|&amp;amp; Eta;|&amp;amp; #919;|Θ|&amp;amp; Theta;|&amp;amp; #920; |Ι|&amp;amp; Iota;|&amp;amp; #921;|Κ|&amp;amp; Kappa;|&amp;amp; #922;|Λ|&amp;amp; Lambda;|&amp;amp; #923; |Μ|&amp;amp; Mu;|&amp;amp; #924;|Ν|&amp;amp; Nu;|&amp;amp; #925;|Ξ|&amp;amp; Xi;|&amp;amp; #926; |Ο|&amp;amp; Omicron;|&amp;amp; #927;|Π|&amp;amp; Pi;|&amp;amp; #928;|Ρ|&amp;amp; Rho;|&amp;amp; #929; |Σ|&amp;amp; Sigma;|&amp;amp; #931;|Τ|&amp;amp; Tau;|&amp;amp; #932;|Υ|&amp;amp; Upsilon;|&amp;amp; #933; |Φ|&amp;amp; Phi;|&amp;amp; #934;|Χ|&amp;amp; Chi;|&amp;amp; #935;|Ψ|&amp;amp; Psi;|&amp;amp; #936; |Ω|&amp;amp; Omega;|&amp;amp; #937;|α|&amp;amp; alpha;|&amp;amp; #945;|β|&amp;amp; beta;|&amp;amp; #946; |γ|&amp;amp; gamma;|&amp;amp; #947;|δ|&amp;amp; delta;|&amp;amp; #948;|ε|&amp;amp; epsilon;|&amp;amp; #949; |ζ|&amp;amp; zeta;|&amp;amp; #950;|η|&amp;amp; eta;|&amp;amp; #951;|θ|&amp;amp; theta;|&amp;amp; #952; |ι|&amp;amp; iota;|&amp;amp; #953;|κ|&amp;amp; kappa;|&amp;amp; #954;|λ|&amp;amp; lambda;|&amp;amp; #955; |μ|&amp;amp; mu;|&amp;amp; #956;|ν|&amp;amp; nu;|&amp;amp; #957;|ξ|&amp;amp; xi;|&amp;amp; #958; |ο|&amp;amp; omicron;|&amp;amp; #959;|π|&amp;amp; pi;|&amp;amp; #960;|ρ|&amp;amp; rho;|&amp;amp; #961; |ς|&amp;amp; sigmaf;|&amp;amp; #962;|σ|&amp;amp; sigma;|&amp;amp; #963;|τ|&amp;amp; tau;|&amp;amp; #964; |υ|&amp;amp; upsilon;|&amp;amp; #965;|φ|&amp;amp; phi;|&amp;amp; #966;|χ|&amp;amp; chi;|&amp;amp; #967; |ψ|&amp;amp; psi;|&amp;amp; #968;|ω|&amp;amp; omega;|&amp;amp; #969;|ϑ|&amp;amp; thetasym;|&amp;amp; #977; |ϒ|&amp;amp; upsih;|&amp;amp; #978;|ϖ|&amp;amp; piv;|&amp;amp; #982;|•|&amp;amp; bull;|&amp;amp; #8226; |…|&amp;amp; hellip;|&amp;amp; #8230;|′|&amp;amp; prime;|&amp;amp; #8242;|″|&amp;amp; Prime;|&amp;amp; #8243; |‾|&amp;amp; oline;|&amp;amp; #8254;|⁄|&amp;amp; frasl;|&amp;amp; #8260;|℘|&amp;amp; weierp;|&amp;amp; #8472; |ℑ|&amp;amp; image;|&amp;amp; #8465;|ℜ|&amp;amp; real;|&amp;amp; #8476;|™|&amp;amp; trade;|&amp;amp; #8482; |ℵ|&amp;amp; alefsym;|&amp;amp; #8501;|←|&amp;amp; larr;|&amp;amp; #8592;|↑|&amp;amp; uarr;|&amp;amp; #8593; |→|&amp;amp; rarr;|&amp;amp; #8594;|↓|&amp;amp; darr;|&amp;amp; #8595;|↔|&amp;amp; harr;|&amp;amp; #8596; |↵|&amp;amp; crarr;|&amp;amp; #8629;|⇐|&amp;amp; lArr;|&amp;amp; #8656;|⇑|&amp;amp; uArr;|&amp;amp; #8657; |⇒|&amp;amp; rArr;|&amp;amp; #8658;|⇓|&amp;amp; dArr;|&amp;amp; #8659;|⇔|&amp;amp; hArr;|&amp;amp; #8660; |∀|&amp;amp; forall;|&amp;amp; #8704;|∂|&amp;amp; part;|&amp;amp; #8706;|∃|&amp;amp; exist;|&amp;amp; #8707; |∅|&amp;amp; empty;|&amp;amp; #8709;|∇|&amp;amp; nabla;|&amp;amp; #8711;|∈|&amp;amp; isin;|&amp;amp; #8712; |∉|&amp;amp; notin;|&amp;amp; #8713;|∋|&amp;amp; ni;|&amp;amp; #8715;|∏|&amp;amp; prod;|&amp;amp; #8719; |∑|&amp;amp; sum;|&amp;amp; #8721;|−|&amp;amp; minus;|&amp;amp; #8722;|∗|&amp;amp; lowast;|&amp;amp; #8727; |√|&amp;amp; radic;|&amp;amp; #8730;|∝|&amp;amp; prop;|&amp;amp; #8733;|∞|&amp;amp; infin;|&amp;amp; #8734; |∠|&amp;amp; ang;|&amp;amp; #8736;|∧|&amp;amp; and;|&amp;amp; #8743;|∨|&amp;amp; or;|&amp;amp; #8744; |∩|&amp;amp; cap;|&amp;amp; #8745;|∪|&amp;amp; cup;|&amp;amp; #8746;|∫|&amp;amp; int;|&amp;amp; #8747; |∴|&amp;amp; there4;|&amp;amp; #8756;|∼|&amp;amp; sim;|&amp;amp; #8764;|≅|&amp;amp; cong;|&amp;amp; #8773; |≈|&amp;amp; asymp;|&amp;amp; #8776;|≠|&amp;amp; ne;|&amp;amp; #8800;|≡|&amp;amp; equiv;|&amp;amp; #8801; |≤|&amp;amp; le;|&amp;amp; #8804;|≥|&amp;amp; ge;|&amp;amp; #8805;|⊂|&amp;amp; sub;|&amp;amp; #8834; |⊃|&amp;amp; sup;|&amp;amp; #8835;|⊄|&amp;amp; nsub;|&amp;amp; #8836;|⊆|&amp;amp; sube;|&amp;amp; #8838; |⊇|&amp;amp; supe;|&amp;amp; #8839;|⊕|&amp;amp; oplus;|&amp;amp; #8853;|⊗|&amp;amp; otimes;|&amp;amp; #8855; |⊥|&amp;amp; perp;|&amp;amp; #8869;|⋅|&amp;amp; sdot;|&amp;amp; #8901;|⌈|&amp;amp; lceil;|&amp;amp; #8968; |⌉|&amp;amp; rceil;|&amp;amp; #8969;|⌊|&amp;amp; lfloor;|&amp;amp; #8970;|⌋|&amp;amp; rfloor;|&amp;amp; #8971; |⟨|&amp;amp; lang;|&amp;amp; #9001;|⟩|&amp;amp; rang;|&amp;amp; #9002;|◊|&amp;amp; loz;|&amp;amp; #9674; |♠|&amp;amp; spades;|&amp;amp; #9824;|♣|&amp;amp; clubs;|&amp;amp; #9827;|♥|&amp;amp; hearts;|&amp;amp; #9829; |♦|&amp;amp; diams;|&amp;amp; #9830;
重要的国际标记 |显示|名称|编号|显示|名称|编号|显示|名称|编号 |&amp;mdash;&amp;ndash; |&amp;ldquo;|&amp;amp; quot;|&amp;amp; #34;|&amp;amp;|&amp;amp; amp;|&amp;amp; #38;|&amp;lt; |&amp;amp; lt;|&amp;amp; #60; |&amp;gt;|&amp;amp; gt;|&amp;amp; #62;|Œ|&amp;amp; OElig;|&amp;amp; #338;|œ|&amp;amp; oelig;|&amp;amp; #339; |Š|&amp;amp; Scaron;|&amp;amp; #352;|š|&amp;amp; scaron;|&amp;amp; #353;|Ÿ|&amp;amp; Yuml;|&amp;amp; #376; |ˆ|&amp;amp; circ;|&amp;amp; #710;|˜|&amp;amp; tilde;|&amp;amp; #732;| |&amp;amp; ensp;|&amp;amp; #8194; | |&amp;amp; emsp;|&amp;amp; #8195;| |&amp;amp; thinsp;|&amp;amp; #8201;|‌|&amp;amp; zwnj;|&amp;amp; #8204; |‍|&amp;amp; zwj;|&amp;amp; #8205;|‎|&amp;amp; lrm;|&amp;amp; #8206;|‏|&amp;amp; rlm;|&amp;amp; #8207; |–|&amp;amp; ndash;|&amp;amp; #8211;|—|&amp;amp; mdash;|&amp;amp; #8212;|‘|&amp;amp; lsquo;|&amp;amp; #8216; |’|&amp;amp; rsquo;|&amp;amp; #8217;|‚|&amp;amp; sbquo;|&amp;amp; #8218;|“|&amp;amp; ldquo;|&amp;amp; #8220; |”|&amp;amp; rdquo;|&amp;amp; #8221;|„|&amp;amp; bdquo;|&amp;amp; #8222;|†|&amp;amp; dagger;|&amp;amp; #8224; |‡|&amp;amp; Dagger;|&amp;amp; #8225;|‰|&amp;amp; permil;|&amp;amp; #8240;|‹|&amp;amp; lsaquo;|&amp;amp; #8249; |›|&amp;amp; rsaquo;|&amp;amp; #8250;|€|&amp;amp; euro;|&amp;amp; #8364;
</content>
    </entry>
    
     <entry>
        <title>MySQL、Postgres、Oracle、SQL server、DB2、Teradata、Netezza数据类型比较</title>
        <url>https://mryqu.github.io/post/mysqlpostgresoraclesql_serverdb2teradatanetezza%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%AF%94%E8%BE%83/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mysql</tag><tag>postgres</tag><tag>oracle</tag><tag>sql server</tag><tag>db2</tag><tag>teradata</tag><tag>netezza</tag>
        </tags>
        <content type="html"> mysqlpostgresoraclemssqldb2teredatanetezzabigint8bytes
-9223372036854775808~9223372036854775807
0~184467440737095516158bytes
-9223372036854775808~9223372036854775807Not support8bytes
-9223372036854775808~92233720368547758078bytes (precision of 19digits)
-9223372036854775808~92233720368547758078bytes
-9223372036854775808~92233720368547758078bytes
-9223372036854775808~9223372036854775807decimal(m,n)65digits
64: MySQL 5.0.3~5.0.5
8?: MySQL 5.0.3 beforeno limit?
1000digits?m:1 ~38
n:-84~127.m:1 ~38
n:0~m1~31digitsm:1~18m:1 ~38
n:0~mnumeric=decimalnumeric=decimalnumeric=decimalnumeric=decimalnumeric=decimalinteger4bytes
-2147483648~2147483647
0~42949672954bytes
-2147483648~2147483647INTEGER=NUMBER(38)
int
4bytes
-2147483648~21474836474bytes (precision of 10digits)
-2147483648~21474836474bytes
-2147483648~21474836474bytes
-2147483648~2147483647varchar(n)0~255characters : MySQLbefore
0~65,535characters : MySQL 5.0.3 and above
unlimited lengthvarchar2(n)
4000 bytes800032672bytes64000 bytes64000 characterschar(n)0~255characters?&amp;nbsp;unlimited length2000 bytes80001~25464000 bytes64000 characterstextTINYTEXT:255(2^8)bytes
TEXT:65535(2^16)bytes
MEDIUMTEXT:16777215 (2^24)bytes
LONGTEXT:4294967295 (2^32)bytesunlimited lengthCLOB: (4 gigabytes - 1) * (database block size).2147483647(2^31-1)CLOB:2 gigabytesCLOB?Not supportdatedatedatedate: oracle date also include time partdatetime: MSSQL only support datetime and smalldatetimedatedatedatetimestamptimestamp:4bytes, UTC format, related with time zonedatetime: 8 bytes, same format with input, no realtion with timezonetimestamp without time zonetimestampdatetime: MSSQL only support datetime and smalldatetimetimestamptimestamptimestamp </content>
    </entry>
    
     <entry>
        <title>Shell参数扩展</title>
        <url>https://mryqu.github.io/post/shell%E5%8F%82%E6%95%B0%E6%89%A9%E5%B1%95/</url>
        <categories>
          <category>Tool</category><category>Linux</category>
        </categories>
        <tags>
          <tag>bash</tag><tag>shell</tag><tag>parameter</tag><tag>expansion</tag>
        </tags>
        <content type="html">  在hadoop-env.sh中，有如下语句：
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-&amp;quot;/etc/hadoop&amp;quot;}  这种用法在Shell Parameter Expansion中进行了详尽的介绍，系统学习一下。 Bash中的$符号的作用是参数替换，将参数名替换为参数所代表的值。对于$来说，大括号是可选的，即$ABC和${ABC}代表同一个参数。但是它可以防止变量被错误解析，比如：${hello}world、${arr[1]}。 参数扩展 下列Bash对参数的测试项为未设置和null。如果略掉冒号，则仅测试未设置。
|表达式|含义 |&amp;mdash;&amp;ndash; |${parameter:-word}|如果parameter没有被声明或者其值为空的话，则表达式替换成word；否则替换成parameter的值。 |${parameter:=word}|如果parameter没有被声明或者其值为空的话，则parameter设为word之后表达式返回parameter的值；否则替换成parameter的值。 |${parameter?word}|如果parameter没有被声明或者其值为空的话，则word被写往标准错误输出和Shell，非可交互的情况下退出；否则替换成parameter的值。 |${parameter:&#43;word}|如果parameter没有被声明或者其值为空的话，则不进行替换；否则替换成parameter的值。 |${!varprefix}
${!varprefix@}|匹配之前所有以varprefix开头进行声明的变量 |${!name[@]}
${!name[]}|如果name是数组对象，返回数组下标列表；如果name以设置但不为数组对象，返回0；否则返回null。
字符串操作 |表达式|含义 |&amp;mdash;&amp;ndash; |${% raw %}{#{% endraw %}parameter}|parameter的长度。 |${parameter:offset}|在parameter中，从位置offset开始提取子串。 |${parameter:offset:length}|在parameter中，从位置offset开始提取长度为length的子串。 |${parameter#word}
${parameter##word}|从头开始扫描parameter对应值，将匹配word正则表达式的字符删除掉#为最短匹配，##为最长匹配。 |${parameter%word}
${parameter%%word}|从尾开始扫描parameter对应值，将匹配word正则表达式的字符删除掉%为最短匹配，%%为最长匹配。 |${parameter/pattern/string}
${parameter//pattern/string}|将parameter对应值的pattern代替为string。/表示只替换一次，//表示全部替换。 |${parameter^pattern}
${parameter^^pattern}|如果pattern是单个字符，将parameter对应值中匹配pattern的字符转换为大写。^表示只转换匹配的首字母，^^表示全部转换。 |${parameter,pattern}
${parameter,,pattern}|如果pattern是单个字符，将parameter对应值中匹配pattern的字符转换为小写。,表示只转换匹配的首字母，,,表示全部转换。
</content>
    </entry>
    
     <entry>
        <title>Java对象的内存使用量分析</title>
        <url>https://mryqu.github.io/post/java%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E9%87%8F%E5%88%86%E6%9E%90/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>对象</tag><tag>内存使用量</tag>
        </tags>
        <content type="html"> 前段时间做GemFire中对象的内存使用量分析时，学习了一下Java对象的内存使用量分析。 如果不知道一个没有变量的空对象占8字节，空数组占12字节，空字符串对象占40字节的话，有必要看一下下面的链接进行学习。 Memory usage of Java Strings and string-related objects
Determining Memory Usage in Java
Java对象内存结构
主题：如何获取一个对象的在内存中的大小？
JAVA Objects Memory Size Reference
</content>
    </entry>
    
     <entry>
        <title>发现Hibernate 3.2.6统计中一个bug</title>
        <url>https://mryqu.github.io/post/%E5%8F%91%E7%8E%B0hibernate_3.2.6%E7%BB%9F%E8%AE%A1%E4%B8%AD%E4%B8%80%E4%B8%AAbug/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Hibernate</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>Statistics</tag>
        </tags>
        <content type="html"> 今天使用Hibernate的统计类，分析一下结果。结果发现了一个bug，不能获得查询缓存中的查询语句。 这个bug倒在3.6.8已经修改了，不过还是影响我的工作。
Statistics stat = sessionFactory.getStatistics(); logger.info(&amp;quot;isStatisticsEnabled:&amp;quot;&#43;stat.isStatisticsEnabled()); logger.info(&amp;quot;stat=&amp;quot;&#43;stat.toString()); logger.info(&amp;quot;queries=&amp;quot;&#43;Arrays.toString(stat.getQueries()));  org.hibernate.stat.StatisticsImpl.java
public String[] getQueries() { return ArrayHelper.toStringArray( queryStatistics.keySet()); }  org.hibernate.util.ArrayHelper.java (Hibernate 3.2.6)
public static String[] toStringArray(Collection coll) { return (String[]) coll.toArray(EMPTY_STRING_ARRAY); }  org.hibernate.util.ArrayHelper.java (Hibernate3.6.8)
public static String[] toStringArray(Collection coll) { return (String[]) coll.toArray( new String[coll.size()]); }  </content>
    </entry>
    
     <entry>
        <title>模型评估笔记</title>
        <url>https://mryqu.github.io/post/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>model</tag><tag>evaluation</tag><tag>data-mining</tag><tag>classification</tag><tag>regression</tag>
        </tags>
        <content type="html">  模型评估简介 模型评估是模型开发过程的不可或缺的一部分。它有助于发现表达数据的最佳模型和所选模型将来工作的性能如何。在数据挖掘中，使用训练集中的数据评估模型性能是不可接受的，因为这易于生成过于乐观和过拟合的模型。数据挖掘中有两种方法评估模型，验证（Hold-Out）和交叉验证（Cross-Validation）。为了避免过拟合，这两种方法都使用（模型没有遇到过的）测试集来评估模型性能。
验证（Hold-Out） 使用这种方法时，通常大的数据集会被_随机_分成三个子集： - 训练集：用于构建预测模型。 - 验证集：用于评估训练阶段所得模型的性能。它为模型参数优化和选择最优模型提供了测试平台。不是所有模型算法都需要验证机。 - 测试集或之前未遇到的样本用于评估模型未来可能的性能。如果模型与训练集拟合的好于测试集，有可能是过拟合所致。
交叉验证（Cross-Validation） 当仅有有限数量的数据时，为了对模型性能进行无偏估计，我们可以使用_k_折交叉验证（k-foldcross-validation）。使用这种方法时，数据被分成_k_份数目相等的子集。我们构建_k_次模型，每次留一个子集做测试集，其他用作训练集。如果_k_等于样本大小，这也被称之为留一验证（leave-one-out）。
分类模型评估 混淆矩阵（Confusion Matrix） 混淆矩阵显示了分类模型相对数据的真实输出（目标值）的正确预测和不正确预测数目。矩阵为_N_x_N_，其中_N_为目标值（类）数目。这类模型的性能通常使用矩阵中的数据评估。下表为两个类别（阳性和阴性）的2x2混淆矩阵。 混淆矩阵目标&amp;nbsp;阳性阴性模型阳性TPFP阳性预测值
TP/(TP&#43;FP)阴性FNTN阴性预测值
TN/(FN&#43;TN)&amp;nbsp;灵敏度特异度准确度&amp;nbsp;=(TP&#43;TN)/(TP&#43;FP&#43;FN&#43;TN)
TP/(TP&#43;FN)TN/(FP&#43;TN)
术语： - 阳性 (P, positive) - 阴性 (N, Negative) - 真阳性 (TP, true positive)：正确的肯定。又称：命中 (hit) - 真阴性 (TN, true negative)：正确的否定。又称：正确拒绝 (correctrejection) - 伪阳性 (FP, false positive)：错误的肯定，又称：假警报 (falsealarm)、第二型错误 - 伪阴性 (FN, false negative)：错误的否定，又称：未命中(miss)、第一型错误 - 灵敏度(sensitivity)或真阳性率(TPR, true positive rate)：又称：召回率（recall）、命中率 (hit rate)在阳性值中实际被预测正确所占的比例。TPR = TP / P = TP / (TP&#43;FN) - 伪阳性率(FPR, false positive rate)：又称：错误命中率，假警报率 (false alarm rate)FPR = FP / N = FP / (FP &#43; TN) = 1-SPC - 特异度 (SPC, Specificity)或真阴性率(TNR, true negativerate)：在阴性值中实现被预测正确所占的比例。SPC = TN / N = TN / (FP&#43;TN) = 1-FPR - 假发现率 (FDR, false discovery rate)：FDR = FP / (FP &#43; TP) = 1-TPR - 准确度 (ACC, accuracy）：预测正确的数占样本数的比例。ACC = (TP &#43; TN) / (P &#43; N) - 阳性预测值 (PPV, positive predictive value)或精度(precision)：阳性预测值被预测正确的比例。PPV = TP / (TP &#43; FP) - 阴性预测值 (NPV, negative predictive value)：阴性预测值被预测正确的比例。NPV = TN / (TN &#43; FN) - F1评分：精度和灵敏度的调和平均数。F1 = 2 precision * recall / (precision&#43;recall) =2TP/(2TP&#43;FP&#43;FN) - Matthews相关系数 (MCC)，即 Phi相关系数：(TP*TN - FP*FN)/ sqrt{(TP&#43;FP)(TP&#43;FN)(TN&#43;FP)(TN&#43;FN)}
示例: 混淆矩阵目标&amp;nbsp;阳性阴性模型阳性7020阳性预测值
0.78阴性3080阴性预测值
0.73&amp;nbsp;灵敏度特异度准确度&amp;nbsp;= 0.75
0.700.80
增益（Gain）和提升（Lift）图 增益和提升是分类模型有效性指标，由通过模型获得的结果和没有模型获得的结果之间的比率计算而成。增益图和提升图使用与评估分类模型性能的可视化工具。然而，与混淆矩阵评估的是整个总体上的模型性能不同，增益图和提升图评估的是总体一部分上的模型性能。
增益图 增益图实际上描述的是整体覆盖率(精度)指标。按照模型预测出的概率从高到低排列，将每一个百分位数内的覆盖率指标标注在图形区域内，就形成了非累积的增益图。如果对每一个百分位及其之前的覆盖率求和，并将值标注在图形区域内，则形成累积的增益图。 - 显然，累积图通常能够更好的表现模型性能，而非累积图则更有利于指出模型中可能存在问题的地方。 - 采用训练集绘制的增益图一般都很不错，虽然没什么意义。用验证集绘制的收益图则未必。
示例: 提升图 提升图实际上是把各个百分位点上的提升度予以描述，同样也分累积的和非累积的。提升图显示与联系随机顾客样本相比我们有可能获得多少阳性响应。例如，通过预测模型仅联系10%的顾客，可以获得不使用模型的三倍响应。 K-S（柯尔莫诺夫-斯米尔诺夫，Kolmogorov-Smirnov）图 K-S图衡量分类模型的性能。更准确的说，K-S阳性和阴性分布之间分离度指标。如果评分将总体分成两组，一组全是阳性，一组全是阴性，则K-S为100。如果模型无法区分阳性和阴性，模型选择的效果类似从总体中随机抽取，K-S将为0。对大多数分类模型，K-S在0到100之间，值越高表示模型分离阳性和阴性的效果越好。 示例： 下例显示分类模型结果。模型对每个阳性（目标）和阴性（非目标）输出赋予0到1000之间的评分。 接收者操作特征曲线（ROC）图 ROC图与增益图或提升图类似，它们都提供了比较分类模型的一种途径。ROC图在X轴显示伪阳性率（真值为0、目标值为1的概率，即1-特异度），Y轴为真阳性率（即灵敏度）。理想情况下，曲线快速爬向左上，表示模型准确预测数据。红色斜对角线表示随机模型。 曲线下面积(AUC，Area Under the Curve) ROC曲线下面积经常用作衡量分类模型质量的指标。随机分类的AUC为0.5，而完美分类的AUC等于1。在实践中，大多数分类模型的AUC在0.5和1之间。 举个例子来说，AUC为0.8表示从目标值为1的组内随机抽取的案例有80%的可能比目标值为0的组内随机抽取的案例评分大。当分类器无法区分两组，区域等于0.5(ROC曲线与斜线一致)。当两组完美分离，例如没有重叠的分布，ROC曲线将直接到达左上角纵坐标1的位置.
基尼系数 判定方法：基尼系数应大于60%，就算好模型。 Gini ＝ 2*AUC － 1
回归模型评估 当创建一些不同的回归模型后，有大量的标准可被评估和比较。
均方根误差（Root Mean Squared Error，RMSE） RMSE是一个衡量回归模型误差率的常用公式。 然而，它仅能比较误差是相同单位的模型。 相对平方误差（Relative Squared Error，RSE） 与RMSE不同，RSE可以比较误差是不同单位的模型。 平均绝对误差（Mean Absolute Error，MAE) MAE与原始数据单位相同， 它仅能比较误差是相同单位的模型。量级近似与RMSE，但是误差值相对小一些。 相对绝对误差（Relative Absolute Error，RAE) 与RSE不同，RAE可以比较误差是不同单位的模型。 决定系数 (Coefficient of Determination) 决定系数 (R2)回归模型汇总了回归模型的解释度，由平方和术语计算而得。 R2描述了回归模型所解释的因变量方差在总方差中的比例。R2很大，即自变量和因变量之间存在线性关系，如果回归模型是“完美的”，SSE为零，则R2为1。R2小，则自变量和因变量之间存在线性关系的证据不令人信服。如果回归模型完全失败，SSE等于SST，没有方差可被回归解释，则R2为零。
标准化残差图（Standardized Residuals Plot） 标准化残差图是一个对在标准化尺度上显示残差分散图有帮助的可视化工具。标准化残差图与普通残差图之间没有实质不同，唯一区别是在Y轴的标准化可以更容易检测到潜在的异常值。 参考 Model Evaluation
分类模型的性能评估——以SAS Logistic回归为例(1): 混淆矩阵
分类模型的性能评估——以SAS Logistic回归为例(2): ROC和AUC
分类模型的性能评估——以SAS Logistic回归为例(3): Lift和Gain
WIKI：Gain(information retrieval)
WIKI：Receiver operating characteristic
WIKI：F1 score
WIKI：Gini coefficient
[](http://mund-consulting.com/Blog/understanding-uate-model-in-microsoft-azure-machine-learning/)
[](http://www.analyticsvidhya.com/blog/2015/01/model-performance-metrics-classification/)
[](http://www.analyticsvidhya.com/blog/2015/01/model-perform-part-2/)
</content>
    </entry>
    
     <entry>
        <title>用GemFire做Hibernate二级缓存</title>
        <url>https://mryqu.github.io/post/%E7%94%A8gemfire%E5%81%9Ahibernate%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category><category>GemFire</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>gemfire</tag><tag>缓存</tag>
        </tags>
        <content type="html">  打开二级缓存 &amp;lt;property name=&amp;quot;hibernate.cache.use_second_level_cache&amp;quot;&amp;gt;true&amp;lt;/property&amp;gt;  为查询缓存设置Cache Factory &amp;lt;property name=&amp;quot;hibernate.cache.region.factory_class&amp;quot;&amp;gt; com.gemstone.gemfire.modules.hibernate.GemFireRegionFactory &amp;lt;/property&amp;gt;  共享缓存模式 ENABLE_SELECTIVE|DISABLE_SELECTIVE|ALL|NONE  ENABLE_SELECTIVE (默认值及推荐值): 仅标注为可缓存的实体会被缓存。 DISABLE_SELECTIVE: 仅标注为不可缓存的实体才不会被缓存。 ALL: 即使实体标为不可缓存也会被缓存。 NONE: 即使实体标为可缓存也不会被缓存。该选项意味着完全禁止二级缓存。
GemFire相关配置 设置GemFire缓存属性
&amp;lt;property name=&amp;quot;gemfire.PROPERTY_NAME&amp;quot;&amp;gt;PROPERTY_VALUE&amp;lt;/property&amp;gt;  设置GemFire缓存默认region类型
&amp;lt;property name=&amp;quot;gemfire.default-region-attributes-id&amp;quot;&amp;gt; REGION_ATTRIBUTE &amp;lt;/property&amp;gt;  REGION_ATTRIBUTE是预定义region类型快捷定义中的任一个。默认为REPLICATE_HEAP_LRU。其他有效region快捷定义包括:REPLICATE、 REPLICATE_PERSISTENT、 PARTITION、 PARTITION_PERSISTENT、PARTITION_REDUNDANT、 PARTITION_REDUNDANT_PERSISTENT。
设置特定GemFire缓存region属性
&amp;lt;property name=&amp;quot;gemfire.region-attributes-for: com.foo.Bar&amp;quot;&amp;gt; REGION_ATTRIBUTE &amp;lt;/property&amp;gt;  缓存映射 @Cache ( CacheConcurrencyStrategy usage(); String region() default &amp;quot;&amp;quot;; String include() default &amp;quot;all&amp;quot;; )  usage: 缓存并发策略(NONE, READ_ONLY, NONSTRICT_READ_WRITE, READ_WRITE,TRANSACTIONAL) region (可选项，默认为实体类的全类名或集合的全类名加属性名):缓存region名 include (选项项，默认为all): all则缓存所有实体属性，non-lazy仅缓存非懒惰加载的实体属性。
对缓存实体使用注释 @Entity @Cacheable @Cache(usage = CacheConcurrencyStrategy.NONSTRICT_READ_WRITE) public class Forest { ... }  对缓存集合使用注释 @OneToMany(cascade=CascadeType.ALL, fetch=FetchType.EAGER) @JoinColumn(name=&amp;quot;CUST_ID&amp;quot;) @Cache(usage = CacheConcurrencyStrategy.NONSTRICT_READ_WRITE) public SortedSet getTickets() { return tickets; }  GemFire支持READ_ONLY、NONSTRICT_READ_WRITE、READ_WRITE和TRANSACTIONAL缓存并发策略。
缓存模式 CacheMode 参数用于控制具体的 Session 如何与二级缓存进行交互: - CacheMode.NORMAL(默认值)：从二级缓存中读写数据。 - CacheMode.GET：从二级缓存中读取数据，但不会向二级缓存写数据。 - CacheMode.PUT：仅向二级缓存写数据，但不从二级缓存中读数据。 - CacheMode.REFRESH：仅向二级缓存写数据，但不从二级缓存中读数据。通过hibernate.cache.use_minimal_puts 的设置，强制二级缓存从数据库中读取数据，刷新缓存内容。
void Session.setCacheMode(CacheMode cacheMode): 设置会话的缓存模式 Query Query.setCacheMode(CacheMode cacheMode):为本次查询覆盖当前会话缓存模式 Criteria Criteria.setCacheMode(CacheMode cacheMode):为本次查询覆盖当前会话缓存模式
</content>
    </entry>
    
     <entry>
        <title>PostgreSQL JDBC setFetchSize</title>
        <url>https://mryqu.github.io/post/postgresql_jdbc_setfetchsize/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>postgresql</tag><tag>hibernate</tag><tag>fetch_size</tag><tag>jdbc</tag>
        </tags>
        <content type="html"> 今天看到我们的Hiberante配置没有设hibernate.jdbc.fetch_size。 Fetch Size是设定JDBC的Statement读取数据的时候每次从数据库中取出的记录条数。例如一次查询1万条记录，对于Oracle的JDBC驱动来说，是不会1次性把1万条取出来的，而只会取出FetchSize条数，当纪录集遍历完了这些记录以后，再去数据库取Fetch Size条数据。因此大大节省了无谓的内存消耗。当然FetchSize设的越大，读数据库的次数越少，速度越快；FetchSize越小，读数据库的次数越多，速度越慢。这有点像平时我们写程序写硬盘文件一样，设立一个缓冲，每次写入缓冲，等缓冲满了以后，一次写入硬盘，道理相同。 看了一下Postgres，它的JDBC驱动却是一次将查询的所有结果都返回。相反使用游标、设置fetchsize倒是麻烦不少。 http://jdbc.postgresql.org/documentation/head/query.html
</content>
    </entry>
    
     <entry>
        <title>Hibernate3 HQL: ClassCastException解决办法</title>
        <url>https://mryqu.github.io/post/hibernate3_hql_classcastexception%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Hibernate</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>hql</tag><tag>ClassCastException</tag><tag>ResultTransformer</tag>
        </tags>
        <content type="html"> 下面的示例代码跑在Hibernate 3.6.8，会抛出异常java.lang.ClassCastException:[Ljava.lang.Object 无法转换成实体对象。通过调试可知返回的仍然是标量字段对象数组，而不是实体对象。
public class DemoEntity { public static final String DEMO_ENTITY_ID = &amp;quot;demoEntityID&amp;quot;; public static final String USER_ID = &amp;quot;userID&amp;quot;; public static final String OFFICE_ID = &amp;quot;officeID&amp;quot;; private Integer demoEntityID; private String userID; private Integer officeID; private String subjectTxt; public List findDemoEntitys(int startID, int limit, boolean includeStartIdInResults) { try { StringBuilder querySb = new StringBuilder(); querySb.append(&amp;quot;select demo.&amp;quot;).append(DemoEntity.DEMO_ENTITY_ID); querySb.append(&amp;quot;, demo.&amp;quot;).append(DemoEntity.USER_ID); querySb.append(&amp;quot;, demo.&amp;quot;).append(DemoEntity.OFFICE_ID); querySb.append(&amp;quot; from DemoEntity demo where demo.&amp;quot;).append(DemoEntity.DEMO_ENTITY_ID); if (includeStartIdInResults) { querySb.append(&amp;quot; &amp;gt;= &amp;quot;).append(startID); } else { querySb.append(&amp;quot; &amp;gt; &amp;quot;).append(startID); } querySb.append(&amp;quot; order by demo.&amp;quot;).append(DemoEntity.DEMO_ENTITY_ID).append(&amp;quot; asc&amp;quot;); Query query = sessionFactory.getCurrentSession().createQuery(querySb.toString()); query.setResultTransformer(Transformers.aliasToBean(DemoEntity.class)); query.setMaxResults(limit); List results = query.setCacheable(true).list(); return results; } catch (RuntimeException re) { throw re; } } }  通过分析Hibernate3.6.8源码，可知ClassicQueryTranslatorFactory使用了org.hibernate.hql.classic.QueryTranslatorImpl（继承自BasicLoader），它没有将标量字段数组转换成实体对象。
protected List getResultList(List results, ResultTransformer resultTransformer) throws QueryException { if ( holderClass != null ) { for ( int i = 0; i &amp;lt; results.size(); i&#43;&#43; ) { Object[] row = ( Object[] ) results.get( i ); try { results.set( i, holderConstructor.newInstance( row ) ); } catch ( Exception e ) { throw new QueryException( &amp;quot;could not instantiate: &amp;quot; &#43; holderClass, e ); } } } return results; }  通过分析Hibernate3.6.8源码，可知ASTQueryTranslatorFactory则使用org.hibernate.hql.ast.QueryTranslatorImpl，它使用的是QueryLoader，而QueryLoader会将标量字段数组转换成实体对象。
protected List getResultList(List results, ResultTransformer resultTransformer) throws QueryException { // meant to handle dynamic instantiation queries... HolderInstantiator holderInstantiator = buildHolderInstantiator( resultTransformer ); if ( holderInstantiator.isRequired() ) { for ( int i = 0; i &amp;lt; results.size(); i&#43;&#43; ) { Object[] row = ( Object[] ) results.get( i ); Object result = holderInstantiator.instantiate(row); results.set( i, result ); } if ( !hasSelectNew() &amp;amp;&amp;amp; resultTransformer != null ) { return resultTransformer.transformList(results); } else { return results; } } else { returnresults; } }  解决方案1：修改querytranslator factory 将Hibernate配置文件中hibernate.query.factory_class由org.hibernate.hql.classic.ClassicQueryTranslatorFactory修改为org.hibernate.hql.ast.ASTQueryTranslatorFactory,这样就能工作正常。 ClassicQueryTranslatorFactory是手写的传统HQL语法解析器工厂类，ASTQueryTranslatorFactory是更新的、基于ANTLR语法解析器生成的HQL语法解释器工厂类。
&amp;lt;propertyname=&amp;quot;hibernate.query.factory_class&amp;quot;value=&amp;quot;org.hibernate.hql.ast.ASTQueryTranslatorFactory&amp;quot;/&amp;gt;  解决方案2：不使用setResultTransformer函数，自己对结果进行转换
Query query = sessionFactory.getCurrentSession().createQuery(querySb.toString()); //query.setResultTransformer(Transformers.aliasToBean(DemoEntity.class)); query.setMaxResults(limit); List&amp;lt;Object[]&amp;gt; queryRes = query.setCacheable(true).list(); List&amp;lt;DemoEntity&amp;gt; results = new ArrayList&amp;lt;DemoEntity&amp;gt;(); if(queryRes!=null &amp;amp;&amp;amp; !queryRes.isEmpty()) { String[] alias = {DemoEntity.DEMO_ENTITY_ID, DemoEntity.USER_ID, DemoEntity.OFFICE_ID); ResultTransformer resTransformer = Transformers.aliasToBean(DemoEntity.class); for(int i=0;i&amp;lt;queryRes.size();i&#43;&#43;) { Object[] objs = (Object[]) queryRes.get(i); if(objs!=null) { results.add((DemoEntity) resTransformer.transformTuple(objs, alias)); } } } return results;  </content>
    </entry>
    
     <entry>
        <title>Hibernate3.X升级到4.X实践</title>
        <url>https://mryqu.github.io/post/hibernate3.x%E5%8D%87%E7%BA%A7%E5%88%B04.x%E5%AE%9E%E8%B7%B5/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Hibernate</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>hibernate3</tag><tag>hibernate4</tag><tag>升级</tag><tag>实践</tag>
        </tags>
        <content type="html">  Jar调查 |jar文件|Hibernate 4.2.2|当前的Hibernate|操作 |&amp;mdash; |antlr.jar|required, 2.7.7|3.2.0|无需改变。 |dom4j.jar|required, 1.6.1|1.6.1|无需改变。 |hibernate-commons-annotations.jar|required, 4.0.2|3.3.1|替换。 |hibernate-corel.jar|required, 4.2.2|3.2.6|替换。 |hibernate-jpa-2.0-api.jar|required, 1.0.1||增加。 |javassist.jar|required, 3.15.0|3.15.0|无需改变。 |jboss-logging.jar|required, 3.1.0||增加。 |jboss-transaction-api_1.1_specl.jar|required, 1.0.1||一开始增加，后来去掉了。 |hibernate-annotations.jar|null|3.3.1|去掉。从Hibernate3.6.0开始hibernate-annotations被合并到hibernate-core。 |hibernate-entitymanager.jar|jpa, 4.2.2|3.2.2|替换。
修改HibernateUtil 将AnnotationConfiguration替换成Configuration； 在使用ServiceRegistry;
Configuration config = new Configuration().configure(); ServiceRegistry serviceRegistry = newServiceRegistryBuilder() .applySettings(config.getProperties()).buildServiceRegistry(); sessionFactory = config.buildSessionFactory(serviceRegistry);  Hibernate4不支持Ant HibernateToolTask。 使用Hibernate3和HibernateToolTask创建hibernate.cfg.xml 使用Hibernate4编译。 https://community.jboss.org/thread/177200
修改Web容器下的jar文件  替换hibernate3.jar为hibernate-core.jar 替换hibernate-commons-annotations.jar 删除ejb3-persistence.jar 删除hibernate-annotations.jar 复制hibernate-entitymanager.jar 复制hibernate-jpa-2.0-api.jar 复制jboss-logging.jar 复制jboss-transaction-api_1.1_spec.jar （最终没有复制）  通过删除jboss-transaction-api_1.1_spec.jar解决TransactionManager冲突 org.springframework.jndi.TypeMismatchNamingException: Objectof type [class com.atomikos.icatch.jta.J2eeTransactionManager]available at JNDI location [java:comp/env/TransactionManager] isnot assignable to [javax.transaction.TransactionManager]  通过删除ejb3-persistence.jar解决javax.persistence冲突 java.lang.NoSuchMethodError:javax.persistence.OneToMany.orphanRemoval()  ejb3-persistence.jar和hibernate-jpa-2.0-api.jar冲突。hibernate-jpa-2.0-api.jar无法删除，否则会导致java.lang.ClassNotFoundException:javax.persistence.Access。
Atomikos没有正式支持Hibernate4，使用临时方案 java.lang.ClassNotFoundException:org.hibernate.transaction.JTATransactionFactory  AtomikosJTATransactionFactory扩展org.hibernate.transaction.JTATransactionFactory(实现org.hibernate.transaction.TransactionFactory接口). 在hibernate4实现中，JTATransactionFactory已被org.hibernate.engine.transaction.internal.jta.JtaTransactionFactory(实现org.hibernate.engine.transaction.spi.TransactionFactory接口)所取代。 http://fogbugz.atomikos.com/default.asp?community.6.2843.5
通过修改hibernate.cfg.xml配置解决NoClassDefFoundError java.lang. NoClassDefFoundError: Could not load requestedclass :org.hibernate.hql.classic.ClassicQueryTranslatorFactory  将hibernate.query.factory_class设置为org.hibernate.hql.internal.classic.ClassicQueryTranslatorFactory
java.lang.NoClassDefFoundError:org/hibernate/cache/QueryCacheFactory  将hibernate.cache.query_cache_factory设置为org.hibernate.cache.internal.StandardQueryCacheFactory
</content>
    </entry>
    
     <entry>
        <title>多维数据遍历</title>
        <url>https://mryqu.github.io/post/%E5%A4%9A%E7%BB%B4%E6%95%B0%E6%8D%AE%E9%81%8D%E5%8E%86/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>olap</tag><tag>遍历</tag>
        </tags>
        <content type="html">  在联机分析处理（OLAP）系统中，需要对存储在数据库或数据仓库中的数据提供分析。由于数据维数不定，无法采用多重for循环进行数据遍历。我在开发过程中一般使用扁平化下标对多维数据进行遍历，今天尝试了一下递归方式，效率更高一些，但是对栈的消耗也更多一些。下面的代码示例使用两种不同的方式对多维数据进行遍历： - 递归 - 采用扁平化下标
示例代码 package com.yqu.collection; import java.util.ArrayList; import java.util.List; public class MultipleDimensionTraveling &amp;lt;T&amp;gt;{ private List&amp;lt;List&amp;lt;T&amp;gt;&amp;gt; mdList; public MultipleDimensionTraveling(){ mdList = new ArrayList&amp;lt;List&amp;lt;T&amp;gt;&amp;gt;(); } public MultipleDimensionTraveling(List&amp;lt;List&amp;lt;T&amp;gt;&amp;gt; mdList){ this.mdList = mdList; } public void addDimension(List&amp;lt;T&amp;gt; dim){ mdList.add(dim); } public void travelByRecursion(){ if(!mdList.isEmpty()) travelByRecursion(0, new ArrayList&amp;lt;T&amp;gt;(mdList.size())); } private void travelByRecursion(int dimIdx, List&amp;lt;T&amp;gt; crossing){ for(int i=0;i&amp;lt;mdList.get(dimIdx).size();i&#43;&#43;){ if(crossing.size()&amp;lt;mdList.size()) crossing.add(dimIdx, mdList.get(dimIdx).get(i)); else crossing.set(dimIdx, mdList.get(dimIdx).get(i)); if(dimIdx==mdList.size()-1){ System.out.println(crossing.toString()); } else { travelByRecursion(dimIdx&#43;1, crossing); } } } public void travelByFlatIndice(){ if(!mdList.isEmpty()){ int crossingNum = 1; int[] cardinalities = new int[mdList.size()]; int[] multipliers = new int[mdList.size()]; for(int i=0;i&amp;lt;mdList.size();i&#43;&#43;) { List&amp;lt;T&amp;gt; dim = mdList.get(i); cardinalities[i] = dim.size(); crossingNum *= cardinalities[i]; } for(int i=cardinalities.length-1;i&amp;gt;=0;i--) { multipliers[i] = (i == multipliers.length - 1)? 1 : cardinalities[i&#43;1]*multipliers[i&#43;1]; } for(int flatIndice=0;flatIndice&amp;lt;crossingNum;flatIndice&#43;&#43;){ handleCrossing(flatIndice, multipliers); } } } private void handleCrossing(int flatIndice, int[] multipliers){ List&amp;lt;T&amp;gt; crossing = new ArrayList&amp;lt;T&amp;gt;(multipliers.length); for (int dimIdx = 0; dimIdx &amp;lt; multipliers.length; dimIdx&#43;&#43;){ int mbrIdx = flatIndice / multipliers[dimIdx]; crossing.add(dimIdx, mdList.get(dimIdx).get(mbrIdx)); flatIndice = flatIndice % multipliers[dimIdx]; } System.out.println(crossing); } public static void main(String[] args) { MultipleDimensionTraveling&amp;lt;String&amp;gt; mdlObj = new MultipleDimensionTraveling&amp;lt;String&amp;gt;(); List&amp;lt;String&amp;gt; dim = new ArrayList&amp;lt;String&amp;gt;(); dim.add(&amp;quot;dim1A&amp;quot;); dim.add(&amp;quot;dim1B&amp;quot;); dim.add(&amp;quot;dim1C&amp;quot;); mdlObj.addDimension(dim); dim = new ArrayList&amp;lt;String&amp;gt;(); dim.add(&amp;quot;dim2A&amp;quot;); dim.add(&amp;quot;dim2B&amp;quot;); dim.add(&amp;quot;dim2C&amp;quot;); dim.add(&amp;quot;dim2D&amp;quot;); mdlObj.addDimension(dim); dim = new ArrayList&amp;lt;String&amp;gt;(); dim.add(&amp;quot;dim3A&amp;quot;); dim.add(&amp;quot;dim3B&amp;quot;); mdlObj.addDimension(dim); System.out.println(&amp;quot;Travling by flat indice:&amp;quot;); mdlObj.travelByFlatIndice(); System.out.println(&amp;quot;Travling by recursion:&amp;quot;); mdlObj.travelByRecursion(); } }  运行结果 Multiple dimension list travling by flat indice: [dim1A, dim2A, dim3A] [dim1A, dim2A, dim3B] [dim1A, dim2B, dim3A] [dim1A, dim2B, dim3B] [dim1A, dim2C, dim3A] [dim1A, dim2C, dim3B] [dim1A, dim2D, dim3A] [dim1A, dim2D, dim3B] [dim1B, dim2A, dim3A] [dim1B, dim2A, dim3B] [dim1B, dim2B, dim3A] [dim1B, dim2B, dim3B] [dim1B, dim2C, dim3A] [dim1B, dim2C, dim3B] [dim1B, dim2D, dim3A] [dim1B, dim2D, dim3B] [dim1C, dim2A, dim3A] [dim1C, dim2A, dim3B] [dim1C, dim2B, dim3A] [dim1C, dim2B, dim3B] [dim1C, dim2C, dim3A] [dim1C, dim2C, dim3B] [dim1C, dim2D, dim3A] [dim1C, dim2D, dim3B] Multiple dimension list travling by recursion: [dim1A, dim2A, dim3A] [dim1A, dim2A, dim3B] [dim1A, dim2B, dim3A] [dim1A, dim2B, dim3B] [dim1A, dim2C, dim3A] [dim1A, dim2C, dim3B] [dim1A, dim2D, dim3A] [dim1A, dim2D, dim3B] [dim1B, dim2A, dim3A] [dim1B, dim2A, dim3B] [dim1B, dim2B, dim3A] [dim1B, dim2B, dim3B] [dim1B, dim2C, dim3A] [dim1B, dim2C, dim3B] [dim1B, dim2D, dim3A] [dim1B, dim2D, dim3B] [dim1C, dim2A, dim3A] [dim1C, dim2A, dim3B] [dim1C, dim2B, dim3A] [dim1C, dim2B, dim3B] [dim1C, dim2C, dim3A] [dim1C, dim2C, dim3B] [dim1C, dim2D, dim3A] [dim1C, dim2D, dim3B]  </content>
    </entry>
    
     <entry>
        <title>Servlet</title>
        <url>https://mryqu.github.io/post/servlet/</url>
        <categories>
          <category>Service&#43;JavaEE</category>
        </categories>
        <tags>
          <tag>servlet</tag><tag>java</tag>
        </tags>
        <content type="html"> 现在用的VFabric tc Server 2.8.0基于Apache的Tomcat7.0.3版本，支持Servlet3.0和JSP 2.2规范。 Java Servlet3.1规范这周可以下载了，目前正在学习当中。将Servlet的不同版本的资料汇总一下，利人利己。 JSR 340：Java Servlet 3.1 Specification
JSR 315：Java Servlet 3.0 Specification
JSR 154：Java Servlet 2.4 Specification
JSR 53：Java Servlet 2.3 and JavaServer Pages 1.2 Specification
Servlet API 2.2 的新特性
Servlet 3.0 新特性详解
</content>
    </entry>
    
     <entry>
        <title>Hibernate缓存</title>
        <url>https://mryqu.github.io/post/hibernate%E7%BC%93%E5%AD%98/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category>
        </categories>
        <tags>
          <tag>hibernate</tag><tag>缓存</tag><tag>一级缓存</tag><tag>二级缓存</tag><tag>查询缓存</tag>
        </tags>
        <content type="html">  Hibernate缓存 Hibernate带有三种不同缓存机制：一级缓存、二级缓存和查询缓存。
SessionFactory和Session SessionFactory(在JEE中叫做EntityManager)的用途是创建会话，初始化JDBC链接并（使用例如C3P0之类的可插拔provider）进行池化。SessionFactory是非可变的，通过hibernate.cfg.cml文件或Springbean配置中提供的匹配信息、缓存信息等配置进行创建。会话是最低级的工作单元，对应一个数据库事物。当会话创建后并对Hibernate实体机型一些操作，比如设置实体的一个属性，Hibernate不会立即更新底层数据库表。相反Hibernate记录实体的状态（是否为脏数据），并在会话最终刷新更新到数据库。这就是Hibernate所谓的一级缓存。
一级缓存 一级缓存是Hibernate记录正在进行的会话加载和接触的实体有可能的脏数据状态。正在进行的会话代表工作单元，始终使用，无法关闭。一级缓存的用途是隐藏对数据库许多SQL查询或更新，并在会话最终批量一起执行。当想起一级缓存的时候就应该想到会话。
二级缓存 二级缓存是进程范围内的缓存，与一个SessionFactory绑定。二级缓存可被相同（通常一个应用程序仅一个）SessionFactory的所有会话共享。默认二级缓存没有使能。二级缓存不存储任何实体实例，而是存储“脱水”状态，即字符串或整形数组代表实体的属性，一个实体id指向“脱水”的实体。概念上可以认为它是一个映射，id作为键，数组作为值。或像下面用于缓存region的这些东西：
public class Person { private Person parent; private Set&amp;lt;Person&amp;gt; children; public void setParent(Person p) { parent = p; } public void setChildren(Set&amp;lt;Person&amp;gt; set) { children = set; } public Set&amp;lt;Person&amp;gt; getChildren() { return children; } public Person getParent() { return parent; } }  Hibernate映射配置如下:
&amp;lt;class name=&amp;quot;org.javalobby.tnt.hibernate.Person&amp;quot;&amp;gt; &amp;lt;cache usage=&amp;quot;read-write&amp;quot;/&amp;gt; &amp;lt;id name=&amp;quot;id&amp;quot; column=&amp;quot;id&amp;quot; type=&amp;quot;long&amp;quot;&amp;gt; &amp;lt;generator class=&amp;quot;identity&amp;quot;/&amp;gt; &amp;lt;/id&amp;gt; &amp;lt;property name=&amp;quot;firstName&amp;quot; type=&amp;quot;string&amp;quot;/&amp;gt; &amp;lt;property name=&amp;quot;middleInitial&amp;quot; type=&amp;quot;string&amp;quot;/&amp;gt; &amp;lt;property name=&amp;quot;lastName&amp;quot; type=&amp;quot;string&amp;quot;/&amp;gt; &amp;lt;many-to-one name=&amp;quot;parent&amp;quot; column=&amp;quot;parent_id&amp;quot; class=&amp;quot;Person&amp;quot;/&amp;gt; &amp;lt;set name=&amp;quot;children&amp;quot;&amp;gt; &amp;lt;key column=&amp;quot;parent_id&amp;quot;/&amp;gt; &amp;lt;one-to-many class=&amp;quot;Person&amp;quot;/&amp;gt; &amp;lt;/set&amp;gt; &amp;lt;/class&amp;gt;  Hibernate概念上为此类持有如下记录:
*-----------------------------------------* | Person Data Cache | |-----------------------------------------| | 1 -&amp;gt; [ &amp;quot;John&amp;quot; , &amp;quot;Q&amp;quot; , &amp;quot;Public&amp;quot; , null ] | | 2 -&amp;gt; [ &amp;quot;Joey&amp;quot; , &amp;quot;D&amp;quot; , &amp;quot;Public&amp;quot; , 1 ] | | 3 -&amp;gt; [ &amp;quot;Sara&amp;quot; , &amp;quot;N&amp;quot; , &amp;quot;Public&amp;quot; , 1 ] | *-----------------------------------------*  因此，本例中Hibernate保留三个字符串和一个用于“多对一”父子关系的可序列化标识符，而不是保留真正的对象实例。这非常重要：一、Hibernate不用担心客户代码操作对象会破坏缓存；二、关联关系和关联由于是简单标识符不会变得‘陈旧’，并易于更新到最新状态。缓存不是对象树，而是概念上的数组映射。概念一词被连续使用，是因为Hibernate为Cache做了更多的幕后工作，用户如果不想实现自己的provider则无需担心其内部实现。Hibernate将对象的状态称之为“脱水的”，因为它含有对象所有重要的信息，但是对Hibernate更为可控。 你有可能注意到子关联在缓存中被省略掉了。Hibernate的控制颗粒度可以让你决定哪些关联应被加入缓存，从二级缓存恢复业务数据时哪些关联应重新判决。当关联有可能被其他类修改但不想对已缓存的类做显式级联修改时，提供了更多控制选项。 默认设置是不缓存关联。如果你对此不了解并对Hibernate缓存如果工作一无所知，仅仅简单打开缓存，有可能仅仅增加了管理缓存的负担而没有获得相应的好处。毕竟，缓存的最大优点是无需通过N&#43;1数据库查询而获得复杂关联。 在此例中，我们仅处理person类，并知道关联能被适当地管理，因此更新映射以让子关联可以被缓存。
 &amp;lt;set name=&amp;quot;children&amp;quot;&amp;gt; &amp;lt;cache usage=&amp;quot;read-write&amp;quot;/&amp;gt; &amp;lt;key column=&amp;quot;parent_id&amp;quot;/&amp;gt; &amp;lt;one-to-many class=&amp;quot;Person&amp;quot;/&amp;gt;  下面是person数据缓存的更新版本:
*-----------------------------------------------------* | Person Data Cache | |-----------------------------------------------------| | 1 -&amp;gt; [ &amp;quot;John&amp;quot; , &amp;quot;Q&amp;quot; , &amp;quot;Public&amp;quot; , null , [ 2 , 3 ] ] | | 2 -&amp;gt; [ &amp;quot;Joey&amp;quot; , &amp;quot;D&amp;quot; , &amp;quot;Public&amp;quot; , 1 , [] ] | | 3 -&amp;gt; [ &amp;quot;Sara&amp;quot; , &amp;quot;N&amp;quot; , &amp;quot;Public&amp;quot; , 1 , [] ] | *-----------------------------------------------------*  需要再次指出的是仅仅缓存相关部分的ID。
当我们从不使用缓存数据库加载ID为‘1’，将执行下列查询:
select * from Person where id=1 ; 加载id为1的person select * from Person where parent_id=1 ; 加载id为1的person的孩子(将返回2, 3) select * from Person where parent_id=2 ; 加载id为2的person的孩子(将返回none) select * from Person where parent_id=3 ; 加载id为3的person的孩子(将返回none)  当使用缓存（假设已完成全部加载）时，直接加载将不再对数据库进行查询，因为它能通过标识从缓存中查找。但假如我们没有对关联进行缓存，需要对数据库进行如下查询:
select * from Person where parent_id=1 ; 加载id为1的person的孩子(将返回2, 3) select * from Person where parent_id=2 ; 加载id为2的person的孩子(将返回none) select * from Person where parent_id=3 ; 加载id为3的person的孩子(将返回none)  SQL查询的数量同没有使用缓存近似一样。这就是尽可能缓存关联为什么很重要的原因。 如果我们使用更复杂的不是基于ID进行查询，例如姓名，Hibernate在这种情况下仍会对数据库进行一次查询。示例代码如下：
Query query = session.createQuery(&amp;quot;from Person as p wherep.firstName=?&amp;quot;); query.setString(0, &amp;quot;John&amp;quot;); List l = query.list();  这会对数据库进行如下查询(即使所有关联已被缓存).
select * from Person where firstName=&#39;John&#39;  查询结果为‘1’，然后缓存可以用来查询其他孩子信息。查询缓存可用于对上述的查询进行缓存。
查询缓存 Hibernate查询缓存默认没有使能。它使用两个叫做org.hibernate.cache.StandardQueryCache和org.hibernate.cache.UpdateTimestampsCache的缓存region。第一个将带有参数的查询作为键进行存储，第二个记录查询结果。如果缓存查询的实体部分被更新，查询缓冲逐出查询及查询结果。因此查询缓冲必须设置逐出则略。一个对ID的简单加载不会使用查询缓存，但是下面的这样查询就会使用查询缓存：
Query query = session.createQuery(&amp;quot;from Person as p where p.parent.id=? and p.firstName=?&amp;quot;); query.setInt(0, Integer.valueOf(1)); query.setString(1, &amp;lt;font color=&amp;quot;red&amp;quot;&amp;gt;&amp;quot;Joey&amp;quot;); query.setCacheable(**true**); List l = query.list();  查询缓存:
*----------------------------------------------------------------------------------------* | Query Cache | |----------------------------------------------------------------------------------------| | [&amp;quot;from Person as p where p.parent.id=? and p.firstName=?&amp;quot;, [ 1 , &amp;quot;Joey&amp;quot;] ] -&amp;gt; [ 2 ] ] | *----------------------------------------------------------------------------------------*  键为查询和参数值组合，值为查询结果标识符列表。注意查询可能修改返回对象的关联时从内部看这将变得更为复杂更不要说查询可能不是对整个对象进行查询而是对对象部分标量属性的查询。也就是说，这仅仅是从概念上对查询缓存的一种看法。 如果二级缓存使能(用于查询缓存返回的对象),id为2的Person及其他关联“脱水”对象将从二级缓存获取并转换成实体对象。
二级缓存并发策略 二级缓存有四个可选的并发策略： - 只读（Read-only）: 这种策略对于从来不修改、只需要频繁读取的数据是非常有用的，也是最简单、性能最好的缓存策略。但必须保证数据不会被修改或删除，否则就会出错。 - 非严格读写（Nonstrict read/write）:这种策略不保证不能保证缓存与数据库中数据的一致性。如果存在两个事务并发地访问缓存数据的可能，则应该为该数据配置一个很短的过期时间，以减少读脏数据的可能。因此，更适用于频繁读取数据、偶尔修改数据并容忍偶尔脏读的场合。 - 读写（Read/write）:如果数据需要被更新，Read/write缓存策略可能是比较合适的。这种策略比read-only消耗更多的资源。在非JTA环境中，每个事务必须在Session.close()或Session.disconnect()调用之前结束。Hibenate在此模式下作为轻量级XA协调器自己管理事务。所有数据库操作必须在一个事务中，无法使用自动提交模式。在flush时，Hibernate搜索会话并搜索所有更新的/插入的/删除的对象并刷新至数据库，之后锁定并更新缓存。此时其他事物无法读写缓存。如果事物回滚，仅释放锁并将这些对象逐出缓存，之后其他事物可以读写缓存。如果事物提交，仅释放锁，之后其他事物可以读写缓存。读写策略提供了“readcommitted&amp;rdquo;数据库隔离级别。对于经常被读但很少修改的数据可以采用这种策略，它可以防止读脏数据。 - 事物（Transactional）:这是一个完全事务支持的策略，可以在JTA环境中使用。它提供了RepeatableRead事务隔离级别。它可以防止脏读和不可重复读这类的并发问题。
注意事项  Hibernate内部始终透明地使用一级缓存。当查询缓存关闭时，HQL查询会对数据库执行SQL查询而不是从一级缓存加载。Hibernate需要键用来从一级缓存加载对象。应此，如果已经获得键的情况下，使用load或者get比HQL查询更适合。此外Iterator方法读写一级缓存，List方法只写不读一级缓存。 一级缓存不能禁用，但可以通过Session的clear方法和evict方法清理一级缓存，从而达到禁止写缓存的效果 Get，Load，Iterator方法读写二级缓存，List方法只写不读二级缓存。在一次会话中可以通过设置会话的CacheMode为Put来禁用读操作，设置CacheMode为Get来禁用写操作。也可以通过SessionFacotry的evict方法清理二级缓存来达到禁止写二级缓存的效果。 当会话的创建时戳(txTimestamp)比二级缓存中缓存对象的更新时戳大时，它会从二级缓存加载；否则，对数据库执行SQL查询。 查询缓存会缓存Hibernate查询结果的键。这些键之后用于Hibernate从一级缓存或二级缓存加载对象。List方法读写查询缓存，Iterator不读查询缓存。  附注 1. Hibernate二级缓存文档 http://docs.jboss.org/hibernate/orm/3.2/reference/en/html/performance.html#performance-cache
http://docs.jboss.org/hibernate/orm/3.6/reference/en-US/html/performance.html
http://docs.jboss.org/hibernate/orm/4.2/manual/en-US/html/ch20.html#performance-cache
2. Hibernate缓存介绍 http://acupof.blogspot.com/2008/01/background-hibernate-comes-with-three.htmlhttp://apmblog.compuware.com/2009/02/16/understanding-caching-in-hibernate-part-one-the-session-cache/  http://apmblog.compuware.com/2009/02/16/understanding-caching-in-hibernate-part-two-the-query-cache/  http://apmblog.compuware.com/2009/03/24/understanding-caching-in-hibernate-part-three-the-second-level-cache/http://www.javalobby.org/java/forums/t48846.htmlhttp://www.javacodegeeks.com/2012/02/hibernate-cache-levels-tutorial.htmlhttp://consultingblogs.emc.com/manjunathasubbarya/archive/2011/10/30/cache-using-hibernate.aspx http://itindex.net/detail/40549-hibernate-缓存-缓存 http://blog.csdn.net/woshichenxu/article/details/586361
3. Hibernate二级缓存并发策略 http://stackoverflow.com/questions/8662609/hibernate-l2-cache-read-write-or-transactional-cache-concurrency-strategy-on-cl  http://stackoverflow.com/questions/8186890/nonstrict-read-write-vs-read-write-in-hibernate
http://wangbt5191-hotmail-com.iteye.com/blog/1711129
4. Hibernate缓存限制 http://squirrel.pl/blog/2011/08/24/hibernate-cache-is-fundametanlly-broken/https://hibernate.atlassian.net/browse/HHH-6600
</content>
    </entry>
    
     <entry>
        <title>JavaSE 新增特性</title>
        <url>https://mryqu.github.io/post/javase_%E6%96%B0%E5%A2%9E%E7%89%B9%E6%80%A7/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>新特性</tag><tag>jdk</tag><tag>java</tag>
        </tags>
        <content type="html">  参考 Wiki：Java version history
JDK各版本很cool的特性
JDK6中httpserver实例
IBM developerWorks：JDK 7 新特性 - 总览
Try-with-resources in Java 7
Java 7 的新特性一览表
编程没有银弹：探讨 Java 8 新增特性的优缺点
IBM developerWorks：Java 8 新特性概述
Java 8 的新特性和改进总览
Java 8 正式发布，新特性全搜罗
</content>
    </entry>
    
     <entry>
        <title>用GemFire做Mybatis二级缓存</title>
        <url>https://mryqu.github.io/post/%E7%94%A8gemfire%E5%81%9Amybatis%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category><category>GemFire</category>
        </categories>
        <tags>
          <tag>gemfire</tag><tag>mybatis</tag><tag>二级缓存</tag>
        </tags>
        <content type="html">  MyBatis支持第三方二级缓存实现，目前支持Ehcache、Hazelcast和OSCache。 GemFire不在支持的范围，但是可以通过实现org.apache.ibatis.cache.Cache接口来使用。
MyBatis的Cache配置及实现  设置MyBatis的Cache全局使用开关：默认是true，如果它配成false，其余各个MapperXML文件配成支持cache也没用。  &amp;lt;settings&amp;gt; &amp;lt;setting name=&amp;quot;cacheEnabled&amp;quot; value=&amp;quot;true&amp;quot;/&amp;gt; &amp;lt;/settings&amp;gt;  各个Mapper XML文件，默认是不采用cache。在配置文件加一行就可以支持cache：  &amp;lt;cache /&amp;gt;  实现GemfireCache
package com.yqu.mybatis.caches.gemfire; import com.gemstone.gemfire.cache.AttributesFactory; import com.gemstone.gemfire.cache.CacheFactory; import com.gemstone.gemfire.cache.Region; import java.util.concurrent.locks.ReadWriteLock; import java.util.concurrent.locks.ReentrantReadWriteLock; import org.apache.ibatis.cache.Cache; import org.apache.ibatis.cache.CacheException; public final class GemfireCache implements Cache { private static Region&amp;lt;object&amp;gt; mybatis_region = null; private Region&amp;lt;object&amp;gt; region = null; private final ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); private String id; public void setId(String id) { this.id = id; } public void setRegion(Region&amp;lt;object&amp;gt; region) { this.region = region; } public Region&amp;lt;object&amp;gt; getRegion() { return region; } private static synchronized Region&amp;lt;object&amp;gt; getParentRegion() { if(mybatis_region==null) { com.gemstone.gemfire.cache.Cache cache = new CacheFactory() .set(&amp;quot;name&amp;quot;, &amp;quot;mybatis_gemfire_cache&amp;quot;) .set(&amp;quot;cache-xml-file&amp;quot;, &amp;quot;gemfire.xml&amp;quot;).create(); mybatis_region = cache.getRegion(&amp;quot;mybatis_gemfire_region&amp;quot;); } return mybatis_region; } @SuppressWarnings({ &amp;quot;deprecation&amp;quot;, &amp;quot;unchecked&amp;quot;, &amp;quot;rawtypes&amp;quot; }) public GemfireCache(String id) { if (id == null) { throw new IllegalArgumentException(&amp;quot;Cache instances require an ID&amp;quot;); } this.id = id; region = getParentRegion().getSubregion(id); if (null == region) { AttributesFactory attrFactory = new AttributesFactory(mybatis_region.getAttributes()); region = mybatis_region.createSubregion(id, attrFactory.create()); } } public void clear() { if (null != region) { try { region.clear(); } catch (Throwable t) { throw new CacheException(t); } } } public String getId() { return this.id; } public Object getObject(Object key) { Object retVal = null; if (null != region) { if (region.containsKey(key.hashCode())) { try { retVal = region.get(key.hashCode()); } catch (Throwable t) { throw new CacheException(t); } } } return retVal; } public ReadWriteLock getReadWriteLock() { return this.readWriteLock; } public int getSize() { if (null != region) { try { return region.size(); } catch (Throwable t) { throw new CacheException(t); } } return 0; } public void putObject(Object key, Object value) { if (null != region) { try { region.put(key.hashCode(), value); } catch (Throwable t) { throw new CacheException(t); } } } public Object removeObject(Object key) { if (null != region) { try { return region.remove(key.hashCode()); } catch (Throwable t) { throw new CacheException(t); } } return null; } @Override public boolean equals(Object obj) { if (this == obj) { return true; } if (obj == null) { return false; } if (!(obj instanceof Cache)) { return false; } Cache otherCache = (Cache)obj; return this.id.equals(otherCache.getId()); } @Override public int hashCode() { return this.id.hashCode(); } @Override public String toString() { return &amp;quot;GemfireCache{&amp;quot; &#43; this.id &#43; &amp;quot;}&amp;quot;; } }  Mapper XML文件配置支持cache后，文件中所有的Mapperstatement就支持了。此时要个别对待某条可以通过useCache禁止使用cache：
&amp;lt;select id=&amp;quot;inetAton&amp;quot; parameterType=&amp;quot;string&amp;quot; resultType=&amp;quot;integer&amp;quot; useCache=&amp;quot;false&amp;quot;&amp;gt; select inet_aton(#{name}) &amp;lt;/select&amp;gt;   参考 iBatis OSCache example tutorial
http://www.webdbtips.com/60455/
MyBatis的Cache实际意义不大
EhcacheCache.java
</content>
    </entry>
    
     <entry>
        <title>Java缓存规范JCache API(JSR107)</title>
        <url>https://mryqu.github.io/post/java%E7%BC%93%E5%AD%98%E8%A7%84%E8%8C%83jcache_apijsr107/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category>
        </categories>
        <tags>
          <tag>java缓存规范</tag><tag>jsr107</tag><tag>jsr347</tag>
        </tags>
        <content type="html"> 今天看了一下Java缓存规范JCacheAPI（JSR107），它对Java对象缓存进行标准化，方便高效开发，让程序员摆脱实现缓存有效期、互斥、假脱机（spooling）和缓存一致性等负担。该规范提供了API、RI（参考实现）和TCK（技术兼容性套件）。 从设计的角度看，基本组成部分有一个CacheManager，用来持有、控制缓存集合。缓存里存放键值对条目。 整个规范包括了如下内容： - 支持原子操作的缓存读写 - 缓存事件监听器 - 统计 - 事务 - 注解
JSR107从2001年开始，中间搁置了一段时间，后来Terracotta（产品：EhCache）和Oracle（产品：Coherence）在2010年加强了对JSR-107的投入，原本有望放入JAVAEE7(JSR342)中，可惜在期限内完不成，直到2012年底才推出了草案。 我更关注数据网格（JSR347），那个是JSR107的超集，关注缓存的逐出、复制和分布化，以及事务。可惜连草案也还没影呢。 http://jcp.org/en/jsr/detail?id=107
http://jcp.org/en/jsr/detail?id=347
</content>
    </entry>
    
     <entry>
        <title>GemFire 数据逐出和持久化</title>
        <url>https://mryqu.github.io/post/gemfire_%E6%95%B0%E6%8D%AE%E9%80%90%E5%87%BA%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category><category>GemFire</category>
        </categories>
        <tags>
          <tag>gemfire</tag><tag>数据逐出</tag><tag>持久化</tag><tag>分布式缓存</tag>
        </tags>
        <content type="html">  为什么逐出数据? 如果有多于JVM内存的数据想放入Region，数据逐出是可能使用的一种解决方案。 一种备选方案是对数据进行分区。然而分区Region也可能无法将所有数据放入内存，所以有可能需要对分区Region采用数据逐出。 另一种备选方案是让数据过一定期限后从内存删除，这是基于时间而不是空间的一种方案。
数据逐出如何工作？ 当使用数据逐出时需要决定所采用的算法和动作。 算法规定了需要检查的所耗费资源的最大值，可为基于条目数量、内存消耗字节数和可用堆消耗百分比的LRU（最近最少使用）算法。 - 条目数量和绝对内存消耗量完全由GemFire逐出控制器基于Region级别进行管理。EntryLRU是最简单的算法，适用于每个条目消耗相同大小的内存。Memory LRU则适用于每个条目消耗不同大小的内存。 - Heap消耗百分比由GemFire逐出控制器基于缓存级别进行管理。最大值设置在管理器配置的缓存下配置。当管理器断定需要进行数据逐出时，它命令逐出控制器对所有逐出算法设为lru-heap-percentage的region采取数据逐出，直到管理器停止这一命令。注意的是，当其他非LRU资源甚至非GemFire缓存消耗内存时，这一算法也会导致HeapLRUregion的数据逐出。
对于一个region，数据逐出操作会逐出最近最少使用到的条目。几乎所有操作（包括读写）都认为是对条目的使用，除了以下的操作： - Region.containsKey - Region.containsValue - Region.getEntry
当使用MemoryLRU或HeapLRU算法时需要实现ObjectSizer接口。这让GemFire可以调用自己的代码来计算条目的字节大小。让条目大小估算精确很重要，但同时需要注意的是复杂ObjectSize实现会花费较长时间并导致性能下降。如果条目的所有值都是String或byte[]类型，GemFire会自动计算内存大小，无须实现ObjectSizer。
分区Region在数据逐出的不同之处 对于分区region，基于条目数量和内存消耗量的逐出行为当节点数据超过本地缓存主副本和冗余副本组合的限制后发生。对于基于堆消耗百分比的逐出行为由管理器驱动。 因为维护整个分区region或者同一节点所有桶（bucket）的LRU条目信息代价太大，GemFire是基于桶来维护LRU条目信息的。此外，对分区region的所有桶施行数据组出会导致数据分布失衡。 因此，对分区region进行的数据逐出可能会保留相对本地节点其他桶或其他分布系统节点相对更老的条目。它可能在主副本中保留条目而在第二副本中逐出条目，或者相反。 LRU逐出对每个桶单独进行: - 对基于内存和条目数的数据逐出，LRU逐出在操作新条目时有可能执行，直到Region的桶组合整体内存下降到门限下结束。对于内存逐出，分区region最大内存门限会忽略lru-memory-size设置，始终是local-max-memory。 - 对于基于堆的数据逐出，每个分区region桶被当作单独region来处理，每个逐出动作仅考虑桶内的LRU，而不是整体分区region。
动作为本地删除条目的数据逐出无法用于复制region，因为不允许对复制分区进行本地写操作，这会违反所有数据在复制分区都可见的契约。如果需要使用本地删除条目的数据逐出，可以考虑使用预加载数据策略，其行为在初始化时与复制分区相同并允许动作为本地删除条目的数据逐出。
| |数据无持久化|数据持久化 |&amp;mdash;&amp;ndash; |EvictionAction.NONE|条目将在内存中一直保存。|条目将在内存和磁盘中一直保存。 |EvictionAction.LOCAL_DESTROY|条目（键和值两部分）将从内存中释放。仅当被逐出数据可从外部数据源加载时可用。| |EvictionAction.OVERFLOW_TO_DISK|条目将被逐出到磁盘但是不会持久化 (当缓存关闭时磁盘文件将被删除)，条目的键部分始终在内存中保存。|条目(键和值两部分)一直在磁盘中保存。条目的值部分将被逐出，键部分始终在内存中保存。
磁盘存储文件名和扩展名 磁盘存储文件包括存储管理、访问控制文件和操作日志（oplog，记录了删除和其他所有操作）。下面的表描述了文件名和扩展名及示例。
文件名 文件名包括三部分:
第一部分: 使用标识 |值|用途|示例 |&amp;mdash;&amp;ndash; |OVERFLOW|仅为溢出region和队列的操作日志数据。|OVERFLOWoverflowDS1_1.crf |BACKUP|持久化、持久化&#43;溢出rgion和队列操作日志数据。|BACKUPoverflowDS1.if, BACKUPDEFAULT.if |DRLK_IF|访问控制 - 对磁盘存储上锁。|DRLK_IFoverflowDS1.lk, DRLK_IFDEFAULT.lk
第二部分: 磁盘存储名 |值|用途|示例 |&amp;mdash;&amp;ndash; |&amp;lt;磁盘存储名&amp;gt;|非默认磁盘存储。|name=&amp;ldquo;overflowDS1&amp;rdquo; DRLK_IFoverflowDS1.lk,
name=&amp;ldquo;persistDS1&amp;rdquo; BACKUPpersistDS1_1.crf |DEFAULT|默认磁盘存储名，当对region或队列指定持久化或溢出但没有命名磁盘存储时使用。|DRLK_IFDEFAULT.lk, BACKUPDEFAULT_1.crf
第三部分: 操作日志序列号 |值|用途|示例 |&amp;mdash;&amp;ndash; |序列号格式为_n|仅用于操作日志。编码从1开始。|OVERFLOWoverflowDS1_1.crf, BACKUPpersistDS1_2.crf, BACKUPpersistDS1_3.crf
文件扩展名 |文件扩展名|用途|注释 |&amp;mdash;&amp;ndash; |if|数据存储元数据|存放在存储所列的第一个目录。文件很小可以忽略不计-在文件大小控制中不考虑。 |lk|磁盘存储访问控制|存放在存储所列的第一个目录。文件很小可以忽略不计-在文件大小控制中不考虑。 |crf|Oplog: 创建、更新和“使无效”操作|在创建时其文件大小预分配为max-oplog-size的90%。 |drf|Oplog: 删除操作|在创建时其文件大小预分配为max-oplog-size的90%。 |krf|Oplog: 键和crf偏移量信息|当操作日志文件大小达到max-oplog-size后创建。用于增强启动时的性能。
数据持久化 GemFire在持久化配置后，确保放入region的所有数据将写入磁盘，以用于下次创建region时恢复数据。这让机器或进程故障后或按特定次序重启GemFire后，数据可以恢复。 GemFire缓存可以驻留多个region，任何region都可以设为持久化。与传统数据库管理系统不同，应用设计师使用GemFire可以考虑哪些数据集放入内存和哪些放入磁盘，数据在任何时候在分布系统中应该有多少可用数据副本。这样的颗粒度控制可以让应用设计师在基于内存的性能和基于磁盘的持久性之间进行权衡。 GemFire使用无共享磁盘存储模型。任意两个缓存节点在写操作时不会共享磁盘文件。这让GemFire应用可以部署在商品级硬件中并获得高的吞吐量。 所有对oplog文件的持久化写操作都是通过向oplog文件附加完成的。如果配置了同步持久化则在写操作完成前将从附加内容从JVM堆刷新到文件系统缓冲区。为了提供更好的性能，不会完全刷新到磁盘。通过使用GemFire复制，多个数据副本保留在内存中，所以完全刷新到磁盘对大多数用例而言是不需要的。 对于同步持久化的一个用例是当存储在GemFire的数据集在其他地方没有被管理（至少在一段时间内）。例如，在金融交易应用中，来自客户的订单以高于数据库可以处理的速度到达，GemFire是管理数据持久性的唯一数据存储库。数据可能会被复制到数据仓库，但多个应用需要数据集在数据中心任何时候都可用。应此，将数据同步持久化到分布系统中至少一个节点的磁盘上是有意义的。 持久化也可被配置为异步的。在此模式下，任何数据变动都会缓冲到内存，知道可被写入磁盘。这意味着当系统崩溃时数量可配置的数据可能会丢失，但是对于可以容忍数据丢失的应用可以带来更大的性能。对于异步持久化的一个用例是当GemFire被用于会话状态管理。上千用户的会话状态可能变化非常快，所以需要异步写提高速度。
持久化如何工作？ 当持久化region被创建后，它会检查持久化文件是否在配置的磁盘目录中存在以用于数据恢复。如果没有发现存在持久化文件，它会创建新的；否则，通过这些文件的数据初始化region的内容。一旦恢复过程结束，region就已经被创建并可被应用和客户端使用。任何对region的写操作都会将条目写入磁盘。 条目写入oplog，oplog包含所有对缓存的逻辑操作。每个跟新会附加到当前oplog的尾部。在某一时刻，oplog会被认为已满，新的oplog会被创建。对oplog的更新可以同步或异步完成。
使用的磁盘和目录 默认情况下，JVM当前目录会用于存放持久化数据。可以配置不同磁盘下的多个目录，这可以突破文件系统空间限制并带来性能优化。
Oplog创建 当oplog被创建，会试图在磁盘上预留max-oplog-size90%的文件大小。这仅仅是设置oplog的文件大小而不是真正写入内容。如果预留请求失败，会记录告警日志并尝试以没有预留空间的方式创建oplog文件。告警提示用户磁盘空间将满。 _max-oplog-size_是以兆字节为单位的oplog文件最大长度。如果长度小于_max-oplog-size_，操作可以写入；否则会创建新的oplog。默认为1GB。 forceRoll方法会强制创建新的oplog文件。这让引用有机会强制GemFire考虑当前oplog长度达到最大值。注意的是该方法未必会真正导致oplog文件滚动，因为这取决于系统参数roll-oplogs是否设为true。 通过设置max-oplog-size足够大以防止当前oplog在使用高峰时变满，防止滚动发生可以增加数据处理吞吐量。一旦使用高峰结束，可以调用forceRoll()来触发滚动。
持久化region恢复 每次启动JVM，都必须通过cache.xml或JavaAPI重新创建持久化region。当持久化region被创建，就会对磁盘上已有的持久化文件进行检查。如果找到文件，则用来进行数据恢复。然而在此之前会检查是否有含有此region的其他GemFire节点存在。如果有这样的节点存在，由于其他节点可能含有比磁盘上更新的数据，就会试图从其他节点获得初始数据进行恢复。原有持久化文件会被备份，并在使用其他节点初始数据恢复成功后删除备份的持久化数据，或者在使用其他节点初始数据恢复失败后用于数据恢复。 默认下条目仅含有从磁盘发序列化获得的键对象并指导如何从磁盘找到值对象。但是值对象在数据恢复阶段并不放入内存,会在应用读取条目时懒惰加载入内存。 如果期望所有的值在数据恢复期间加载到内存，需要设置系统属性gemfire.disk.recoverValues为true。这会降低region创建速度但会给应用读数据带来更好的性能。另一个优点是region创建时会分配所需的全部堆内存，因此在缓存创建时可知是否给JVM提供了足够多的堆。如果同时配置了溢出磁盘，最好在数据恢复时不加载值，除非有很高百分比的数据能装入内存。
何时数据写入磁盘? 当任何在持久化region上的写操作完成后，数据会写入磁盘。下边描述了region写操作：
|写操作|写入的数据|方法 |&amp;mdash;&amp;ndash; |条目创建|一个包含键、值和条目id的oplog记录|create, put, putAll, get due toload, region creation due to initialization frompeer |条目更新|一个包含新值和条目id的oplog记录|put(), putAll(), invalidate(), localInvalidate(),Entry.setValue() |条目删除|一个条目id的oplog记录|remove(), destroy(), localDestroy() |region关闭|关闭所有文件但是保留在磁盘|close(), Cache.close() |region删除|关闭并从磁盘删除所有文件|destroyRegion(), localDestroyRegion() |region清空|从磁盘删除所有文件并创建新的空文件|clear(), localClear() |region使无效|对每个条目付为null的新值|invalidateRegion(), localInvalidateRegion()
即使配置了同步磁盘写，GemFire仅同步写入文件系统缓冲区，而不是磁盘本身。这意味着在文件系统缓冲区的数据有可能在机器崩溃时无法写入磁盘。但这能让持久化region所在的JVM崩溃时数据得以保护。
 注意 可以配置为同步oplog写刷新到磁盘，但是通常会造成显著性能下降。如果使用告诉磁盘或固态内存，可以通过设置系统参数gemfire。syncWrites为true让同步oplog写刷新到磁盘，该参数仅在启动GemFire节点之前可改。
 异步写 使用异步写可以获得更好的性能，其代价是使用更多的内存（用于缓冲）和写操作完成后数据仍存在内存而非写入磁盘的风险。 不是想同步写那样立即附加到当前oplog文件，异步写将当前操作加入异步缓冲。当缓冲区满（基于_bytes-threshold_设置）或当时间超时（基于_time-interval_设置），或者当强制刷新（通过调用DiskStore.flush），将会将异步缓冲的内容刷新到当前oplog文件。 当操作加入异步缓冲区，有可能会对内存上的更新操作进行合并。例如，异步缓冲区已经包含了对键为X的条目的创建，此后完成了对键为X的条目的删除，则缓冲区在刷新到磁盘时无需操作。如果在异步缓冲区刷新到磁盘之前有五次对键为X的条目的更改，则缓冲区中最新的修改需要写入磁盘。 DiskStore.flush()方法强制将异步缓冲区刷新到当前oplog。这允许应用在DiskStore.flush()调用前的异步缓冲区内容已被移出JVM内存并放入文件系统缓冲区。
业务数据如何被写入磁盘 持久化region将每个条目的键和值进行序列化，并写入磁盘。 Oplog滚动（Rolling） 当操作日志满，GemFire会自动关闭日志并创建一个带有下一个序列号的新日志。这叫做oplog滚动。也可以调用APIDiskStore.forceRoll来请求oplog滚动。可在压缩磁盘存储之前进行调用，这样最新的oplog也可以被压缩。
 注意 日志压缩能改变磁盘存储文件的名称。一些已有日志被删除或者被更高序列号的文件替换，文件序列号也经常改变。GemFire始终为新日志用比已有数字更高的数字。
 处理磁盘错误 如果region发生致命磁盘异常，它将记录错误日志并关闭region。磁盘上的文件会被保留用以恢复。取决于磁盘错误，用户需要删除或归档这些文件。如果由于文件毁坏导致恢复失败，需要删除这些数据文件并重新创建region。由于磁盘空间不足导致的错误，可以保留这些文件用于解决磁盘空间问题后重新创建region的数据恢复。
溢出到磁盘 溢出region可以将数据驱逐到磁盘但是不会持久化。 但溢出region被通过cache.xml或API创建时，它会检查在配置的目录下是否存在溢出文件。如果存在且被其他进程锁定，则溢出region创建失败。如果没有锁定，则会删除，然后创建自己的溢出文件。 在region上的操作可能导致数据从内存删除并写入磁盘。写操作跟持久化region一样：附加记录到当前oplog，即可以同步也可以异步，业务数据也是通过序列化写入磁盘。 如果读一个值被逐出到硬盘的条目，值会重新读入内存，同时其他条目的值被逐出到硬盘。 老的oplog文件在内容全部废弃后被删除。
溢出分区region 分区region可以配置为溢出到磁盘。用于存储JVM中分区region数据的桶有自己的文件和统计。对分区region的max-oplog-size会除以最大桶个数来决定每个桶的实际max-oplog-size。 因为所有桶的oplog都存放在相同目录，在相同JVM中对不同桶的并发写会导致额外的磁盘头移动。当溢出使能时，需要降低分区region的桶总个数来获得更好的性能。
何时数据写入磁盘? 条目仅值部分会被逐出到磁盘，键始终保留在内存里。 当超出溢出region逐出控制器门限，数据将写入磁盘。当对region进行写操作或者资源管理检测到JVM内存过低时，发生溢出。 如果溢出由写操作导致，操作在数据从内存溢出到磁盘、region的资源下降到门限值后完成。 注意下列写操作释放资源而不是消耗资源，所以它们不会导致数据逐出： - 对条目进行更新，新的值对象大小比老的值对象小 - 删除条目 - 不会导致数据逐出的region级操作
写操作也有可能导致数据写入磁盘。一旦达到逐出控制器门限，读操作需要从磁盘加载数据到内存，其他条目的值对象需要被逐出以释放空间。 注意：由于溢出非持久化region不会进行数据恢复，可以安全配置异步写操作，缺点就是需要使用更多额外的内存。 对于溢出持久化Regina，所有值在写操作时已经立即写入磁盘，因此需要被逐出的值对象仍可以在内存被间接引用。
装载值对象 当读值对象被逐出到磁盘的条目时，需要读取包含当前值对象的oplog文件将其加载回内存。该oplog文件可能是非活跃oplog，默认最多7个非活跃oplog可以保持打开用来进行读取。系统属性gemfire.MAX_OPEN_INACTIVE_OPLOGS定义了可以同时打开的非活跃oplog文件最大个数。如果需要对已关闭的非活跃oplog进行读取，可能需要关系最近最少读的已打开的非活跃oplog。
滚动 Oplog滚动是压缩不再写入的oplog过程，基本上是垃圾回收过程。对溢出非持久化region的滚动与持久化region不同，它从非活跃oplog拷贝记录到当前的活跃oplog。如果所有非活跃oplog的值都拷贝到当前活跃oplog，即没有节省空间又浪费CPU和磁盘IO。所以默认如果少于一半的值需要从非活跃oplog复制到活跃oplog，才会考虑。可以设置系属性gemfire.OVERFLOW_ROLL_PERCENTAGE（浮点数）来设置这个百分比。例如设置其为25%，则配置为‘0.25’。这意味着当非活跃oplog含有25%的有效值对象（75%的垃圾）才会滚动到当前活跃oplog。 即使禁止滚动，溢出非持久化region在oplog不包含有效值对象时仍将删除非活跃oplog。因此如果可知加入溢出非持久化region的数据有相对短的生存时间，可以安全禁止滚动。 被驱逐到磁盘的值如仍被缓存使用就一直有效，直到条目被修改或删除。
注：以上文档是通过GemFire手册和多个社区文档整合而成，由于原文档版本不同而且没有源码对照，加上个人理解有限，难免有误，敬请谅解。
</content>
    </entry>
    
     <entry>
        <title>GemFire查询</title>
        <url>https://mryqu.github.io/post/gemfire%E6%9F%A5%E8%AF%A2/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category><category>GemFire</category>
        </categories>
        <tags>
          <tag>gemfire</tag><tag>查询</tag><tag>oql</tag><tag>对象查询语言</tag>
        </tags>
        <content type="html">  GemFire在region中存储的数据为键值对，其中值可以为任何对象，例如简单的字节数组或者复杂的嵌套对象。GemFire提供了一种查询机制可以获得满足特定条件的键、值或条目集合。GemFire支持的查询语义和语法是OQL（对象查询语言）的一个子集。OQL是由对象数据管理组制定的ODMG3.0对象模型的重要组件之一，与SQL很相似，可以查询复杂对象、对象属性和方法，支持完整的ASCII和Unicode字符集。 为了提高查询执行效率，GemFire像数据库一样支持索引。在查询执行时，查询引擎使用数据存储上的索引可以减少查询处理时间。查询是GemFire很强大的功能，但它也需要大量性能优化和容量规划也确保不拖垮系统。
Region存储示例 本文的查询示例基于类Porfolio和Positon的对象。
Portfolio.java package query; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import java.io.Serializable; import java.util.HashMap; import java.util.Iterator; import java.util.Map; import com.gemstone.gemfire.DataSerializable; import com.gemstone.gemfire.DataSerializer; public class Portfolio implements Serializable, DataSerializable { private int ID; public String pkid; public Position position1; public Position position2; public String description; public HashMap positions = new HashMap(); String type; public String status; public String [] names={&amp;quot;aaa&amp;quot;,&amp;quot;bbb&amp;quot;,&amp;quot;ccc&amp;quot;,&amp;quot;ddd&amp;quot;}; public int getID() { return ID; } public String getPk() { return pkid; } public HashMap getPositions() { return positions; } public Position getP1() { return position1; } public Position getP2() { return position2; } public boolean isActive() { return status.equals(&amp;quot;active&amp;quot;); } public static String secIds[] = { &amp;quot;SUN&amp;quot;, &amp;quot;IBM&amp;quot;, &amp;quot;YHOO&amp;quot;, &amp;quot;GOOG&amp;quot;, &amp;quot;MSFT&amp;quot;, &amp;quot;AOL&amp;quot;, &amp;quot;APPL&amp;quot;, &amp;quot;ORCL&amp;quot;, &amp;quot;SAP&amp;quot;, &amp;quot;DELL&amp;quot;, &amp;quot;RHAT&amp;quot;, &amp;quot;NOVL&amp;quot;, &amp;quot;HP&amp;quot;}; public Portfolio() { } public Portfolio(int i) { ID = i; if(i % 2 == 0) { description = null; } else { description = &amp;quot;XXXX&amp;quot;; } pkid = &amp;quot;&amp;quot; &#43; i; status = i % 2 == 0 ? &amp;quot;active&amp;quot; : &amp;quot;inactive&amp;quot;; type = &amp;quot;type&amp;quot; &#43; (i % 3); position1 = new Position(secIds[Position.cnt % secIds.length], Position.cnt * 1000); if (i % 2 != 0) { position2 = new Position(secIds[Position.cnt % secIds.length], Position.cnt * 1000); } else { position2 = null; } positions.put(secIds[Position.cnt % secIds.length], new Position( secIds[Position.cnt % secIds.length], Position.cnt * 1000)); positions.put(secIds[Position.cnt % secIds.length], new Position( secIds[Position.cnt % secIds.length], Position.cnt * 1000)); } public boolean equals(Object o) { if (!(o instanceof Portfolio)) { return false; } Portfolio p2 = (Portfolio)o; return this.ID == p2.ID; } public int hashCode() { return this.ID; } public String toString() { String out = &amp;quot;Portfolio [ID=&amp;quot; &#43; ID &#43; &amp;quot; status=&amp;quot; &#43; status &#43; &amp;quot; type=&amp;quot; &#43; type &#43; &amp;quot; pkid=&amp;quot; &#43; pkid &#43; &amp;quot;\n &amp;quot;; Iterator iter = positions.entrySet().iterator(); while (iter.hasNext()) { Map.Entry entry = (Map.Entry) iter.next(); out &#43;= entry.getKey() &#43; &amp;quot;:&amp;quot; &#43; entry.getValue() &#43; &amp;quot;, &amp;quot;; } out &#43;= &amp;quot;\n P1:&amp;quot; &#43; position1 &#43; &amp;quot;, P2:&amp;quot; &#43; position2; return out &#43; &amp;quot;\n]&amp;quot;; } public String getType() { return this.type; } public void fromData(DataInput in) throws IOException, ClassNotFoundException { this.ID = in.readInt(); this.pkid = DataSerializer.readString(in); this.position1 = (Position)DataSerializer.readObject(in); this.position2 = (Position)DataSerializer.readObject(in); this.positions = (HashMap)DataSerializer.readObject(in); this.type = DataSerializer.readString(in); this.status = DataSerializer.readString(in); this.names = DataSerializer.readStringArray(in); this.description = DataSerializer.readString(in); } public void toData(DataOutput out) throws IOException { out.writeInt(this.ID); DataSerializer.writeString(this.pkid, out); DataSerializer.writeObject(this.position1, out); DataSerializer.writeObject(this.position2, out); DataSerializer.writeObject(this.positions, out); DataSerializer.writeString(this.type, out); DataSerializer.writeString(this.status, out); DataSerializer.writeStringArray(this.names, out); DataSerializer.writeString(this.description, out); } }  Position.java package query; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import java.io.Serializable; import java.util.HashSet; import java.util.Set; import com.gemstone.gemfire.DataSerializable; import com.gemstone.gemfire.DataSerializer; public class Position implements Serializable, DataSerializable { private long avg20DaysVol=0; private String bondRating; private double convRatio; private String country; private double delta; private long industry; private long issuer; private double mktValue; private double qty; public String secId; private String secLinks; public String secType; private double sharesOutstanding; public String underlyer; private long volatility; private int pid; public static int cnt = 0; public Position() {} public Position(String id, double out) { secId = id; sharesOutstanding = out; secType = &amp;quot;a&amp;quot;; pid = cnt&#43;&#43;; this.mktValue = (double)cnt; } public boolean equals(Object o) { if (!(o instanceof Position)) return false; return this.secId.equals(((Position)o).secId); } public int hashCode() { return this.secId.hashCode(); } public static void resetCounter() { cnt = 0; } public double getMktValue() { return this.mktValue; } public String getSecId(){ return secId; } public int getId(){ return pid; } public double getSharesOutstanding(){ return sharesOutstanding; } public String toString(){ return &amp;quot;Position [secId=&amp;quot; &#43; this.secId &#43; &amp;quot; out=&amp;quot; &#43; this.sharesOutstanding &#43; &amp;quot; type=&amp;quot; &#43; this.secType &#43; &amp;quot; id=&amp;quot; &#43; this.pid &#43; &amp;quot; mktValue=&amp;quot; &#43; this.mktValue &#43; &amp;quot;]&amp;quot;; } public Set getSet(int size){ Set set = new HashSet(); for(int i=0;i set.add(&amp;quot;&amp;quot;&#43;i); } return set; } public Set getCol(){ Set set = new HashSet(); for(int i=0;i&amp;lt;2;i&#43;&#43;){ set.add(&amp;quot;&amp;quot;&#43;i); } return set; } public void fromData(DataInput in) throws IOException, ClassNotFoundException { this.avg20DaysVol = in.readLong(); this.bondRating = DataSerializer.readString(in); this.convRatio = in.readDouble(); this.country = DataSerializer.readString(in); this.delta = in.readDouble(); this.industry = in.readLong(); this.issuer = in.readLong(); this.mktValue = in.readDouble(); this.qty = in.readDouble(); this.secId = DataSerializer.readString(in); this.secLinks = DataSerializer.readString(in); this.sharesOutstanding = in.readDouble(); this.underlyer = DataSerializer.readString(in); this.volatility = in.readLong(); this.pid = in.readInt(); } public void toData(DataOutput out) throws IOException { out.writeLong(this.avg20DaysVol); DataSerializer.writeString(this.bondRating, out); out.writeDouble(this.convRatio); DataSerializer.writeString(this.country, out); out.writeDouble(this.delta); out.writeLong(this.industry); out.writeLong(this.issuer); out.writeDouble(this.mktValue); out.writeDouble(this.qty); DataSerializer.writeString(this.secId, out); DataSerializer.writeString(this.secLinks, out); out.writeDouble(this.sharesOutstanding); DataSerializer.writeString(this.underlyer, out); out.writeLong(this.volatility); out.writeInt(this.pid); } }  OQL查询示例 SELECT DISTINCT * FROM /exampleRegion WHERE status =&#39;active&#39; 此查询从/exampleRegion获取满足条件&amp;rdquo;status = active&amp;rdquo;的非重复（distinct）对象。
OQL与SQL的对比 OQL和SQL的查询语法很相似。例如，数据库portfolio表含有id和status列，同样一个Portfolio对象含有id和status属性。 一个简单的SQL SELECT语法如下：
SELECT [projection-list] FROM [From clause] WHERE [Where clause] -- SELECT id FROM portfolio WHERE status = &#39;active&#39;  OQL语法看起来很类似：
SELECT [projection-list] FROM [From clause] WHERE [Where clause] -- SELECT ID FROM /exampleRegion WHERE status = &#39;active&#39; -- exampleRegion是GemFire缓存中的region，Portfolio对象是作为其值存储。  此外相对于SQL可用于查询两维数据（列和行），OQL允许查询嵌套对象。对象数据通常是嵌套的，需要从外部数据层下钻访问内嵌数据。 假如Portfolio含有多个Positions属性。数据库模型中，它被映射为扁平化结构，以两个不同表protfolio和position来表现之间的关系。在对象模型中，它被表达为Portfolio对象包含一些列Position对象。 为了获得有效portfolios中所有position的market值：
-- SQL将查询两个表： SELECT pos.mktValue FROM portfolio p, position pos WHERE p.status = &#39;active&#39; AND p.id = pos.id -- OQL会通过从上层遍历到下层来查询一个region： SELECT pos.mktValue FROM /exampleRegion portfolio, positions.values pos WHERE portfolio.status = &#39;active&#39;  OQL仅涉及SELECT语句，SQL包含DDL和DML语句。
编写并执行查询 同数据库查询一样，OQL查询非常灵活并依赖数据结构。它需要认真学习数据、查询的不同表达方式以及索引的使用。 - 决定需要查询的信息及其来源。可以从客户端或本地缓存查询服务器数据。 - 编写查询语句。对于一个SELECT语句： - 编写FROM子句，包含对象类型和迭代器变量。 - 编写WHERE子句过滤数据。 - 编写投影列表。 - 决定是否使用索引。 - 编写执行查询的代码并处理查询结构。 - 根据测试性能和优化结构添加索引。
查询语句 一个查询语句是能传送给查询引擎并对数据集执行的完整OQL语句。为了创建查询语句，需要组合支持的关键词、表达式和操作符，以获得所需信息。 查询语句表达式遵从查询语言语法的各种规则，它可以包含： - 路径表达式 - 属性名 - 方法调用 - 操作符 - 字面值（Literals） - 查询参数 - IS_DEFINED、IS_UNDEFINED、ELEMENT、NVL和TO_DATE函数 - SELECT语句
注意的是对于查询语句上面都不是必须的。例如，SELECT语句通常被认为是查询的起始部分，但其实不是必要的。下面的表达式就是一个有效查询，如果/exampleRegion存储了超过100个值就会返回true。 /exampleRegion.size &amp;gt; 100
别名和同义词 在查询语句中，路径表达式(regions及其对象)可以使用别名定义，别名可以在查询的其他部分使用或引用。下面的查询语句中p用作/exampleRegion的别名。 SELECT DISTINCT * FROM /exampleRegion p WHERE p.status =&#39;active&#39;
查询代码示例 -- 定义查询语句。 String queryString = &amp;quot;SELECT DISTINCT * FROM /exampleRegion&amp;quot;; -- 通过Cache获得QueryService。 QueryService queryService = cache.getQueryService(); -- 创建Query对象。 Query query = queryService.newQuery(queryString); -- 在本地执行查询，返回结果集。 SelectResults results = (SelectResults)query.execute(); -- 获得ResultSet的大小。 int size = results.size(); -- 通过ResultSet迭代遍历。 Portfolio p = (Portfolio)results.iterator().next();  查询参数执行 这与SQL预处理语句类似，查询参数可以在查询执行时设置。它允许用户创建一次查询，通过运行时传递查询条件执行多次。 注意： 客户端到服务器端的查询不支持。 查询参数由美元符号$及参数在参数数组中的下标标识。下标计数从1开始，$1指第一个绑定属性，$2指第二个绑定属性，以此类推。 对象数组的第0个元素用于第一个查询参数，以此类推。如果参数个数或参数类型与查询语句不一致，则会在执行时抛出异常。如果参数个数不一致，则会抛出QueryParameterCountInvalidException异常。如果参数类型不一致，则会抛出TypeMismatchException异常。 示例：
-- 定义查询语句。 String queryString = &amp;quot;SELECT DISTINCT * FROM /exampleRegion p WHERE p.status = $1&amp;quot;; -- 通过Cache获得QueryService QueryService queryService = cache.getQueryService(); -- 创建Query对象。 Query query = queryService.newQuery(queryString); -- 设置查询参数。 Object[] params = new Object[1]; params[0] = &amp;quot;active&amp;quot;; -- 在本地执行查询，返回结果集。 SelectResults results = (SelectResults)query.execute(params); -- 获得ResultSet的大小。 int size = results.size();  此外查询引擎支持region路径作为查询参数。为了在FROM子句中使用参数，参数引用必须为一个集合。 例如:SELECT DISTINCT * FROM $1 p WHERE p.status = $1
Select 语句结果(Result Set) SELECT语句返回结果可能是UNDEFINED或者实现SelectResults接口的集合。 返回的SelectResults可以是: - 两种情况下返回的对象集合: - 当投影列表仅有一个表达式且该表达式没有显式使用字段名：表达式语法 - 当投影列表为*且FROM子句仅指定一个集合 - 包办对象的结构体集合
当结构体被返回时，结构体中每个字段的名称通过下列顺序决定： - 如果字段通过字段名：表达式方式被显式使用，字段名将被使用。 - 如果SELECT投影类表为*且FROM子句中使用了显示迭代器表达式，迭代器变量名用作字段名称。 - 如果字段与region或属性路径相关联，路径上最后一个属性名被使用。 - 如果无法通过上述规则决定名称，查询处理器会生成任意唯一名称。
下面为SELECT投影列表和FROM子句表达式使用示例：
SELECT DISTINCT * FROM/exampleRegion -- 返回portfolios集合(Region将Portfolio作为值)。 SELECT DISTINCT secId FROM /exampleRegion, positions.values TYPEPosition WHERE status = &#39;active&#39; --返回有效potifolios的positions属性对象的secld属性的集合。 SELECT DISTINCT &amp;quot;type&amp;quot;, positions FROM /exampleRegion WHEREstatus = &#39;active&#39; --返回有效portfolios的结构体集合。结构体的第二个属性为Map ( jav.utils.Map )对象，将positionsmap作为值。 SELECT DISTINCT * FROM /exampleRegion, positions.values TYPEPosition WHERE status = &#39;active&#39; --返回有效portfolios的结构体集合 SELECT DISTINCT * FROM /exampleRegion portfolio, positionspositions TYPE Position WHERE portfolio.status =&#39;active&#39; --返回有效portfolios的结构体集合。  限制结果数量 LIMIT关键字可选择性放在查询语句后面限制返回的行数。例如，下面的查询最到返回10行：SELECT * FROM /exampleRegions LIMIT 10 如果使用limit关键字，不要对查询结果集合做任何汇总性质的操作，因为这样并无意义。例如对使用了limit子句的查询结果使用add或addAll方法会抛出异常。
可查询数据 对于查询的region，所有对象必须同一类型。对非同一类的数据进行查询和索引会导致异常，但是可以使用不同子类型。为了确保数据同查询一致，可以使用region属性key-constraint和value-constraint限制region中的键和值类型。对于查询的对象，需要实现equals和hashCode方法。如果这些方法不存在，可能会放回不一致的查询结构。
数据可见性 查询引擎根据查询范围命名空间来解析名称和路径表达式。查询初始命名空间由下面几项组成： - 查询上下文的region。region名称由全路径指定，全路径由斜杠/起始并在region名之间用斜杠/分隔。 - Region查询属性。可以通过region路径访问对象的对象的公开字段和方法。 - 顶级region数据。可以通过region路径访问条目键或值数据。 - /exampleRegion.keySet 返回region中条目的键集合。 - /exampleRegion.entrySet 返回Region.Entry对象集合。 - /exampleRegion.values 返回region中条目的值集合。 - /exampleRegion 返回region中条目的值集合。
下钻 新的命名空间加入基于SELECT语句FROM子句的查询范围。在下面的查询示例中，FROM表达式鉴定为/exampleRegion内条目值集合。这些值的属性加入初始查询范围，status从新的查询范围内被解析出来。 SELECT DISTINCT * FROM /exampleRegion p WHERE p.status =&#39;active&#39; 每个FROM子句表达式必须解析为对象集合，且集合可用于查询表达式进行迭代。在上例中，条目值集合被WHERE子句迭代，将status字段与字符串‘active’比较，并将匹配的值对象放入结果集合。 在下面的查询中，FROM子句的第一个表达式指定的集合用于SELECT语句的其他部分（包括FROM子句第二个表达式）。 SELECT DISTINCT * FROM/exampleRegion, positions.values positions WHERE positions.qty &amp;gt;1000.00
属性可见性 可以访问当前查询范围内任何对象或对象属性。在查询过程中，对象属性可被映射为对象的公开字段和方法。在FROM规范中，查询范围内的任何对象都是有效的，因此在查询一开始所有本地缓存region及其属性都在查询范围内。 对于存在公开getter方法“getSecId()”的属性Position.secId 查询语句可以为如下方式：
SELECT DISTINCT * FROM /exampleRegion p where p.position1.secId= &#39;1&#39; SELECT DISTINCT * FROM /exampleRegion p where p.position1.SecId= &#39;1&#39; SELECT DISTINCT * FROM /exampleRegion p wherep.position1.getSecId() = &#39;1&#39;  查询引擎首先尝试使用公开字段值解析属性，如果公开字段不存在则尝试使用字段的getter方法。
名称范围 当两个不同名称范围（包）下的同名类用于OQL查询时需要指定对象的类名。IMPORT 语句用于建立查询中的类名。
IMPORT package.Position;****SELECT DISTINCT * FROM /exampleRegion, positions.valuespositions TYPE Position WHERE positions.mktValue &amp;gt;=25.00  指定对象类型 指定对象类型有助于查询引擎以更优的速度处理查询。除了在配置中指定对象类型 (使用key-constraint和value-constraint)，也可以在查询语句中显式指定对象类型。
SELECT DISTINCT * FROM /exampleRegion, positions.valuespositions TYPE Position WHERE positions.mktValue &amp;gt;=25.00  性能考虑 同在关系数据库上运行的查询处理器一样，查询的写法对执行性能有很大关系。此外，是否使用索引取决于每个查询是如何制定的。下面是在优化查询性能时需要考虑的一些事情： - 索引不用于包含NOT的表达式。因此在查询的WHERE语句中qty &amp;gt;= 10可以对qty建立索引来提高性能，然而NOT(qty&amp;lt; 10)不会采用索引。 - 对于AND操作符，如果使用索引的条件能放在其他查询条件前，查询会更高效。
索引 GemFire查询引擎支持索引。使用索引，查询性能能显著提高。查询在没有索引帮助下需要迭代遍历集合中的每个对象。如果存在匹配部分或全部查询规范的索引，查询将仅迭代有索引的集合上，因此能检查查询处理时间。 当创建索引时，需要注意的是： - 当被索引的数据改变时索引也需要更新，这导致维护代价。需要很多更新且不常使用的索引相比不用索引消耗更多系统资源。 - 索引消耗内存。 - 索引在溢出region仅有限支持。
索引创建 索引可以通过程序创建或者使用xml定义。
java QueryService qs = cache.getQueryService(); qs.createIndex(&amp;quot;myFuncIndex&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;status&amp;quot;, &amp;quot;/exampleRegion&amp;quot;); qs.createIndex(&amp;quot;myPrimIndex&amp;quot;, IndexType.PRIMARY_KEY, &amp;quot;id&amp;quot;, &amp;quot;/exampleRegion&amp;quot;);  xml &amp;lt;region name=&amp;quot;portfolios&amp;quot;&amp;gt; &amp;lt;region-attributes . . . &amp;gt; &amp;lt;/region-attributes&amp;gt; &amp;lt;index name=&amp;quot;myIndex&amp;quot;&amp;gt; &amp;lt;functional from-clause=&amp;quot;/exampleRegion&amp;quot; expression=&amp;quot;status&amp;quot;/&amp;gt; &amp;lt;/index&amp;gt; &amp;lt;index name=&amp;quot;myKeyIndex&amp;quot;&amp;gt; &amp;lt;primary-key field=&amp;quot;id&amp;quot;/&amp;gt; &amp;lt;/index&amp;gt; &amp;lt;entry&amp;gt;  索引类型 函数索引 排序索引允许属性与常量的比较。比较可以用任何关系操作符，且不限于属性和属性路径。比较可以是带有参数的复杂函数，只要属性、属性路径或函数是支持Comparable接口的对象。其次，索引表达式对复杂对象也是可用的。下面是有效可比较的对象示例： java.util.Date objectjava.lang.Integer Floatjava.lang.String Stringjava.lang.Boolean Boolean
主键 它让查询服务了解region中键和值的关系。 注：主键索引无法排序。没有排序，仅能使用相等测试。无法进行其他比较。为了获得主键上的排序索引，在用作主键的属性上创建函数索引。 注：查询服务不会自动获知region键和值的关系。因此，必须创建主键索引。 用于主键索引的FROM子句必须就是region路径。索引表达式是作用于条目值处理键的表达式。例如，region将Portfolios作为值，Portfolios的id字段作为键，索引的表达式为id。
索引维护(同步或异步的) 索引自动与引用的region数据保持一致。region属性IndexMaintenanceSynchronous指定当region数据改变时同步地更新索引还是通过后台线程异步地更新索引。异步索引维护对同一region主键批量执行多个更新。默认同步模式，它能对region数据提供最大的一致性。 AttributesFactory.setIndexMaintenanceSynchronous() 下面的声明式索引创建设置维护模式为异步的：
&amp;lt;region-attributes index-update-type=&amp;quot;asynchronous&amp;quot;&amp;gt; &amp;lt;/region-attributes&amp;gt;  索引内部结构 RangeIndex和CompactRangeIndex是用于维护（在可查询对象上创建的）索引的两种内部数据结构。
CompactRangeIndex CompactRangeIndex 是具有简单数据结构、范围最小化用于索引维护的一种范围索引。此索引不支持投影属性存储。 当前CompactRangeIndex仅支持在region路径上创建的索引。当下列条件成立时被选为索引实现： - 索引维护是同步模式。 - 索引表达式为路径表达式。 - FROM子句仅有一个迭代器。即对于每个region条目仅有一个值且索引直接用于region的值上(不支持键、条目)。
RangeIndex 当CompactRangeIndex无法使用时使用。下列为RangeIndex使用时的示例。
createIndex(&amp;quot;statusIndex&amp;quot;,&amp;quot;status&amp;quot;,&amp;quot;/portfolios, positions&amp;quot;); createIndex(&amp;quot;secIdIndex&amp;quot;,&amp;quot;b.secId&amp;quot;,&amp;quot;/portfolios pf, pf.positions.values b&amp;quot;); createIndex(&amp;quot;intFunctionIndex&amp;quot;,&amp;quot;intFunction(pf.getID)&amp;quot;,&amp;quot;/portfolios pf, pf.positions b&amp;quot;); createIndex(&amp;quot;kIndex&amp;quot;,&amp;quot;pf&amp;quot;,&amp;quot;/portfolios.keys pf&amp;quot;);  索引示例 -- Primary key index. The field doesn&#39;t has to be present. createIndex(&amp;quot;pkidIndex&amp;quot;, IndexType.PRIMARY_KEY, &amp;quot;p.pkid1&amp;quot;, &amp;quot;/root/exampleRegion p&amp;quot;); createIndex(&amp;quot;Index4&amp;quot;, IndexType.PRIMARY_KEY,&amp;quot;ID&amp;quot;,&amp;quot;/portfolios&amp;quot;); -- Simple index createIndex(&amp;quot;pkidIndex&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;p.pkid&amp;quot;, &amp;quot;/root/exampleRegion p&amp;quot;); -- On Set type createIndex(&amp;quot;setIndex&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;s&amp;quot;, &amp;quot;/root/exampleRegion p, p.sp s&amp;quot;); -- Positions is a map createIndex(&amp;quot;secIdIndex&amp;quot;, IndexType.FUNCTIONAL,&amp;quot;b.secId&amp;quot;,&amp;quot;/portfolios pf, pf.positions.values b&amp;quot;); -- Index expression as function. createIndex(&amp;quot;intFunctionIndex&amp;quot;, IndexType.FUNCTIONAL,&amp;quot;intFunction(pf.getID)&amp;quot;,&amp;quot;/portfolios pf, pf.positions b&amp;quot;); -- On Keys createIndex(&amp;quot;keyIndex&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;keys&amp;quot;,&amp;quot;/portfolios.keySet keys&amp;quot;); createIndex(&amp;quot;kIndex&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;pf&amp;quot;,&amp;quot;/portfolios.keys pf&amp;quot;); createIndex(&amp;quot;keyIndex&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;ks.hashCode&amp;quot;,&amp;quot;/portfolios.keys ks&amp;quot;); createIndex(&amp;quot;k1Index&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;key&amp;quot;,&amp;quot;/portfolios.entries&amp;quot;); -- On Entry createIndex(&amp;quot;entryIndex&amp;quot;, IndexType.FUNCTIONAL, &amp;quot;value.getID()&amp;quot;,&amp;quot;/portfolios.entrySet pf&amp;quot;); -- Misc. createIndex(&amp;quot;cIndex&amp;quot;, IndexType.FUNCTIONAL,&amp;quot;pf.getCW(pf.ID)&amp;quot;,&amp;quot;/portfolios pf&amp;quot;); createIndex(&amp;quot;funcReturnSecIdIndex&amp;quot;, IndexType.FUNCTIONAL,&amp;quot;pf.funcReturnSecId(element(select distinct pos from /portfolios pf, pf.positions.values as pos where pos.sharesOutstanding = 5000))&amp;quot;,&amp;quot;/portfolios pf, pf.positions b&amp;quot;); createIndex(&amp;quot;NVLIndex1&amp;quot;,IndexType.FUNCTIONAL, &amp;quot;nvl(pf.position2, pf.position1).secId&amp;quot;, &amp;quot;/portfolios pf&amp;quot;);  索引使用指南 使用索引优化查询需要仔细计划、测试和调优。定义不佳的索引不会提高查询性能，相反会降低查询性能。
一般规则  如果查询的FROM子句与索引非常匹配，一般情况下索引会提高查询性能。 查询评估引擎没有复杂的基于成本的优化器，而是基于索引数量和须评估的操作数选择一个最佳索引或多个索引。  对单个region查询使用索引  带有一个比较操作的查询可通过主键或功能键（取决于比较的属性是否为主键）提高性能。 SELECT DISTINCT * FROM/exampleRegion portfolio WHERE portfolio.pkid =&#39;123&#39; 如果pkid是/exampleRegionregion的键，在pkid上创建主键索引是最佳选择，因为主键索引不会增加维护负担。如果pkid不是键，在pkid上创建功能索引能够提高性能。 对于多个比较操作，可以在一个或多个属性上创建功能索引。尝试如下：  在期望获得最小结果集的条件创建一个索引。检查使用索引后的性能。 保留第一个索引，在第二个条件上增加索引。第二个索引可能会降低性能。如果性能降低的话，删除第二个索引、仅保留第一个索引。查询中比较的次序会影响性能。通常在OQL查询中，应该将获得最少结构的比较放在前面。 对于下面的查询，应该在name、age或两者上尝试功能索引：  SELECT DISTINCT * FROM /exampleRegion portfolio WHEREportfolio.status = &#39;active&#39; and portfolio.ID &amp;gt; 45  对于嵌套查询，深入到最低级索引以及查询，会获得较好性能。下面的查询深入了一级：  SELECT DISTINCT * FROM/exampleRegion portfolio, portfolio.positions.values positionswhere positions.secId = &#39;AOL&#39; and positions.MktValue &amp;gt;1    在同等联接（Equi-join）查询中使用索引 同等联接查询是在WHERE子句中以同等条件联接两个region进行查询。 - 在每个同等联接条件的每一侧创建功能索引。查询引擎会迭代左侧键和右侧索引进行相等匹配来快速评估查询的同等联接条件。 注意：同等联接查询需要功能索引。主键索引不会用于同等联接查询。 对下面的查询：
 SELECT DISTINCT inv.name, ord.orderID, ord.status FROM /investors inv, /orders ord WHERE inv.investorID = ord.investorID  创建两个索引： FROM子句索引的表达式/investors invinv.investorID/orders ordord.investorID - 如果在查询中有单region查询及同等联接查询，仅在能至少为查询中每一region创建一个索引的情况下为单region创建索引。任何对region的子集所加的索引都会降低性能。 对于下面的查询：
 SELECT DISTINCT * FROM /investors inv, /securities sc, inv.heldSecurities inv_hs WHERE sc.status = &amp;quot;active&amp;quot; AND inv.name = &amp;quot;xyz&amp;quot; AND inv.age &amp;gt; 75 AND inv_hs.secName = sc.secName  对同等联接条件创建如下索引： FROM子句索引的表达式/investors inv, inv.heldSecuritiesinv_hsinv_hs.secName/securities scsc.secName 之后，如果能创建更多索引，可在sc.status和inv.age或inv.name两者之一或全部上加索引。
在溢出OverflowRegion使用索引 可在溢出region查询中使用索引，限制如下： - 必须使用同步索引维护方式。 - 索引from子句必须仅指定一个迭代器，而且必须引用键或条目值，而不能是region的entrySet。 - 索引数据自身不会存储和溢出到磁盘上。
在使用多个region的同等关联查询中使用索引 识别所有同等联接条件。之后，在联接所有region的同时为同等联接条件创建尽可能多的索引。如果为了更好地过滤数据存在冗余的同等联接条件用于联接两个region，为这些联接创建冗余的索引会对性能不利。仅为每个region对的一个同等联接条件创建索引。
查询分区region 分区region的基本存储单元是桶，存在于GemFire节点并包好所有映射到一个哈希值的全部条目。对分区region进行查询，系统会分发查询到所有节点的所有桶上，然后将结果集合并再返回查询结果。
查询限制 分区region查询与非分区region查询一样，除了下面的限制。如果对分区region的查询不符合限制会产生异常UnsupportedOperationException。 - 分区region及分区region和复制region之间的联接仅通过函数服务提供支持。对分区region的联接查询不支持客户端服务器API。 - 如果分区region和其他region同在一个节点上，可以对分区region及分区region和复制region之间进行联接查询。同等联接查询仅在同一节点上的分区region可行，且同一节点上的列用于查询的WHERE子句中。对于多列分区，一个AND子句需要在WHERE子句中。 - 同等联接查询允许用于分区region及分区region和本地复制region，只要复制region存在于所有分区region节点之上。为了在分区region和其他region（无论是否分区region）执行联接查询，需要在函数服务执行上下文中使用query.execute方法。 - 查询必须仅为一个SELECT表达式(相对于任意的OQL表达式)，前接零个或多个IMPORT语句。 - 只要仅有一个分区region被引用，SELECT表达式可以任意复杂，包括嵌套SELECT表达式。 - 分区region引用仅可在第一个FROM子句迭代器。其他FROM子句迭代器不能引用任何region(例如下钻到分区region的值). - 第一个FROM子句迭代比仅包含一个对分区region的引用(引用可以为参数，例如$1)。 - 第一个FROM子句迭代器不能包含子查询，但其他FROM子句迭代器可以有子查询。 - 可在分区region查询中使用ORDER BY，但是ORDER BY指定的字段必须在投影列表中。
带有大结果集的查询 查询结果的大小取决于查询限定和整个数据集大小。分区region可以装载比其他region类型更多的数据，因此更有可能在分区region查询中获得大的结果集。如果结果集特别大，有可能导致接收结果的节点内存溢出。
查询扩展 函数 查询语言支持下面这些函数： - ELEMENT(expr) -从集合或数组中抽出单个元素。如果参数并非含有一个元素的集合或数组，则方法抛出FunctionDomainException异常。 示例：ELEMENT(SELECT DISTINCT * FROM /exampleRegion WHERE id =&#39;XYZ-1&#39;).status = &#39;active&#39; - IS_DEFINED(expr) -如果表达式结果不为UNDEFINED返回TRUE。 - IS_UNDEFINED(expr)- 如果表达式结果为UNDEFINED返回TRUE。在大多数查询中，查询结果不会包含未定义值。IS_UNDEFINED函数允许包含未定义值，因此可以识别带有未定义值的元素。示例：SELECT DISTINCT * FROM /exampleRegion p WHEREIS_UNDEFINED(p.status) - NVL(expr1, expr2) -如果expr1为空则返回expr2。表达式可以为绑定参数、路径表达式或字面值。 - TO_DATE (date_str, format_str) -返回Java Date类对象。date_str表示日期，format_str表示date_str所使用的日期格式。format_str会通过java.text.SimpleDateFormat进行解析。
字面值 GemFire支持下面的字面值类型： - boolean - 布尔值，TRUE或FALSE - integer和long -如果后缀为ASCII字母L则整型字面值为long类型，否则为init类型。 - floating point- 如果后缀为ASCII字母L则浮点型字面值为float类型，否则为double类型（可选择性使用ASCII字母D作为后置）。浮点型字面值可选择性使用指数后缀E或e，指数后缀后面可为有符号或无符号数值。 - string - 字符串字面值通过单引号分割。内嵌的单引号使用两个表示。例如，字符串&amp;rsquo;Hello&amp;rsquo;会认为是数值Hello，字符串&amp;rsquo;Hesaid, &amp;ldquo;Hello&amp;rdquo;&amp;lsquo;会认为是Hesaid, &amp;lsquo;Hello&amp;rsquo;。嵌入的新行会被认为是字符字面值的一部分。 - char -前缀为CHAR的字符串字面值为char类型，否则为string类型。单引号的字符字面值为CHAR &amp;ldquo;&amp;rdquo;(四个单引号)。 - date -前缀为DATE的使用JDBC格式的java.sql.Date对象：DATEyyyy-mm-dd。yyyy代表年，mm代表月，dd代表日。年必须用四位数字表示，而不允许用两位数字。 - time -前缀为TIME的使用JDBC格式（基于24小时制）的java.sql.Time对象：TIMEhh:mm:ss。hh代表小时，mm代表分钟，ss代表秒。 - timestamp - 前缀为TIMESTAMP的使用JDBC格式的java.sql.Timestamp对象：TIMESTAMPyyyy-mm-dd hh:mm:ss.fffffffff。yyyy-mm-dd代表日期，hh:mm:ss代表时间，fffffffff代表秒的小数部分(最大九位数字)。 - NIL &amp;ndash; 与NULL等效。 - NULL &amp;ndash;与Java中的null相同。 - UNDEFINED&amp;ndash;可用于任意数据类型的特殊字面值。当访问一个空值属性的属性时期结果为UNDEFINED。注意：当访问一个值为null的属性时，其结果不是未定义的。例如查询访问属性address.city且address为null，则结果为未定义的。如果查询访问address，结果不是未定义的，而是NULL。
使用java.util.Date比较数值:可以使用java.util.Date值比较临时字面值DATE、TIME和TIMESTAMP。查询语言没有用于java.util.Date的字面值。
在查询语句中使用注释 可在查询语句中包含注释。单行注释可以用两个双破折号起始。多行注释以/起始，以/结尾。
SELECT DISTINCT * FROM/exampleRegion WHERE status = &#39;active&#39; -- here is anothercomment SELECT DISTINCT * FROM /* here is a comment */ /exampleRegion WHERE status=&#39;active&#39;  方法调用 要在查询中使用方法，需要将属性名映射到想要调用的公开方法上。
SELECT DISTINCT * FROM/exampleRegion p WHERE p.positions.size &amp;gt;= 2 --映射为positions.size()  当通过查询处理器调用返回值为void的方法，其返回值为null。
调用无参方法 如果属性名映射为无参公开方法，则在查询语句中可将方法名当作属性使用。例如emps.isEmpty等同于emps.isEmpty()，查询会在positions上调用isEmpty方法并返回所有没有positions的portfolios集合。
SELECT DISTINCT * FROM/exampleRegion p WHERE p.positions.isEmpty  调用有参方法 SELECT DISTINCT * FROM/exampleRegion p WHERE p.name.startsWith(&#39;Bo&#39;)  对于重载方法，查询处理器通过运行时参数类型与方法所需参数类型进行匹配决定所调用的方法。如果仅有一个方法签名匹配所提供的参数，它会被调用。查询处理使用运行时类型匹配方法签名。 如果多于一个方法可被调用，查询处理器选择与给定参数最匹配的方法。例如，一个重载方法有多个相同参数个数的版本，一个用Person类型参数，另一个用Person子类Employee类型参数，且Employee是最匹配的对象类型。如果传递给方法的参数同两种类型都相符，查询处理采用带有Employee参数类型的方法。 参数处理器使用方法名和参数运行时类型决定适于调用的方法。因为使用运行时类型，值为null的参数没有任何类型信息，因此可以与任何类型的参数匹配。当使用了值为null的参数且查询处理器无法判断适用的方法，将抛出异常AmbiguousNameException。
操作符 比较操作符 比较两个值并返回结果，TRUE或FALSE。支持的比较操作符为：
= 等于 &amp;lt;&amp;gt; 不等于 != 不等于 &amp;lt; 小于 &amp;lt;= 小于等于 &amp;gt; 大于 &amp;gt;= 大于等于  等于和不等于操作符同其他比较操作符相比具有最低优先级。它们可用于同null比较。如果相同UNDEFINED比较，使用IS_DEFINED或IS_UNDEFINED操作符而不是这些比较操作符。
逻辑操作符 支持的逻辑操作符为AND和OR，当表达式同事使用两种操作符时，AND优先级高于OR。
一元操作符 一元操作符对单个值或表达式进行操作，优先级低于表达式中的比较操作符。GemFire支持一元操作符NOT，其操作数须为boolean。
Map和下标操作符 Map和下标操作符可以访问键值对（例如map和region）及有序集合（例如数组、列表、字符串）中的元素。操作符表现为集合名跟中括号“[]”。映射和索引在中括号中指定。 数组、列表和字符串单元可以通过下标访问，下标从零起始。myList[index]代表myList列表里第（index&#43;1）个元素。Map和region值可以通过键用相同语法访问。对于region，map操作符仅在本地缓存获取而不会使用netSearch。myRegion[keyExpression]等同于myRegion.getEntry(keyExpression).getValue。
点和斜杠操作符 点操作符‘.’分割路径表达式上的属性名，与其等效的写法是右箭头&amp;rdquo;-&amp;gt; &amp;ldquo;。斜杠操作符用于访问子region时分割region名称。
LIKE断言 GemFire有限支持like断言，表示为：
WHERE x LIKE &#39;&#39;  LIKE可用于表达“等于”：
SELECT DISTINCT * FROM/exampleRegion p where p.status LIKE &#39;active&#39;  如果字符串加通配符&amp;rsquo;%&amp;lsquo;或&amp;rsquo;*&amp;lsquo;，表现为“起始”。
SELECT DISTINCT * FROM/exampleRegion p where p.status LIKE &#39;activ%&#39;  通配符仅能用于句尾。如果在其他位置发现通配符将抛出异常。可以将转义的通配符放在字符串的任何位置。like断言会使用存在的索引。
IN表达式 IN表达式是boolean类型表达式，指示一个表达式是否包含于兼容类型表达式集合中。 如果e1和e2是表达式，e2是一个集合且e1是与e2同类型或其子类的对象或字面值，则e1 INe2是一个boolean类型的表达式。其返回值为： - TRUE 若e1不是UNDEFINED且包含于集合e2 - FALSE 若e1不是UNDEFINED且不包含于集合e2 - UNDEFINED 若e1为UNDEFINED 例如2 IN SET(1, 2, 3)为TRUE。另一个例子是所查询的集合由子查询定义。
SELECT DISTINCT * FROM/exampleRegion p WHERE p.ID IN (SELECT p2.ID FROM /exampleRegion2p2 WHERE p2.status = &#39;active&#39;)  内部SELECT语句返回 /exampleRegion2中所有status为active的条目id集合。外部SELECT迭代/exampleRegion，将每个条目的id同集合比较。对于每个条目，如果IN表达式返回TRUE，相关联的名称和地值加入外部SELECT集合。
类型转换 GemFire查询处理器在某些情况下为了执行包含不同类型的表达式会采取隐含类型转换和提升。查询处理器执行双目数值提升,方法调用转换和临时类型转换。
双目数值提升（Binary numericpromotion） 查询处理器遇到比较操作符（&amp;lt;、 &amp;lt;=、&amp;gt;、&amp;gt;=、=和&amp;lt;&amp;gt;）时对操作数执行双目数值提升，按照下面的规则次序将操作数提升为操作数中宽度最大类型。 - 如果有操作数为double类型，则其他操作数转换为double类型 - 如果有操作数为float类型，则其他操作数转换为float类型 - 如果有操作数为long类型，则其他操作数转换为long类型 - 操作数转换为int char类型
方法调用转换 查询语言中方法调用转换遵从Java方法调用转换，除了查询语言使用运行时类型而不是编译时类型，并且处理null参数时与Java不同。因为使用运行时类型，值为null的参数没有任何类型信息，因此可以与任何类型的参数匹配。当使用了值为null的参数且查询处理器无法判断适用的方法，将抛出异常AmbiguousNameException。
临时类型转换 查询语言支持的临时类型包括java.util.Date、java.sql.Date、java.sql.Time和java.sql.Timestamp，这些类型被当作相同类型处理，能够互相按照纳秒数进行比较并用于索引。 查询语言预留字 预留字 下列单词为查询语言预留不能用作标识符。标注星号的单词目前没有被GemFire使用，但预留用于未来实现。
abs\* all\* and andthen\* any\* array as asc\* avg\* bag\* boolean by\* byte char collection count\* date declare\* define\* desc\*dictionary distinct double element enum\* except\* exists\* false first\* flatten\* float for\* from\* group\* having\* import in int intersect\* interval\*is_defined is_undefined last\* like limit list\* listtoset\* long map max\* min\* mod\* nil not null nvl octet or order\* orelse\* query\* select set short some\* string struct\* sum\* time timestamp to_date true type undefine\* undefined union\* unique\* where  访问任何用查询预留字相同的方法、属性或命名对象，使用双括号将名称括起来。例如：SELECT DISTINCT &amp;quot;type&amp;quot; FROM /root/portfolios WHERE status =&#39;active&#39;****SELECT DISTINCT * FROM /region1 WHERE emps.&amp;quot;select&amp;quot;() &amp;lt;100000
语言注记  查询语言关键字例如SELECT、NULL和DATE是大小写不敏感的。标识符例如属性名、方法名和路径表达式是大小写敏感的。 注释行以 &amp;ndash; (双破折号)起始。 注释块以/起始，以/结尾。 字符串字面值用单引号分隔，内嵌单引号使用两个表示。例如：  &#39;Hello&#39; value = Hello   &#39;He said, &#39;&#39;Hello&#39;&#39;&#39; value = He said, &#39;Hello&#39;  字符字面值使用CHAR关键字加上使用单引号括起来的字符表示。单引号字符本省表示为&amp;rdquo;&amp;rdquo; (使用了四个单引号)。 TIMESTAMP字符值小数部分最大长度为9位数字。  </content>
    </entry>
    
     <entry>
        <title>GemFire Region分类</title>
        <url>https://mryqu.github.io/post/gemfire_region%E5%88%86%E7%B1%BB/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Cache</category><category>GemFire</category>
        </categories>
        <tags>
          <tag>gemfire</tag><tag>分布式缓存</tag><tag>region</tag>
        </tags>
        <content type="html"> GemFire开发指南6.5的第4.2节仅列举了分区、复制（分布式）、分布式（非复制）和本地四种Region类型，但RegionShortcut类却定义了23个快捷预定义属性Region。Region的主要行为取决于数据策略、关注策略、范围、本地最大内存和冗余拷贝数（仅用于分区Region）、逐出算法和动作。
Region快捷预定义属性数据策略范围本地
最大
内存
注1冗余
拷贝
数
注1逐出算法逐出动作LOCALNORMALLOCAL    LOCAL_HEAP_LRUNORMALLOCAL  LRU_HEAPLOCAL_DESTROYLOCAL_OVERFLOWNORMALLOCAL  LRU_HEAPOVERFLOW_TO_DISKLOCAL_PERSISTENTPERSISTENT_REPLICATELOCAL    LOCAL_PERSISTENT_OVERFLOWPERSISTENT_REPLICATELOCAL  LRU_HEAPOVERFLOW_TO_DISKPARTITIONPARTITION     PARTITION_HEAP_LRUPARTITION   LRU_HEAPLOCAL_DESTROYPARTITION_OVERFLOWPARTITION   LRU_HEAPOVERFLOW_TO_DISKPARTITION_PERSISTENTPERSISTENT_PARTITION     PARTITION_PERSISTENT_OVERFLOWPERSISTENT_PARTITION   LRU_HEAPOVERFLOW_TO_DISKPARTITION_PROXYPARTITION 0   PARTITION_PROXY_REDUNDANTPARTITION 01  PARTITION_REDUNDANTPARTITION  1  PARTITION_REDUNDANT_HEAP_LRUPARTITION  1LRU_HEAPLOCAL_DESTROYPARTITION_REDUNDANT_OVERFLOWPARTITION  1LRU_HEAPOVERFLOW_TO_DISKPARTITION_REDUNDANT_PERSISTENTPERSISTENT_PARTITION  1  PARTITION_REDUNDANT_
PERSISTENT_OVERFLOWPERSISTENT_PARTITION  1LRU_HEAPOVERFLOW_TO_DISKREPLICATEREPLICATEDISTRIBUTED_ACK    REPLICATE_HEAP_LRUPRELOADDISTRIBUTED_ACK  LRU_HEAPLOCAL_DESTROYREPLICATE_OVERFLOWREPLICATEDISTRIBUTED_ACK  LRU_HEAPOVERFLOW_TO_DISKREPLICATE_PERSISTENTPERSISTENT_REPLICATEDISTRIBUTED_ACK    REPLICATE_PERSISTENT_OVERFLOWPERSISTENT_REPLICATEDISTRIBUTED_ACK  LRU_HEAPOVERFLOW_TO_DISKREPLICATE_PROXYEMPTYDISTRIBUTED_ACK     注1：仅用于分区region http://www.gemstone.com/docs/current/product/docs/japi/com/gemstone/gemfire/cache/RegionShortcut.html |数据策略|行为 |&amp;mdash;&amp;ndash; |EMPTY|在本地缓存中没有数据存储。Region始终表现为空。没有内存成本的、零数据存储占用的生产者本地缓存收发其他缓存节点的时间，零数据存储占用的消费者仅接受事件。为了使空Region接受事件，需设置关注策略为ALL。 |NORMAL(默认)|本地使用的数据存储在本地缓存。此策略允许缓存内容不用于其他缓存节点的内容。 |REPLICATE|Region使用其他节点缓存的数据初始化。初始化后，分布式Region的所有事件会自动复制到本地缓存，在本地缓存保持整个分布式Region的复制。不允许会导致内容与其他缓存节点不一致的操作。此策略与本地范围共用时，行为与正常Region相同。 |PARTITION|通过使用自动数据分布，数据在本地和远程缓存中分区。 |PERSISTENT_REPLICATE|行为与复制Region一样同时数据在硬盘中持久化。 |PERSISTENT_PARTITION|行为与分区Region一样同时数据在硬盘中持久化。 |PRELOADED|初始化时行为像复制Region，之后行为像正常Region。
http://www.gemstone.com/docs/current/product/docs/japi/com/gemstone/gemfire/cache/DataPolicy.html
 |关注策略|行为 |&amp;mdash;&amp;ndash; |ALL|注册对分布式或分区Region中所有条目的事件进行关注，无论这些条目是否存在于本地缓存。 |CACHE_CONTENT(默认)|仅对存在于本地缓存的条目事件进行关注。对于分区Region，本地节点必须保存条目数据的主备份。
http://www.gemstone.com/docs/current/product/docs/japi/com/gemstone/gemfire/cache/InterestPolicy.html
 |范围|行为 |&amp;mdash;&amp;ndash; |GLOBAL|条目更新过程中自动使用GemFire锁服务。这确报即使两个节点通过更新同一条目也保持一致性。一个更新会在所有节点先执行，然后第二个跟新才执行。对于下面的分布式确认或分布式无确认，两个节点同时更新同一条目时，该条目在两个节点的值可能会不一致。 |DISTRIBUTED_ACK|条目更新过程中不会上锁，但执行更新的节点会在获得其他节点响应之后结束操作，因此可以避免简单通信问题（例如网络传输层临时故障期间分布失效）。 |DISTRIBUTED_NO_ACK(默认)|与确认式分布类似，但节点执行更新时不会等待其他节点响应。两个节点除了可能更新后数据不一致，而且无法保证更新被分布成功。 |LOCAL|非分布式。Region尽对在该GemFire节点内运行的线程可见。
http://www.gemstone.com/docs/current/product/docs/japi/com/gemstone/gemfire/cache/Scope.html
 |逐出算法|行为 |&amp;mdash;&amp;ndash; |NONE (默认)|无逐出。 |LRU_ENTRY|通过Region中条目数量驱动逐出行动的算法。 |LRU_HEAP|通过当前所消耗的Java虚拟机堆空间百分比驱动逐出行动的算法。 |LRU_MEMORY|通过Region所消耗内存字节数驱动逐出行动的算法。
http://www.gemstone.com/docs/current/product/docs/japi/com/gemstone/gemfire/cache/EvictionAlgorithm.html
 |逐出动作|行为 |&amp;mdash;&amp;ndash; |NONE|无逐出。 |LOCAL_DESTROY (默认)|对最近最少使用到的条目执行本地销毁。 |OVERFLOW_TO_DISK|将最近最少使用到的条目写入硬盘并将内存中条目的值置为null以释放堆空间。该动作仅在Region配置为可访问磁盘数据时可用。
http://www.gemstone.com/docs/current/product/docs/japi/com/gemstone/gemfire/cache/EvictionAction.html
</content>
    </entry>
    
     <entry>
        <title>为Unix终端或Windows命令行设置UTF-8编码</title>
        <url>https://mryqu.github.io/post/%E4%B8%BAunix%E7%BB%88%E7%AB%AF%E6%88%96windows%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AE%BE%E7%BD%AEutf-8%E7%BC%96%E7%A0%81/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>locale</tag><tag>chcp</tag><tag>mode</tag><tag>console</tag><tag>encoding</tag>
        </tags>
        <content type="html">  Unix终端 使用“locale –a” 命令检查支持的编码 servermt&amp;gt; locale -a C POSIX ……… en_US.ISO8859-1 en_US.ISO8859-15 en_US.US-ASCII en_US.UTF-8 ………  设置环境变量 LANG= en_US.UTF-8  Windows命令行 通过chcp命令设置 Displays or sets the active code page number: CHCP [nnn] C:\&amp;gt;chcp Active code page: 437 C:\&amp;gt;chcp 65001 Active code page: 65001  通过mode con命令设置 Select code page: MODE CON[:] CP SELECT=yyy Code page status: MODE CON[:] CP [/STATUS] C:\&amp;gt;mode con cp /status Status for device CON: ---------------------- Code page: 437 C:\&amp;gt;mode con cp select=65001 Status for device CON: ---------------------- Lines: 300 Columns: 160 Keyboard rate: 31 Keyboard delay: 1 Code page: 65001  代码页代号 |Identifier|.NET Name|Additional information |----- |037|IBM037|IBM EBCDIC US-Canada |437|IBM437|OEM United States |500|IBM500|IBM EBCDIC International |708|ASMO-708|Arabic (ASMO 708) |709||Arabic (ASMO-449&#43;, BCON V4) |710||Arabic - Transparent Arabic |720|DOS-720|Arabic (Transparent ASMO); Arabic (DOS) |737|ibm737|OEM Greek (formerly 437G); Greek (DOS) |775|ibm775|OEM Baltic; Baltic (DOS) |850|ibm850|OEM Multilingual Latin 1; Western European (DOS) |852|ibm852|OEM Latin 2; Central European (DOS) |855|IBM855|OEM Cyrillic (primarily Russian) |857|ibm857|OEM Turkish; Turkish (DOS) |858|IBM00858|OEM Multilingual Latin 1 &#43; Euro symbol |860|IBM860|OEM Portuguese; Portuguese (DOS) |861|ibm861|OEM Icelandic; Icelandic (DOS) |862|DOS-862|OEM Hebrew; Hebrew (DOS) |863|IBM863|OEM French Canadian; French Canadian (DOS) |864|IBM864|OEM Arabic; Arabic (864) |865|IBM865|OEM Nordic; Nordic (DOS) |866|cp866|OEM Russian; Cyrillic (DOS) |869|ibm869|OEM Modern Greek; Greek, Modern (DOS) |870|IBM870|IBM EBCDIC Multilingual/ROECE (Latin 2); IBM EBCDIC Multilingual Latin 2 |874|windows-874|ANSI/OEM Thai (same as 28605, ISO 8859-15); Thai (Windows) |875|cp875|IBM EBCDIC Greek Modern |932|shift_jis|ANSI/OEM Japanese; Japanese (Shift-JIS) |936|gb2312|ANSI/OEM Simplified Chinese (PRC, Singapore); Chinese Simplified (GB2312) |949|ks_c_5601-1987|ANSI/OEM Korean (Unified Hangul Code) |950|big5|ANSI/OEM Traditional Chinese (Taiwan; Hong Kong SAR, PRC); Chinese Traditional (Big5) |1026|IBM1026|IBM EBCDIC Turkish (Latin 5) |1047|IBM01047|IBM EBCDIC Latin 1/Open System |1140|IBM01140|IBM EBCDIC US-Canada (037 &#43; Euro symbol); IBM EBCDIC (US-Canada-Euro) |1141|IBM01141|IBM EBCDIC Germany (20273 &#43; Euro symbol); IBM EBCDIC (Germany-Euro) |1142|IBM01142|IBM EBCDIC Denmark-Norway (20277 &#43; Euro symbol); IBM EBCDIC (Denmark-Norway-Euro) |1143|IBM01143|IBM EBCDIC Finland-Sweden (20278 &#43; Euro symbol); IBM EBCDIC (Finland-Sweden-Euro) |1144|IBM01144|IBM EBCDIC Italy (20280 &#43; Euro symbol); IBM EBCDIC (Italy-Euro) |1145|IBM01145|IBM EBCDIC Latin America-Spain (20284 &#43; Euro symbol); IBM EBCDIC (Spain-Euro) |1146|IBM01146|IBM EBCDIC United Kingdom (20285 &#43; Euro symbol); IBM EBCDIC (UK-Euro) |1147|IBM01147|IBM EBCDIC France (20297 &#43; Euro symbol); IBM EBCDIC (France-Euro) |1148|IBM01148|IBM EBCDIC International (500 &#43; Euro symbol); IBM EBCDIC (International-Euro) |1149|IBM01149|IBM EBCDIC Icelandic (20871 &#43; Euro symbol); IBM EBCDIC (Icelandic-Euro) |1200|utf-16|Unicode UTF-16, little endian byte order (BMP of ISO 10646); available only to managed applications |1201|unicodeFFFE|Unicode UTF-16, big endian byte order; available only to managed applications |1250|windows-1250|ANSI Central European; Central European (Windows) |1251|windows-1251|ANSI Cyrillic; Cyrillic (Windows) |1252|windows-1252|ANSI Latin 1; Western European (Windows) |1253|windows-1253|ANSI Greek; Greek (Windows) |1254|windows-1254|ANSI Turkish; Turkish (Windows) |1255|windows-1255|ANSI Hebrew; Hebrew (Windows) |1256|windows-1256|ANSI Arabic; Arabic (Windows) |1257|windows-1257|ANSI Baltic; Baltic (Windows) |1258|windows-1258|ANSI/OEM Vietnamese; Vietnamese (Windows) |1361|Johab|Korean (Johab) |10000|macintosh|MAC Roman; Western European (Mac) |10001|x-mac-japanese|Japanese (Mac) |10002|x-mac-chinesetrad|MAC Traditional Chinese (Big5); Chinese Traditional (Mac) |10003|x-mac-korean|Korean (Mac) |10004|x-mac-arabic|Arabic (Mac) |10005|x-mac-hebrew|Hebrew (Mac) |10006|x-mac-greek|Greek (Mac) |10007|x-mac-cyrillic|Cyrillic (Mac) |10008|x-mac-chinesesimp|MAC Simplified Chinese (GB 2312); Chinese Simplified (Mac) |10010|x-mac-romanian|Romanian (Mac) |10017|x-mac-ukrainian|Ukrainian (Mac) |10021|x-mac-thai|Thai (Mac) |10029|x-mac-ce|MAC Latin 2; Central European (Mac) |10079|x-mac-icelandic|Icelandic (Mac) |10081|x-mac-turkish|Turkish (Mac) |10082|x-mac-croatian|Croatian (Mac) |12000|utf-32|Unicode UTF-32, little endian byte order; available only to managed applications |12001|utf-32BE|Unicode UTF-32, big endian byte order; available only to managed applications |20000|x-Chinese_CNS|CNS Taiwan; Chinese Traditional (CNS) |20001|x-cp20001|TCA Taiwan |20002|x_Chinese-Eten|Eten Taiwan; Chinese Traditional (Eten) |20003|x-cp20003|IBM5550 Taiwan |20004|x-cp20004|TeleText Taiwan |20005|x-cp20005|Wang Taiwan |20105|x-IA5|IA5 (IRV International Alphabet No. 5, 7-bit); Western European (IA5) |20106|x-IA5-German|IA5 German (7-bit) |20107|x-IA5-Swedish|IA5 Swedish (7-bit) |20108|x-IA5-Norwegian|IA5 Norwegian (7-bit) |20127|us-ascii|US-ASCII (7-bit) |20261|x-cp20261|T.61 |20269|x-cp20269|ISO 6937 Non-Spacing Accent |20273|IBM273|IBM EBCDIC Germany |20277|IBM277|IBM EBCDIC Denmark-Norway |20278|IBM278|IBM EBCDIC Finland-Sweden |20280|IBM280|IBM EBCDIC Italy |20284|IBM284|IBM EBCDIC Latin America-Spain |20285|IBM285|IBM EBCDIC United Kingdom |20290|IBM290|IBM EBCDIC Japanese Katakana Extended |20297|IBM297|IBM EBCDIC France |20420|IBM420|IBM EBCDIC Arabic |20423|IBM423|IBM EBCDIC Greek |20424|IBM424|IBM EBCDIC Hebrew |20833|x-EBCDIC-KoreanExtended|IBM EBCDIC Korean Extended |20838|IBM-Thai|IBM EBCDIC Thai |20866|koi8-r|Russian (KOI8-R); Cyrillic (KOI8-R) |20871|IBM871|IBM EBCDIC Icelandic |20880|IBM880|IBM EBCDIC Cyrillic Russian |20905|IBM905|IBM EBCDIC Turkish |20924|IBM00924|IBM EBCDIC Latin 1/Open System (1047 &#43; Euro symbol) |20932|EUC-JP|Japanese (JIS 0208-1990 and 0121-1990) |20936|x-cp20936|Simplified Chinese (GB2312); Chinese Simplified (GB2312-80) |20949|x-cp20949|Korean Wansung |21025|cp1025|IBM EBCDIC Cyrillic Serbian-Bulgarian |21027||(deprecated) |21866|koi8-u|Ukrainian (KOI8-U); Cyrillic (KOI8-U) |28591|iso-8859-1|ISO 8859-1 Latin 1; Western European (ISO) |28592|iso-8859-2|ISO 8859-2 Central European; Central European (ISO) |28593|iso-8859-3|ISO 8859-3 Latin 3 |28594|iso-8859-4|ISO 8859-4 Baltic |28595|iso-8859-5|ISO 8859-5 Cyrillic |28596|iso-8859-6|ISO 8859-6 Arabic |28597|iso-8859-7|ISO 8859-7 Greek |28598|iso-8859-8|ISO 8859-8 Hebrew; Hebrew (ISO-Visual) |28599|iso-8859-9|ISO 8859-9 Turkish |28603|iso-8859-13|ISO 8859-13 Estonian |28605|iso-8859-15|ISO 8859-15 Latin 9 |29001|x-Europa|Europa 3 |38598|iso-8859-8-i|ISO 8859-8 Hebrew; Hebrew (ISO-Logical) |50220|iso-2022-jp|ISO 2022 Japanese with no halfwidth Katakana; Japanese (JIS) |50221|csISO2022JP|ISO 2022 Japanese with halfwidth Katakana; Japanese (JIS-Allow 1 byte Kana) |50222|iso-2022-jp|ISO 2022 Japanese JIS X 0201-1989; Japanese (JIS-Allow 1 byte Kana - SO/SI) |50225|iso-2022-kr|ISO 2022 Korean |50227|x-cp50227|ISO 2022 Simplified Chinese; Chinese Simplified (ISO 2022) |50229||ISO 2022 Traditional Chinese |50930||EBCDIC Japanese (Katakana) Extended |50931||EBCDIC US-Canada and Japanese |50933||EBCDIC Korean Extended and Korean |50935||EBCDIC Simplified Chinese Extended and Simplified Chinese |50936||EBCDIC Simplified Chinese |50937||EBCDIC US-Canada and Traditional Chinese |50939||EBCDIC Japanese (Latin) Extended and Japanese |51932|euc-jp|EUC Japanese |51936|EUC-CN|EUC Simplified Chinese; Chinese Simplified (EUC) |51949|euc-kr|EUC Korean |51950||EUC Traditional Chinese |52936|hz-gb-2312|HZ-GB2312 Simplified Chinese; Chinese Simplified (HZ) |54936|GB18030|Windows XP and later: GB18030 Simplified Chinese (4 byte); Chinese Simplified (GB18030) |57002|x-iscii-de|ISCII Devanagari |57003|x-iscii-be|ISCII Bengali |57004|x-iscii-ta|ISCII Tamil |57005|x-iscii-te|ISCII Telugu |57006|x-iscii-as|ISCII Assamese |57007|x-iscii-or|ISCII Oriya |57008|x-iscii-ka|ISCII Kannada |57009|x-iscii-ma|ISCII Malayalam |57010|x-iscii-gu|ISCII Gujarati |57011|x-iscii-pa|ISCII Punjabi |65000|utf-7|Unicode (UTF-7) |65001|utf-8|Unicode (UTF-8)  </content>
    </entry>
    
     <entry>
        <title>Windows服务的创建、查询及删除操作</title>
        <url>https://mryqu.github.io/post/windows%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%88%9B%E5%BB%BA%E6%9F%A5%E8%AF%A2%E5%8F%8A%E5%88%A0%E9%99%A4%E6%93%8D%E4%BD%9C/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>windows service</tag><tag>sc</tag>
        </tags>
        <content type="html">  SC是与Windows服务管理器和服务通信的命令行程序。
查询SC帮助 创建Windows服务 示例：sc create akxService binPath= C:\Test\akxService.exe
查询Windows服务 删除Windows服务 示例：sc delete akxService 如果服务名含有空格，可在服务名上加双引号。
</content>
    </entry>
    
     <entry>
        <title>技术博文链接</title>
        <url>https://mryqu.github.io/post/%E6%8A%80%E6%9C%AF%E5%8D%9A%E6%96%87%E9%93%BE%E6%8E%A5/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          
        </tags>
        <content type="html">  Webkit Webkit内核探究【1】——Webkit简介
Webkit内核探究【2】——Webkit CSS实现
WebKit内核源代码分析（一）
WebKit内核源代码分析（二）
WebKit内核源代码分析（三）
WebKit内核源代码分析（四）
WebKit内核源代码分析（五）
 安全存储密码 如何安全的存储密码 - hash、salt 以及更多
MD5&#43;Salt加密机制
 ActiveMQ ActiveMQ in Action (1) 关于Session.DUPS_OK_ACKNOWLEDGE的注解： 当使用DUPS_OK_ACKNOWLEDGE会话应答模式，会话延迟应答消息的传递情况。 当JMS出现问题，这可能导致一些消息的重复传递，所以仅用于消费者能容忍重复消息的情况下。 其优点是通过减少会话防止重复消息的工作来减少会话的负载。 ActiveMQ in Action (2) ActiveMQ in Action (3) ActiveMQ in Action (4) ActiveMQ in Action (5) ActiveMQ in Action (6) ActiveMQ in Action (7) 飞鸟Blog:优化ActiveMQ性能
 Tomcat集群 负载均衡技术 Apache &#43; Tomcat集群配置详解 （1） Apache &#43; Tomcat集群配置详解 （2） IP组播与组播协议 Tomcat集群Cluster实现原理剖析 利用JMX监控Tomcat集群
 负载均衡 http://en.wikipedia.org/wiki/Load_balancing_(computing) http://zh.wikipedia.org/wiki/负载均衡_(计算机)
 Web Application Server架构 Tomcat 系统原理分析 Tomcat 设计模式分析 Jetty 的工作原理以及与 Tomcat 的比较
 Spring Security Spring Security3源码分析
 并发 聊聊并发（一）——深入分析Volatile的实现原理 聊聊并发（二）——Java SE1.6中的Synchronized 聊聊并发（三）——JAVA线程池的分析和使用 聊聊并发（四）——深入分析ConcurrentHashMap 聊聊并发（五）——原子操作的实现原理 聊聊并发（六）——ConcurrentLinkedQueue的实现原理分析 并发编程网
 未分组 Javamex: Java tutorials and performance information
  </content>
    </entry>
    
     <entry>
        <title>尝试了一下jacob</title>
        <url>https://mryqu.github.io/post/%E5%B0%9D%E8%AF%95%E4%BA%86%E4%B8%80%E4%B8%8Bjacob/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>jacob</tag><tag>java</tag><tag>com</tag><tag>组件</tag>
        </tags>
        <content type="html"> JACOB开源项目 Jacob是Java与COM组件桥接的缩写，即JAVA-COMBridge。通过使用Jacob类库，我们可以很方便地在Java程序中使用JNI来进行本地调用COM库。它可运行在x86和（支持32位和64位Java虚拟机的）X64环境。Jacob最初是由美国人DanAlder在Inventure公司担任CTO时编写的，目的是为了方便众多的程序员在Java2虚拟机上，调用Win32平台上COM自动化服务器中的组件。当Jacob项目以开源的方式在网络上公布以后，越来越多的人开始参与项目的研发与改进中去。
JACOB 相关博文： Jacob的简单介绍
Jacob使用入门及问题解析
</content>
    </entry>
    
     <entry>
        <title>孔多塞投票悖论</title>
        <url>https://mryqu.github.io/post/%E5%AD%94%E5%A4%9A%E5%A1%9E%E6%8A%95%E7%A5%A8%E6%82%96%E8%AE%BA/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>悖论</tag><tag>孔多塞</tag><tag>投票</tag>
        </tags>
        <content type="html">  简介 投票悖论指的是在通过“多数原则”实现个人选择到集体选择的转换过程中所遇到的障碍或非传递性，这是阿罗的不可能定理衍生出的难题。公共选择理论对投票行为的研究假设投票是那些其福利受到投票结果影响的人们进行的，投票行为的作用是将个人偏好转化为社会偏好。在多数投票原则下，可能没有稳定一致的结果。 投票悖论是公共选择理论中的概念，又称为循环的大多数，是指在集体投票时容易出现投票结果随投票次序的不同变化，大部分甚至全部备选方案在比较过程中都有机会轮流当选的循环现象。 18世纪时，法国著名思想家提出了所谓的“投票悖论”，后人称其为“孔多塞投票悖论”。其描述为：设有A、B、C三个人，他们对X、Y、Z的偏好如下： A： X&amp;gt;Y&amp;gt;Z B： Y&amp;gt;Z&amp;gt;X C： Z&amp;gt;X&amp;gt;Y
如果按照社会少数服从多数的原则，对X与Y进行民主表决，那么结果如下： A： X&amp;gt;Y B： Y&amp;gt;X C： X&amp;gt;Y 社会偏好：X&amp;gt;Y
因为A、C两个人都偏好X，只有一个人B偏好Y，所以根据少数服从多数原则，我们最后得到X&amp;gt;Y的社会偏好。 同理，我们如果再按照少数服从多数的原则，对Y与Z以及X与Z进行民主表决。通过三次选择，于是我们得到一个矛盾的社会偏好： X&amp;gt;Y；Y&amp;gt;Z；Z&amp;gt;X
从上面的论述中，孔多塞认为，大众投票并不会真正传递社会偏好，投票制度并不能真正保证社会合意的结果。因为，根据常理推断，社会偏好应该具有某种传递特征。因此，大众投票并不能保证出最优的，投票从本质上不具有传递性，甚至会有某种非理性。
同时，孔多塞悖论还给我们另一层的其实：认为设计的投票顺序和规则，可以对投票的结果产生非常重大的影响。
例如三个女孩A、B、C一块出去吃午饭，可以选择肯德基、麦当劳、必胜客其中之一。她们的喜好如下： A： 麦当劳&amp;gt;肯德基&amp;gt;必胜客 B： 肯德基&amp;gt;麦当劳&amp;gt;必胜客 C： 必胜客&amp;gt;麦当劳&amp;gt;肯德基
由于偏好难以达成一致，于是三人决定投票，并定投票规则为：先在麦当劳和必胜客之间选一个，然后再在胜者和肯德基之间选一个。
假如三人都毫无心机，那么第一轮麦当劳胜出；第二轮又将胜出。这实际上是最佳选择，也就是说麦当劳在三人中的综合评论是最高的。
但是如果B为了达到自己的目的而动用一些策略，投票的结果将完全改变。B可以在第一轮故意投票给必胜客，淘汰掉麦当劳，必胜客胜出，第二轮肯德基铁定胜出，因此B通过策略实现了自己的最爱，但是这一结果却不符合总体的最大利益。
发展 1972年诺贝尔经济学奖的获得者肯尼思·阿罗，在他的《社会选择与个人价值》（1951）中，证明了著名的阿罗不可能性定理，把这个投票悖论形式化了。在该书中，他运用数学工具把孔多塞的观念严格化和一般化了。那么，能不能设计出一个消除循环投票，做出合理决策的投票方案呢？
阿罗的结论 根本不存在一种能保证效率、尊重个人偏好、并且不依赖程序 (agenda)的多数规则的投票方案。
阿罗证明 不存在同时满足如下四个基本公理的社会选择函数： 1）个人偏好的无限制性，即对一个社会可能存在的所有状态，逻辑上可能的个人偏好都不应当先验地被排除； 2）弱帕累托原则， 3）非相关目标独立性，即关于一对社会目标的社会偏好序不受其它目标偏好序变化的影响； 4）社会偏好的非独裁性。
简单地说，阿罗的不可能定理意味着，在通常情况下，当社会所有成员的偏好为已知时，不可能通过一定的方法从个人偏好次序得出社会偏好次序，不可能通过一定的程序准确地表达社会全体成员的个人偏好或者达到合意的公共决策。投票悖论表明：根本不存在一种能满足阿罗五个假设条件的社会选择原理。解决投票悖论的方法是限制投票偏好，即将多峰偏好改为单峰偏好。
解决 1998年诺贝尔经济学奖获得者阿马蒂亚·森在20世纪70年代提出对“投票悖论”的解决方法。阿马蒂亚·森所提出的解决投票悖论、绕过“阿罗不可能定理”的方法就是改变甲、乙、丙其中一个人的偏好次序，以解决投票悖论的问题。
举例 比如将A的偏好次序从（X&amp;gt;Y&amp;gt;Z）改变为（X&amp;gt;Z&amp;gt;Y），新的偏好次序排列如下： A：X&amp;gt;Z&amp;gt;Y B：Y&amp;gt;Z&amp;gt;X C：Z&amp;gt;X&amp;gt;Y
于是得到三个社会偏好次序——（X&amp;gt;Y）（Z&amp;gt;Y）（Z&amp;gt;X），这样就能避开投票悖论，当然它却改变了A的偏好次序。
阿马蒂亚·森选择模式 阿马蒂亚·森把这个发现加以延伸和拓展，得出了解决投票悖论的三种选择模式： 一、所有人都同意其中一项选择方案并非是最佳； 二、所有人都同意其中一项选择方案并非是次佳； 三、所有人都同意其中一项选择方案并非是最差。
阿马蒂亚·森表示在上述三种选择模式下，投票悖论不会再出现，取而代之的结果是得大多数票者获胜的规则总是能达到唯一的决定。但是有一个问题是为了追求一致性，改变、忽略、牺牲了个人偏好次序。
</content>
    </entry>
    
     <entry>
        <title>t分布的由来</title>
        <url>https://mryqu.github.io/post/t%E5%88%86%E5%B8%83%E7%9A%84%E7%94%B1%E6%9D%A5/</url>
        <categories>
          <category>DataScience</category>
        </categories>
        <tags>
          <tag>杂谈</tag><tag>t分布</tag><tag>学生分布</tag>
        </tags>
        <content type="html"> 一直对student分布的名字莫名其妙，搜了一下，原来t分布是由统计学家哥威廉·戈塞在都柏林的A.吉尼斯父子酿酒厂对小样本中平均数比例对其标准误差的分布所做的研究，由于吉尼斯酿酒厂的规定禁止戈塞发表关于酿酒过程变化性的研究成果，因此戈塞不得不于1908年，首次以“学生”(Student)为笔名，发表自己的研究成果。因此t分布又称为学生分布。 http://baike.baidu.com/view/1419652.htm
http://baike.baidu.com/view/1332600.htm
</content>
    </entry>
    
     <entry>
        <title>学会了用excel制作甘特图</title>
        <url>https://mryqu.github.io/post/%E5%AD%A6%E4%BC%9A%E4%BA%86%E7%94%A8excel%E5%88%B6%E4%BD%9C%E7%94%98%E7%89%B9%E5%9B%BE/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>excel</tag><tag>2010</tag><tag>甘特图</tag>
        </tags>
        <content type="html"> 参考文档 http://dreamtails.pixnet.net/blog/post/22059710-用excel畫甘特圖! http://www.excel123.cn/Article/tuxinghetubiao/201201/917.html
</content>
    </entry>
    
     <entry>
        <title>[Flex] Explicitly mapping ActionScript and Java objects</title>
        <url>https://mryqu.github.io/post/flex_explicitly_mapping_actionscript_and_java_objects/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>flex</tag><tag>action script</tag><tag>java</tag><tag>object</tag><tag>mapping</tag>
        </tags>
        <content type="html"> Flex和Java对象的映射 http://livedocs.adobe.com/blazeds/1/blazeds_devguide/help.html?content=serialize_data_3.html
</content>
    </entry>
    
     <entry>
        <title>Eclipse中解决远程调试超时的设置</title>
        <url>https://mryqu.github.io/post/eclipse%E4%B8%AD%E8%A7%A3%E5%86%B3%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95%E8%B6%85%E6%97%B6%E7%9A%84%E8%AE%BE%E7%BD%AE/</url>
        <categories>
          <category>Tool</category><category>Eclipse</category>
        </categories>
        <tags>
          <tag>eclipse</tag><tag>remote</tag><tag>debug</tag><tag>timeout</tag><tag>tomcat</tag>
        </tags>
        <content type="html">  主要是加大调试器超时和启动超时设置 Tomcat设置 catalina.bat中添加如下：
:doStart shift if &amp;quot;%TITLE%&amp;quot; == &amp;quot;&amp;quot; set TITLE=Tomcat set _EXECJAVA=start &amp;quot;%TITLE%&amp;quot; %_RUNJAVA% set CATALINA_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,address=5558,server=y,suspend=n if not &amp;quot;&amp;quot;%1&amp;quot;&amp;quot; == &amp;quot;&amp;quot;-security&amp;quot;&amp;quot; goto execCmd shift echo Using Security Manager set &amp;quot;SECURITY_POLICY_FILE=%CTALINA_BASE%\conf\catalina.policy&amp;quot; goto execCmd  Eclipse设置 </content>
    </entry>
    
     <entry>
        <title>[Flex] 显式使用public namespace</title>
        <url>https://mryqu.github.io/post/flex_%E6%98%BE%E5%BC%8F%E4%BD%BF%E7%94%A8public_namespace/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>flex</tag><tag>namespace</tag>
        </tags>
        <content type="html"> 今天看代码时，发现public namespace的使用，搜了一下flex4 in action，貌似没有。 放狗搜了一下，学习学习。 myspace.as
package { public namespace myspace =&amp;quot;http://myspace&amp;quot;; }  TestClass.as
package { import myspace; public class TestClass { public function foo():void { trace(&amp;quot;Public foo is called&amp;quot;); } myspace function foo():void { trace(&amp;quot;MySpace foo is called&amp;quot;); } private function fooPrivate():void { trace(&amp;quot;Called private function&amp;quot;); } protected function fooProtected():void { trace(&amp;quot;Called protected function&amp;quot;); } public function callFoo(t:TestClass):void { // call the private/protected members on the object. t.fooPrivate(); t.fooProtected(); } } }  testApp.mxml
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt; &amp;lt;mx:Application xmlns:mx=&amp;quot;http://www.adobe.com/2006/mxml&amp;quot; layout=&amp;quot;absolute&amp;quot; creationComplete=&amp;quot;testFunc()&amp;quot;&amp;gt; &amp;lt;mx:Script&amp;gt; &amp;lt;![CDATA[ use namespace myspace; private function testFunc():void { var test:TestClass = new TestClass; //public can be also used as namespace name to call the correct function. test.public::foo(); // call the myspace namespace function test.myspace::foo(); var test2:TestClass = new TestClass; // call the function to demonstrate that private/protected functions // can be called on test2 object. test.callFoo(test2); } ]]&amp;gt; &amp;lt;/mx:Script&amp;gt; &amp;lt;/mx:Application&amp;gt;  原文 Using public namespace explicitly
</content>
    </entry>
    
     <entry>
        <title>表驱动法</title>
        <url>https://mryqu.github.io/post/%E8%A1%A8%E9%A9%B1%E5%8A%A8%E6%B3%95/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>表驱动法</tag><tag>代码大全</tag>
        </tags>
        <content type="html"> 表提供了一种复杂逻辑和继承结构的替换方案。如果发现自己对某个应用程序的逻辑或者继承树关系感到困惑，可以问问自己它是否可以通过一个查询表来加以简化。 使用表的一项关键决策是决定如何访问表。可以通过直接访问、索引访问或者阶梯访问。 使用表的另一项关键决策是决定应该把什么内容放入表中。
</content>
    </entry>
    
     <entry>
        <title>Tomcat JNDI 数据源配置</title>
        <url>https://mryqu.github.io/post/tomcat_jndi_%E6%95%B0%E6%8D%AE%E6%BA%90%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>Service&#43;JavaEE</category><category>Web Application Server</category>
        </categories>
        <tags>
          <tag>tomcat</tag><tag>datasource</tag><tag>jndi</tag><tag>global naming resource</tag><tag>context</tag>
        </tags>
        <content type="html"> 学写一下Tomcat:JNDI Datasource HOW-TO，JNDI数据源可以配置到两个位置。Tomcat详细介绍了第二种方式，我目前的项目使用的是第一种方式。第一种方式的好处是，多个Web应用程序可以共享Tomcat全局命名空间里的资源。 - Tomcat全局命名空间 $CATALINA_HOME$/conf/server.xml $CATALINA_HOME$/conf/localhost/javatest.xml - Web应用程序环境(Context)</content>
    </entry>
    
     <entry>
        <title>数据库常用操作笔记</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E7%AC%94%E8%AE%B0/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>数据库</tag><tag>greenplum</tag><tag>postgresql</tag><tag>mysql</tag><tag>db2</tag>
        </tags>
        <content type="html">  GreenPlum/PostGreSQL操作  Create a new database: createdb medb Drop database: dropdb medb Access database: psql -h gpserver -d gpdatabase -U gpuser -W Get help: medb=# \h Quit: medb=# \q Read command from file: medb=# \i input.sql To dump a database: pg_dump medb &amp;gt; db.out To reload the database: psql -d database -f db.out Dump all database: pg_dumpall &amp;gt; /var/lib/pgsql/backups/dumpall.sql Restore database: psql -f /var/lib/pgsql/backups/dumpall.sql medb Show databases: psql -l medb=# \l; Show users: medb=# select * from &amp;quot;pg_user&amp;quot;; Show tables: medb=# select * from &amp;quot;pg_tables&amp;quot;; Set password: medb=# upadte pg_shadow set passwd = &#39;new_password&#39; where usename = &#39;username&#39;; Clean all databases (Should be done via a daily cron): vacuumdb --quiet --all check column of table medb=# select * from INFORMATION_SCHEMA.COLUMNS where table_name = &#39;table_name&#39;; medb=# \d table_name; check function medb=# select * from pg_catalog.pg_proc p where p.proname=&#39;function_name&#39;; medb=# \df; check privilege medb=# select * from INFORMATION_SCHEMA.role_table_grants; schema relate command  medb=#\dn; medb=# select np.nspname as &amp;quot;schema&amp;quot;, usr.usename as &amp;quot;owner&amp;quot; from pg_namespace as np, pg_user as usr where np.nspowner=usr.usesysid; medb=# select distinct schemaname from pg_tables where tableowner=&#39;userName&#39;; medb=# ALTER SCHEMA name RENAME TO newname medb=# ALTER SCHEMA name OWNER TO newowner  search path relate command  medb=# show search_path; medb=# set search_path=&#39;new_search_path&#39;;  check postgresql server status pg_ctl status -D server_data_folder start postgresql server pg_ctl start -D server_data_folder Top 10 Biggest Tables at database  medb=# SELECT table_name, pg_relation_size(table_name) as size FROM information_schema.tables WHERE table_schema NOT IN (&#39;information_schema&#39;, &#39;pg_catalog&#39;) ORDER BY size DESC LIMIT 10;   连接GreenPlum/PostGreSQL的JDBC代码 import java.sql.Connection; import java.sql.DriverManager; import java.util.Properties; public class TestConnection { public static void connectWithProps() { Connection connection; Properties props; try { Class.forName(&amp;quot;org.postgresql.Driver&amp;quot;); props = new Properties(); props.put(&amp;quot;user&amp;quot;, &amp;quot;aUser&amp;quot;); props.put(&amp;quot;password&amp;quot;, &amp;quot;aPassword&amp;quot;); connection = DriverManager.getConnection(&amp;quot;jdbc:postgresql://aGreenplumServer:5432/aDatabase&amp;quot;, props); System.out.println(&amp;quot;Connection successful&amp;quot;); connection.close(); } catch (Exception e) { System.out.println(&amp;quot;error &amp;quot; &#43; e); } } public static void connectWithUserInfo() { Connection connection; try { Class.forName(&amp;quot;org.postgresql.Driver&amp;quot;); connection = DriverManager.getConnection(&amp;quot;jdbc:postgresql://aGreenplumServer:5432/aDatabase&amp;quot;, &amp;quot;aUser&amp;quot;, &amp;quot;aPassword&amp;quot;); System.out.println(&amp;quot;Connection successful&amp;quot;); connection.close(); } catch (Exception e) { System.out.println(&amp;quot;error &amp;quot; &#43; e); } } public static void main(String argv[]) { connectWithUserInfo(); connectWithProps(); } }  MySQL操作  To get the current database character set  mysql&amp;gt; SHOW VARIABLES LIKE &#39;character_set_database&#39;; &#43;------------------------&#43;-------&#43; | Variable_name | Value | &#43;------------------------&#43;-------&#43; | character_set_database | utf8 | &#43;------------------------&#43;-------&#43; 1 row in set (0.00 sec)  To know MySQL Server default Character Set  mysql&amp;gt; SHOW VARIABLES LIKE &#39;character_set_server&#39;; &#43;----------------------&#43;-------&#43; | Variable_name | Value | &#43;----------------------&#43;-------&#43; | character_set_server | utf8 | &#43;----------------------&#43;-------&#43; 1 row in set (0.00 sec)  my.ini default-character-set=utf8 To know storage engine mysql&amp;gt; show table stauts from the_database; my.ini default-storage-engine=INNODB  Teradata操作  Use BTEQWIN to logon Teradata server  Teradata BTEQ 12.00.00.00 for WIN32. Copyright 1984-2007, NCR Corporation. ALL RIGHTS RESERVED. Enter your logon or BTEQ command: logon meserver/metest .logon meserver/metest Password: ****** ** Logon successfully completed. ** Teradata Database Release is 13.00.00.14 ** Teradata Database Version is 13.00.00.15 ** Transaction Semantics are BTET. ** Character Set Name is &#39;UTF8&#39;. *** Total elapsed time was 2 seconds. BTEQ -- Enter your DBC/SQL request or BTEQ command:  To run a sql script bteq &amp;lt; script.sql &amp;gt; script.out To get help info about BTEQ help bteq; To connect to a database inside bteq .logon TDPID/User ID To exit the bteq session quit; To run file (batch mode) in Teradata run file myfile.sql To run operation system command in BTEQ os dir os rm some_file   To switch/access a database database database_name; 7. To display current session help session; 8. To list tables in a database help database database_name; 9. To show a table information help table table_name; 10. To show a table details show table table_name; 11. To explain a sql command explain sql_command 12. To list Teradata version show version 13. Some useful teradata SQL commands  select database;` select user; select date; select time; 
DB2操作  To start a db2 server, use command db2start To stop a db2 server, use command db2stop To manage DB2 Administrative Server, use command db2admin Tools DB2BATCH- Reads SQL statements from either a flat file or standard input, dynamically prepares and describes the statements and returns an answer set: Authorization: sysadmin .and Required Connection -None..eg db2batch -d databasename -f filename -a userid/passwd -r outfile DB2expln - DB2 SQL Explain Tool DB2exfmt - Explain Table Format Tool DB2icrt - Create an instance DB2idrop - Dropan instance DB2ilist - List instances DB2imigr - Migrate instances DB2iupdt - Update instances Db2licm - Installs licenses file for product ; db2licm -a db2entr.lic DB2look - DB2 Statistics Extraction Tool  参考  PostgreSQL表空间、数据库、模式、表、用户、角色之间的关系 postgres命令 MySQL privileges explored  </content>
    </entry>
    
     <entry>
        <title>数据库catalog的定义</title>
        <url>https://mryqu.github.io/post/%E6%95%B0%E6%8D%AE%E5%BA%93catalog%E7%9A%84%E5%AE%9A%E4%B9%89/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>数据库</tag><tag>catalog</tag>
        </tags>
        <content type="html"> The database catalog of a database instance consists ofmetadata in which definitions of database objects such as basetables, views (virtual tables), synonyms, value ranges, indexes,users, and user groups are stored.
The SQL standard specifies a uniform means to access thecatalog, called the INFORMATION_SCHEMA, but not all databasesfollow this, even if they implement other aspects of the SQLstandard. For an example of database-specific metadata accessmethods, see Oracle metadata.
</content>
    </entry>
    
     <entry>
        <title>Flex启动次序</title>
        <url>https://mryqu.github.io/post/flex%E5%90%AF%E5%8A%A8%E6%AC%A1%E5%BA%8F/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>flex</tag><tag>组件</tag><tag>容器</tag><tag>应用程序</tag><tag>启动顺序</tag>
        </tags>
        <content type="html"> 所有Flex组件在启动过程都会触发一些事件。这些事件指示何时组件首次被创建、在内部进行描绘、在屏幕上进行绘制。这些事件也指示何时组件结束创建，当组件为容器时指示何时子组件被创建。 组件被实例化、加入或链接父组件，之后在容器内确定大小并布局。组件创建顺序如下： 下例展示了组件创建生命期内分发的一些重要事件： 容器和组件的创建顺序是不同的，因为容器可为其他组件的父组件。容器内的组件也必须经历创建顺序。如果一个容器是另一个容器的父组件，内部容器的子组件也必须经历创建顺序。 下例展示了容器创建生命期内分发的一些重要事件： 当所有组件被创建并在屏幕绘制，Application对象会分发一个applicationComplete事件。这是程序启动的最后一个被分发的事件。 multiview容器(navigators)的启动顺序与标准容器不同。默认情况下，导航器(navigator)的所有顶级视图会被实例化。然而，Flex仅创建初始可见视图的子组件。当用户切到导航器的其他视图，Flex才会为此视图创建子组件。
</content>
    </entry>
    
     <entry>
        <title>FLEX组件的生命周期</title>
        <url>https://mryqu.github.io/post/flex%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>flex</tag><tag>组件</tag><tag>生命周期</tag>
        </tags>
        <content type="html"> 组件实例化生命周期描述了用组件类创建组件对象时所发生的一系列步骤,作为生命周期的一部分,flex自动调用组件的的方法,发出事件,并使组件可见。 下面例子用as创建一个btn控件,并将其加入容器中
var boxContainer:Box = new Box(); //设置Box容器 ... //创建btn var b:Button = new Button(); b.label = &amp;quot;Submit&amp;quot;; ... //将btn添加到Box容器中 boxContainer.addChild(b);  下面的步骤显示了用代码创建一个Button控件，并将这个控件添加到Box容器中时所发生的一切： 1. 调用了组件的构造函数; var b:Button = new Button(); 2. 通过设置组件的属性对组件进行设置: //Configure the button control. b.label = &amp;rdquo;Submit&amp;rdquo;; 组件的setter方法将会调用invalidateProperties()、invalidateSize()、invalidateDisplayList()方法。 3. 调用addChild()方法将该组件添加到父组件。 //Add the Button control to the Box container. boxContainer.addChild(b); 4. 将component的parent的属性设置为对父容器的引用. 5. 计算组件样式(style)设置。 6. 在组件上发布priininialize事件。 7. 调用组件的createChildren()方法。 8. 调用invalidateProperties(),invalidateSize(),invalidateDisplayList()方法以触发后续到来的,下一个&amp;rdquo;渲染事件&amp;rdquo;(render event)期间对commitProperties(),measure(),updateDisplayList()方法的调用.这个规则唯一一个例外就是当用户设置组件的height和width属性时,Flex不会调用measure()方法。 9. 在组件上分发initialize事件。此时，组件所有的子组件都被初始化，但是组件没有改更size和处理布局。可以利用这个事件在组件布局之前执行一些附加的处理。 10. 在父容器上分发childAdd事件。 11. 在父容器上分发initialize事件。 12. 在下一个&amp;rdquo;渲染事件&amp;rdquo;(render event)中,Flex执行以下动作: - 调用组件的commitProperties()方法。 - 调用组件的measure()方法。 - 调用组件的layoutChrome方法。 - 调用组件的updateDisplayList()方法。 - 在组件上发布updateComplete事件。 13. 如果commitProperties(),measure,updateDisplayList方法调用了invalidateProperties(),invalidateSize(),或invalidateDisplayList()方法,则Flex会分发另外一个render事件。 14. 在最后的render事件发生后,Flex执行以下动作: - 通过设置组件的visible属性使组件变为可视. - 在组件上分发creationComplete事件.组件的大小(size)和布局被确定. 这个事件只在组件创建时分发一次. - 在组件上分发updateComplete事件.无论什么时候,只要组件的布局(layout),位置,大小或其它可视的属性发生变化就会分发这事件,然后组件被更新，以使组件能够被正确地显示.
原文：http://blog.csdn.net/stonywang/article/details/2667551
</content>
    </entry>
    
     <entry>
        <title>解：failed to create task or type cobertura-instrument</title>
        <url>https://mryqu.github.io/post/%E8%A7%A3failed_to_create_task_or_type_cobertura-instrument/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>cobertura</tag><tag>测试覆盖率</tag><tag>ant</tag><tag>code coverage</tag>
        </tags>
        <content type="html"> 由于项目需要，学习使用cobertura检查junit测试覆盖率。 结果ant总是报错：
BUILD FAILEDbuild.xml:54: Problem: failed to create task or typecobertura-instrumentCause: The name is undefined.Action: Check the spelling.Action: Check that any custom tasks/types have been declared.Action: Check that any&amp;lt;presetdef&amp;gt;/&amp;lt;macrodef&amp;gt;declarations have taken place.  搜了一些文章，都无解，最后看cobertura的example，找到病根。 解决方法： 将改成就好了。
</content>
    </entry>
    
     <entry>
        <title>实践记录：将MDF MSSQL数据库内容导入MySQL</title>
        <url>https://mryqu.github.io/post/%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95%E5%B0%86mdf_mssql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%85%E5%AE%B9%E5%AF%BC%E5%85%A5mysql/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>数据库</tag><tag>mysql</tag><tag>导入</tag><tag>mdf</tag><tag>sqlserver</tag>
        </tags>
        <content type="html"> 玩一个例程源码，它使用的是SQL server数据库，虽然我机子装了VS2005带的SQL server2005，我还是想用MySQL。 把自己的转换过程草草记录下来，也不算太无聊吧。
【环境】VS2005附带的SQL server 2005 &amp;amp; MySQL 5.1
1、下载并安装Microsoft SQL Server Management StudioExpress（SSMSE）http://www.microsoft.com/downloadS/details.aspx?familyid=C243A5AE-4BD1-4E3D-94B8-5A0F62BF7796&amp;amp;displaylang=en
2、 配置SQL2005
==SQL server configuration manager== SQL server service -&amp;gt; 选择SQL serverbrower属性，修改模式为手动，然后启动SQL server brower
==SQL Server Surface Area Configuration -&amp;gt;Surface Area Configuration for Services and Connections== SQLEXPRESS -&amp;gt; Database Engine -&amp;gt;Remote connection -&amp;gt; Local and remote connection&amp;amp; using both tcp/ip and named pipes
==SQL server configuration manager== SQL server 2005 network configuration -&amp;gt; protocolsfor SQLEXPRESS -&amp;gt; 选择TCP/TP属性中IPAddresses，删掉动态端口，设定端口为1433
==Microsoft SQL Server Management Studio Express== 选择SQLEXPRESS属性 -&amp;gt; security -&amp;gt; SQLserver and Windows authentication modeSQLEXPRESS -&amp;gt; security -&amp;gt; logins-&amp;gt; 选择sa属性，修改密码
重启SQL server服务
==Microsoft SQL Server Management Studio Express== SQLEXPRESS -&amp;gt; Databases邮件菜单Attach命令，导入MDF数据库文件
4、Mysql migration toolkit ==源数据库连接== MS SQL Server 127.0.0.1：1433 username=sa password=？Database=？
==目的数据库连接== MySQL Server 127.0.0.1：3306 username=root password=？
</content>
    </entry>
    
     <entry>
        <title>读《剑出偏锋 JBoss的过去现在和未来》</title>
        <url>https://mryqu.github.io/post/%E8%AF%BB%E5%89%91%E5%87%BA%E5%81%8F%E9%94%8B_jboss%E7%9A%84%E8%BF%87%E5%8E%BB%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>jboss</tag>
        </tags>
        <content type="html"> 剑出偏锋 JBoss的过去现在和未来 JBoss的确很好用，全是拜Marc老兄带人稳扎稳打做出来的。学习，实践，平常心!
</content>
    </entry>
    
     <entry>
        <title>周末看JMF</title>
        <url>https://mryqu.github.io/post/%E5%91%A8%E6%9C%AB%E7%9C%8Bjmf/</url>
        <categories>
          <category>Java</category>
        </categories>
        <tags>
          <tag>jmf</tag><tag>java</tag><tag>media</tag><tag>framework</tag>
        </tags>
        <content type="html"> 继续hello之旅。
JavaTM Media FrameworkAPI Guide http://java.sun.com/javase/technologies/desktop/media/jmf/2.1.1/guide/index.html
JMF开发指南1.0 中文版 http://hi.baidu.com/bigbigant/blog/item/43021ad8f240123033fa1ce8.html/cmtid/916ed31677780f11972b430a
</content>
    </entry>
    
     <entry>
        <title>UML笔记（JUDE）</title>
        <url>https://mryqu.github.io/post/uml%E7%AC%94%E8%AE%B0jude/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          
        </tags>
        <content type="html">  UML介绍 统一建模语言（UML是 Unified ModelingLanguage的缩写）是用来对软件密集系统进行可视化建模的一种语言。UML为面向对象开发系统的产品进行说明、可视化、和编制文档的一种标准语言。 UML的主要创始人是Jim Rumbaugh、Ivar Jacobson和GradyBooch，他们最初都有自己的建模方法（OMT、OOSE和Booch），彼此之间存在着竞争。最终，他们联合起来创造了一种开放的标准。1997年，OMG组织（ObjectManagementGroup对象管理组织）发布了统一建模语言UML。UML的目标之一就是为开发团队提供标准通用的设计语言来开发和构建计算机应用。UML提出了一套IT专业人员期待多年的统一的标准建模符号。通过使用UML，这些人员能够阅读和交流系统架构和设计规划&amp;ndash;就像建筑工人多年来所使用的建筑设计图一样。 最常用的UML图包括：用例图、类图、序列图、状态图、活动图、组件图和部署图。 类图 类图中的元素 类和接口是类图中的基本元素，其UML表示是一个长方形，垂直地分为三个区：最上层显示类名或接口名，中间的区域列出类的属性，底部的区域列出类的操作。 对于抽象类/方法，类/方法名是斜体的。 对于静态属性/方法，属性/方法下面有下划线。 根据属性/方法的访问权限不同，其图示也不同。
|属性/方法访问权限|图示 |&amp;mdash;&amp;ndash; |公开|&#43; |包内访问|~ |保护|# |私有|-
在关联建模中，存在一些情况下，你需要包括其它类，因为它包含了关于关联的有价值的信息。对于这种情况，你会使用关联类来绑定你的基本关联。关联类和一般类一样表示。不同的是，主类和关联类之间用一条相交的点线连接。关联类类似查询表，可以凭借关联信息从一个类查找到另一个类。 UML中的软件包类似于Java中的包，使建模者能够组织模型分类器到不同的名字空间中，便于管理。 类图中的六大关系 |关系名|介绍|体现|图示 |&amp;mdash;&amp;ndash; |泛化关系generalization|表示一般与特殊的关系|类与类之间的继承，接口与接口之间的继承|用一条实线加空三角来表示|实现关系realization|表示类与接口的关系|类对接口的实现|用一条虚线加空三角来表示|依赖关系dependency|类与类之间的连接，表示一个类依赖于另外一个类的定义；依赖关系仅仅描述了类与类之间的一种使用与被使用的关系|局部变量、方法/函数的参数或者是对静态方法的调用|用一条虚线加箭头来表示|关联关系association|类与类之间的连结，关联关系使一个类知道另外一个类的属性和方法；通常含有“知道”，“了解”的含义依赖关系是具有偶然性的、临时性的、非常弱的，方向是单向的；关联关系是固定的、长期的对应关系，方向可以是单向或者双向的。对类而言依赖存在的理由有：B作为一个参数被传递给A内所定义的一个方法(参数可见性)；B在A的一个方法内被声明为局部对象(局部声明可见性)；B对A全局可见(全局可见性)。而关联一般应来描述普通的属性可见性(B是A的一个属性，是一种相对长久的可见性, 是普遍存在的)。|成员变量|用一条实线来表示关联关系的一段带箭头的是可访问的（Navigableassociation）；带叉号的是不可访问或禁止访问的（Non-navigableassociation）；什么都不带的是未特别指出的关系（Unspecifiedassociation），例如无法直接访问但是可以间接访问。 |聚合关系aggregation|关联关系的一种，是一种强关联关系；聚合关系是整体和个体/部分之间的关系；关联关系的两个类处于同一个层次上，而聚合关系的两个类处于不同的层次上，一个是整体，一个是个体/部分；在聚合关系中，代表个体/部分的对象有可能会被多个代表整体的对象所共享|成员变量|用一条实线加空心菱形来表示|组合关系composition|也是关联关系的一种，但它是比聚合关系更强的关系。组合关系要求聚合关系中代表整体的对象要负责代表个体/部分的对象的整个生命周期；组合关系不能共享；在组合关系中，如果代表整体的对象被销毁或破坏，那么代表个体/部分的对象也一定会被销毁或破坏；而聚在合关系中，代表个体/部分的对象则有可能被多个代表整体的对象所共享，而不一定会随着某个代表整体的对象被销毁或破坏而被销毁或破坏。|成员变量|用一条实线加实心菱形来表示继承和实现体现的是类与类、或者类与接口间的纵向关系；依赖、关联、聚合和组合关系则体现的是类与类、或者类与接口间的引用、横向关系，是比较难区分的，这几种关系都是语义级别的，所以从代码层面并不能完全区分各种关系。 但总的来说，后几种关系所表现的强弱程度依次为：组合&amp;gt;聚合&amp;gt;关联&amp;gt;依赖。
序列图 同步消息的图示为一条实线加实三角，异步消息的图示为一条实线加箭头。 交互框常见操作符 |操作符|含义 |&amp;mdash;&amp;ndash; |alt|多选一的片段；只有条件为真者会执行 |opt|可选的；该片段只在所给条件为真时执行，等同于只有一个片断的alt |par|并行；每一个片断并行运行 |loop|循环；片断可以执行多次；警戒条件表示循环的条件 |region|关键区域；片断一次只有一个线程执行 |neg|否定；片断展示无效的交互 |ref|引用；引用到另一张图中定义的交互。画一个框盖住交互设计的生命线。你可以定义参数和返回值 |sd|序列图；圈出一张完整的序列图，如果你愿意的话
参考 《UML精粹：标准对象建模语言简明指南》 JUDE文档
IBM的“UML 基础”系列文章。
</content>
    </entry>
    
     <entry>
        <title>关于JavaScript框架</title>
        <url>https://mryqu.github.io/post/%E5%85%B3%E4%BA%8Ejavascript%E6%A1%86%E6%9E%B6/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>dojo</tag><tag>mootools</tag><tag>prototype</tag><tag>extjs</tag>
        </tags>
        <content type="html">  JS库一览 http://www.slideshare.net/jeresig/javascript-library-overview
http://www.webjx.com/javascript/jsajax-8545.html
JS库评估 http://wiki.freaks-unidos.net/javascript-libraries
为什么选择DOJO？ 原文版 http://dojotoolkit.org/book/dojo-book-0-9/introduction/why-dojo
中文版 http://bigqiangbigqiang.spaces.live.com/blog/cns!64A5E0FB4DFCD63F!606.entry
http://bigqiangbigqiang.spaces.live.com/blog/cns!64A5E0FB4DFCD63F!607.entry
为什么选择mootools,抛弃了prototype http://www.javaeye.com/topic/122425
不要使用ExtJS http://pablotron.org/?cid=1556
ExtJS源自YUI，功能更强，许可更苛刻。
</content>
    </entry>
    
     <entry>
        <title>Adobe Flash、Flex、AIR和ColdFusion</title>
        <url>https://mryqu.github.io/post/adobe_flashflexair%E5%92%8Ccoldfusion/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>Flash</tag><tag>Flex</tag>
        </tags>
        <content type="html">  Adobe Flash Adobe Flash，前称ShockwaveFlash并流行地简称Flash，前身FutureSplash，既指Adobe FlashProfessional 多媒体创作程序，也指Adobe Flash Player。Adobe公司于2005年12月3日收购Macromedia公司，因此Flash成了Adobe公司的软件。
特性  被大量应用于因特网网页的矢量动画文件格式。 使用向量运算（VectorGraphics）的方式，产生出来的影片占用存储空间较小。 使用Flash创作出的影片有自己的特殊文件格式（swf） 该公司声称全世界97%的网络浏览器都内置Flash播放器（FlashPlayer） 是Adobe提出的“富因特网应用”（RIA）概念的实现平台 Flash6之后版本纳入面向对象程序概念。与其他语言比较，不论是在数据库、XML、PHP等各种平台上，都能更进一步的相互结合应用。  主要文件格式  swf这是一个完整的影片档，无法被编辑。有时会被念作“swiff”或“swaif”[1]。Swf在发布时可以选择保护功能，如果没有选择，很容易被别人输入到他的源文件中使用。然而保护功能依然阻挡不了为数众多的破解软件，有不少闪客专门以此来学习别人的代码和设计方式。 flaFlash的源文件，只能用AdobeFlash打开编辑。 as（ActionScript的缩写）是一种编程语言的简单文本文件.FLA文件能够直接包含 ActionScript, 但是也可以把它存成AS档做为外部链接文件(如定义ActionScript类则必须在写在as文件里,再通过import加入类)，以方便共同工作和更高级的程序修改。 swc，是一种供Flash使用的库格式，可以粗略地理解为Flash用的dll。无法被编辑。 FLV，FLV是FlashVideo的简称，是一种网络视频格式，FLV是 FLASH VIDEO的简称，FLV流式媒体格式是一种视频格式，它的出现有效地解决了视频文件导入Flash后，使导出的SWF文件体积庞大，不能在网络上有效使用等缺点。  版本历史 FutureSplash Animator (1996-4-10) - Flash前身，由简单的工具和时间线组成。 - Flash 1 (1996-11) - Macromedia给FutureSplashAnimator更名后为Flash的第一个版本。 - Flash 2 (1997-6) - 引入库的概念。 - Flash 3 (1998-5-31) - 影片剪辑、Javascript插件、透明度和独立播放器。 - Flash 4 (1999-6-15) - 变量、文本输入框、增强的ActionScript、流媒体MP3。 - Flash 5 (2000-8-24) - Javascript、智能剪辑、HTML文本格式。 - Flash MX (2002-3-15) - Unicode、组件、XML、流媒体视频编码。 - Flash MX 2004 (2003-9-10) - 文本抗锯齿、Actionscript2.0、增强的流媒体视频、行为。 - Flash MX 2004 Pro (2003-9-10) - 包括所有Flash MX 2004的特性，加上Web Services、 ActionScript 2.0的面向对象编程、媒体播放组件。 - Flash 8 (2005-9-13)新增了滤镜和层混合模式,增加了BitmapData类，使Flash拥有了全新的位图绘图方式。 - Flash 8 Pro (2005-9-13) - 增强为移动设备开发的功能、方便创建Flash Web、增强的网络视频。 - Flash CS3 / 9 (2007-4-16) - 使用界面和其他的Adobe Creative Suite3应用程序结合。并增强与Photoshop及Illustrator的应用功能。这一版本最重要的改动就是增加了全新的Actionscript3脚本语言，重新设计了命名空间的结构并增强了对面向对象的支持，并在其Flash Player 9中增加了针对Actionscript3而完全重新编写的虚拟机AVM 2。 - Flash CS4 /10(2008-9-23) -极大的改变了以往的动画编辑方式。新的动画补间不再由时间线的关键帧组成，而是完全基于动画对象而创建,同时还增加了动画编辑器作为新动画方式的辅助工具。FlashCS4还集成了3D变形和反向运动骨骼，增强了字体引擎，并可以直接发布Adobe Air文件。此外，FlashCS4增强了ActionScript3.0的音频类(SoundClass)，使其能够从数据动态输出音频，这为Flash的多媒体应用开发又开拓了更广的领域。
Adobe Flex Adobe Flex是最初由Macromedia公司在2004年3月发布的，基于其专有的MacromediaFlash平台，它是涵盖了支持RIA（RichInternet Applications）的开发和部署的一系列技术组合。
成因 传统的程序员在开发动画应用方面存在困难，Flash平台最初就是因此而产生。Flex试图通过提供一个程序员们已经熟知的工作流和编程模型来改善这个问题。 Flex 最初是作为一个J2EE（Java2 Platform, Enterprise Edition）应用，或者可以说是JSP（JavaServerPages）标签库而发布的。它可以把运行中的MXML（Flex标记语言）和ActionScript编译成FLASH应用程序（即二进制的SWF文件）。最新版的FLEX支持创建静态文件，该文件使用解释编译方式并且不需要购买服务器许可证就可以在线部署。 Flex的目标是让程序员更快更简单地开发RIA应用。在多层式开发模型中，Flex应用属于表现层。 Flex 采用GUI界面开发，使用基于XML的MXML语言。Flex具有多种组件，可实现Web Services，远程对象，drag anddrop，列排序，图表等功能；FLEX内建动画效果和其它简单互动界面等。相对于基于HTML的应用（如PHP、ASP、JSP、ColdFusion及CFMX等）在每个请求时都需要执行服务器端的模板，由于客户端只需要载入一次，FLEX应用程序的工作流被大大改善。FLEX的语言和文件结构也试图把应用程序的逻辑从设计中分离出来。 Flex 服务器也是客户端和XML Web Services及远程对象（Coldfusion CFCs，或Java类，等支持ActionMessage Format的其他对象）之间通讯的通路。 一般被认为可能是 Flex 替代品的是OpenLaszlo和AJAX技术。
最初版本 ( Flex Server 1.0 与 1.5 ) Flex最初版本的目标市场是企业应用开发，价格约 US$15000 每CPU。每个许可证包含五个FlexBuilder许可证。
Adobe Flex 2 Flex2改变了许可模式，开放其&amp;rdquo;Flex Framework&amp;rdquo;技术为免费版。 新版的 Flex Builder 2 基于EclipseIDE，对于需要诸如data push和自动测试等高级功能的用户来说，企业级的服务仍然有效。 Flex2引入了新版本的语言ActionScript3的使用，这需要 Flash Player 9以上版本作为运行时(runtime)。 Flex是第一个被Adobe冠上Adobe商标而重新命名的原Macromedia 产品。
Adobe Flex 3 FLEX3加入的新功能 1. 数据库精灵联机，预设支持Derby、PostgreSQL、SQL Mode-JDBC、MySQL2. 现在视觉编辑上有了移动放大、缩小等功能 3. 程序代码重整部分，有了更名变量功能，可以包含所有项目档 4.你也可以选择不同的SDK版本 5. AS的文件批注更方便作成HTML在线浏览版，好让你日后查询。 6.快速将Flex包装成AIR桌面程序 7.整合LiveCycle Data Services(前身为Flex DataServices)，除了实时大量数据推送外，也可以当点选页面后才会在Server上编译。 FLEX3文件将不再像flex2这么大了，FLEX 2 与FLEX3在加载组件容量上的区别,以及FLEX可以实现体积减小.FLEX 框架将集成到FLASHPLAYER里.如果很多网站都是用FLEX来制作的话，那么框架是一样的,那样减少程序体积就成了可能。就不需要像现在进入一个FLEX程序就要加载一遍框架,电脑在刷新程序代码的时候也吃不消啊.看起来FLEX3 在要求电脑配置上会比FLEX 2小点。
Adobe Flex 4 FLEX4已经推出了SDK下载，新版本的FLEX将于2009发布
Flex 和 ColdFusion Macromedia 把一部份 Flex 1.5 的子集嵌入到了它的ColdFusion MX 7中间件平台中以供在 Flash表单中使用。虽然可以使用这个平台来开发RIA，但是它原来的目的只是为了开发丰富的表单应用，所以这个功能并不为Macromedia所支持。
Flex 应用开发步骤 以下直接来源于Flex 2.0 Beta 3的帮助文件： - 使用一系统预定义组件(窗口，按钮等)来定义一个开发界面。 - 组织安排组件，现在用户自定义的界面设计。 - 使用风格和主题来定义可见设计。 - 增加动态动作，如应用程序之间的互动。 - 定义并在需要时连接上一个数据服务。 - 从源代码生成一个在Flash播放器中运行的SWF文件。
版本历史  Flex 1.0－2004年3月 Flex 1.5－2004年10月 Flex 2.0 (Alpha) - 2005年10月 Flex 2.0 Beta 1－2006年2月 Flex 2.0 Beta 2－2006年3月 Flex 2.0 Beta 3－2006年5月 Flex 2.0 Final - 2006年6月28日 Flex 3.0 Beta - 2008年3月  Adobe AIR Adobe AIR（AIR＝Adobe IntegratedRuntime），开发代号為Apollo[1]，是一个跨操作系統runtime environment用來建造RIA，使用Flash、Flex、HTML与AJAX，可能部署為桌面应用程序。
竞争对手  Microsoft Silverlight JavaFX (來自Sun Microsystems)。 XUL与 XULRunner Google Gears  ColdFusion ColdFusion（直译：冷聚变），是一个动态Web服务器，其CFML（ColdFusionMarkup Language）是一种程序设计语言，类似现在的JSP里的JSTL（JSPStandard Tag Lib），从1995年开始开发，其设计思想被一些人认为非常先进，被一些语言所借鉴。 Coldfusion 最早是由 Allaire 公司开发的一种应用服务器平台，其运行的 CFML（ColdFusionMarkup Language） 针对Web应用的一种脚本语言。文件以*.cfm为文件名，在ColdFusion专用的应用服务器环境下运行。在Allaire 公司被 Macromedia 公司收购以后,推出了 Macromedia ColdFusion5.0，类似于其他的应用程序语言, cfm文件被编译器翻译为对应的 c&#43;&#43; 语言程序，然后运行并向浏览器返回结果。 虽然 .cfc 和custom tag 具有类似的重用性，但 cfc 提供了更加灵活的调用方式，例如 webservice 方式的调用支持。 Macromedia己被Adobe并购，所以ColdFusion亦成為Adobe旗下产品。
</content>
    </entry>
    
     <entry>
        <title>JDK、Ant和Maven开发环境配置</title>
        <url>https://mryqu.github.io/post/jdkant%E5%92%8Cmaven%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
        <categories>
          <category>Tool</category>
        </categories>
        <tags>
          <tag>jdk</tag><tag>ant</tag><tag>maven</tag><tag>配置</tag>
        </tags>
        <content type="html">  JDK  下载JDK并安装到c:\tools下 设置Java环境变量：JAVA_HOME = c:\tools\Java\jdk1.x.0_xxCLASSPATH =.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;path变量 %JAVA_HOME%\bin 运行&amp;rdquo;java -version&amp;rdquo;进行验证  Ant  下载Ant并解压缩到c:\tools下 设置Ant环境变量：ANT_HOME = c:\tools\apache-ant-1.x.xpath变量 %ANT_HOME%\bin 运行&amp;rdquo;ant -version&amp;rdquo;进行验证  maven  下载Maven并解压缩到c:\tools下 设置Maven环境变量：M2_HOME = c:\tools\apache-maven-x.x.xpath变量 %M2_HOME%\bin 运行&amp;rdquo;mvn &amp;ndash;version&amp;rdquo;进行验证  m2eclipse  通过下列update site安装:http://download.eclipse.org/technology/m2e/releases 在Window - Preferences - Maven - Installations添加上一步安装的Maven  </content>
    </entry>
    
     <entry>
        <title>org.gjt.mm.mysql.Driver和com.mysql.jdbc.Driver的区别</title>
        <url>https://mryqu.github.io/post/org.gjt.mm.mysql.driver_%E5%92%8Ccom.mysql.jdbc.driver%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
        <categories>
          <category>db&#43;nosql</category>
        </categories>
        <tags>
          <tag>mysql</tag><tag>驱动</tag>
        </tags>
        <content type="html"> org.gjt.mm.mysql.Driver是早期的驱动名称，后来就改名为com.mysql.jdbc.Driver，现在一般都推荐使用com.mysql.jdbc.Driver。在最新版本的mysqljdbc驱动中，为了保持对老版本的兼容，仍然保留了org.gjt.mm.mysql.Driver，但是实际上org.gjt.mm.mysql.Driver中调用了com.mysql.jdbc.Driver，因此现在这两个驱动没有什么区别。
//org.gjt.mm.mysql.Driver的源代码 package org.gjt.mm.mysql; import java.sql.SQLException; public class Driver extends com.mysql.jdbc.Driver { // ~Constructors//----------------------------------------------------------- public Driver() throws SQLException {super();} }  由源代码可以看出，仅仅是为了兼容，才保留了该名字，所以建议直接使用com.mysql.jdbc.Driver
</content>
    </entry>
    
     <entry>
        <title>Meta的http-equiv属性详解</title>
        <url>https://mryqu.github.io/post/%E4%BB%8A%E5%A4%A9%E7%9C%8B%E7%9A%84%E4%B8%9C%E4%B8%9Cmeta%E7%9A%84http-equiv%E5%B1%9E%E6%80%A7%E8%AF%A6%E8%A7%A3/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>http-equiv</tag><tag>meta</tag><tag>http</tag><tag>html</tag>
        </tags>
        <content type="html"> http-equiv顾名思义，相当于http的文件头作用，它可以向浏览器传回一些有用的信息，以帮助正确和精确地显示网页内容，与之对应的属性值为content，content中的内容其实就是各个参数的变量值。 meta标签的http-equiv属性语法格式是：＜meta http-equiv=&amp;ldquo;参数&amp;rdquo; content=&amp;ldquo;参数变量值&amp;rdquo;＞；其中http-equiv属性主要有以下几种参数： 1. Expires(期限) 说明：可以用于设定网页的到期时间。一旦网页过期，必须到服务器上重新传输。 用法：＜meta http-equiv=&amp;ldquo;expires&amp;rdquo; content=&amp;ldquo;Wed, 20 Jun 2007 22:33:00 GMT&amp;rdquo;＞ 注意：必须使用GMT的时间格式。 2. Pragma(cache模式) 说明：禁止浏览器从本地计算机的缓存中访问页面内容。 用法：＜meta http-equiv=&amp;ldquo;Pragma&amp;rdquo; content=&amp;ldquo;no-cache&amp;rdquo;＞ 注意：这样设定，访问者将无法脱机浏览。 3. Refresh(刷新) 说明：自动刷新并指向新页面。 用法：＜meta http-equiv=&amp;ldquo;Refresh&amp;rdquo; content=&amp;ldquo;2；URL=http://www.net.cn/&amp;quot;＞ 注意：其中的2是指停留2秒钟后自动刷新到URL网址。 4. Set-Cookie(cookie设定) 说明：如果网页过期，那么存盘的cookie将被删除。 用法：＜meta http-equiv=&amp;ldquo;Set-Cookie&amp;rdquo; content=&amp;ldquo;cookievalue=xxx; expires=Wednesday, 20-Jun-2007 22:33:00 GMT； path=/&amp;ldquo;＞ 注意：必须使用GMT的时间格式。 5. Window-target(显示窗口的设定) 说明：强制页面在当前窗口以独立页面显示。 用法：＜meta http-equiv=&amp;ldquo;Window-target&amp;rdquo; content=&amp;rdquo;_top&amp;rdquo;＞ 注意：用来防止别人在框架里调用自己的页面。 6. content-Type(显示字符集的设定) 说明：设定页面使用的字符集。 用法：＜meta http-equiv=&amp;ldquo;content-Type&amp;rdquo; content=&amp;ldquo;text/html; charset=gb2312&amp;rdquo;＞ 7. Pics-label(网页等级评定) 用法：网页等级评定 说明：在IE的internet选项中有一项内容设置，可以防止浏览一些受限制的网站，而网站的限制级别就是通过meta属性来设置的。 8. 还有Page_Enter、Page_Exit……
补充： 设定进入页面时的特殊效果
设定离开页面时的特殊效果
Duration的值为网页动态过渡的时间，单位为秒。
Transition是过渡方式，它的值为0到23，分别对应24种过渡方式。如下表： 0 盒状收缩 1 盒状放射
2 圆形收缩 3 圆形放射
4 由下往上 5 由上往下
6 从左至右 7 从右至左
8 垂直百叶窗 9 水平百叶窗
10 水平格状百叶窗 11 垂直格状百叶窗
12 随意溶解 13 从左右两端向中间展开
14 从中间向左右两端展开 15 从上下两端向中间展开
16 从中间向上下两端展开 17 从右上角向左下角展开
18 从右下角向左上角展开 19 从左上角向右下角展开
20 从左下角向右上角展开 21 水平线状展开
22 垂直线状展开 23 随机产生一种过渡方式
</content>
    </entry>
    
     <entry>
        <title>JavaScript编辑器</title>
        <url>https://mryqu.github.io/post/javascript%E7%BC%96%E8%BE%91%E5%99%A8/</url>
        <categories>
          <category>前端</category>
        </categories>
        <tags>
          <tag>javascript</tag><tag>editor</tag>
        </tags>
        <content type="html">  Antechnus公司的javascript editor http://www.c-point.com/index.html InterAKTonline公司的JSEclipse http://www.interaktonline.com/Products/Eclipse/JSEclipse/Overview/ Teniga 据说这个最强，下次有空试试 https://sourceforge.net/projects/teniga  放弃自己搜索了，这个强帖太厉害http://blog.csdn.net/holym/archive/2007/09/29/1805887.aspx
</content>
    </entry>
    
     <entry>
        <title>[C&#43;&#43;] 类型转换</title>
        <url>https://mryqu.github.io/post/c&#43;&#43;_%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/</url>
        <categories>
          <category>C&#43;&#43;</category>
        </categories>
        <tags>
          <tag>C&#43;&#43;</tag><tag>cast</tag><tag>type conversion</tag>
        </tags>
        <content type="html">  C&#43;&#43;中的强制转换函数共有以下几种： - C 风格（C-style）强制转型: (Type) expr - 函数风格（Function-style）强制转型: Type( expr )要注意的是Type(expr)语法上等同(Type)expr，但是要避免使用。Type(expr,expr_else)是安全的。 - static_cast &amp;lt;type-id&amp;gt; ( expr )：用于非多态类型转换。static_cast是第一个应该尝试的类型转换。它完成类似隐性类型转换（例如int转float，指针转void*）这样的工作，也能调用显式（或隐式）类型转换函数。在很多情况下，显式使用static_cast没有必要。static_cast也能在继承层次上进行类型转换。在进行上行转换（子类转父类）是没有必要的，下行转换只要没有虚拟继承的情况下也可用，但是它不会做任何检查，下行转换为非该对象真正的类型时行为不明确。type-id和expr必须是指针、引用、算术类型或枚举类型。 - dynamic_cast &amp;lt;type-id&amp;gt; ( expr )：用于多态类型转换。dynamic_cast是几乎唯一用于处理多态类型转换的。你可以将一个指针或引用转换成其他类的多态类型（一个多态类型至少有一个虚函数，不管是声明的还是继承的）。它不仅经可用于下行转换，还可以横向转换或上行转换到另一个继承链。dynamic_cast会检查转换是否可行，如果可行则返回期望的对象，否则原表达式是指针的话返回空指针、原表达式是引用的话抛出std::bad_cast异常。dynamic_cast有一些限制。当继承层次上有相同类型的多个对象（DiamondDerivationproblem，菱形派生问题）而又没有使用虚拟继承时，无法工作。它仅能遍历公开继承，在遍历保护继承或私有继承时总是失败。非公开的继承很少使用，所以这种问题也很少见。Type-id必须是类的指针、类的引用或者void*；如果type-id是类指针类型，那么expr也必须是一个指针，如果type-id是一个引用，那么exp也必须是一个引用。 - const_cast &amp;lt;type-id&amp;gt; ( expr)：用来修改类型的const、volatile和__unaligned属性。const_cast可用于对一个变量添加或删除const属性，其他C&#43;&#43;类型转换（甚至reinterpret_cast）没有删除const的能力。需要注意的是原有变量是const的，如果修改之前的常量值会造成不确定的行为。如果一个const引用指向非常量，对引用去掉const是安全的。当重载的成员函数是const的时候非常有用，例如你可以对一个对象添加const以调用重载的成员函数。const_cast也能对volatile属性进行修改，只是会更少被用到。除了const 或volatile修饰之外， type_id和expr的类型是一样的。 - reinterpret_cast &amp;lt;type-id&amp;gt; ( expr )：对类型简单重新解释reinterpret_cast是最危险的类型转换，应该尽可能少地使用。它直接将一个类型转换成另外一个，例如将一个指针获得的值转换成另一种类型、将指针存储成整型值、或其他一些丑陋的转换。基本上，reinterpret_cast仅能保障转换回原类型是正常的，你能在中间类型不小于原有类型的情况下获得相同的值。有很多reinterpret_cast不能做的转换。主要用于转义转换和二进制处理，例如将原始数据流转成实际数据、或将数据存储在对齐指针的低bit位中。type-id必须是一个指针、引用、算术类型、函数指针或者成员指针。
其中前两种称为旧风格（old-style）的强制转型，后四种为标准C&#43;&#43;的类型转换符。
旧风格的强制转型可以看成按下列顺序排列的第一个成功的类型转换组合： - const_cast - static_cast (忽略访问限制) - static_cast接着const_cast - reinterpret_cast - reinterpret_cast接着const_cast
旧风格的强制转型比较危险，因为可能被解析成reinterpret_cast，而且解析成static_cast时会忽略访问权限控制（能做其他类型转换无法实现的功能）。此外，使用旧风格的强制转型也不如C&#43;&#43;类型转换容易查找，所以一般不推荐使用。
参考 Type conversions
MSDN：Casting Operators
When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used?
总结C&#43;&#43;中的所有强制转换函数(const_cast，reinterpret_cast，static_cast，dynamic_cast)
In C&#43;&#43;, why use static_cast(x) instead of (int)x?
</content>
    </entry>
    
     <entry>
        <title>敏捷开发</title>
        <url>https://mryqu.github.io/post/%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91/</url>
        <categories>
          
        </categories>
        <tags>
          <tag>敏捷开发</tag><tag>agile</tag>
        </tags>
        <content type="html">  什么是敏捷开发？ 九十年代末，几种方法学开始不断获得公众的注意。每种方法学都是对已有开发方法、新开发方法以及转变后的已有开发方法进行不同组合。但是它们都强调研发团队和业务专家之间的紧密协作、面对面的沟通（因为比文档更高效）、频繁地交付新的可部署的商业价值、紧密的自我管理的团队、精致开发代码和管理团队的方法，以使不可预见的需求不再是一场灾难。
敏捷开发的历史 2001年2月11日到13日，在美国犹他州Wasatch山的滑雪圣地Snowbird的一幢大楼里，17位轻量级过程专家通过这次会议形成了敏捷软件开发运动，其中包括了：极限编程（eXtremeProgramming, XP）、Scrum、动态系统开发方法（Dynamic SystemsDevelopment Method，DSDM）、自适应软件开发（AdaptiveSoftware Development，ASD）、Crystal方法、特性驱动方法（Feature-DrivenDevelopment，FDD）、实用程序设计等。这些业界专家概括出了一些可以让软件开发团队具有快速工作、响应变化能力的价值观和原则，目的是推进敏捷开发方法的研究和应用。会议的结果是17名与会者共同签署并发布了“敏捷软件开发宣言”（TheManifesto for Agile Software Development）,敏捷联盟（AgileAlliance）由此诞生。
敏捷开发的核心价值-特点和优势  个体和交互胜过过程和工具 人是获得成功的最为重要的因素。如果团队中没有优秀的成员，那么就是使用好的过程也不能从失败中挽救项目，但是，不好的过程却可以使最优秀的团队成员失去效用。如果不能作为一个团队进行工作，那么即使拥有一批优秀的成员也一样会惨败。团队的构建要比环境的构建重要得多。许多团队和管理者就犯了先构建环境，然后期望团队自动凝聚在一起的错误。相反，应该首先致力于构建团队，然后再让团队基于需要来配置环境。 可以工作的软件胜过面面俱到的文档 没有文档的软件是一种灾难。代码不是传达系统原理和结构的理想媒介。团队更需要编制易于阅读的文档，来对系统及其设计决策的依据进行描述。然而，过多的文档比过少的文档更糟。编制众多的文档需要花费大量的时间，并且要使这些文档和代码保持同步，就要花费更多的时间。如果文档和代码之间失去同步，那么文档就会变成庞大的、复杂的谎言，会造成重大的误导。虽然从代码中提取系统的原理和结构信息可能是困难的，但是代码是惟一没有二义性的信息源。在团队成员的头脑中，保存着时常变化的系统的脉络图（roadmap）。人和人之间的交互是把这份脉络图传授给他人的最快、最有效的方式。 客户合作胜过合同谈判 不能像订购日用品一样来订购软件。你不能够仅仅写下一份关于你想要的软件的描述，然后就让人在固定的时间内以固定的价格去开发它。所有用这种方式来对待软件项目的尝试都以失败而告终。有时，失败是惨重的。告诉开发团队想要的东西，然后期望开发团队消失一段时间后就能够交付一个满足需要的系统来，这对于公司的管理者来说是具有诱惑力的。然而，这种操作模式将导致低劣的质量和失败。成功的项目需要有序、频繁的客户反馈。项目的需求基本处于一个持续变化的状态。大的变更是很平常的。在这期间，也会出现整个功能块被减掉，而加进来另外一些功能块。然而，合同和项目都经受住了这些变更，并获得成功。成功的关键在于和客户之间真诚的协作，并且合同指导了这种协作，而不是试图去规定项目范围的细节和固定成本下的进度。 响应变化胜过遵循计划 响应变化的能力常常决定着一个软件项目的成败。当我们构建计划时，应该确保计划是灵活的并且易于适应商务和技术方面的变化。计划不能考虑得过远。  敏捷开发的十二条原则  我们最优先要做的是通过尽早地、持续地交付有价值的软件来使客户满意。 即使到了开发的后期，也欢迎改变需求。敏捷过程利用变化来为客户创造竞争优势。 经常性地交付可以工作的软件，交付的间隔可以从几个星期到几个月，交付的时间间隔越短越好。 在整个项目开发期间，业务人员和开发人员必须天天都在一起工作。 围绕被激励起来的个人来构建项目。给他们提供所需的环境和支持，并且信任他们能够完成工作。 在团队内部，最具有效果并富有效率的传递信息的方法，就是面对面的交谈。 可以工作的软件是首要的进度度量标准。 敏捷过程提倡可持续的开发速度。责任人、开发者和用户应该能够保持一个长期的、恒定的开发速度。 不断地关注优秀的技能和好的设计会增强敏捷能力。 简单——把无需做的工作最大化的艺术——是最根本的。 最好的构架、需求和设计出于自我组织的团队。 每隔一定时间，团队会在如何才能更有效地工作方面进行反省，然后相应地对自己的行为进行调整  敏捷开发的一些方法  完整团队XP项目的所有参与者（开发人员、客户、测试人员等）一起工作在一个开放的场所中，他们是同一个团队的成员。这个场所的墙壁上随意悬挂着大幅的、显著的图表以及其他一些显示他们进度的东西。 计划游戏计划是持续的、循序渐进的。每2周，开发人员就为下2周估算候选特性的成本，而客户则根据成本和商务价值来选择要实现的特性。 客户测试作为选择每个所期望的特性的一部分，客户可以根据脚本语言来定义出自动验收测试来表明该特性可以工作。 结对编程所有的产品软件都是由两个程序员、并排坐在一起在同一台机器上构建的。 测试驱动开发 编写单元测试是一个验证行为，更是一个设计行为。同样，它更是一种编写文档的行为。编写单元测试避免了相当数量的反馈循环，尤其是功能能验证方面的反馈循环。自动单元测试可以使研发工作和测试工作并行化。 改进设计随时利用重构方法改进已经腐化的代码，保持代码尽可能的干净、具有表达力。 持续集成团队总是使系统完整地被集成。只要有可能就进行代码集成，周期可以在几个小时，最好不要超过一天。持续集成可以避免错误的积累，可以增加可重用的代码。在一个结对小组认为适当的时候并通过了所有的单元测试，就可以进行集成，集成后的代码必须通过测试。 集体代码所有权任何结对的程序员都可以在任何时候改进任何代码。没有程序员对任何一个特定的模块或技术单独负责，每个人都可以参与任何其它方面的开发。 不要加班 尽可能不要加班，大多数加班并不能挽回已有的延迟，连续超过两个星期的加班说明有问题存在。向一个已经延迟的项目填加人员也不是一个好的选择。  附录-极限编程 1999年KentBeck提出的ExtremeProgramming（XP）是轻量级方法中最引人注目的一个。XP基于沟通、简单、反馈、勇气四个核心价值提出了完整团队（WholeTeam）、计划博弈（PlanningGame）、隐喻（Metaphor）、小规模交付（Smallrelease）、单元测试（Unittest）、简单设计（SimpleDesign）、结对开发（PairProgramming）、重构（Refactoring）、持续集成（ContinuousIntegration）、代码集体所有制（Collective codeOwnership）、编码标准（CodingStandard）、可持续步调（SustainablePace）等十二个核心实践。XP是目前发展应用得最活跃的方法，适用于需求模糊和挥发性强的场合。从2000年起，关于XP研究的年会年年召开，对XP方法的过程、原则、适用性等各方面的讨论大大丰富了XP的方法和理论。Mark C.Paulk提出XP是CMM的一个截面，LaurieWilliams在结对编程方面进行了深入的研究，Roy W.Miller对XP实践进行了修订，提出XP的19个实践。
附录-SCRUM 1993年KenSchwaber和JeffSutherland提出SCRUM，是对迭代式面向对象方法的改进。SCRUM提出的SCRUMMeeting、Sprint等模式。SCRUM将工业过程控制中的概念应用到软件开发中来，认为软件开发过程更多是经验性过程（EmpiricalProcess），而不是规约性过程（DefinedProcess）。
附录-Crystal 1999年Alistair Cockburn提出CrystalMethodologies。与其它敏捷方法的提出者不同，Cockburn的研究基于对IBM公司近四十个项目案例的调查。他认为不同的项目需采用不同的开发方法，并随着开发，进行连续不断的过程改进。据此他提出了一系列方法（CrystalClear、CrystalYellow、CrystalOrange、CrystalRed等）。Crystal方法强调以人和沟通为中心，强调对方法的选择和调整要考虑两个因素，一是充分发挥考虑人的特长，二是满足待开发软件的可靠性要求。刚好够用的方法论成为Crystal的基本原则之一。相对于人和团队，过程是第二位的，因此，过程应该被最小化，即“刚好够用”。
附录-DSDM 1994年DynamicSystems Development Methodology（DSDM）由英国16家公司的联盟发起，应用范围也不再限于IT行业。DSDM的基本观点是，任何事情都不可能一次性的圆满完成，在时间进度和可用资源预先固定的情况下，力争需求的最大化满足，提出了时间框（TimeBox）技术、MoSCoW（MustdO，Shoulddo，CoulddO，Won&amp;rsquo;tdo）优先级排序、工作间（Workshop）等方法。
附录-FDD Feature DrivenDevelopment（FDD）是由PeterCoad、Jeff deLuca、EricLefebvre共同开发的一套针对中小型软件开发项目的开发模式。所谓的特征点（Feature）是一些用户认为有用的小功能项，一个特征点能在两周或更短的时间内被实施，且产生可见的、能运行的代码。FDD将开发过程分为五个过程，每个过程指南采用ETVX（EntryCriteria，入口准则；Task，任务；Verification，评审确认；eXitCriteria，出口准则）方法描述，并明确了哪些角色参与哪些子任务，哪些子任务是可选的、哪些是必须的。
附录-自适应软件开发 1994年J.Holland在圣达菲（SantaFe Institute，SFI）研究所正式提出了比较完整的复杂自适应系统（ComplexAdaptive System，CAS）理论，认为在一定环境中的主体相互竞争和合作，导致系统产生突变。JimHighsmith基于复杂自适应系统理论提出了自适应软件开发（AdaptiveSoftware Development，ASD），旨在通过提高组织的自适应力以应对极度变化、难以预测的快速软件开发要求。开发组织的首要目标是快速响应变化，即提高适应力，而适应力只能孕育，不能通过命令和控制来获得，ASD提出“领导—协作”模型来提高组织的自适应力。Highsmith提出了基于有机原则的模式概念以区别于机械的过程，认为过程的实现方式必须能让项目团队成为一个有机的活跃的生态系统。他给出了一种过程分类方案：严密过程、灵活过程、问题求解过程，并强调问题求解过程是软件开发的创新核心。
</content>
    </entry>
    
     <entry>
        <title>面向对象设计的SOLID原则和设计模式</title>
        <url>https://mryqu.github.io/post/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1%E7%9A%84solid%E5%8E%9F%E5%88%99%E5%92%8C%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
        <categories>
          <category>Tech</category>
        </categories>
        <tags>
          <tag>OOAD</tag><tag>S.O.L.I.D</tag><tag>design pattern</tag><tag>设计模式</tag><tag>UML</tag>
        </tags>
        <content type="html">  S.O.L.I.D S.O.L.I.D是面向对象设计和编程(OOD&amp;amp;OOP)中几个重要编码原则(ProgrammingPrinciple)的首字母缩写。 - SRP :The Single Responsibility Principle单一责任原则 - OCP :The Open Closed Principle 开放封闭原则 - LSP :The Loskop Substitution Principle里氏替换原则 - DIP :The Dependency Inversion Principle依赖倒置原则 - ISP :The Interface Segregation Principle接口分离原则
单一责任原则：  当需要修改某个类的时候原因有且只有一个（THERE SHOULD NEVER BE MORE THAN ONE REASONFOR A CLASS TOCHANGE）。换句话说就是让一个类只做一种类型责任，当这个类需要承当其他类型的责任的时候，就需要分解这个类。 比如：报表的内容和报表的格式都会变化改变，但是这两种变化的性质不同，一个是实质内在，一个是表面上的，SRP认为这是问题的两个方面，其实代表不同的职责，应该将它们分离放入不同的类或模块中，而不应该放在一起，否则的话，因为不同原因发生变化，导致对方变动，比如报表格式变新的样式，这个变化是不应该涉及到内容的。 反模式: 一个类处理的事情太多了, 应当进行分解
开放封闭原则  软件实体应该是可扩展，而不可修改的。也就是说，对扩展是开放的，而对修改是封闭的。这个原则是诸多面向对象编程原则中最抽象、最难理解的一个。 反模式: 一个模块的修改将导致其他模块的修改
里氏替换原则  当一个子类的实例应该能够替换任何其超类的实例时，它们之间才具有is-A关系 子类可以代替基类, 客户使用基类, 他们不需要知道派生类所做的事情. 反模式: if(a instanceof TypeA) {&amp;hellip;} 这是一个针对行为职责可替代的原则，如果S是T的子类型，那么S对象就应该在不改变任何抽象属性情况下替换所有T对象。这里的抽象属性是指对象的字段属性。
我们使用接口时经常碰到一个问题，需要使用接口子类中的方法，而接口中没有这个方法，那么只能要么修改接口，要么将接口downcast为具体子类。为什么会出现这个尴尬现象？有几种情况导致，其中一种情况是是将当前的类重构到接口时，没有将类中所有方法extract到接口中，可能因为这些被你漏掉的方法不属于当前接口，那么，它又违背了单一职责原理，说明你当前这个类的方法设计得又不合理。
所以，如果单一职责设计的足够好，那么LSP原则则是检验的方法。LSP原则是对对象职责和协作的一种检验约束方法，此外还有DBC(designby contract)原则，为了保证实现接口的子类职责行为的约束，DBC三要素都必须被重视满足： 1. Preconditions前置条件不能在子类中被强化。 2. Postconditions后置条件不能在子类中被弱化。 3. 子类自身不变性Invariants必须在子类自己中封装满足。这也是前面“不改变任何抽象属性”的意思。
依赖倒置原则  高层模块不应该依赖于低层模块，二者都应该依赖于抽象  抽象不应该依赖于细节，细节应该依赖于抽象  要针对接口编程，不针对实现编程。
接口分离原则  不能强迫用户去依赖那些他们不使用的接口。换句话说，使用多个专门的接口比使用单一的总接口总要好。 反模式：肥类(fat class), 有成堆的方法, 而且用户很少使用 SOLID原则如今在DCI架构中能够得到真正实现和发展。
包的设计原则： 1. 发布/重用等价原则（REP）我们创建包的目的是为了给别人重用，所以重用的粒度就是发布的粒度。 2. 公共闭合原则（CCP）因为相同原因而被修改的类应该放入一个包中，对应于“单一责任原则”。 3. 公共重用原则（CRP）应该尽可能地将只被一个客户使用的包与被多个不同客户使用到的包分开。对应于“接口隔离原则” 4. 非循环依赖原则（ADP）不要在包依赖图中出现循环依赖。如a依赖于b，b依赖于c，同时c又依赖于a。 5. 稳定依赖原则（SDP）要依赖于稳定的包，而不要依赖于经常变化的包。对应于“依赖倒置原则”。 6. 稳定抽象原则（SAP）稳定的包应当是抽象的。对应于“依赖倒置原则”。
设计模式 - DesignPattern 创建型  Abstract Factory（抽象工厂模式） -&amp;gt;(简单工厂模式)  Factory Method（工厂模式）  Builder（生成器模式）  Singleton（单件模式） -&amp;gt;(多例模式)  Prototype（原型模式）  结构型  Adapter（适配器模式）  Bridge（桥接模式）  Composite（组合模式）  Decorator（装饰模式）  Facade（外观模式，门面模式）  Flyweight（享元模式） -&amp;gt;(不变模式)  Proxy（代理模式）  行为型  Chain of Responsibility（职责链模式）  Command（命令模式）  Interpreter（解释器模式）  Iterator（迭代器模式）  Mediator（中介者模式）  Memento（备忘录模式） Observer（观察者模式）  State（状态模式）  Strategy（策略模式） TemplateMethod（模板方法模式）  Visitor（访问者模式）  </content>
    </entry>
    
     <entry>
        <title>Posts</title>
        <url>https://mryqu.github.io/post/</url>
        <categories>
          
        </categories>
        <tags>
          
        </tags>
        <content type="html"> </content>
    </entry>
    
</search>