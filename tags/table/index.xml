<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>table on Mryqu's Notes</title><link>https://mryqu.github.io/tags/table/</link><description>Recent content in table on Mryqu's Notes</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 16 Jul 2018 06:12:39 +0000</lastBuildDate><atom:link href="https://mryqu.github.io/tags/table/index.xml" rel="self" type="application/rss+xml"/><item><title>[Spark] SparkCatalogAPI使用</title><link>https://mryqu.github.io/post/spark_sparkcatalogapi%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 16 Jul 2018 06:12:39 +0000</pubDate><guid>https://mryqu.github.io/post/spark_sparkcatalogapi%E4%BD%BF%E7%94%A8/</guid><description>Catalog API简介 Spark中的DataSet和Dataframe API支持结构化分析。结构化分析的一个重要的方面是管理元数据。这些元数据可能是一些临时元数据（比如临时表）、SQLContext上注册的UDF以及持久化的元数据（比如Hivemeta store或者HCatalog）。 Spark2中添加了标准的API（称为catalog）来访问Spark SQL中的元数据。这个API既可以操作Spark SQL，也可以操作Hive元数据。
Catalog API使用 查询数据库 scala&amp;gt; spark.catalog.listDatabases.show(false) +-------+---------------------+----------------------------------------+ |name |description |locationUri | +-------+---------------------+----------------------------------------+ |default|Default Hive database|hdfs://10.211.55.101/user/hive/warehouse| +-------+---------------------+----------------------------------------+ scala&amp;gt; spark.catalog.currentDatabase res4: String = default 查询表 scala&amp;gt; spark.catalog.listTables.show(false) +----+--------+----------------------------------------+---------+-----------+ |name|database|description |tableType|isTemporary| +----+--------+----------------------------------------+---------+-----------+ |emp |default |null |MANAGED |false | |emp2|default |Imported by sqoop on 2018/07/10 04:23:26|MANAGED |false | |emp3|default |Imported by sqoop on 2018/07/10 06:13:17|MANAGED |false | |yqu1|default |null |MANAGED |false | |yqu2|default |null |MANAGED |false | +----+--------+----------------------------------------+---------+-----------+ 下面的示例用于创建不同TableType的表：</description></item><item><title>[Hibernate Tools] 通过数据库表生成JPA Entity类</title><link>https://mryqu.github.io/post/hibernate_tools_%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E7%94%9F%E6%88%90jpa_entity%E7%B1%BB/</link><pubDate>Sun, 30 Aug 2015 08:56:57 +0000</pubDate><guid>https://mryqu.github.io/post/hibernate_tools_%E9%80%9A%E8%BF%87%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E7%94%9F%E6%88%90jpa_entity%E7%B1%BB/</guid><description>本文与前一博文[Hibernate Tools] 通过JPA Entity类生成数据库表 正好相反，实践一下如何通过数据库表生成JPA Entity类。
在Eclipse中安装JBoss Tools中的Hibernate Tools插件 创建JPA项目PetStoreDemo 使用向导创建JPA项目 项目基本设置 设置JPA Facet 此处选用了Generatic 2.1平台，用户库HIBERNATE_JPA包含如下jar文件：
hibernate-commons-annotations.jar hibernate-core.jar hibernate-jpa-2.1-api.jar 通过数据库表生成JPA Entity类 执行“Generate Entities from Tables” 选择库表 设置库表关联关系 定制生成Entity的默认行为 设置单个Entity 生成结果 下面以Item为例，展示生成结果。
package com.yqu.jpetstore; import java.io.Serializable; import javax.persistence.*; import java.math.BigDecimal; @Entity @Table(name=&amp;#34;item&amp;#34;) @NamedQuery(name=&amp;#34;Item.findAll&amp;#34;, query=&amp;#34;SELECT i FROM Item i&amp;#34;) public class Item implements Serializable { private static final long serialVersionUID = 1L; private String itemid; private String attr1; private String attr2; private String attr3; private String attr4; private String attr5; private BigDecimal listprice; private String status; private BigDecimal unitcost; private Product product; private Supplier supplierBean; public Item() { } @Id @GeneratedValue(strategy=GenerationType.</description></item><item><title>[Hibernate Tools] 通过JPA Entity类生成数据库表</title><link>https://mryqu.github.io/post/hibernate_tools_%E9%80%9A%E8%BF%87jpa_entity%E7%B1%BB%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8/</link><pubDate>Sat, 29 Aug 2015 07:19:26 +0000</pubDate><guid>https://mryqu.github.io/post/hibernate_tools_%E9%80%9A%E8%BF%87jpa_entity%E7%B1%BB%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8/</guid><description>以前用过hbm2ddlAnt任务通过Hibernate映射文件生成数据库DDL。现在使用JPA后，不知道还有没有标准工具了。找了一圈，还是HibernateTools。
在Eclipse中安装JBoss Tools中的Hibernate Tools插件 创建JPA项目CustomerDemo 使用向导创建JPA项目 项目基本设置 设置JPA Facet 此处选用了EclipseLink 2.5.x平台。如选择Generatic2.1平台，在生成数据库Schema时会报“Generate Tables from Entities is notsupported by the Generic Platform”。 用户库ECLIPSELINK_JPA包含如下jar文件：
eclipselink.jar javax.persistence.jar org.eclipse.persistence.jpa.modelgen.jar org.eclipse.persistence.jpars.jar Entity类代码及设置 Customer类 package hello; import javax.persistence.*; @Entity @Table(name=&amp;#34;CUSTOMER&amp;#34;) public class Customer { @Id @Column(name=&amp;#34;CUSTOMER_ID&amp;#34;, nullable=false, updatable=false, unique=true) @GeneratedValue(strategy=GenerationType.AUTO) private Long id; @Column(name = &amp;#34;CUSTOMER_FNAME&amp;#34;) private String firstName; @Column(name = &amp;#34;CUSTOMER_LNAME&amp;#34;) private String lastName; protected Customer() {} public Customer(String firstName, String lastName) { this.firstName = firstName; this.</description></item><item><title>[Hive] Hive 表UDTF和汇聚UDAF学习</title><link>https://mryqu.github.io/post/hive_hive_%E8%A1%A8udtf%E5%92%8C%E6%B1%87%E8%81%9Audaf%E5%AD%A6%E4%B9%A0/</link><pubDate>Thu, 20 Aug 2015 05:48:48 +0000</pubDate><guid>https://mryqu.github.io/post/hive_hive_%E8%A1%A8udtf%E5%92%8C%E6%B1%87%E8%81%9Audaf%E5%AD%A6%E4%B9%A0/</guid><description>在之前的博文[Hive] Hive Macro和UDF实践中，我对Hive的宏和普通UDF进行学习并实践，这里将针对Hive表UDF（UDTF）和汇聚UDF（UDAF）进行学习。 普通UDF可以对一行表数据进行处理输出一个单元格数据；UDTF可以对一行表数据进行处理输出多列甚至多行数据；UDAF可以对整表数据进行处理输出某种汇聚结果。
UDTF Hive支持的内建UDTF有explode()、json_tuple()和inline()等函数。
查看UDTF介绍 选个好理解的explode函数吧。
hive&amp;gt; describe function explode; OK explode(a) - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns Time taken: 0.009 seconds, Fetched: 1 row(s) 测试内建UDTF 像inline函数需要根元素为ARRAY，第二层元素为STRUCT，搭建环境有点麻烦。所以还是接着擼explode函数吧。 上述命令将三行的列a中数组元素拆成7行字符串。那执行&amp;quot;select explode(a), b fromcomplex_datatypes_example;&amp;ldquo;会返回什么呢？结果是7行还是3行？ 谜底就是错误提示&amp;quot;Only a single exp.ression in the SELECT clause issupported with UDTF&amp;rsquo;s.&amp;quot;！
UDTF实现 一个定制UDTF要继承GenericUDTF抽象类并实现initialize、process及close方法。Hive调用initialize方法以将参数类型通知给UDTF。UDTF必须返回UDTF之后产生的行对象相应的对象观察器。一旦initialize()被调用后，Hive将使用process()方法将行传递给UDTF。在process()方法中，UDTF生成行并调用forward()方法将行转给其他运算符。当所有的行都传递给UDTF后，Hive将最终调用close()方法。 通过FunctionRegistry类可知explode函数的实现类为GenericUDTFExplode。下面通过GenericUDTFExplode对照参考四Hive Developer Guide - UDTF学习一下UDTF实现。
GenericUDTFExplode继承了抽象父类GenericUDTF 在initialize方法中，GenericUDTFExplode检查输入列是否为ARRAY或MAP类型，不是的话抛出异常。如果输入列为ARRAY类型，则输出列名为col，类型为输入列数组中元素类型；如果输入列为MAP类型，则输出列名为key和value，类型分别为输入列MAP的键与值相应的类型； 在process方法中，针对输入列ARRAY或MAP的每一个元素调用forward()方法将所生成的行转给其他运算符； 在close()方法中，实现为空。 UDAF Hive支持的内建UDAF有sum()、count()、min()和histogram_numeric()等函数。</description></item><item><title>[Hive] Hive表文件存储格式</title><link>https://mryqu.github.io/post/hive_hive%E8%A1%A8%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</link><pubDate>Fri, 14 Aug 2015 06:25:23 +0000</pubDate><guid>https://mryqu.github.io/post/hive_hive%E8%A1%A8%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</guid><description>Hive支持的内建表文件存储格式如下：
|存储格式|介绍 |&amp;mdash;&amp;ndash; |TEXTFILE|按照纯文本文件格式存储。如果配置hive.default.fileformat没有设置的话，TEXTFILE是默认文件格式。此存储格式下，数据不做压缩的话，磁盘开销大，数据解析开销大。使用Gzip、Bzip2、Snappy等进行压缩使用（系统自动检查，执行查询时自动解压）的话，Hive不能对数据进行切分，从而无法对数据进行并行操作。 |SEQUENCEFILE|按照压缩的Sequence File格式存储。SequenceFile一般是在HDFS FileSystem中生成，供map调用的原始文件。Hive中的SequenceFile继承自Hadoop API 的SequenceFile，不过它的key为空，使用value存放实际的值，这样是为了避免MR在运行map阶段的排序过程。 |RCFILE|按照RCFile (Record Columnar File)格式存储。在Hive0.6.0引入。RCFile是在计算机集群中判断如何存储关系型表的数据存放结构，是Facebook、俄亥俄州立大学、中科院计算所联合研究成果。FCFile结构是由数据存储格式、数据压缩方式、数据读取优化技术等多种模块的系统组合，可以实现数据存放的四个要求：(1)快速加载，(2) 快速处理查询，(3) 高效利用存储空间 (4) 非常适用于动态数据访问模式。它遵循“先按行划分，再垂直划分”的设计理念。当查询过程中，针对它并不关心的列时，它会在IO上跳过这些列。需要说明的是，RCFile在map阶段从远端拷贝仍然是拷贝整个数据块，并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列，并跳到需要读取的列，而是通过扫描每一个row group的头部定义来实现的，但是在整个HDFS Block 级别的头部并没有定义每个列从哪个rowgroup起始到哪个row group结束。所以在读取所有列的情况下，RCFile的性能反而没有SequenceFile高。 |ORC|在Hive 0.11.0引入。ORC(Optimized RowColumnar)存储源自于RCFile。FCFile把每列都当作二进制blob处理，而ORC存储列元数据，针对列类型使用特定的读写器。ORC支持ACID、内建索引和复杂类型。官网上介绍“其性能显著快于RCFile或Parquet”。Facebook和Yahoo等大公司都在使用。 |PARQUET|在Hive 0.13.0引入。Parquet源自于google Dremel系统。Parquet最初的设计动机是存储嵌套式数据，将这类数据存储成列式格式，以方便对其高效压缩和编码，且使用更少的IO操作取出需要的数据，这也是Parquet相比于ORC的优势，它能够透明地将Protobuf和thrift类型的数据进行列式存储，在Protobuf和thrift被广泛使用的今天，与parquet进行集成，是一件非容易和自然的事情。除了上述优势外，相比于ORC,Parquet没有太多其他可圈可点的地方，比如它不支持update操作（数据写成后不可修改），不支持ACID等。 |AVRO|在Hive 0.13.0引入。Avro是数据序列化系统，由Hadoop项目开发的。
测试 $ echo -e &amp;#39;1\x01foo&amp;#39; &amp;gt; tabft.txt $ echo -e &amp;#39;2\x01bar&amp;#39; &amp;gt;&amp;gt; tabft.txt $ hive hive&amp;gt; create table tabft (id int, name string); hive&amp;gt; quit; $ hadoop fs -put tabft.txt /user/hive/warehouse/tabft $ hive hive&amp;gt; create table tabft_txt (id int, name string) STORED AS TEXTFILE; hive&amp;gt; insert into table tabft_txt select * from tabft; hive&amp;gt; create table tabft_seq (id int, name string) STORED AS SEQUENCEFILE; hive&amp;gt; insert into table tabft_seq select * from tabft; hive&amp;gt; create table tabft_rc (id int, name string) STORED AS RCFILE; hive&amp;gt; insert into table tabft_rc select * from tabft; hive&amp;gt; create table tabft_orc (id int, name string) STORED AS ORC; hive&amp;gt; insert into table tabft_orc select * from tabft; hive&amp;gt; create table tabft_parq (id int, name string) STORED AS PARQUET; hive&amp;gt; insert into table tabft_parq select * from tabft; hive&amp;gt; create table tabft_avro (id int, name string) STORED AS AVRO; hive&amp;gt; insert into table tabft_avro select * from tabft; 获取Sequence文件信息 在我的环境下，按照压缩的Sequence File格式存储后的文件是非压缩的。 获取ORC文件信息 参考 Hive 语言手册 - DDL</description></item><item><title>[OpenUI5] 十分钟了解sap.ui.table.Table</title><link>https://mryqu.github.io/post/openui5_%E5%8D%81%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3sap.ui.table.table/</link><pubDate>Wed, 12 Nov 2014 20:00:19 +0000</pubDate><guid>https://mryqu.github.io/post/openui5_%E5%8D%81%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3sap.ui.table.table/</guid><description>转发一篇SAP community network的好帖子：http://scn.sap.com/docs/DOC-54075
Introduction sap.ui.table.Table is commonly used inOpenUI5 desktop application. Many questions (related to thiscontrol) that are posted in this group, it is evident thatdocumentation for this control is lacking and we (developers) haveto dive deep into debugging its source code to figure things out.It is fortunate that Javascript source code is always available;modern browsers provide debugging capability and personally, I amfortunate to have the opportunity to work with someone in SAPUI5team while using this control.</description></item></channel></rss>