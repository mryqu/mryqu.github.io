<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark Sql on Mryqu&#39;s Notes</title>
    <link>https://mryqu.github.io/tags/spark-sql/</link>
    <description>Recent content in Spark Sql on Mryqu&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 03 Aug 2016 06:10:01 +0000</lastBuildDate>
    
	<atom:link href="https://mryqu.github.io/tags/spark-sql/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[Spark]Spark和Hive集成</title>
      <link>https://mryqu.github.io/post/spark_spark%E5%92%8Chive%E9%9B%86%E6%88%90/</link>
      <pubDate>Wed, 03 Aug 2016 06:10:01 +0000</pubDate>
      
      <guid>https://mryqu.github.io/post/spark_spark%E5%92%8Chive%E9%9B%86%E6%88%90/</guid>
      <description>在前一博文[Spark] Spark2集群安装实践中安装了Spark后，发现和Hive还没有集成在一起，此外Hive自己也不好使了。
hadoop@node50064:~$hive ls: cannot access /usr/local/spark/lib/spark-assembly-*.jar: No such file or directory .................  原来Spark assemblyjar在Spark2中已经不存在了，而Hive脚本判断系统存在Spark后仍要使用，需要将$HIVE_HOME/bin/hive中的这部分代码注释掉：
# add Spark assembly jar to the classpath #if [[ -n &amp;quot;$SPARK_HOME&amp;quot; ]] #then # sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar` # CLASSPATH=&amp;quot;${CLASSPATH}:${sparkAssemblyPath}&amp;quot; #fi  至此，Hive本身工作正常。下面开始Spark和Hive集成配置工作。 - Spark SQL CLI需要使用到Hive Metastore，因此需要在[Hive] 安装Hive 1.2.x的基础上继续修改$HIVE_HOME/conf/hive-site.xml：- 将$HIVE_HOME/conf/hive-site.xml软连接到$SPARK_HOME/conf目录中:
 cd $SPARK_HOME/conf ln -s $HIVE_HOME/conf/hive-site.xml   启动Hive Metastore和HiveServer2：  hive --service metastore &amp;amp; hive --service hiveserver2 &amp;amp;   下面进行验证工作：
hadoop@node50064:~$ hive hive&amp;gt; use default; OK Time taken: 0.</description>
    </item>
    
  </channel>
</rss>