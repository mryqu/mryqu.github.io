<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>[Hadoop] 处理跨块边界的InputSplit - Mryqu's Notes</title><meta name=keywords content="mryqu,yandongqu,博客,程序员,架构师,笔记,技术,分享"><meta property="og:title" content="[Hadoop] 处理跨块边界的InputSplit"><meta property="og:site_name" content="Mryqu's Notes"><meta property="og:image" content="/img/author.jpg"><meta name=title content="[Hadoop] 处理跨块边界的InputSplit - Mryqu's Notes"><meta name=description content="mryqu | yandongqu | 博客 | 软件 | 架构 | 技术"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href="/js/vendor/font-awesome/css/font-awesome.min.css?v=4.6.2" rel=stylesheet type=text/css><link href="/js/vendor/fancybox/jquery.fancybox.css?v=2.1.5" rel=stylesheet type=text/css><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css><script type=text/javascript id=hexo.configuration>var NexT=window.NexT||{},CONFIG={scheme:"Pisces",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0}</script></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class="site-meta custom-logo"><div class=custom-logo-site-title><a href=https://mryqu.github.io/ class=brand rel=start><span class=logo-line-before><i></i></span>
<span class=site-title>Mryqu's Notes</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle></p></div><div class=site-nav-toggle><button>
<span class=btn-bar></span>
<span class=btn-bar></span>
<span class=btn-bar></span></button></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/tags/ rel=section><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class=menu-item><a href=/categories/ rel=section><i class="menu-item-icon fa fa-fw fa-categories"></i><br>分类</a></li><li class=menu-item><a href=/post/ rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class=popup><span class="search-icon fa fa-search"></span>
<input type=text id=local-search-input><div id=local-search-result></div><span class=popup-btn-close>close</span></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://mryqu.github.io/post/hadoop_%E5%A4%84%E7%90%86%E8%B7%A8%E5%9D%97%E8%BE%B9%E7%95%8C%E7%9A%84inputsplit/ itemprop=url>[Hadoop] 处理跨块边界的InputSplit</a></h1><div class=post-meta><span class=post-time><span class=post-meta-item-icon><i class="fa fa-calendar-o"></i></span>
<span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2014-01-02">2014-01-02</time></span>
<span class=post-category>&nbsp; | &nbsp;
<span class=post-meta-item-icon><i class="fa fa-folder-o"></i></span>
<span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=/categories/bigdata itemprop=url rel=index><span itemprop=name>BigData</span></a>
&nbsp;</span></span>
<span>&nbsp; | &nbsp;
<span class=post-meta-item-icon><i class="fa fa-eye"></i></span>
<span class=post-meta-item-text>阅读：</span>
<span class=leancloud-visitors-count>41 字 ~1分钟</span></span></div></header><div class=post-body itemprop=articleBody><p>Mapper从HDFS中读取文件进行数据处理的。凭借InputFormat、InputSplit、RecordReader、LineReader等类，Mapper用户代码可以处理输入键值对进行数据处理。下面学习一下MapReduce是如何分割无压缩文本文件输入的。</p><p>涉及的类有：</p><ul><li>InputFormat及其子类<img src=/images/2014/1/0026uWfMzy78Kl76P5W5c.png alt=[Hadoop] 处理跨块边界的InputSplit>InputFormat类执行下列操作：<ul><li>检验作业的输入文件和目录是否存在。</li><li>将输入文件分割策划功能InputSlit，基于文件的InputFormat根据文件大小将文件分割为逻辑上的Split。</li><li>实例化用于对每个文件分割块解析记录的RecordReaderInputFormat类包括下列两个主要的子类：</li><li>TextInputFormat：用于解析文本文件。将文件按行生成记录；键为LongWritable，文件偏移量；值为Text，行的内容。</li><li>SequenceFileInputFormat：用于解析Hadoop压缩二进制文件。SequenceFile可为无压缩、记录压缩或块压缩。与TextInputFormat不同，SequenceFileInputFormat的键值对是泛型的。</li></ul></li><li>InputSplit及其子类<img src=/images/2014/1/0026uWfMzy78Kled9SDb7.png alt=[Hadoop] 处理跨块边界的InputSplit>InputSplit是单个Mapper所要处理的数据子集逻辑表现形式。每个InputSplit很可能不会包含完整记录数，即在输入分割中首尾记录有可能是不完整的，处理全部记录由RecordReader负责。InputSplit的子类包括：<ul><li>FileSplit代表输入文件的GetLength()大小的一个片段。FileSplit由InputFormat.getSplits(JobContext)调用返回，并传给InputFormat类用于实例化RecordReader。</li><li>CombineFileSplit将多个文件并入一个分割内（默认每个文件小于分割大小）</li></ul></li><li>RecordReader及其子类<img src=/images/2014/1/0026uWfMzy78KlnWUdg96.png alt=[Hadoop] 处理跨块边界的InputSplit>RecordReader将输入分割片内的数据分析成Mapper所要处理的键值对。记录跟分割边界/块边界不一定匹配，RecordReader判断记录位置并处理日志边界。RecordReader包括下列子类：<ul><li>LineRecordReader：处理文本文件。</li><li>SequenceFileRecordReader：处理Sequence文件。</li></ul></li><li>LineReader：用于对文件进行读取操作、分析行并获得键值对。</li></ul><p>处理的具体流程如下：</p><ul><li><a href=https://github.com/apache/hadoop/blob/branch-2.2.0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java>FileInputFormat</a>.getSplits(JobContext)方法主要完成计算InputSplit的工作。<ul><li>首先判断输入文件是否可被分割的。如果文件流没有被压缩或者使用bzip2这种可分割的压缩算法，则文件可被分割；否则整个文件作为一个InputSplit。</li><li>如果文件可被分割的话，分割尺寸为max( max( 1,mapreduce.input.fileinputformat.split.minsize), min(mapreduce.input.fileinputformat.split.maxsize, blockSize))。如果没有对分割最小/大值进行设置的话，则分割尺寸即等于块大小，而块大小默认为64MB。</li><li>文件按照上述分割尺寸分割记录文件路径、每一分割的起始偏移量、分割块实际尺寸、输入文件所在机器。只要文件剩余数据量在1.1倍分割尺寸范围内，就会放到一个InputSplit中。<img src=/images/2014/1/0026uWfMzy78KAfdADMba.png alt=[Hadoop] 处理跨块边界的InputSplit></li></ul></li><li><a href=https://github.com/apache/hadoop/commits/branch-2.2.0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java>LineRecordReader</a>主要完成从InputSplit获取键值对的工作。<ul><li>LineRecordReader构造方法获知行分隔符是否为定制分割符；</li><li>initialize(InputSplit,TaskAttemptContext)方法获知InputSplit的start和end(=start+splitLength)，如果start不为0的话，跳过第一行（不用管第一行是否完整）。即处理上一InputSplit的RecordReader处理本InputSplit的第一行，处理本InputSplit的RecordReader处理下一个InputSplit的第一行。</li><li>nextKeyValue()方法处理第一个InputSplit，需要跳过可能存在的UTF BOM。<img src=/images/2014/1/0026uWfMzy78KIBGCNHa1.png alt=[Hadoop] 处理跨块边界的InputSplit></li></ul></li><li><a href=https://github.com/apache/hadoop/blob/branch-2.2.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java>LineReader</a>主要完成从从文件输入流获取数据、如没有定制换行符则需判别CR/LF/CRLF换行符，并获得键值对。<img src=/images/2014/1/0026uWfMzy78M0FIjYr1f.png alt=[Hadoop] 处理跨块边界的InputSplit></li></ul><p>以上类都不涉及对HDFS文件和块的实际读操作，本地和远程读取可学习org.apache.hadoop.hdfs.client.HdfsDataInputStream、org.apache.hadoop.hdfs.DFSInputStream等类的代码。</p><h3 id=参考>参考</h3><p><a href=http://stackoverflow.com/questions/14291170/how-does-hadoop-process-records-split-across-block-boundaries>How does Hadoop process records split across block boundaries?</a></p></div><footer class=post-footer><div class=post-nav><div style="border:1px dashed #e0e0e0;padding:10px;background-color:#fffeee;background-repeat:no-repeat;background-attachment:scroll;background-position:1%;-moz-background-size:auto auto;-moz-background-clip:-moz-initial;-moz-background-origin:-moz-initial;-moz-background-inline-policy:-moz-initial"><div><p style=margin-top:0>标题：[Hadoop] 处理跨块边界的InputSplit<br>作者：<a target=_blank href=/>mryqu</a><br>声明： 本博客所有文章除特别声明外，均采用 <a href=http://creativecommons.org/licenses/by-nc-sa/3.0/cn/>CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p></div></div><div class=clear></div></div><div class=post-tags><a href=/tags/hadoop rel=tag title=hadoop>#hadoop#</a>
<a href=/tags/inputformat rel=tag title=inputformat>#inputformat#</a>
<a href=/tags/inputsplit rel=tag title=inputsplit>#inputsplit#</a>
<a href=/tags/recordreader rel=tag title=recordreader>#recordreader#</a></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://mryqu.github.io/post/hbase_%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%AD%98%E5%82%A8/ rel=next title=[HBase] 原始数据类型存储><i class="fa fa-chevron-left"></i> [HBase] 原始数据类型存储</a></div><div class="post-nav-prev post-nav-item"><a href=https://mryqu.github.io/post/hadoop_mapreduce%E8%BE%93%E5%87%BAsequencefile%E5%AE%9E%E8%B7%B5/ rel=prev title=[Hadoop] MapReduce输出SequenceFile实践>[Hadoop] MapReduce输出SequenceFile实践 <i class="fa fa-chevron-right"></i></a></div></div><div class=comments id=comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://mryqu.github.io/post/hadoop_%E5%A4%84%E7%90%86%E8%B7%A8%E5%9D%97%E8%BE%B9%E7%95%8C%E7%9A%84inputsplit/",this.page.identifier="https://mryqu.github.io/post/hadoop_%E5%A4%84%E7%90%86%E8%B7%A8%E5%9D%97%E8%BE%B9%E7%95%8C%E7%9A%84inputsplit/"};(function(){var e=document,t=e.createElement("script");t.src="//mryqu.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript rel=nofollow>comments powered by Disqus.</a></noscript></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>文章目录</li><li class=sidebar-nav-overview data-target=site-overview>站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/author.jpg alt><p class=site-author-name itemprop=name></p><p class="site-description motion-element" itemprop=description>Programmer & Architect</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>662</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>27</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>1472</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/mryqu target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>
GitHub</a></span>
<span class=links-of-author-item><a href=https://twitter.com/yandongqu target=_blank title=Twitter><i class="fa fa-fw fa-twitter"></i>
Twitter</a></span>
<span class=links-of-author-item><a href=https://www.facebook.com/yandongqu target=_blank title="FB Page"><i class="fa fa-fw fa-facebook"></i>
FB Page</a></span></div><div class="links-of-blogroll motion-element inline"><script type=text/javascript src="//rf.revolvermaps.com/0/0/8.js?i=01h16xrlw6m&m=0&s=220&c=ff0000&cr1=ffffff&f=arial&l=33&bv=35" async></script></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#参考>参考</a></li></ul></li></ul></nav></div></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span itemprop=copyrightYear>&copy;
2009 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Mryqu's Notes</span></div><div class=powered-by>Powered by - <a class=theme-link href=http://gohugo.io target=_blank title=hugo>Hugo v0.105.0</a></div><div class=theme-info>Theme by - <a class=theme-link href=https://github.com/xtfly/hugo-theme-next target=_blank>NexT</a></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i>
<span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript>Object.prototype.toString.call(window.Promise)!=="[object Function]"&&(window.Promise=null)</script><script type=text/javascript src="/js/vendor/jquery/index.js?v=2.1.3"></script>
<script type=text/javascript src="/js/vendor/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
<script type=text/javascript src="/js/vendor/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
<script type=text/javascript src="/js/vendor/velocity/velocity.min.js?v=1.2.1"></script>
<script type=text/javascript src="/js/vendor/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/vendor/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
<script src="/js/vendor/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
<script type=text/javascript src=/js/utils.js></script>
<script type=text/javascript src=/js/motion.js></script>
<script type=text/javascript src=/js/affix.js></script>
<script type=text/javascript src=/js/schemes/pisces.js></script>
<script type=text/javascript src=/js/scrollspy.js></script>
<script type=text/javascript src=/js/post-details.js></script>
<script type=text/javascript src=/js/toc.js></script>
<script type=text/javascript src=/js/bootstrap.js></script>
<script type=text/javascript src=/js/search.js></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] }
  });
</script><script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script></body></html>